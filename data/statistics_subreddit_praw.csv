,time,title,content,top_10_comments,upvotes,no_of_comments,url
0,2023-05-27 14:10:07,[R]I'm looking for statistics about how many people/companies that are working remotely after the pandemic.,"It would be great if the numbers are from 2022.
Does anyone know where I can find it? I have tried eurostat, but with no luck.","['This is a sub where people can seek advice on how to perform statistical methods not ask where to find certain data or analyses that were performed.', 'Fair!']",0,2,https://www.reddit.com/r/statistics/comments/13szowk/rim_looking_for_statistics_about_how_many/
1,2023-05-27 13:38:35,[D] Short Time Series Forecasting,"Hi, I would like to know what sort of approaches are used/ is best for forecasting short time series.

I’m currently looking at a market supply and price forecasting situation. Where I need to forecast the market supply and price on a daily basis. It seems like taking the historical average is a good approach. But I would like to know what else is out there.

Thank you.","[""How short is short?\n\nExponential smoothing, double exponential smoothing ... among many other things. Kind of depends on the situation.\n\nAre you forecasting both together? I'd assume they don't behave independently.\n\nIf there's weekly cycles, there's potential modifications for that.""]",1,1,https://www.reddit.com/r/statistics/comments/13sz55s/d_short_time_series_forecasting/
2,2023-05-27 08:01:27,[Q] is it a good idea to start a job that is not relevant to the degree in statistics if I want a career in data analysis?,"I recently got a job offer for a chemistry assistant job, which is weird to me because the job requires a chemistry/science degree and i would be working in a lab. The job description loosely mentions data analysis. Basically, it’s very disconnected from the job I actually want, which is data analytics/data science. 

I am a recent graduate, so this would be my first job in the real world. My only concern is that it wouldn’t help me at all in getting to my end goal of data analytics, and I’m concerned that I will lose a lot of the skills I gained during my studies by the time I leave. Also I just started applying, so I’m not even sure if I’ll have a hard time finding what I’m looking for. 

What were your entry level positions? Is it standard to get a job not relevant to your degree and then transitioning later? Thank you all so much for the advice!","['Sounds like the data analysis connection is a bit loose tbh', 'Yeah that sounds like barely an analysis job. Oof I see your dilemma.', ""Can you get some clarity on how much analysis you'll be doing? Because if there is a lot of statistical analysis is could be related. What exactly would you be doing according to the job description?"", 'It doesn’t seem like much. Here is the job description: \n\n“Daily sample preparation and data analysis to ensure laboratory runs smoothly and meets required turnaround times.\n\nMethod optimization as required.\n\nEnsures compliance with internal Standard Operating Procedures (SOPs) and quality systems.\n\nEnsures test equipment is clean, maintained and in good working order.\n\nMaintains a safe, clean and sanitary work area.\n\nMaintains an accurate notebook and other required documentation protocols.\n\nFollows all safety procedures and documents.\nEnsures high quality standards are met through performing calibrations, quality checks and proficiency programs.\n\nMaintains inventory tracking as required.\n\nBeing aware of laboratory hazards and safety regulations, including but not limited to wearing appropriate PPE\n\nAdditional duties as assigned.”\n\nIt seems like the most I’ll be doing is cleaning up, preparing equipment, and the occasional write up or light data analysis. No mention of statistical software or anything. I have yet to go to a full job interview, but when I do I’ll ask about it. Hopefully they might be able to assign me more relevant work, but assuming they don’t, I’m not sure if I should take it.', 'Yeah and i don’t really know what to say either. I did apply, but it was more of a “might as well” type of thing. I never thought I’d get any attention since I didn’t have any of the requirements listed. \n\nIf I do attend an interview can I use some judo mind trick to try and get something more fitting what I want? I need a sentence I can say that sounds polite and understandable. It’s not the job I was looking for but it’s not something I want to pass up either.']",2,5,https://www.reddit.com/r/statistics/comments/13sslke/q_is_it_a_good_idea_to_start_a_job_that_is_not/
3,2023-05-27 07:49:20,[Q] Are entry-level data scientist positions tough with two masters degrees (first in Data Science and second in Statistics) with no practical experience?,"I am a foreign student in the United States of America.  
I completed my MS in Data Science and will pursue an MS in Statistical Science.

  
There were a few reasons for pursuing a second MS degree:

1. The Master of Science in Data Science program did not provide a comprehensive understanding of the statistical component within the curriculum. Emphasis was placed on data processing techniques and utilizing Python for model fitting and training, with some exposure to data visualizations to a limited extent.
2. The current state of the job market is highly challenging. Despite my best efforts, I encountered significant difficulties in securing employment as work visa sponsorships are tough to come by. The student visa restrictions add to this predicament, which necessitates obtaining a job within a specified timeframe. Failure to do so might compel me to explore employment opportunities in a different country. Regrettably, the prospects for Data Science positions in my home country are rather bleak.
3. I have a full-ride scholarship including the living expenses for the MS in Statistical Sciences.  


My MS in Data Science in 2023 and plan on completing an MS in Statistics in 2025. I have no practical work experience.

  
I do understand if I create a stellar portfolio of projects and get in summer internship and a Co-op, I would be in much better shape than I am today, and hence I plan to work on those aspects as well.

  
In 2025, will I be overqualified for the entry-level Data Scientist positions?

  
For recruiters, would you see my two master's degrees without any practical work experience as an anchor to my future career in Data Science?","[""You're going to do fine in 2025. Nobody can say what the market will be like in a few years but you'll have valuable, hireable skills regardless.\n\nMake sure you don't undervalue connections with other professionals, communication and people skills. Getting interviews is one thing (which connections you make today will help with), but doing well on the interviews you get requires soft skills. It also makes a huge difference in your career as well."", ""Start applying, get used to murder at technical interviews. You won't be over qualified. Trust me."", 'As a former recruiter, what’s your visa status? One of the biggest hurdles as a recruiter was dealing with different visas and whether or not the company could work with an individuals visa situation. Transferring a visa is one thing, but sponsoring one is a whole other deal.', 'If you can do write python scripts/ SQL/ build work flows. Look at any healthcare system that is attached to a school on the east coast. The will under pay you at 80k, but after 2-3 years experience you can get better paying jobs elsewhere. Gl', 'Where are you doing your MS? I’m looking for funded masters programs because things aren’t looking hot with a BS right now']",6,5,https://www.reddit.com/r/statistics/comments/13ssbn1/q_are_entrylevel_data_scientist_positions_tough/
4,2023-05-27 05:52:03,[Q] What kind of jobs should I be looking for with an undergrad in stats,"I haven’t gotten a job yet and I’ve applied for like 40 jobs so I must be doing something wrong

I have a BS in statistics and spent a year in a statistical genetics lab on campus where I ran analysis on genotype data and data cleaning/manipulation, and I’ve presented many publications to the other lab members. I’m applying for jobs like entry-level data analyst positions for healthcare or clinical research or marketing companies. Jobs that require undergrad-level skills using R and python. I got an interview for a research analyst role in clinical research and I never heard back. I’ve probably applied for at least 20 jobs that I never heard back from. 

Is the job market that bad or am I applying for the wrong jobs? What s","['Data Analyst positions would be great. If you have good projects in R & Python, Data Scientist positions could be feasible.', 'Another place you might consider is research for the government or legislature.', 'I’m in the same boat as you! Job hunt can be tough but we shouldn’t get discouraged by rejection. I’m applying to entry level data analyst jobs or jobs in data science and ml that I find interesting. I’ve even taken a shot at some financial analyst positions because i really only like the companies bahahaha any advice would be helpful from anyone', 'Usually something like Research Analyst.', 'You could also look into business analyst/intelligence type roles']",18,9,https://www.reddit.com/r/statistics/comments/13spl0p/q_what_kind_of_jobs_should_i_be_looking_for_with/
5,2023-05-27 03:56:49,[Q] How to interpret Weibull distribution?,"How should my interpretation of a linear model change when its residuals fit a 3-parameter Weibull (with Threshold) distribution, versus when small p-values reject that hypothesis? What's the practical takeaway, or my next steps to investigate?

For context, I build energy consumption models for whole-building operations, generally with two-year baselines.

If there are any other specifics I should need to provide, please let me know. Thank you.","[""It strikes me as weird for energy consumption data to be Weibull distributed. Afaik you get Weibull data mostly from non-memoryless survival times and from block maxima. Is your data censored or representative of maxima over fixed time intervals? If not, how did you arrive at the use of the Weibull distribution?\n\nEdit: if you're using a GLM with Weibull residuals, you interpret it like any GLM model. I don't think the residuals being Weibull changes much."", 'I\'m as bewildered as you. From what I know about how Weibull distributions are applied, I can\'t make it make sense for our applications.\n\nIt\'s part of the analysis script we run after each model is specified. When a statistic is out of spec, we\'re to make a note of it, what it means for the model, and why we believe the model should still be used.\n\nWhen I\'ve asked about it, my manager says ""it\'s a test to the model\'s longevity,"" which makes no sense to me and sounds like something they heard from someone who heard it from someone else who heard it from someone else.', ""I may have misunderstood something here. I'm going to lay out a couple possibilities here \n\n1) (I had assumed this one was the case) You have a linear regression model with residuals that have been found empirically to be Weibull distributed by some distribution fitting excercise, and you therefore are using a generalized linear model with Weibull residuals. The p values of interested in relate to the beta coefficients of some predictor or predictors of some response variable (energy consumption?)\n\n\n2) You are using a glm with the residual distribution specified as Weibull. The p values of interest relate to the beta coefficients of some predictor of predictors. But there's no good reason to be using Weibull beyond a nonsensical assertion made by your manager (your wildly incompetent manager if true).\n\n\n3) I've completely misunderstood what's going on here. Maybe the p values relate to a kolmogorov smornoff test on your residuals, or you're not using a linear model, or maybe I'm misunderstanding the nature of your data and the response variable is a measure of model performance and you're doing a regression to see of the model has continued to perform adequately over time or something.\n\nCan you please tell me which case is true, and if it's 3 describe your modeling problem and methods in clear terms?"", ""I dont think it can be a glm that theyre discussing (though maybe it ought to have been).\n\n With glms you specify the conditional distribution of the response, rather than a residual or error distribution, and that conditional distribution is exponential family (which this shifted and  truncated Weibull isnt).\n\nYou can fit regression-and-glm-like models with a conditional  Weibull response easily enough, but not directly in a glm (unless it was specifically exponential). I've used (parametric) survival models (log link) to take care of the fitting, which worked very well. You can also do it more directly just by maximizing likelihood but I prefer to use already solid code and readily available  output set up for inference already.\n\nIts hard to be sure since the OP didn't explain their analysis steps cIearly but it sounds to me like the issue is just that OLS yields skewed residuals - which might or might not be all that relevant, depending on the aim and what else was wrong with the model.\n\nI'd worry first about the \ndependence over time, issues related to stationarity etc. This may come up at several points, but I'd ponder it from the start.\n\nThen I'd worry about the conditional mean / link function. With regression problems, that's the biggest one. Get it wring and everything else is moot.\n\nThen I'd consider the conditional variance / variance function.\n\nThe shape of the conditional distributions comes after those in importance and might not matter very much unless prediction intervals are required.\n\nI imagine that glms could be a a good place to start with this, if the time series aspects dont prevent that.""]",1,4,https://www.reddit.com/r/statistics/comments/13smv84/q_how_to_interpret_weibull_distribution/
6,2023-05-27 03:30:38,[Q] Best nomenclature for series of PDFs?,"So I am creating a stochastic model of a digraph where each edge (defined by i,n for reasons not worth getting into) has a PDF of the RV x associated with it.  I've referred to the variations of x based on the edge it's associated to by x_in (not sure that's a good idea)

I have a function that needs to sum integrals of these pdfs but I don't love how it looks.

Can't figure out if the pdfs should be called f(x_in) or f(x)_in or f_in(x), or maybe something else entirely.  But I need to show that we are iterating through the PDFs by i and n.  All of the pdfs are unique, so I'm like 90% sure that f(x_in) is wrong, which is why I'm not sure about even referring to the RV as x_in.

I am not an academic and searching iterative functions ended me up at the wonkiest math wiki I've seen, so apologies if this is a dumb question.

(Ed: underscore is subscript, thought reddit followed that)","['Depending on the exact integral, your integrals are presumably equivalent to some probability calculation or some expectation of the random variables associated with each of the various pdfs. These would be commonly written with capital letter names of the random variables: X_n. In that case you could write the sequence out in standard probability or expectation notation: e.g. E(X_n) or P(X_n in (some set))', '`f_{in}(x)` is the only sensible choice. `f(x_{in})` would imply that all random variables have the same density `f`, while `f(x)_{in}` doesn’t mean anything.\n\nYou don’t need to differentiate the arguments with subscripts, you are using x as an auxiliary variable for integration, so its scope is limited to a single integral and there is no cause for confusion.']",1,2,https://www.reddit.com/r/statistics/comments/13sm8xv/q_best_nomenclature_for_series_of_pdfs/
7,2023-05-27 00:15:34,[Q] Linguistics student in urgent need of help with logical regression - I don't understand anything.,"Hi all, I don't come from a mathematical background at all, so I'm completely lost when it comes to regression or any stats, really. We're using [this website](https://languagevariationsuite.shinyapps.io/Pages/) and I don't understand anything at all really. I'd be very very grateful if someone could get into contact and hopefully explain some things please.

Thanks very much.","['Are you paying?', 'DO YOU FOLLOW THE CONDUCTOR’S LEAD?', ""I wouldn't trust much of what comes out of this website. Pointing stepwise regression at problems blindly and expecting to get truthful outcomes feels like a recipe for false positive inferences.\n\nNot your fault. This site just has the feeling of something somebody whipped up without much concern for how others would use it.\n\nI don't know what alternatives you might have, but I'm happy to discuss your problem if you want to pm me.\n\nedit: Was viewing on my phone earlier and I didn't realize that there's a non-stepwise version. Nevertheless, I feel for you. This is an opaque and frustrating introduction to statistics. My offer still stands for a PM. If you explain your research problem to me, I'll try to offer some advice as to how you can proceed; either using this interface or with something that might be more forgiving for you."", 'A linguistics student?', 'Fellow linguist here. I teach stats to linguists at the graduate level. Feel free to message me.']",0,5,https://www.reddit.com/r/statistics/comments/13shgq9/q_linguistics_student_in_urgent_need_of_help_with/
8,2023-05-26 21:16:24,"[Q] where i can find the ""hot topics in statistics""","Hi, im a former economist and i just started my masters in statistics. I need to write a dissertation in a year and a half, but i dont know the fiel enough to feel in love with a research agenda.

What Twitter pages do you follow? What are the main journals and congress? I need to get inspired, thank you!","['I’d reccomend signing up for daily email notifications from the statistics arXiv in whatever sub-categories you’re loosely interested in ( https://arxiv.org/archive/stat ). The vast majority of new stat papers will be posted on arXiv to get peer feedback/claim intellectual property before going through the formal peer review process with a journal. It’s the closest thing to an active pulse on what’s going on in the stats research community.\n\nRead the paper titles you get sent by email every day to get a sense of what topics come up a lot. Read the abstracts of one or two papers that stick out to you every day, and skim full papers whose abstracts you find especially interesting. If you find a paper on a topic that really intrigues you, then look in the introduction for reference to a review paper on the topic (trust me, there’s almost always one there) and check it out to get a broad overview of that area of research. From there you can go down the rabbit hole as far as you want.\n\nIt won’t happen immediately, but over time you’ll start to develop a very good understanding of the current trends in stats research. You might even start to notice some gaps in the research that you can help fill with your thesis work :) \n\nGood luck!', 'Look into what faculty and students in top departments are publishing.', 'Which are the top departaments?', '> where i can find the ""hot topics in statistics""\n\nTake a look at the major journals related to your specific sub-interests in statistics, read the recent stat uploads to arxiv.org \n\nIf you want general journals, try JASA and JRSS-B\n\nIf your university has a stats dept, the library should have subscriptions to both (among many others).\n\n> What Twitter pages do you follow? \n\nLOL.', 'Are you assigned a faculty mentor for this program?']",19,8,https://www.reddit.com/r/statistics/comments/13sd04y/q_where_i_can_find_the_hot_topics_in_statistics/
9,2023-05-26 20:09:34,[Q] The interpretation of the same independent variable in two separate moderated multiple regressions,"In the case of the interpretation of the results in two moderated multiple regressions with one predictor that is identical in each, does the direct effect of that one IV's contribution to the model not control for the presence of the other IV's/moderators? 

I am asking this, as in a study I am conducting I have this scenario where the two coefficients for the direct effects on an identical IV (on the same sample) is producing coefficients differing in both significance and direction.

Logically, the other IVs would be interacting with this effect in some way for it to be different in the model, but I would like some insight into how exactly this is working.","['I don’t quite follow what you’re asking. Are you asking how a single variable’s coefficient can change markedly when you include different controls?', 'Yes, in the two different models, is the direct effect for the same IV controlling for the effects of the other variables present in the model, hence why the value changes so dramatically?', 'Yes correct. As a simple example, suppose you regressed food security on an indicator for whether a household received food stamps (a government program that provides money for food). You’d see that there’s a negative coefficient on the food stamps, meaning that it looks like this government program makes people *less* food secure! That seems strange. \n\nBut we are of course missing a key variable here: income. Households in the program have lower income than households not in the program, so the coefficient on food stamps also has this difference in it. It’s not really a fair comparison of households with and without the program since these households are very different. Once you control for income, all of a sudden the coefficient is positive! Food stamps increases food security. \n\nThis same idea extends to having a bunch of controls. If you control for different things, you can have very different coefficients on the same variable.', 'Aha, yes I see now. I believe I was confusing myself by discounting the fact that each of the differing factors in the multiple regression act as controls and change it each time. Thanks.']",1,4,https://www.reddit.com/r/statistics/comments/13sbg6g/q_the_interpretation_of_the_same_independent/
10,2023-05-26 19:20:30,[Q] when to remove outliers from a set.,"Please remove if not allowed. I have read the rules and think I am ok. 

My parents have a sheep stud and have participated in a sire evaluation. The study have been conducted as follows. 

* Ewes randomly selected from the same mob
* Inseminated with the different rams semen (provided by different farms) and recorded.
* 760 lambs born to the mob. 42 where from my parents rams. 
* (this is the step they are questioning) they test the different stats of all 760 lambs and remove the outlires. 
* return an adjusted mean of the remaining set. 

On one of the tests almost all of their lambs fall in to the outliers on the better end of the scale. This means that only their lambs that would be considered sub standard in their breeding program are included in the sample set. 

Their question is would it be more appropriate split the set by farms and remove outliers for each farm before recombining taking the mean? 

Obviously this will make their farm look better.","[""Honestly unless the outliers were data entry errors, I tend to not like omitting them. They're valid datapoints that describe what happened. I would just use different measures of central tendency to describe the set. Why not just use median & IQR rather than an adjusted mean at all?"", 'If “almost all” your data points are outliers…. Then they are not outliers', ""Why are they outliers? Is there something wrong with the measurement? It's not a good idea to just chuck data because it's a bit different."", 'So, being a statistics grad from Texas A&M gig-em! I was taught not to remove outliers unless you have legitimate reasons for thinking the data does not ""deserve"" to be in the dataset,  (i.e. If it was known that the observation was not measured correctly, then don\'t use it (that\'s a reason), if you know that the particular observation is an anomaly (in the sense that when the observation was recorded, something funky was going on) (that\'s a reason) ). To answer their question in terms of a spectrum, if the goal is to make their farm look better, go ahead and just remove all the ""bad"" data and report the ""best"" data, and don\'t say what you did to the data. If the goal is to be honest about the quality of sheeps or whatever, then just report the entire dataset and report things such as mean and variance and how you got those quantities (mean and variance that is).', 'The average sheep or the typical sheep? \n\nEither way, outlier detection should be based on expert knowledge established prior to the experiment, not the observed data.\n\nA different strategy might be to allocate monetary value to all sheep and then calculate an average on that scale.']",7,13,https://www.reddit.com/r/statistics/comments/13saeir/q_when_to_remove_outliers_from_a_set/
11,2023-05-26 19:15:17,[Q] Is there a point to Variance aside from using it to find Standard Deviation?,"Super newbie here obviously. 
When we were going over Variance and Standard Deviation (SD) I asked what the difference between the two were. I get that SD is the average distance of each data from the mean value. When I asked “then what is Variance” I was just told that it is SD squared. 
 Are there uses for Variance? If not, why is it a stand alone value. Why not just have the SD formula?

I hope I’m making sense. I can try to clarify if needed.","['In mathematical statistics and probability theory,  it is mainly the variance of a random variable of estimator we care about as efficiency and precision are mainly stated in terms of the variance. The SD, and by extension the SE (standard errors) , are just  square roots of the variance . This is mainly done  to convert the variance back to the original unit of measurement so that we can construct intervals for the variable of interest or compute p-values.', ""Mean and variance are the first two moments of the probability distribution, a representation of the population (data generating process, phenomena at hand) from which you are drawing your samples. These are central moments. Generally, you either have a mental model of the probability distribution (you infer from your theoretical knowledge about the application) or you fit the distribution by using your data. Mean and variance are the first two moments of central tendency, and their definitions arise from probability theory. Mean and variance are defined for most distributions, though there are some that don't have any (Cauchy for example).\n\nIf you are working with the normal distribution or many other common distributions, you can indeed use the **sample moments** (e.g. sample mean, sample variance) to accurately estimate the **population moments**. If the function of the sample values you use to estimate has an expected value that is equal to the population moments, they are called **unbiased.**\n\nNow, for some probability distributions, this may not be true. Then you cannot, without bias, estimate the mean & variance of your probability distribution using the naive sample standard deviation.\n\nWhat I mean is, variance is a defining characteristic of a probability distribution, not the standard deviation. Standard deviation is just another function that estimates variance, though it can indeed be defined as a moment, but it is variance that we are mostly working on, not SD.\n\nCasella & Berger is a classical textbook that explains these topics deeply at an introductory level."", 'Some things are proportional to variance.', 'Yes, it’s a key component of theoretical statistics. It’s the second moment. The covariance is hugely important as well.', 'Amazing information! Thanks everyone!']",0,5,https://www.reddit.com/r/statistics/comments/13saaqr/q_is_there_a_point_to_variance_aside_from_using/
12,2023-05-26 17:47:05,[Q] Tukey HSD results not including all 16 groups (statsmodels pairwise_tukeyhsd),"I run this tukey hsd test:

    tukey = pairwise_tukeyhsd(endog=posVariantsDfArray,
                          groups=posVariantsDf.columns.values,
                          alpha=0.05)

variantsDf = my data, see: [https://i.imgur.com/jAZaP6E.png](https://i.imgur.com/jAZaP6E.png)

I expected to see all 16 groups compared to each other in the results (see [https://i.imgur.com/afD3AWJ.png](https://i.imgur.com/afD3AWJ.png)), but it only picked 4. Why is that? How can I compare all 16 groups?

Goal: I'd like to see which variants means are significantly different from each other.

Thanks!",['Try “tuckey.results_table” and see if that doesn’t include all pairs.  If not what does “posVariantsArray” look like?'],1,1,https://www.reddit.com/r/statistics/comments/13s8l1b/q_tukey_hsd_results_not_including_all_16_groups/
13,2023-05-26 14:45:38,[Q] Regression model with interacting variables,"Hi everyone,  
  
I'm doing a university project which involves creating a regression model to explain something about the world.  
  
My chosen topic is to look at what factors are most important to winning a game of football (proper football for the record, not the American sort). The rationale for the study is so that coaches/managers can consider how much of their budget they would want to spend on the top attacking or defending players.  
  
I've completed the basic multiple regression model, with the win ratio on the Y axis and then goals/game, goals conceded/game and possession as the X variables.  
  
In the regression model, increasing goals/match by 1 increases wins/match by 0.211, while for every conceded/match you reduce wins/match by -0.118, so clearly coaches should focus more on scoring goals than conceding them.  
  
The data I've gathered comes from four different football leagues, so I also want to consider whether these variables change depending on the league. In some leagues, depending on the style of football, preventing goals might actually be more important, or at least relatively important, than scoring them.  
  
Can anyone suggest the best way to test whether there is differences between the leagues, short of running regression models across the leagues individually? I believe there should be a way to do this using dummy and interacting variables, but my brain isn't quite figuring it out.  
  
TYIA","['If ""g"" is the number of leagues, create g-1 dummy code variables, with each coded ""1"" for one league and zero for all the rest. Choose one league to serve as the reference group, which is coded zero for all dummy code variables (each variable will compare one league to the reference group, so ideally the reference group will serve as a useful comparison in some way). Create cross-product terms for each predictor variable by multiplying the predictor variable by each dummy code variable. The set of cross-product terms will represent the interaction between a predictor variable and league (with each cross product term comparing the effect for the group coded ""1"" relative to the reference group. If you center the predictor variables by subtracting their mean before calculating the cross-product terms, you can more easily interpret the lower-order terms in the analysis that includes the interaction effects (the lower-order terms will represent simple slopes for one variable when the other is zero, so centering a variable makes zero more meaningful by making it equal to the variable\'s mean).', 'You can’t make the leap that coaches should focus more on scoring goals than conceding them solely from that model. You’re not showing something causal.', '… also, controlling for league may help account for nonindependence. If residuals are correlated within leagues, your original analysis could have biased standard errors. Controlling for differences between leagues using dummy codes accounts for mean differences between leagues. If your dataset includes more than about 10 leagues, a multilevel model may be a more sensible approach to account for nonindependence and allow you to model how effects vary randomly between leagues using random effects.', 'League dummies won’t take care of no independence between units within a league.', 'Depending on how many predictors there are, interacting each one with dummies for each league is a really bad way to overfit the model and eat up power.']",11,9,https://www.reddit.com/r/statistics/comments/13s5m5p/q_regression_model_with_interacting_variables/
14,2023-05-26 12:51:38,[Q] Question about manual plotting of ROC curve,"I have a binary classifier, that is not an ML method, and am trying to manually plot the ROC curve. We are classifying patients into cancer and non-cancer, where based on biomarker values, we assign them a risk score in the range 0-8.

  
If they are >= 5, they are predicted as a positive cancer case and if they are below, they are predicted as a control patient. I also have the ground-truth values for each patient. I want to calculate the TPR and FPR at each threshold of 0,1,2, etc.

  
I'm following along this tutorial, https://www.statology.org/roc-curve-excel/ where they use a method that at each threshold, they calculate TPR & FPR using

FPR: =1 - (Cumulative Pass / Total Cumulative Pass)

TPR: =1 - (Cumulative Fail / Total Cumulative Fail)

  
I'm confused because this seems to deviate from the standard definitions of:

TPR = TP / (TP + FN)

FPR = FP / (FP + TN)

  
I was hoping to use the cumulative method, because at certain thresholds I'll have TP and FP be zero, which gives me a 0 TPR and 0 FPR. But I am concerned the tutorial is not statistically sound.  
Thanks in advance!","['FPR = 1 - true negative rate (AKA specificity)\nTPR = 1 - False negative rate (AKA recall)\n\nThis is what seems to be being used here, there are so many of these metrics all intertwined that its easy to get lost in the conversions. Id say what you are trying to achieve here is very similar to credit risk, i.e. binning people into certain risk groups. One way to think about the cumulative method I find is to put cumulative goods on the x axis, cumulative bads on the y and then plot points for each bucket. Might be more obvious to you how this forms a ROC curve.']",5,1,https://www.reddit.com/r/statistics/comments/13s3jr3/q_question_about_manual_plotting_of_roc_curve/
15,2023-05-26 03:55:18,"[Question] Should i use Stdev.P or Stdev.S for my data, and can SEM be calculated from both stdev.P and stdev.S?","To keep it simple, I have an experiment in which cells were injected with a certain treatment, and all these cells were measured. There exist no other cells treated with this treatment, so they must comprise the entire population, as there is no sampling and as a result sampling error, correct?  
  
In this case i should use stdev.P?  
  
""The standard error of the mean (SEM) measures how much discrepancy is likely in a sample's mean compared with the population mean.""  
  
But if is is the entire population, and I am using stdev.P, is SEM still meaningful? Is it calculated the same way for both? (SD/sqrt(n))?","['You still have a sample of all possible treated cells. Repeat the experiment and the results will be different. So conceptually this is still a sample, not a census. Use sample standard deviation.', '> There exist no other cells treated with this treatment, so they must comprise the entire population\n\nNo, that\'s not the population in this case. If someone were to very carefully replicate your experiment, they would still get a different set of values to you. You weren\'t interested in a conclusion about the values those *specific* cells, you might have chosen different cells to run the experiment on. Typically the population that would be relevant would be the set of values you\'d have had if you\'d used *all the cells that might have been selected* -- specifically the collection of cells about which you wish your conclusions to apply.\n\nOften that population is notional; I expect, for example, that you would like your conclusions to apply to cells that don\'t even exist today, but might be around in a month, or a year. \n\nUnless you\'re happy for people to say ""well, that experiment contains *no information of any value*, because the conclusions *only apply to that small set of cells that were treated*, but I want to know what would happen with *other* cells of the same kind.""', ""Remember that when doing research the population that you study is something you define and you must be really specific about what are its elements and delimit it in space and time.\n\nMaybe you decide that the population for your study is the fruits on a specific tree near your house, then you can use stdev.p and it will be correct but your conclusions will only apply to the fruits of that specific tree. If you wanted to make claims about all the trees in the world as of today, you'll definitely need to redefine your population and extract a sample."", ""It's important to think about the way statistics is generally formalized- we care, for example, about the entire population of X, so we start with statistics about X, and then recognize our limitations by sampling from a distribution D from X- and formulate from there.\nThere is no reason to assume that you will never have the full population. Obviously in practice, we almost always start from a sample, but Stdev of the population is sometimes required, so why would you not include the option? Especially when it could be used.\nI'll give an easy example: at my old university, it was fairly isolated and vegetarian was artificial. They only cared about the trees within the campus property area. There basically were no trees outside the area.\nAfter meticulous tagging, every tree within the property was tagged and recorded. They used population level analysis for their year to year analysis on the campus tree and vegetation, because that was the entire scope, and they absolutely had nothing to say about trees in general."", ""> But then nothing can ever be population can it?\n\nThis is essentially correct. \n\nIf I want to write a report on the end of year results for Year 12 at Random High School in 2022, then those are the only results that will ever exist for that class and if I want to include a measure of variability, like SD, to summarise the within-year spread, I can do that. But if I do do that, it's probably because I want to compare the class of 2022 with <something>, formally or informally. In which case, Year 12s in 2022 were still a random sample of all possible Year 12s.\n\nThere are times when we have genuine census (ie whole population) data and only really care about describing what it looks like. But if you're calculating SD for any reason other than description, you're probably conceptualising your census as a sample of some kind, whether or not the data for other possible samples have been, or even could be, collected.""]",0,7,https://www.reddit.com/r/statistics/comments/13rrjgz/question_should_i_use_stdevp_or_stdevs_for_my/
16,2023-05-26 02:17:03,[Q] Need Help,"What are the easiest statistical methods to measure effectiveness. It’s for a paper titled Effectiveness of Marketing Strategy on purchase intention of students for online shopping. 

I’m pretty bad at stats and I’ll appreciate any help I can get.",['You’d have to define what you mean by effectiveness first.'],0,1,https://www.reddit.com/r/statistics/comments/13rp1or/q_need_help/
17,2023-05-26 01:35:02,[Q] Stats GRE preparation,"Can anyone give me advice on how to prepare? Is there a book or something I should look at, or do I just go over everything I learned in undergrad? I’m not sure where to start.","[""There is GRE general, which involves reading comprehension, writing skills, and quantitative reasoning.\n\nThere are also subject GRE, specific for subjects like physics, chemistry, mathematics, etc. I don't think there's one for statistics."", 'Is there a subject GRE exam just for statistics?', 'Definitely don’t need to study much (if anything) from undergrad. There are tons of different test prep books or free test prep videos you can use. The math only really covers through college algebra from what I remember. If you’re from the US, think of it as basically a slightly higher level SAT or ACT\n\nA lot of schools waive the GRE these days, so make sure you check that requirement at the programs you’re applying to in case it lets you save the $200 or whatever for the exam fee.', 'I assumed that’s how it works. Is the GRE more like the ACT??']",2,4,https://www.reddit.com/r/statistics/comments/13rnzit/q_stats_gre_preparation/
18,2023-05-26 01:30:29,[Question] What statistical methods are appropriate for analyzing ML results? Are ML results normally distributed?,"Say when we want to compare the results of 3 different treatments of input and their effect on the same Machine Learning model. We got the evaluation/testing accuracy per epoch per treatment, like epoch 1 has Accuracy of 0.83 for treatment A, 0.76 for treatment B, 0.33 for treatment C, etc.   
Due to the limitation in expense and time, we only have about 5 runs per treatment (say each run has 10 epochs), so 5 x 10 = 50 data points per treatment.   
The question is: what are the correct methods to use on these ML results? Can we assume the data follows normal distribution in order to use something like ANOVA? If not, then can we use something like Kruskal-Wallis?","['You’d have to inspect the data to see what the distribution looks like', '> What statistical methods are appropriate for analyzing ML results? \n\nThe question is impossibly general. Different approaches might be reasonable in different cases. \n\n> Are ML results normally distributed?\n\nThat\'s a bit like asking ""are fish 35 cm long?""   \n... a few are very close to 35cm long, but for most purposes that won\'t matter.\n\n> epoch 1 has Accuracy of 0.83 for treatment A\n\nThat particular fish (""Accuracy of A"") can\'t actually be 35cm long. That\'s probably not of major consequence, but likely there\'s better choices of a model than ""this fish is exactly 35 cm long"" ... I mean than ""the population of values of which we have a sample are normally distributed"". \n\nIt may well be that equality null hypothesis tests are not the ideal choice of approach whatever distributional model you have, but you haven\'t explained what you were originally trying to find out *before* you tried to turn it into a stats problem.\n\nI presume by *Accuracy* that you\'re talking about classification accuracy. Is that correct? If so, why was your title  question not more specific? Not all ML results are classification accuracies.\n\nIf it is classification accuracy, it will be a count divided by another count, in which case variance will be different at very high accuracy than it will be at accuracies nearer to 0.5. (the number of correct-classifications will presumably be either something like binomial or -more likely- a mixture of binomials with varying p\'s). There\'s not enough detail about the situation for *us* to choose a model for you.\n\nYou\'re also not clear about what exactly you wan to compare with what. Are you comparing within epochs? Presumably there\'s cross-epoch differences. \n\n> Can we assume the data follows normal distribution in order to use something like ANOVA? If not, then can we use something like Kruskal-Wallis?\n\nWhat led you to consider ANOVA, in the first place? Was the original question of interest about *average* accuracy? If so, Kruskal-Wallis would not answer that question. Use an analysis that responds to your question of interest; there\'s many more options than ordinary ANOVA.', 'That’s too vague for us to help.', 'No, considering the accuracy as function of epoch is completely irrelevant. I assume what you want to do is compare different models performance for the same problem? Its honestly a complicated topic, but something simple and decent to do is to use k-fold data splitting (into training and validation), train model A and model B in each data split and do a paired t-test on these performances on the validation folds.\nEdit I would use a metric that doesnt depend on the threshold which is a choice, better use auroc', 'Clear as mud but this might be a helpful place to start: [Meta-Analysis of Diagnostic Accuracy with mada](https://cran.r-project.org/web/packages/mada/vignettes/mada.pdf)\n\nYou may or may not want to use that package but the references should be helpful.']",0,13,https://www.reddit.com/r/statistics/comments/13rnvdz/question_what_statistical_methods_are_appropriate/
19,2023-05-26 00:54:04,[Question] Is a Multiple Hypothesis Test correction needed when running an A/A test?,"I am running a test to understand the churn rate on an application page. To confirm the experiment was set up correctly, I initially ran an A/A test before any changes were made for the three subsets of our users before starting the A/B test.

  
Do I need to include the A/A test hypothesis tests in the multiple hypothesis test correction?","[""I wouldn't think so since you weren't really testing anything."", 'No. The A/A test is to answer the question ""Are there bugs in my experiment setup?"". Once that is answered you run your A/B test assuming there are no bugs.']",2,2,https://www.reddit.com/r/statistics/comments/13rmxox/question_is_a_multiple_hypothesis_test_correction/
20,2023-05-25 22:23:49,[Q] What methods can I use to show the statistical significance of differences in my data?,"For my thesis I ran a study, where I presented the participants a handful of attributes of online shops. On a scale (-3, -2, -1, 0, 1, 2, 3) they had to choose how much they would trust an unknown online shop given different attributes.

Now I have a table like this: [https://imgur.com/d6a78733-bced-44c7-8fb8-4803d423afcf](https://imgur.com/d6a78733-bced-44c7-8fb8-4803d423afcf)

&#x200B;

What I did so far:

\- Using python (pandas, matplotlib) creating a dataframe based on all the data and calculating the means and standard deviation for all attributes

Problem: The means lie closely to each other, which is problematic for my further process. I need some way to tell if differences between the means are statistically relevant.

My first approach would be to display the means and deviation on a graph using matplotlib (though I haven't figured out yet how to get the deviation in there as well).

Do you have any suggestions what methods I could choose to encounter this?

Cheers","[""It sounds like you're approach this all wrong.\n\n\\>The means lie closely to each other, which is problematic for my further process. I need some way to tell if differences between the means are statistically relevant.\n\nThere is no scenario where the values determine if there is statistical relevancy. The values are what they are. It seems you want those values to show something different, so you get the results you want.\n\nYou don't 'show' statistical significance. Significance is not a goal but the result of a test."", 'You can use tukey hsd to see differences among the groups.\nGood luck with your analysis.', '>display the means and deviation on a graph using matplotlib (though I haven\'t figured out yet how to get the deviation in there as well).\n\nImport matplotlib.pyplot as plt\n\nplt.errorbar(xdata,ydata,yerrs=errordata,fmt=""o"",fillstyle=""none"",ecolor=""r"",elinewidth=0.2,capsize=2,capthick=1)\n\nplt.show()\n\nSomething like that. xdata and ydata and errordata are arrays containing your values.\n\nAlso consider showing not the standard deviations, but the standard error of the mean, which is likely more relevant for what you want to convey.', ""Also, it's not a problem that the means lie close to each other, if the errors are just small enough. In physics we somtiles compare values that are only like 0.000001% different.\n\nIf the means lie outside the errors (not the deviations!!!) of each other, then it points towards them being statistically significantly different."", ""Of course. Still, I'd like to properly show that. Tukey's range test should do it (thanks u/whythigh).""]",2,12,https://www.reddit.com/r/statistics/comments/13rj698/q_what_methods_can_i_use_to_show_the_statistical/
21,2023-05-25 20:38:58,"[Question] ""Average"" distance between binary vectors","I want to determine the average distance between multiple binary vectors.

1. I use (for now) the Sokal-Michener distance to calculate the distance between the vectors, pair wise (Requirement for the calculation I recon)
2. Then I take the sum of all distances between pairs
3. Then I either calculate the mean or median

* My supervisor has concerns that the result might be skewed, when there is a cluster with say 4 out 5 points closely, but one outlier.
   * I tried to account for that with the use of the median (before I had the mean)
* However he said ""try something else, I'm sure there is something different"", but after some poking around I did not find something to calculate the distance between multiple points or vectors. (Even leaving the binary part out of it for the sake of research) 
* Has anyone pointers for me or an idea where to look? Or can justify my median approach?","[""Can you weight the ones which are linked together multiply? IE you get an extra point for the lengths of chains? k nearest neighbors maybe?\n\nYou've probably already googled, but here are some links which at least discuss the thing.\n\nhttps://stats.stackexchange.com/questions/274731/how-to-statistically-validate-that-one-clustering-technique-is-more-effective-th\n\nhttps://www.coursehero.com/file/p2k4009/Sokal-and-Michener-1958-the-distance-between-two-clusters-is-defined-as-the/\n\nAssociations among similarity and distance measures for binary data in ... https://mz.mf.uni-lj.si/article/download/204/293/334\n\nhttps://documentation.sas.com/doc/en/pgmsascdc/9.4_3.3/statug/statug_cluster_details01.htm"", 'Start with using the Hamming distance', 'Thank you for your help, gonna check the links tomorrow.\n\nI also had a meeting with him today and we talked about it and he had the idea I calculate the sum of the distances of the nearest neighbour.\n\nKinda like your idea. Will have to look into it', 'Why you opt for the hamming?\n\nThe Sokal Michener seemed good for my purposes as it takes same zeros also in consideration. But I already had a ""test"" where i threw every distance under the sun on the problem and it mainly just gives different values if the distance is fitting.\n\nThe squared euclidean also didn\'t look to bad as it gives bigger numbers, but the patterns are all the same. Except the 1 part on zero-one scale distances, this might be interesting if I can differentiate the higher areas better\n\n&#x200B;\n\n    Soakl Michener\n    [[0. 0.42105263 0.43283582 0.5106383 0.84615385]\n    [0.42105263 0. 0.40909091 0.53146853 1. ]\n    [0.43283582 0.40909091 0. 0.5 0.85245902]\n    [0.5106383 0.53146853 0.5 0. 0.77906977]\n    [0.84615385 1. 0.85245902 0.77906977 0. ]]\n    haming\n    [[0. 0.26666667 0.27619048 0.34285714 0.73333333]\n    [0.26666667 0. 0.25714286 0.36190476 1. ]\n    [0.27619048 0.25714286 0. 0.33333333 0.74285714]\n    [0.34285714 0.36190476 0.33333333 0. 0.63809524]\n    [0.73333333 1. 0.74285714 0.63809524 0. ]]\n    seuclidean\n    [[ 0. 10.86853256 10.42832681 11.61895004 19.46150046]\n    [10.86853256 0. 10.69462482 12.47497495 22.29069313]\n    [10.42832681 10.69462482 0. 11.45643924 19.55760722]\n    [11.61895004 12.47497495 11.45643924 0. 18.4729532 ]\n    [19.46150046 22.29069313 19.55760722 18.4729532 0. ]]']",1,4,https://www.reddit.com/r/statistics/comments/13rgow4/question_average_distance_between_binary_vectors/
22,2023-05-25 19:45:52,How much optimization do stats MS/PhD students need to know? [Q],"Is a book like this, by boyd:

https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf

Overkill for stats PhD students in terms of optimization? How much of this would a stat PhD student use?","['It is a great book for anyone who uses optimization in their research. With this kind of math, you never know when you will need it and if you study it in advance, it also helps you question your problems better.', ""Even though it is one of my research areas, it is not something most statisticians, even PhD statisticians need to know.  There are a few very simple things: difference between global and local optima, consequences of convexity for global optima and uniqueness of optima, various characterizations of convexity, the Dennis-Moré theorem, Kuhn-Tucker conditions.   But they don't need to know most of that book or any optimization book unless they want to use it in research.  I know most of [Rockafellar (1970)](https://www.amazon.com/Analysis-Princeton-Landmarks-Mathematics-Physics/dp/0691015864/) and [Rockafellar and Wets (1998)](https://www.amazon.com/Variational-Analysis-Grundlehren-mathematischen-Wissenschaften/dp/3642083048/) but I wouldn't say every statistician should know that."", ""It's super helpful to know some optimization; I'd recommend the Ken Lange textbook though, more focused on statistical applications."", 'Statisticians (that do research) routinely need to think on how to implement estimation procedures, which are always optimization problems.', 'Yes.  Convex optimization, outside of exponential families, is not what you have in statistics.  So ""mostly focus on convex"" is definitely the [Wrong Thing](http://catb.org/jargon/html/W/Wrong-Thing.html), especially since Rockafellar and Wets (citation above) generalize all of that to the nonconvex case (and even vastly improve theory under convexity).  You don\'t really understand convexity until you read Rockafellar and Wets.']",35,31,https://www.reddit.com/r/statistics/comments/13rfk1q/how_much_optimization_do_stats_msphd_students/
23,2023-05-25 18:25:18,[Q] What do the odd diagonal lines in my ZPRED*ZRESID plot in SPSS mean?,"I am currently working on my Bachelor's thesis and to test my hypotheses I am doing a regression analysis (I am unsure as to which one yet). To check the assumptions for a linear regression I plotted my standardised residuals against my standardised predicted values for the independent variable in a scatterplot. However, the values are not equally distributed above and below zero and there is no equal coverage of values along all ranges. But even more strangely, I see a lot of odd diagonal lines in my plot. What do these lines indicate? I have never seen them before.

I am using IBM SPSS 28 to run my analyses.",['post the graph'],1,1,https://www.reddit.com/r/statistics/comments/13rdyks/q_what_do_the_odd_diagonal_lines_in_my/
24,2023-05-25 17:04:35,[Q] How to denote the correlation (r or ρ)?,"Hello, I just have a quick question about notation in a scientific paper. I am supposed to mention how the effect size measure I used can be assessed in terms of magnitude. I would like to use the Pearson   
correlation coefficient and refer to Cohen (1988). If I now give the rating of  = .1 (small) / = .3 (moderate) / = .5 (large), do I have to denote the correlation with an r or with ρ (rho)?

I'm not sure what's right since I am talking about the coefficient in general, independent of a specific calculation.

If this is not the right subreddit, please point me to where I might find the answer. Thanks! :)","['Convention is that rho is use for the (unknown) population parameter and r is used for a sample. Either way, clearly define notation in your paper.']",1,1,https://www.reddit.com/r/statistics/comments/13rcj3q/q_how_to_denote_the_correlation_r_or_ρ/
25,2023-05-25 07:58:41,[Question] Help to find a proper test to perform,"Help on which test to perform?

Hey y’all I have this statistics project where I’m trying to compare the predicted price of crude oil and the actual price from 200-2008. I’m stuck on which test to actually perform. I have a full data set and was thinking about comparing the regression lines of predicted and actual prices but still not sure which test would accomplish this. I’m stuck in excel for this for whatever that’s worth. Any help would be appreciated.","['Assuming you’re talking about predicting future prices based on past prices, you should use a time series model (such as ARIMA), not a conventional linear regression model.\n\nTime series model predictive performance is typically evaluated by computing mean-square-prediction error (MSPE) with an expanding or rolling training window.\n\nI don’t use excel, so I can’t help you there.']",1,1,https://www.reddit.com/r/statistics/comments/13r25pj/question_help_to_find_a_proper_test_to_perform/
26,2023-05-25 04:07:02,[E] Value of Terminal Masters,Is there any value for a terminal masters in stats (particularly an online one) following a bs in stats? It seems like the two have very significant overlap and I see that many of my UG courses would be repeated in one of these programs. Do these programs have any value following bs in stats?,"['You\'ll have to tell us a bit more about what this ""terminal masters program"" includes: to most of us (at least in the USA), a masters in statistics is by definition not terminal, because PhDs are frequently awarded in the field. \n\nRegular master\'s degrees are worth a fair bit in terms of what types of jobs they get you past the resume screener for (there are not many jobs doing real statistics, as opposed to data entry/basic number crunching, that a BS in statistics will get you.) Yours might also be.', 'I’ve seen “professional masters” in statistics programs before, assuming that’s what you’re referring too. They are designed to be less mathematically rigorous and focused more on how to use different existing tools to analyze a few different types of data. So, fairly similar to a BS in stats.\n\nA professional masters programs will probably make you more confident in your ability to analyze some different types of data than just having a BS in stats, but I don’t think you’ll come out with a very deep understanding of how the methods work. A professional masters will probably prepare you more for a job as a data analyst than just a bachelor’s degree would, but a traditional master’s degree will prepare you better for a wider variety of careers both in terms of understanding how things work and how to implement them.\n\nYou didn’t ask about this, but I figured I’d mention this for anyone interested: If you want to eventually get a PhD, do NOT get a terminal masters. Get a traditional master’s with a thesis/research project option. Or go straight to a PhD (very common).', 'I know what you’re talking about and did it - DM me', 'A professional master’s in stats won’t give strong evidence of research potential or the ability to perform well in rigorous coursework. So, you’ll be paying potentially a lot of money for something that won’t strengthen your PhD application nearly as much as a traditional master’s.\n\nI’m not saying a professional master’s will hurt your application, but it probably won’t be a strong indicator of academic rigor and research potential, which are two of the main things that admissions committees look for. If you have the funds and really want to do the professional program, go for it.\n\nIn general, I think professional masters programs may be a good option for someone who wants to do certain data analyst type roles in industry, but not necessarily for someone interested in a PhD or further research.', ""The terminal masters I've seen are noted as professional and do not include research. Also their curriculum does not get into too much depth compared to non terminal. It seems to usually be 30 credits of core and some electives and the only prereqs are usually just calc 3 and lin alg. Coursework seems to introduce stats at a graduate level without depth and overlaps a lot with my current ug.""]",2,6,https://www.reddit.com/r/statistics/comments/13qw6e8/e_value_of_terminal_masters/
27,2023-05-25 02:44:14,[Q] Interpreting Multiple Regression Coefficients After Square Transforming All Variables,"Hello,  
I'm knocking the dust off on my stats. I'm currently dealing with an entire data set that is all on the same scales. All the variables have negative skewness ranging from mild (-\~.4) to extreme (<-1.X).  
  
A square transformation to every variable REALLY cuts this down. I know that interpreting multiple regression output after non-linear transformations on the dependent is muddy. I'm wondering if interpreting coefficients for a multiple regression with all variables having the same non-linear transformation is better, and, if so, how I might do that.  
  
Thank you for any input.","['You can absolutely interpret transformed variables. You just have to look at predicted effects instead of ""a one unit increase in x is associated with a beta increase in y"". The basic idea is that you\n\n* choose a low value of one of your x variables, as well as a high value. This might be the bottom 25th percentile vs the 75th percentile.\n* predict the y value for the low x value and the high x value, holding all other variables at their mean (or median, or anything else really)\n* see if the difference between the predicted y values is meaningful to your context. \n\n[Here\'s](https://www.reddit.com/r/statistics/comments/12yq9x7/q_question_about_holding_variables_constant/?utm_source=share&utm_medium=web2x&context=3) a reddit thread where I commented more on holding all other variables constant.\n\n[Here\'s](https://stats.oarc.ucla.edu/stata/dae/logistic-regression/) a UCLA page on using predicted probabilities to interpret a logistic regression; it relies on the same principles; predicted probabilities are just a particular kind of predicted value. And the version [in R](https://stats.oarc.ucla.edu/r/dae/logit-regression/). The UCLA site is generally fantastic.']",1,1,https://www.reddit.com/r/statistics/comments/13qtxkp/q_interpreting_multiple_regression_coefficients/
28,2023-05-25 02:14:56,[Q] is there any literature on using predicted scores in a subsequent model?,"Let’s say I use a multiple regression model to predict individual scores and then use those predicted scores as an independent variable in a subsequent model. 

Would I be concealing variability in that second model since the predicted scores are point estimates and are not including the variability about those estimates?","['It’s called stacking and there’s some literature out there, see for instance Wolpert (1994) and (1996) for some of the earliest work. There’s also blending which is conceptually similar. Both of these techniques are mostly focused on prediction and not statistical inference, that is more “data science” than “statistics”.', '>Would I be concealing variability in that second model since the predicted scores are point estimates and are not including the variability about those estimates?\n\nYes. But you could do a bootstrap where you include both models to capture this variability', 'I appreciate you pointing me in this direction!']",1,3,https://www.reddit.com/r/statistics/comments/13qt5ot/q_is_there_any_literature_on_using_predicted/
29,2023-05-25 01:26:02,[Q] >99th percentile vs 99th percentile,"My son had is 1 year checkup, and they measured various things and the paperwork has percentiles for those. Some statistics, like head circumference say things like 97th percentile, which i understand, but his height and weight both say >99th percentile, rather than just 99th percentile. Does that just mean 99th percentile or does it mean something else? Is our health provider just weird? (Kaiser in California) I wasn't at the appointment so I didn't get to ask, and googling hasn't brought me anything. Thanks for any help.","['It means greater than the 99th percentile, or greater than 99% of other 1 year olds.', '> Is our health provider just weird?\n\nNo, this is pretty conventional for medical stuff\n\n> Does that just mean 99th percentile or does it mean something else? \n\nIt means ""above the 99th percentile"" without specifying whether it\'s 99.1 or 99.7 or 99.99.\n\nSo somewhere within the biggest 1% of the population at that age for both height and weight.', 'Thanks.  That makes sense. He has had ones before that just say 99%, but this time they added the >']",1,3,https://www.reddit.com/r/statistics/comments/13qrtuo/q_99th_percentile_vs_99th_percentile/
30,2023-05-25 01:17:05,[Education] [PSA] [Rant] Don't you dare write or post about Gamma distributions without saying what parameterization you are using.,"I mean, really. I've spent the last several days working a model involving old-school ARD priors for factor weights, using a Gamma prior, and related topics.

And ALMOST NONE of the 100+ web pages and PDFs I've been reading EVER take the simple step of explicitly saying what parameterization for Gamma they are referring to in their paper/post. Is it shape? Is it rate? Who knows? 

No, I don't know what's common in your discipline. And I suspect  you don't, either.

No, I can't know for sure just because you use a ""beta"" instead of a ""theta"". Sure, the wikipedia notation is more popular than it used to be, but not everyone uses those consistently.

So if you are one of those people that write about the Gamma distribution without explicitly saying whether you are using shape, rate (or some other!!) parameterization, YOU ARE A BAD PERSON. May all your models fail to converge. May all your reviewers be ""Reviewer #3"". May your IRB committee require you to get informed consent in triplicate not just from subjects, but from subject's parents and grandparents and roomates' cousins' uncles.

My next PSA will be called: ""If you use priors in a paper with empirical results but never tell us what numbers you used for your top-level priors, YOU ARE A BAD PERSON. Even if you are a famous stats god who helped develop a whole field.""","[""> use priors in a paper with empirical results but never tell us what numbers you used for your top-level priors\n\nHere in the wild west of data science they just say stuff like the 'tool uses Bayesian statistics to power its algorithms' or something most of the time leaving me to sit there like ok? and?"", 'You tell ‘em! Why can’t they just write the damn density with the symbols they use?', 'undergrad stats major here! my bayesian stats prof went on a 10 minute rant about specifying gamma parameterizations this past semester so dw us future generations will make sure to specify', ""It'd be one thing if they could simply write Gamma(shape, rate) or something like that next to the distribution reference. But maybe that takes away the mystery? Everyone likes a good mystery!\n\nPersonally, I'm going to start writing all my papers and models using the flugelhorn Gamma parameterization, a rather obscure version of Gamma promulgated by Tibetan econometrician monks in the late 70s. Of course, I'm still going to \\*write\\* it as Gamma(alpha, beta). But I'm sure people can figure it out."", ""Ha ha, your prof is doing god's work. :D""]",138,28,https://www.reddit.com/r/statistics/comments/13qrll4/education_psa_rant_dont_you_dare_write_or_post/
31,2023-05-25 01:04:11,[Q] How to handle known dependence of explanatory variables,"I have a set of data of county median income. I want to explain the variation in this data by variation in county characteristics. The county characteristics that I have are all fractions, such as ""fraction of degree holders who received their first degree in engineering.""  
  
These characteristics can be split into different groups where every characteristic within each group is known to be dependent on each other member characteristic of the group, as the sum of each characteristic in a certain group is 1.0 for each county.  
  
For example, one group that I know are all dependent is: pop fraction with less than high school education, fraction with highschool education, fraction with some college, with bacchelors, with graduate degree.  
  
How do I handle this? Alternately what search terms should I use to learn more about analysis techniques for this situation, or what books (or chapters from books) would be able to teach me more about this situation?","['If using a linear model you can just drop 1 of the variables. E.g. if 5 variables always sum to 1, drop one and do regression with remaining ones. This is common when e.g. changing a categorical variable to one-hot encoded and dropping 1 of them.\nAlternatively you could use e.g. a lasso regularized model.', 'You have *compositional* data. You want to search for models with compositional predictors.', 'Thank you so much, this is exactly what I was looking for']",1,3,https://www.reddit.com/r/statistics/comments/13qr9aa/q_how_to_handle_known_dependence_of_explanatory/
32,2023-05-25 00:08:10,"[Education] Masters in EU, Canada and Oceania that place well into top US PhD programs?","As per the question I want to know if there are good programs in the EU and Oceania that prepare students well and get placed well into top PhD programs. I am aware of top departments like ETH Zurich, Oxford, UCL. Maybe even Paris Saclay, Sorbonne, Unimelb as well? Mcgill seems to fund all of their masters students but I have no data regarding how many students get placed in good PhDs. I want to know if they are better for industry or furthur education. Funded programs would be better but I will try my hand in anything.","['Are you asking about Canada as well? UToronto and UBC are the most well known schools, all of them tend to be more academically focused in Canada so are best for further education, but theyre all internationally recognized enough to be able to get into industry roles\n\nEdit - i had McGill listed but found that they dont have MSc for math/stats', ""hello! thank you for your reply. ive heard great things about ubc and UToronto but I'm worried about their PhD placements in the US since I couldnt find any official data.\n\nRegarding Mcgill, when I checked their website a few days ago, MSc thesis and MA thesis option showed up for me :O""]",2,2,https://www.reddit.com/r/statistics/comments/13qpsv7/education_masters_in_eu_canada_and_oceania_that/
33,2023-05-24 22:39:56,"[Software] I made a free and simple web app to generate dummy data (no ads, no tracking, no signup, no BS)","If you want to check it out, it's at [www.DataSmith.click](https://www.DataSmith.click)","['Neat!', ""I have meaning to do something like this, but I don't know the web dev part. Maybe, we could also specify data types for each column, also the distribution."", 'Awesome. \nAn option to include numerical rather than string data would be great.']",9,3,https://www.reddit.com/r/statistics/comments/13qnl2m/software_i_made_a_free_and_simple_web_app_to/
34,2023-05-24 19:10:35,[Education] [Research] PCA (principal component analysis) evaluation,"Hi,

I have an issue understanding and learning how to use PCA to evaluate my data, interpret it and evaluate the results. Hopefully you know where I can find more Information about it or maybe you got some information yourself. I'd be extremely glad to hear from you, cause I need it for my Bachelor Thesis and it's also very intrigueing too, but I can't quite wrap my head around it. 

Thank, you in advance!","['Statquest pca playlist:\nhttps://youtube.com/playlist?list=PLLViszK5qws1SUpVeEiGxHpjnJC-_ZzMy', ""Why do you think you need PCA? That's a good place to start. \n\nWhile PCA has a myriad of uses, it's mostly a transformation technique to reduce dimensionality by creating vectors from your variables to represent the greatest variance. Plenty of places to go from there but there are a whole other range of statistical methods to consider."", 'In short, the idea is to reduce the amount of features while still maintaining as much information as possible evaluating among other metrics the variance of the vector created by each feature. A concept I personally struggled with when I first started experimenting with it is that it can ONLY be used on numeric continuous variables, not even numeric indicator variables (1, 0): https://builtin.com/data-science/step-step-explanation-principal-component-analysis', ""We're going to need a bit more detail to assist. What exactly is tricky for you (e.g., scree plot, eigenvalues, etc.)?"", ""I need it cause I want to research, wether aboveground and belowgeround traits of different plants correlate or not.\nFrom what it looked like PCA is able to tell me this, but I'm not sure how I can detect correlation/extract it from the plot I yield.""]",7,7,https://www.reddit.com/r/statistics/comments/13qih23/education_research_pca_principal_component/
35,2023-05-24 18:49:10,[R] Looking for an expert in (descriptive) Bayesian statistics for an phd project on folk psychology and typological thinking,"Hello everyone,

As a PhD candidate working in experimental philosophy, I am in the process of exploring a unique idea in the realm of folk psychology. This concept, while still somewhat nebulous, is starting to take shape and I believe that with the right expertise, it could develop into an exciting study.

At this stage, I am eager to tap into the wisdom and experience of this community to assess the feasibility of this idea. Constructive criticism and feedback are not only welcomed, but highly appreciated.

My ultimate goal is to find a statistician who shares a keen interest in this topic and possesses the skills necessary to help refine and guide this study. For those who express interest, I can provide a research outline to give a clearer picture of the project.

Should we find our interests and ideas aligning, I look forward to entering into a paid collaboration, where your role would encompass:
Assisting in the preliminary setup of an experimental design tailored to the research question(s).

Conducting thorough statistical analyses on future collected data.
Providing insightful feedback and suggesting modifications as the project evolves.

I’m looking for someone with a deep understanding of (descriptive) Bayesian statistics, experience in experimental design and data analysis, and strong communication skills.

If this prospect interests you or you know someone who might be a good fit, please message me directly or leave a comment below.

Thank you!","['Why Bayesian?', '> find a statistician who shares a keen interest in this topic\n\nI\'ve never even *heard* of this topic before just now. I\'m a statistician, not a psychologist (*dammit, Jim*...). I don\'t know what ""folk psychology"" and ""typological thinking"" are.', 'Send me a pm.', ""Sounds interesting, my advice is to cast the net wide as you search for collaborators. Bear in mind that r/statistics is a very small, quiet backwater in the world of statistics; you would do well to look for a bigger audience -- I get the impression that stats.stackexchange.com is quite a lot more numerous. Maybe you are already searching more widely, if so, that's great, carry on.""]",1,4,https://www.reddit.com/r/statistics/comments/13qi2er/r_looking_for_an_expert_in_descriptive_bayesian/
36,2023-05-24 12:48:41,"[S] R-Studio - First time reading R output, need help to read data","https://imgur.com/a/HAK4v0V
^
Title, what does the different numbers mean?

I color-coded them, so its easier to explain. I have been to statistics lectures for 6 months, so i have some knowledge, but not when reading outputs in R.","['Orange colour: it\'s about the coefficient of the regression, with their corresponding p value (last column)\n\nRed colour: * is used to indicate the p value hit which level of significance threshold \n\nThe other boxes are some about statistics concepts, that is not unique to ""R output"" (for example degree of freedom and R squared). If you don\'t understand them, it\'s better to learn the concepts before reading R output.', 'Have you ever read a regression table from any other program? Or are you new to them altogether?', 'People may be able to help more if you post your code and dataset for people to see.\n\n1Q and 3Q refer to [quartiles](https://en.wikipedia.org/wiki/Quartile). The other things you squared are labelled. If you forgot the statistical concept, you can look them up. For example, the red square refers to how `lm()` labels statistical significance.', ""purple are the quartiles (look it up if you don't know what that is)  \n\norange are stuff about the coefficients of your regression. Estimate of their value, their standard deviation, and then t-value and p-values to assess whether they are significantly different from 0. At the end of their line you see a number of stars, here 3.  \n\nred: it's a legend telling you what the stars mean. For example 2 stars mean that the pvalue is < 0.001.\n\nFor the rest at the bottom, they tell you what this is."", 'Try https://bookdown.org/brianmachut/uofm_analytics_r_hw_sol_2/linreg.html']",0,18,https://www.reddit.com/r/statistics/comments/13qbeyg/s_rstudio_first_time_reading_r_output_need_help/
37,2023-05-24 11:17:17,[Software] Question about constructing the design matrix in R,"I am trying to construct the design matrix to fit a logistic regression model with lasso penalty-glmnet. I want to include the main effects & 2nd order interaction terms. I have few variables which are factors. When I create the design matrix it seems that the reference category for the factor variable is included as a column in the design matrix.

The following is the code on the mtcars dataset for illustration only

data(mtcars)

\#### select specific columns: mpg,cyl,am(binary response) ####

data\_fit\_model <- mtcars\[,c(1,2,9)\]

\##### convert number of cylinders to a factor ######

data\_fit\_model$cyl <- factor(data\_fit\_model$cyl,levels=c(""4"",""6"",""8""))

\#### specify the formula for main effects & 2nd order interaction without intercept #####

model\_formula <- as.formula(am\~.+.\^2-1)

\#### build the design matrix #####

design\_mat <- model.matrix(model\_formula,data=data\_fit\_model)

However if I specify the following

model\_formula <- as.formula(am\~.+.\^2)

for the model formula then the column for reference category is not included in the design matrix. Can anyone tell me how to write the model formula correctly so that there is no intercept term & the reference category for factor variables is not included as a column?","["">  Can anyone tell me how to write the model formula correctly so that there is no intercept term & the reference category for factor variables is not included as a column?\n\nYou can't have both. In a straight one-way model, *either* you omit the reference category and include the intercept or you include the reference category and omit the intercept. I strongly suggest the former, because the latter doesn't generalize to more variables."", 'What you said is correct. The intercept term would be the logodds for the reference category for number of cylinder & mpg 0 in the example above. While having both intercept & reference category would make the columns of the design matrix linearly dependent.']",2,2,https://www.reddit.com/r/statistics/comments/13q9izi/software_question_about_constructing_the_design/
38,2023-05-24 06:16:58,[Q] Am I wasting my time on indeed?," I just graduated with my bachelors in stats, and Iv been applying a lot on indeed this past week. Only issue is that these positions usually have 100-500 applicants. Am I wasting my time applying for these jobs? How did you get your first position outside of college? Where do data analysts usually start?

Thank you, sorry if this post is a little irrelevant to the subreddit. I wanted to hear from people in my field of interest.","['There was a comment on the data science subreddit a little while ago. And while I don’t have the link, here’s the general gist. Spoken from their perspective. \n\nWe put out an advert for a data analyst at our company and within 24 hours we had 500 applications so we took it down. \n\nHalf of those applicants didn’t have the right to work. Down to 250\n\nAnother 80% didn’t meet the necessary qualifications. \nDown to 50\n\nThen all but five had poorly written resumes, no projects, no initiative etc. \n\nOf the 5 interviewed. 3 were vastly overqualified and the other two were considered for the job. \n\nEnd comment. \n\nSo I guess what I’m trying to say is that you shouldn’t be concerned about the big numbers. If you have your right to work, meet the requirements and a polished resume. I say just apply, eventually you’ll get lucky and land something. \n\nAnd heck sometimes even if you don’t meet the requirements I’d throw in a application just for kicks.', ""I recommend to keep applying on both Indeed and LinkedIn. A lot of those resumes never reach human eyes, because they are filtered by keywords before they reach the hiring team. When applying, read the job description a couple times, pull some keywords from it and add them to your resume. \n\nPersistence and patience go a long way. Don't be afraid to pour a lot of time into your resume/cover letters. It's the first thing people see. You can search through this forum and find some resume tips. \n\nDon't let the mass of applicants discourage you. Highlight your skills. No need to be humble when applying! \n\nI ended up going into the field of GIS, so my experience is a little different. But once I polished my resume, I finally secured a great job."", 'Take LinkedIn’s application numbers with a grain of salt. LinkedIn doesn’t have access to the actual application process on a company’s website and tracks every click of the “apply now” button as an application. \n\nThere’s also a big difference between qualified applicants and unqualified applicants throwing a prayer at a company.', 'I got my first job through indeed, as a data analyst', 'I see. For one of the positions I found the same job on both indeed and LinkedIn. Should I apply to both or just LinkedIn? The LinkedIn position had like 43 applicants which is pretty good for me but I’m still not sure.']",10,14,https://www.reddit.com/r/statistics/comments/13q2on4/q_am_i_wasting_my_time_on_indeed/
39,2023-05-23 23:09:10,[Research] Adjusting Statistical Methodologies for Pandemic-Influenced Data,"Are there any good recent papers that examined how we as statisticians should adjust our methods for pandemic-influenced data in longitudinal studies? There are tons of public health before/during/after studies, but I am looking specifically for published papers aimed at statisticians.","['Yes, exactly. While we may already know how to adjust our models, I\'m looking for ""best practices"" type papers for this topic. I\'m compiling a list of refs for a paper I\'m writing with some colleagues.', ""> Ergo, the incidence of handwashing increased at these handwashing stations, but not due entirely to the treatment (increased access to handwashing facilities). \n\nThis is a 100% fine and I fail to see how its an issue? If it affected both the treatment and control groups equally\n\n> Rather, the incidence of handwashing is now confounded with people's heightened adherence to avoiding the virus, which included behaviors like washing hands more often in public. \n\nHow is this confounding?\n\n> It's not straightforward how to tease apart the treatment effect versus the pandemic effect in studies like this. Hence, the need exists for some guidelines, especially for junior statisticians.\n\nWhy?"", 'How would we adjust our models? Not sure I understand.', 'Can you give an example?', ""Study with a high probability of impact: Intervention to Improve Hand Hygiene\n\n- We collected baseline data before introducing the intervention\n- We introduced the intervention (i.e., enhanced access to hand-washing facilities) to the population\n- COVID-19 raised the perception of risk\n- Result: The pandemic, rather than the treatment intervention, largely influenced adherence to hand-washing. Therefore, we should control for the impact of the pandemic in the analysis, even if the study wasn't designed as a pre-/post-pandemic investigation.\n\nThere were a multitude of public health studies planned well before the pandemic. The researchers had firmly established the hypotheses, treatment, and data collection protocols. Then, unexpectedly, a global public health crisis arrives. These ongoing longitudinal studies didn't consider the pandemic in their design. How do we approach these studies *statistically* that the pandemic now confounds - with risk perception behaviors as the root cause of these confounds?""]",3,8,https://www.reddit.com/r/statistics/comments/13pr8pk/research_adjusting_statistical_methodologies_for/
40,2023-05-23 22:42:22,[Q] How would you go about creating a multiple regression to predict outcomes of 2 player games such as badminton or tennis?,"What type of dependent variable would be suitable? And would you consider the two opponents of a match as two separate points, or would they be the same? If you use two opponents as different points, would that affect the model in anyway since they technically are coming from the same match?","['https://en.m.wikipedia.org/wiki/Bradley–Terry_model\n\nhttps://cran.r-project.org/web/packages/footBayes/vignettes/footBayes_a_rapid_guide.html', ""Make a research question first. We don't even know what you are interested in knowing. How can you build a model without knowing what the point of it is?"", 'Have thought of this before. \nDepends completely on the game. \nTake prev matches as data and maybe rank players depending on their overall winnings? \n\n\nSo if player one is 6 and player 2 is 4 then player one will win is the prediction. \n\n\nOr else we can run a logistic regression with some variables ig.\nMaybe like some players do better on clay courts so variables like that. \nNow I kinda want to work on this', 'Let’s say I want to use a multiple regression model to examine how different variables such as ranking, home court, and height will affect the outcome of a game. Since both opponents have different have different input variables, but are playing in the same match, should you consider both players as different data sets? The Match outcome is dependent on both players, so how would you go about adjusting for that?']",2,4,https://www.reddit.com/r/statistics/comments/13pqj3v/q_how_would_you_go_about_creating_a_multiple/
41,2023-05-23 22:41:45,[Q] can somebody explain Correspondence analysis for dummies?,"I am writing a paper for my archaeology studies on the usage of CA in archaeology. My goal is to explain it more detailed than archaeologists did before (put the contingency table into software and push button) but simpler than dedicated literature. I understand the contingency table and calculating the probability of independence, but I am stuck in understanding how to put the chi-squared equation into it to get an inertia and translate it to vectors.

Tl;dr: can someone explain CA to a stupid archaeologist like he's 5?","['Imagine you have a big toy box filled with different types of toys. You want to see if there\'s any connection between the toys that kids like to play with. To do this, you start by organizing the toys into rows and columns in a table, like a game board.\n\nNow, you ask a bunch of kids to come and play with the toys. As they play, you observe which toys they choose and mark it down on the game board. Each cell in the table represents a combination of two toys, and you count how many times those toys were chosen together.\n\nOnce you have this table, you want to find patterns and understand which toys are more closely related to each other. Here\'s where Correspondence Analysis comes in.\n\nFirst, you calculate the expected probability of two toys being chosen together if they were picked independently (meaning one toy\'s choice doesn\'t influence the other). This is like assuming the kids are randomly picking toys.\n\nNext, you compare the actual counts of toy combinations to these expected probabilities. You subtract the expected probabilities from the actual counts and square the result. Then you add up all these squared differences.\n\nNow, you have a number that represents how much the actual toy choices deviate from what you would expect if the toys were picked randomly. We call this number inertia. The bigger the inertia, the more related the toys are.\n\nTo understand this better, think of inertia as a measure of how surprised you are by the toy choices. If the kids consistently pick certain toys together more often than expected, you\'d be more surprised, and the inertia would be higher.\n\nTo visualize this, you can imagine the table of toys as a big map. The inertia tells you which parts of the map are ""hotspots"" where certain toys are popular together. You can draw arrows from the middle of the map to these hotspots to represent the relationships between toys. The length and direction of the arrows show the strength and direction of the relationship.\n\nSo, Correspondence Analysis helps you discover hidden connections between toys and see which toys tend to go together. It\'s like making a map of toy preferences based on how kids play and showing the important relationships between toys using arrows.', ""Thank you so much, it's really a great example. I still feel like running into a wall. I can follow the logic to creating a contingency table of the observed numbers and creating one of expected numbers, then I found out how to calculate chi-squared values but after that it seems like magic and you get coordinates with certain distances to the mean profile in a point cloud and somehow you get Eigenvalue and row and column scores.\nI guess I'm way in over my head with this...""]",6,2,https://www.reddit.com/r/statistics/comments/13pqijt/q_can_somebody_explain_correspondence_analysis/
42,2023-05-23 17:18:48,[E] Statistics Plan for Public Policy — Need Advice for Streamlining,"Cherissime amis. I am trying to see if I am going about my career/education plan the wrong way.

**Current status**: CSULB Statistics student since fall 23.  
**Current plan**: Complete Statistics degree. Double-major in Economics. During or after, intern/part-time[1] with public sector for work experience (possibly for 2 years if class schedule works out). Path splits here: enter public policy/administration program, then work OR work, then come back for public policy/administration program. Same end result since the MPA program at CSULB prefers for people to have work experience anyway. MPP at UCI is also an option.  
**Current education**: Will learn R, SAS, machine learning, survey sampling, and *probably* SQL at my university for stats. Python is introduced but not done in detail (PANDAS not covered). Will cover basic forecasting, urban/public economics, and first-semester graduate econometrics for economics. No plan for real/numerical analysis right now. Have taken a class on public policy. Planning for undergraduate thesis to involve public policy as part of my CSULB program.  
**Potential education options**: I'm open to taking community college classes for cheap, but I'm uncertain if these are respected enough/would add any value. That said, there's a few basic accounting classes and [data science classes](https://catalog.cccd.edu/coastline/pathways/technology/computer-information-systems/data-science-certificate-achievement/#requirementstext) that I could take, but the latter seems to overlap decently with my bachelor's. One of the directors for the program really [upsells it in a video the CC made](https://www.youtube.com/watch?v=l6RKrexzjh4), so I'm feeling a little unsure.  
**The pipe dream**: Take MPA courses in undergrad. After undergrad, go straight into a PhD program in public policy.  
**Absolute ideal sector/career**: Use statistics and economics knowledge to investigate economic effectiveness of educational policies (i.e. educational economics). Preferably operating in public sector. Open to being a professor/lecturer part-time/full-time.  
**Minimum**: Any public sector job involving statistics/data.  
**[1] List of internships/opportunities I found/know of:** Some volunteer-internship opportunities with local government agencies. PPIC summer internships. Federal government student trainee options. JusticeCorps. Maybe something involving education.  

In short, my concerns are as follows:  
1. Will it be difficult for me to find employment within the public sector? I'm not *really* opposed to private as a stopgap/networking between my education to the public sector, but if my job just involves making a funny finance value go up, I don't think I'll be satisfied. Consequently, I'd like to do something decent/more memorable. ~~Also job security.~~  
2. Is my desire to pivot from statistics to public policy viable, or would I need more work experience/classes beyond what I've listed (undergraduate thesis)?  
3. Is there anything else I'm missing in my plans that would make me a better fit for entering into/maintaining an effective career in the public sector/as a statistician?  
4. Are there any other career endpoints/educational options that would make more sense than an MPA/MPP/PhD?  
5. God dang it do I need to take real analysis  
6. Do I *really* need a PhD?  
7. Considering my desired field involves researching education but my undergrad doesn't directly have it, is it worth paying for extra classes in it, or is it probably just a work experience game?

Many thanks for your responses. I wasn't sure if this was better here or in /r/askstatistics.","['What is your goal, i.e. what do you want to do? If you want to be a serious researcher, stats with some good math knowlegde, ie calculus and linear algebra, is a huge advantage in the social sciences and public policy. 5.  Speaking as someone who had only one calculus class in undergrad and is now in a poli sci phd program, I would certainly take more math if I had to go back and do my undergrad over again. Time spent relearning math during a phd is time that could be spent on papers and acquiring substantive knowledge. It will also help you greatly in the phd application process as better programs will be more confident in your ability to understand what is going on under the hood. Will real analysis help you? Maybe, maybe not. If you want to get a PhD public policy, it probably cant hurt you, although I would differ to someone else on this. 4./6. If you really want to do research, i.e. designing experiments, conducting studies, and understanding the interaction between education and economics, a masters is the minimum in most cases, and a PhD is probably required for most research oriented positions. 7. I would concentrate on getting work experience, which will definitely help you with MPP/MPA applications. You can always upskill when you get to the program and it seems like you have a solid base, unless getting an entry level position requires skills you dont have or cant learn on your own. 2. A switch to public/education policy is completely viable with your background, and your background will actually give you an advantage over someone who has never taken stats, econ, or calculus before.', 'It seems like you are worried that your lack of substantive knowledge of education policy/economics might hold your application back. I think this depends in part on what type of PhD you want to get and what area in. A PhD in Public Policy or Economics with a focus on education policy for your dissertation topic may only allow you to take 1 or 2 courses on education policy, while the rest will be broadly focused on the policy process and policy analysis(public policy phd) or on economic theory (econ phd), with both requiring alot of technical training in econometrics.  Given that it seems like you want to focus on the intersection of economics and education an econ or public policy phd makes the most sense. This is different from a PhD in education or education policy from an education school that is going to be more focused on traditional education topics, like does x program to raise math scores for underperforming kids actually work? If you wanted to do that type of work then I would say that your lack of Ed knowledge might be more of an issue. Would substantive knowledge of Ed policy help you? Absolutely, but you can build that up through masters courses or reading on your own. For example, my own research tends to focus on the politics of policing. I have never taken a class related to policing in either my PhD or Masters and have no formal training in the area other than what I have read on my own or learned from professors (political scientists dont study policing really). Acquiring substantive knowledge is relatively easy, just read more or do a directed reading. It is much harder to gain technical or math skills, which is what most phd programs want to see. I would take some classes now or in your masters on Ed Policy or start to read joirnal articles on topics in that area that interest you. Have an idea of what you want to study for your phd, but realize that it might change.', ""Thank you for the reply.\n\n> What is your goal, i.e. what do you want to do?  \n\nI have an *area* I want (educational economics), but past that, I'm not deadset on a specific career. Insofar, policy analyst or researcher in that area would work just fine. I'm not too picky purely because I can't guarantee anything.  \n\n\n>  Speaking as someone who had only one calculus class in undergrad and is now in a poli sci phd program, I would certainly take more math if I had to go back and do my undergrad over again. Time spent relearning math during a phd is time that could be spent on papers and acquiring substantive knowledge.  \n\nInteresting. I was worried that my lack of formal political science knowledge would be an issue and I wouldn't even get my application looked at, but it sounds like your specific program emphasizes mathematical/quantitative techniques?\n\n\n> It will also help you greatly in the phd application process as better programs will be more confident in your ability to understand what is going on under the hood.  \n\nIs my lack of *formal* political science background not as much of an issue, or is it just a matter of making my niche clear/getting work experience/finding the right professors who are similar to what I want to do?  \n\n\n> A switch to public/education policy is completely viable with your background, and your background will actually give you an advantage over someone who has never taken stats, econ, or calculus before.  \n\nMy concern was that similar individuals might have experience/more knowledge in political science/education that'll keep me locked out of the field, but at least hearing it's *possible* is the main assurance I needed."", ""Thanks again. I don't really have any other professor/mentor who's in this area (undergrad in stats doesn't exactly attract an educational policy analyst...), so...\n\n> It seems like you are worried that your lack of substantive knowledge of education policy/economics might hold your application back.   \n\nCorrect. I'm not afraid of learning new material in my PhD/master's, I'm just worried they wouldn't even take it seriously if it's not close enough to the field. Herein, it's a switch from STEM to Social Sciences, but I conjectured that this decision would look intentional enough.\n\n> while the rest will be broadly focused on the policy process and policy analysis(public policy phd) or on economic theory (econ phd), with both requiring alot of technical training in econometrics  \n\nFrom what I heard from other students at my uni trying for PhDs/master's in econ, real analysis will ~apparently~ be necessary/very good for that level of econ, which was why I stressed it so much in my posts here. I'm not very into mathematical proofs, but in the name of my goals, it's an option.\n\n> This is different from a PhD in education or education policy from an education school that is going to be more focused on traditional education topics, like does x program to raise math scores for underperforming kids actually work?  \n\nWhile I would *like* a general understanding on these things so I can sort of talk about the topic with people, I'm not interested in outright designing/criticizing those traditional education topics. I'm more interested in policy side, as of now.  \n\n> Would substantive knowledge of Ed policy help you? Absolutely, but you can build that up through masters courses or reading on your own... Acquiring substantive knowledge is relatively easy, just read more or do a directed reading. \n\nFor my public policy class, I did do a short analysis on ESEA 1965-1975, so I at least have the toolset to approach this subject according to *some* of the standards of my uni's MPA. Sometimes it gets bad enough I'll talk about ESEA 1965-1975 for 5-30 minutes...\n\n> It is much harder to gain technical or math skills, which is what most phd programs want to see.  \n\nInteresting. One of my concerns was that my application wouldn't appear to be literate in political science/theory/methods, but that sounds rectifiable with internships/my undergrad thesis, anyway. Hoping that tripling down on technical skills from statistics + field knowledge of economics + work experience from public policy will be enough.\n\n> I would take some classes now or in your masters on Ed Policy or start to read joirnal articles on topics in that area that interest you. \n\nI was actually thinking of taking educational research/statistics offered at my uni. That said, most of the hot stuff is in grad-level (e.g. a course in educational program evaluation, educational administration), but I'll probably fish around and see what I can squeeze in not in a hurry. As for reading, I really ought to make time for that...\n\n> Have an idea of what you want to study for your phd, but realize that it might change.  \n\nI feel pretty set on either k-12 education or community college. Realizing it might change might be the bigger difficulty.""]",1,4,https://www.reddit.com/r/statistics/comments/13pj9fx/e_statistics_plan_for_public_policy_need_advice/
43,2023-05-23 09:40:50,[E] Recommendations for introductory statistics course or self-study textbook,,['Stat2: Modeling with regression and ANOVA.\n\nIt has the advantage of being introductory (i.e. not calc based like some of the other mentions) as well as practical. You can also find R code for all examples as well as exercises online.'],2,1,/r/AskStatistics/comments/13p4abr/recommendations_for_introductory_statistics/
44,2023-05-23 09:37:22,[Career] I got a 100K job. Does a masters in stats make sense?,"Currently I work in the financial industry with 3 years of experience under my belt. The job it’s boring, not mentally challenging and the skills earned in the job are not really that transferable. I have a degree in economics with an emphasis in econometrics (I took some math and stats courses to prepare for a Masters in economics). I was wondering if a masters in statistics it’s the right move in my career given that I’m already making good money. I enjoy the mathematical rigor required in stats and coding machine learning models.

I know the decision is personal, but I wanted to know what factors would you guys look at, or if it’s just a dumb decision to leave a good paying job.

PD: I don’t mean to sound entitled or ungrateful.","[""I'd look into part time MS programs. You can work and get your MS at the same time, which is what I did. Downside is that it could take up to 5 years depending on your pace, so it's not for everyone. \n\nI don't think the MS actually taught me skills that would get me hired, but it definitely helped introduce me to a lot of theory that makes learning about machine learning concepts a bit easier to grasp. Also I think it helps you get more interviews compared to not having a master's"", ""Some perspective from someone with a similar career path. An ms in stats won't really teach you more than what you can find online. It's not enough to push you into a quant role and if you are already making 100k+ it's more likely to open up a lateral role. I'm at six figures after starting back office in finance, moving to fo and then switching industries to healthcare. Take a course and learn python and maybe SQL/other language. Automate your work and think about what you would rather be doing with all of your free time. I'm working on my free school money to get a master's in HR just to give me some flexibility."", 'I second this!\n\nI already have an MSc in Stats (UK) and work as a senior marketing analyst. I am doing a second MSc in Marketing Management but part time while I work.\n\nI have to say that it’s a real grind but doing a masters that’s applied to your daily work is a real bonus! You learn so much more this way because your ideas from learning bleed instantly into your work. This counteracts one of the problems with doing a MSc program and THEN working, it’s the lack of real world, industry specific, applied problem solving knowledge. I realise this is not quite the same with op since they already work.', ""1. As others have said, don't leave your day job.  If you do you will have to get hired back in.\n2. You may find that your current employer doesn't notice your new degree; you were Fred before, and are still Fred.  You might have to change employers to benefit.\n3. The most important - **talk with people**, both within and outside your current employer.  See who values what, where and when."", ""I didn't regret pursuing stats as a MS even though I never used it - also working in finance and it didn't really make sense to change careers. I don't know how much programs charge these days but my 2 year program didn't break the bank, I spread it out over 4 years, and I loved the material. Maybe what I'm saying is what do you have to lose?""]",40,34,https://www.reddit.com/r/statistics/comments/13pa6km/career_i_got_a_100k_job_does_a_masters_in_stats/
45,2023-05-23 09:35:11,[Q] What type of sampling should I say I’m using for my study?,"I’m conducting statistical social science research (it’s my first time doing independent research) and I’m a bit confused on whether I’m using self selection sampling, convenience sampling , purposive sampling, or something else? 

My data was collected via an anonymous survey posted on various people’s social media platforms, with most responses coming from an influencer based in the country whose population I am interested in (Pakistan). 

My survey is in English (not the country’s primarily spoken language) and my research is about gender-based discrimination in the financially advantaged segment of Pakistan’s female population (a very different group from the millions of people who live in rural areas and villages). 

About ~ 30 responses came from me sending out the survey to people I know. Additionally, only people who have lived 5 or more years in Pakistan were allowed to participate. 

I’d appreciate any advice on how to phrase the sampling method I used, ideally in as few words as possible because i have minimal space on my poster","['This sounds like a volunteer response sample to me.', 'I believe this is a rather clear case of convenience sampling. :)', 'I got you\nrechardpeter09@gmail.com for any assistance you may need in data analysis']",0,3,https://www.reddit.com/r/statistics/comments/13pa4sg/q_what_type_of_sampling_should_i_say_im_using_for/
46,2023-05-23 09:33:25,[Q] How do I determine statistical significance when working with twitter data?,"If i'm running text analytics (specifically sentiment analysis in R) on a hashtag from twitter for a computational linguistics class. How many tweets do I need to download for statistical significance? One book said 1800, one said 3000, and one said 15000. What do y'all think? I'm also going to need to defend why I chose that amount so is there some mathematical way of determining what's significant? What do y'all think?","['You should perform a power analysis.', 'I think you need to have a better understanding of statistics before articulating a question like this, there are plenty of resources. OpenIntro is a good place to start. Properly understand what statistical significance is and isn’t is imperative.', 'Also we should never ever try and achieve statistical significance, it’s not a goal.', 'It’s not clear what test you are doing in order to achieve statistical significance. You would need to explain more about your test and experimental design.', ""It depends on the question you want to address and the model you want to use to answer it. It's usually always better to have more data though, so keep that in mind""]",1,11,https://www.reddit.com/r/statistics/comments/13pa3ff/q_how_do_i_determine_statistical_significance/
47,2023-05-23 05:23:54,[Q] How should I perform clustering on angular data?,"I'm currently performing an analysis on users' event timestamps.  Each user has at least one timestamp of interest.  I am specifically interested in answering the following question (use case paraphrased):  **What groupings are there in terms of hour and day-of-the-week in which users prefer to visit a website?**.  For example, one potential finding could be ""there's a group of users who prefers to visit around 5-6PM on weekdays, another group of users who visits in daytime hours throughout the weekend, and a third group who prefers to visit between 8-10AM on weekdays."" However, I can't just treat hours and days of the weeks as linear features because they're cyclical, as Hour 0 is closer to Hour 23 than it is to Hour 4 and Sunday (0) is closer to Saturday (7) than it is to Tuesday (2).

After a lot of research I discovered [directional statistics](https://en.wikipedia.org/wiki/Directional_statistics).  It seems like the most sensible way to represent this data for clustering is to transform hour to points on the unit circle via e.g. 22.3 -> (sin(22.3/24 * 2pi), cos(22.3/24 * 2pi)) and similar for day of week, but with a denominator of 7 instead of 24 (see [StackOverflow](https://datascience.stackexchange.com/questions/5990/what-is-a-good-way-to-transform-cyclic-ordinal-attributes/6335#6335), which gives a transformation that treats the vertical line at y=0 as the reference direction).  This ensures that Hour 0 is closer to Hour 23 than it is to Hour 2 when taking Euclidean distances.  As a result, each timestamp is transformed to a coordinate pair on two different unit circles - one unit circle for hours and another for days-of-week.

I also started skimming through Murda and Jupp (2000) to better understand my options.  It seems like I could also just treat the hours and days-of-week as angles from a reference point (Hour 0 for hours; Sunday=0 for day of week) and somehow work with those.  However, it's not obvious how to do the clustering if I work with the angles directly.  Additionally, there are complications because we have _two_ circular variables that may or may not be independent, and I'm not sure whether it's more sensible to treat the problem as clustering torus data or spherical data.  (Note that I did consider taking one transformation with a separate pair for each hour/dayOfWeek combination, but realized that the distances wouldn't have the properties I wanted.)

Keeping the context of the problem in mind:

* What is the most sensible approach to cluster hours and days-of-the-week to identify groupings of activity?  Euclidean distance on two sets of unit circle coordinates?  Some other approach on a torus or unit sphere?

* How should I deal with the fact that each user has multiple timestamps?  When I initially treated these features as linear, I transformed my data such that one row == one user and made compositional features of the form ""percent of visits in Hour 0"", ""percent of visits in Hour 1"", etc. and similar for day of week such that sum(hour features) == 1 and sum(day_of_week features) == 1.  However, it's not obvious how to do something similar with continuous angular data.  I thought about using a Gaussian mixture model on the unit circle coordinates with partial pooling on userId, but I don't know how to do that in an unsupervised way in R.  (I tried the flexmix package for that.)

* This isn't as important as the first two questions, but it's still somewhat important.  I'm interested in clustering _local_ hour, rather than UTC hour as the data is currently represented.  However, no one logged the time zones!  I know that time zone is determined at the user-level, rather than at the timestamp level, and that all users are within the US.  Is there an approach to clustering that will treat hours and days-of-week in an isometric way?  That is, treat bumps at Hours 20 and 21 for one user the same as for a different user with the same-size bumps at Hours 5 and 6.

Thank you!","['There is also k-means clustering in the presence of periodic boundary conditions. [This package](https://github.com/kpodlaski/periodic-kmeans) and the accompanying paper give an implementation with several examples including the NYC taxi dataset.', ""When it comes to clustering, think about distances.\n\n> there's a group of users who prefers to visit around 5-6PM on weekdays, another group of users who visits in daytime hours throughout the weekend, and a third group who prefers to visit between 8-10AM on weekdays\n\nThis sounds like you consider the distance between, say, 5pm Monday and 5:03 Tuesday to be further appart than the distance between 4pm Friday and 7pm Friday? If so it looks like you're not after 2D coordinates over a torus nor sphere, but 1D coordinates on a circle in terms of *hour-of-the-week*. Otherwise you would need some way to choose the radii of the torus, that is, how far appart is Monday and Tuesday compared to 5pm and 6pm.\n\nTo compute the distance between two hour-of-the-week locations, you could transform with sin and cos and use euclidean distance, but the distance would be the length of the chord of the circle. Another measure you could choose is the distance around the circumference, a.k.a the arc length. The later seems more natural, but clustering is more of an art than a science, and sometimes the less natural approach can work better."", ""Arc length is better, simpler and also linear. Just divide the unit circle (2π) to 24 hrs and u get π/12 per hour. Meaning u can transform clock hours (T) to radians using π(T)/12. Calculating the arc legth is just the absolute difference between those angles. Now, obviously if u do this to hour 23 and hour 1 you'll get a longer distance but u can set a conditional such that if the arc legth (s) is greater than π then the new arc length (d) = 2π - s.\n\nEdit: Taking euclidean distance is not linear. Take a regular clock for example, hr 12 to hr 6 should be twice of what hr 12 to hr 3 is. But in Euclidean distance hr 12 to hr 6 is 2 and hr 12 to hr 3 is sqrt(2)  which made farther points closer than they should have been."", 'Mapping to the unit circle is just doing this but also taking the cosine. If you only take sine you have differential scaling between various points, rather than properly preserving equivalent intervals on the circle.', ""K-means is one of the most widely-used algorithms to cluster data. However, it has several limitations: a) it requires the use of L2 distance for efficient clustering, which also b) restricts the data you're clustering to be vectors, and c) doesn't require the means to be datapoints in the dataset.\n\nUnlike in k-means, the k-medoids problem requires cluster centers to be actual datapoints, which permits greater interpretability of your cluster centers. k-medoids also works better with arbitrary distance metrics, so your clustering can be more robust to outliers if you're using metrics like L1. Despite these advantages, most people don't use k-medoids because prior algorithms were too slow.\n\nYou can check our BanditPAM v4.0 (includes R, Python and C++ versions). [paper](https://arxiv.org/abs/2006.06856)\n\nIt's written in C++ for speed, but callable from Python and R. It also supports parallelization and intelligent caching at no extra complexity to end users. Its interface also matches the sklearn.cluster.KMeans interface, so minimal changes are necessary to existing code. [repo](https://github.com/motiwari/BanditPAM)""]",17,9,https://www.reddit.com/r/statistics/comments/13p3uis/q_how_should_i_perform_clustering_on_angular_data/
48,2023-05-23 01:58:53,[Q] One way anova,"I did a one way anova in excel where i compared 8 groups. The p-value was non significant, but when I made the figure and inserted the standard deviations on the different bars some of the deviations on the bars did not overlap. Are they still significantly different from each other even though my p-value was above 0.05? Or does the deviations not matter when the p-value was non significant?","['""made the figure"" is highly ambiguous. You\'re going to have to explain the circumstances in more detail, and what you plotted.\n\nIf your overall anova p-value was not significant you would not normally go on to do pairwise comparisons. If you were going to do pairwise comparisons regardless, what was the point of the ANOVA?', 'If you had planned a priori to compare each mean with each other mean you could have used the Tukey hsd and not done an ANOVA. This is valid since Tukey controls the Type I error rate on its own. You would  slightly increases the Type I error rate if the Tukey test we’re done following a non-significant ANOVA  but for your own education you could see how it comes out.', ""If you're checking if the means of the 8 groups are the same, you can do K-way Anova.\n\nThen if not the same, you proceed to do multiple comparison which has methods including TukeyHSD, Bonferroni method, LSD. They basically carry out pairwise comparison of means of every 2 groups but with some correction."", ""I dont know anything about statistics, which is why I'm here. I did the analysis in excel, and used the means of the groups to make a simple bar chart. Then I calculated the standard deviation errors which gave me the error bars that was placed on the bar chart to look for overlap. As far as I know some groups in an anova can be different from one another and not, even though the p-value is significant. I did an anova instead of doing multiple pairwise comparisons. What I'm asking is; can the few groups that dont overlap still be significantly different from one another even though the p-value is not for the whole analysis.\n\nIm trying to see if the amount of prey delivered to a nest varies during the time of day. The day is segmented into 8 different groups and each group contain data from multiple nests""]",0,4,https://www.reddit.com/r/statistics/comments/13oy36t/q_one_way_anova/
49,2023-05-22 22:02:13,[R] Another Bonferroni question! When to reset the FWER? Theoretical?,"I am reviewing my write-up Results section and thinking about how I can improve on the consideration I gave to random error in running so many tests. So I'm calculating Family-Wise Error Rate (FWER) so as to look back on results and demarcate what might be more likely to be a random result.What I'm trying to figure out is what constitutes a fresh analysis and fresh calculation of FWER? I'm presuming it is partly theoretical - for example when a hypothesis is looking for correlations, ANOVA, and a few linear regressions in order to answer the question, these might be considered the 'family of tests' relating to that hypothesis.But I'm guessing one might also view it as random Type I error risk relating to the tests run on the whole sample of participants, rather than per hypothesis.So, for example, let's say we had a sample of 100 people who sprinted from point A to B to C, producing time data.

1. Compare Point A results to B and C, and B to C. Also run correlations between A B and C.
2. Split the sample into Fastest Sprinters from A to B, and Slowest from A to B. Compare Fastest versus Slowest groups on A to B and C, and B to C.
3. Split the Fastest and Slowest Sprinters groups into people who drank Red Bull beforehand and didn't. Compare Fastest versus Slowest drinkers of Red Bull versus none on times between point A to B and C, and B to C. After looking at correlations, use Linear Regression to predict the time from B to C for Slowest Sprinters who drink Red Bull based on the predictor variables of red bull cans per week and time from A to B..
4. The same participants take the test 6 months later, compare the results again.

I am thinking FWER can be calculated for all tests, something like 6+6+23=approx 35 tests to be run, is a FWER of 1 - (1 - .05) to the power of 35 = 83% chance of Type I error and a proposed alpha level of .0014 to avoid Type I error.But could we also look at it as different hypotheses? Needing separate FWER calculations? For example the whole sample calculations, could be considered differently to part of the sample's calculations when putting them into a subgroup and looking at a different dependent variable.And what of the fourth circumstance - should we reset FWER completely given the test statistics were gathered 6 months after the first set of statistics?","['All questions you ask of a sample must be part of FWER correction. Only exception is if you do the same analysis twice, e.g. you calculate a correlation between y and x and then you do a regression.. dont do the same thing twice. A new sample does not need to be corrected for hypotheses tested on another sample. The scientific method is hypothesis first, write them on a piece of stone and then test just those while correcting for FWER. For next experiment you can write other or new hypotheses. This procedure can be followed with data splitting. Say you randomly split data into 80% and 20%. You use 80% to discover say 10 genes out of 5000 that seem interesting and make hypotheses for just those 10 genes and then you test just those in the 20% while correcting only for 10 tests because this is a new sample. But the 20% is use once. If you go back and forth between whats discovery and confirmation, youll quickly get in trouble and cease doing science aka data leakage.', ""Just want to check I understand - so regardless of all the different ways I've split up the sample or whatever tests I'm doing, ALL must be accounted for in the one family of tests. This changes with a new sample - even if they're the same participants doing the same thing a week after the stats were done on the first sample. It's a 'luck' on the day kinda thing. Save the emerging hypotheses for another day's testing.\n\nWith correlation, if I run a follow up regression say to check for the amount of variance in the DV explained, isn't that only 1/2 of the tests a regression runs? It reports ANOVA, Pearson's - maybe that's what you meant, that regression counts as 1 test, although if there are a few predictors, would they each also be counted as a separate test? I'm guessing they would.\n\nIn my thesis, I'll be using ancillary hypotheses as a form of exploratory testing, I read somewhere that mightn't need the rigid Bonferroni correction as it is being viewed exploratorily, also would make sense I guess.""]",1,2,https://www.reddit.com/r/statistics/comments/13orsoc/r_another_bonferroni_question_when_to_reset_the/
50,2023-05-22 21:57:46,[Q] Zipf: An Appropriate Statistical Distribution of Computational Power in the World?,"https://top500.org gives the top 500 supercomputers and their respective FLOPS (computational power).  
Do you think all computational power per device would be a Zipf Distribution or some other distribution? I want to use the data from Top 500 to estimate the total computational power in the entire world.","[""Standard test is to bin a bit then do log-log plot of x vs frequency of x. If the log-log plot is linear and negative, you've got a power law. (Or at least something like it, some people are very persnickety about saying nothing is ever _really_ a power law).""]",1,1,https://www.reddit.com/r/statistics/comments/13oro8t/q_zipf_an_appropriate_statistical_distribution_of/
51,2023-05-22 20:12:02,[Q] Reasonable FIML estimates?,"Is it reasonable for a regression coefficient to have a +21% increase following full information maximum likelihood to account for missing data? For example, from 1.9 with missing data to 2.3 after FIML?","[""A 21% increase in a regression coefficient following the use of Full Information Maximum Likelihood (FIML) to account for missing data can be considered reasonable, especially if missing data is related to the variables in the model. FIML is a method used to estimate missing data by considering all available information in the dataset. By incorporating this additional information, FIML can improve the precision and accuracy of parameter estimates.\r  \n\rThe increase in the regression coefficient suggests that the imputation of missing data through FIML has provided a more reliable estimate of the relationship between the variables. It indicates that the missing data, when properly accounted for, has influenced the parameter estimate and increased its magnitude.\r  \n\rHowever, it is important to interpret these results cautiously and consider other factors. Firstly, it's crucial to assess the statistical significance of the coefficient estimates and determine if the increase is statistically significant. Secondly, the specific context and nature of your data and research question should be taken into account when evaluating the meaningfulness of the increase. Finally, consider whether the change in coefficient aligns with your expectations and theoretical understanding of the relationship between the variables.\r  \n\rIn summary, while a 21% increase in a regression coefficient following FIML for missing data can be reasonable, it is important to consider statistical significance, the context of the analysis, and theoretical expectations to ensure appropriate interpretation.""]",1,1,https://www.reddit.com/r/statistics/comments/13op57w/q_reasonable_fiml_estimates/
52,2023-05-22 17:55:07,[D] Risk of studying from old textbooks,I got a bunch of statistics textbooks for free from work and they're published around 1990. The topics look really interesting and I've started skimming through some of them. What do you guys think the dangers are of studying from books that old.,"['If the textbooks are about math or statistics, then there’s no problem learning from old books. \n\nIn statistics, certain more recent trends like the Bayesian approach to statistics or bootstrap techniques might not be mentioned or emphasized.\n\nWhat matters most is just who the authors are and how well they presented the material.', 'I personally find old book are very good to read. They are harder to read too, but its good for you to train your ability to read mathematics, once you understand those the newer one are usually not hard to read.', ""I have much older books that I swear by. (An old copy of Feller for example. I wish I had a good copy of Kendall and Stuart Vol I -- 3rd edition would suit me fine)\n\nit depends on the topic; some topics have barely changed in 50 years, others have change a lot in the last 20.\n\ne.g. If you're learning mathematical statistics you're probably fine.\n\nIf you were trying to learn say MCMC or be current on Statistical Learning, you might want a more recent book since a fair bit has happened in 30 years."", ""I don't have them with me right now, I'll post a picture of the collection next time I go home - which is in 5 days"", 'Out of curiosity, which books did you receive?']",4,6,https://www.reddit.com/r/statistics/comments/13ome42/d_risk_of_studying_from_old_textbooks/
53,2023-05-22 15:24:11,[Q] Where to go next after finishing Statistical Rethinking by Richard McElreath?,"I was part of a reading group and finished Statistical Rethinking by Richard McElreath. 
What book/course would be a logical next step? (Preferably on the applied side, but not necessary)","['Perhaps Bayesian Data Analysis by Gelman:\n\nhttps://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954', 'To piggyback off this (great) suggestion: the whole book is [available for free in PDF form](http://www.stat.columbia.edu/~gelman/book/)', 'And the author’s website for the book: \nhttps://avehtari.github.io/BDA_course_Aalto/\n\nEdit: Also the other author’s website: \nhttps://www.stat.columbia.edu/~gelman/book/\n\nEdit2: The lecture videos: \nhttps://m.youtube.com/playlist?list=PLBqnAso5Dy7O0IVoVn2b-WtetXQk5CDk6', 'Doing Bayesian Data Analysis by John Kruschke. I read this one first and just started on Statistical Rethinking. I found Kruschke’s chapter regarding experimental goals, sample size, and power helpful for experimental design when you expect to perform Bayesian analysis post-experiment.', 'You can always dive in and then backtrack to fill any gaps in knowledge as you go.\n\nAnother great book that covers probability fundamentals is Introduction to Probability by Blitzstein and Hwang: https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573\n\nHighly recommended!']",43,14,https://www.reddit.com/r/statistics/comments/13ojo95/q_where_to_go_next_after_finishing_statistical/
54,2023-05-22 14:04:45,Regression Interpretation [Q],"In a little side project I’m doing at work to look at some of the relationships impacting sick leave on the railway I have the following regression equation

Sick leave = -0.12 - Driver(0.039) + Age(0.005) + Stops (0.001) + Km’s(0.002) 

Where,

Sick leave running count for corresponding diagrams.

Driver is a dummy indicating whether the employee is a Guard or Driver

Age is the employees age

Stops represents the number of stations they will stop at in the diagram

Km’s represents the kilometres on there schedule.

With this equation, I’m just a little confused on the interpretation.

If I wanted to evaluate the impact of 20 additional stops on increasing the chance of sick leave for a Driver of 50 years of age would I plug this in like this.

Sl = 
-0.12-1(0.039)+50(0.005)+20(0.001)
+0(0.002) = 0.125

Then would I just take the percentage change of this using the base case when stops equals zero, but age still equals 50.

Thanks for any input 🙏🏼","['The impact of 20 additional stops on sick leave, in this model , is the same if the driver is 6 months old or 100 years.', 'This takes me back to college days. I’m a bit rusty but I’m also curious to read what people say. I’d also take the difference between stops equals zero and stops equals 20.', 'Difference should be 0.001 or .1%', ""From your equation, the incremental impact is 20*0.001. Basically if you hold everything else except the number of stops constant, the regression boils down to just the equation of a straight line with slope= 0.001. Intuitively, the slope is the rate of change (the result of the first order differential with respect to stops).\n\nGiven the model, though I'd suggest you look at multicollinearity issues with the model. Based on feature names, I suspect stops and kms are correlated. If so, you can't draw the conclusion I proposed above as the coefficient is biased."", 'I have no idea of what I’m talking about, but shouldn’t more stops also increase the km?']",0,5,https://www.reddit.com/r/statistics/comments/13oi7im/regression_interpretation_q/
55,2023-05-22 13:30:27,MS in Statistics make up for bad undergrad stats? (PhD stats admission) [Q],"I applied to PhD programs in statistics this cycle and didn’t get into any. I got into one funded MS program in statistics at Miami university in Oxford oh. My undergrad stats were BS in stats with minor in mathematics, so I took math up to two semesters of real analysis. I had a 3.4 cumulative gpa as an undergrad. 

I am doing a funded MS, and hoping to reapply to PhD programs. If I get a 4.0 gpa, and do good in the coursework, can this boost my chances to getting into a whole lot more phd programs in 2 years? How do adcoms view students who did well in MS in Stats at another school? I’d have to reapply, since Miami is a terminal MS, and there’s no phd program there. So I’d also like to hear what I should do in my MS to boost my shot at getting into PhD programs?

Love to hear your thoughts.","[""I guess masters in statistics would be more beneficial than just bachelor's no matter it's either academia or corporates.\nGood luck with your studies."", ""3.4 isn't great these days. A terminal masters program typically will prepare you for statistical consulting gigs. You may find that's the level you want to stop at.\n\nI'd encourage working up your GRE scores and get involved in research during the masters. I too had not a great undergrad GPA, but aced the GRE and had relevant experience. It's ok to be weak in one place (grades) if you make it up in the other areas."", 'Regardless, do a funded MS. Not in stats but I had a low undergrad gpa (although about a 3.9 in the last year after getting my shit together), did a MA and did well, got into a top 5 PhD program. \n\nAlso, PhD is research, so get research experience', 'So an MA would greatly improve my chances?', 'Could high grades like a 3.9+ in a MS help tho? Or would they really still focus on my 3.4 in undergrad? I mean, it’s a terminal MS, but it’s only because the department is small and doesn’t have a PhD. It’s highly theoretical and places kids in PhD programs.']",1,11,https://www.reddit.com/r/statistics/comments/13ohk1z/ms_in_statistics_make_up_for_bad_undergrad_stats/
56,2023-05-22 12:15:26,[Q] Should we control for this variable?,"I will give a hypothetical example. I am not saying it is true, it is just a random hypothetical. Here are the assumptions for this made up example:

\- everybody will get depressed at some point in their life

\- there is a significant/observable cut off point between non suicidal depression and suicidal depression

\- anti-depressants have been shown to significantly reduce the chances of developing suicidal depression for a small subset of the population, around 5%

\- A significant proportion, for example, about half, of people with both non suicidal and suicidal depression end up with chronic nausea

\- The excessive crying in suicidal depression has been show to be able to cause chronic nausea (the level of crying in non suicidal depression is not sufficient to cause chronic nausea).

\- The lack of appetite in non suicidal depression has been shown to be able to cause chronic nausea. Antidepressants do not stop the lack of appetite in either non suicidal depression, Those with suicidal depression get chronic nausea due to excessive crying, and not due to lack of appetite.  

You are running a trial to see the effect of antidepressants on chronic nausea. You have a sample of 1000, you divide it into 500 being in the intervention group (these people receive the antidepressants) and 500 being in the control group (these people receive a placebo). 

Given that antidepressants only stop 5% of people from developing suicidal depression, given that suicidal depression uniquely can cause chronic nausea via its excessive crying, given that lack of appetite in both nonsuicidal and suicidal depression can also cause chronic nausea: shouldn't you exclude suicidal depression participant from the study?  

Think about it. Given that antidepressants can reduce chronic nausea in 5% of people, by stopping suicidal depression (and therefore stopping excessive crying, which can cause chronic nausea), wouldn't that mean if you don't control for the variable ""suicidal depression"" the antidepressant would mean less chronic nausea in the antidepressant group? 

Wouldn't that show that antidepressants have an overall effect on reducing the levels of chronic nausea? But given that they only do this for 5% of the population, if you don't control for suicidal depression, wouldn't it mean for 95% of people this reduction of chronic nausea from the antidepressant wouldn't apply? Because if they get chronic nausea, it would be because of the lack of appetite, which antidepressants have no effect on. So shouldn't you exclude those with suicidal depression from the study? Am I making a mistake here?",['This sounds like suicidal depression is a mediator. Do not control for it.'],0,1,https://www.reddit.com/r/statistics/comments/13og3en/q_should_we_control_for_this_variable/
57,2023-05-22 12:10:51,What are the big research areas still around in statistical learning? [Q],"I was wondering what stats departments do active research in statistical learning. Furthermore, what topics are big or “hot topics” in statistical learning. I’ve been reading quite a bit of elements of statistical learning and developed an interest in ensemble learning, as well as high dimensional regression and topics related to shrinkage for linear models. 

I was wondering if research, say in these areas still goes on? For example, what is the research going on with tree based methods / ensemble learning. 

Or is statistical learning an “old” research area with not much new happening?","['I assume you mean something that is not deep learning. (Elements of Statistical Learning discusses neural networks, as does An Introduction to Statistical Learning, so I think neural networks count as “statistical learning”.)', 'What?', 'Really? Like the stuff in the elements of statistical learning book?', ""Gelman (who is awesome), isn't an author on either of those books...?"", 'Lmao whoops, got my quintessential texts mixed up']",6,12,https://www.reddit.com/r/statistics/comments/13og069/what_are_the_big_research_areas_still_around_in/
58,2023-05-22 05:10:27,"[E] For graduate programs, does anyone have experience with substituting calculus or linear algebra coursework with CLEP test results?","Currently working in environmental consulting. Graduated with a MS in geology 12 years ago and currently use statistics in projects in my work. I am looking to enroll in a graduate statistics certificate program (currently looking at either Colorado State University or Penn State), which my job would pay for, however, I need to complete calculus prerequisites as I did not need to take these courses for my undergraduate program (also geology). I don't think my job would pay for prerequisites. 

To save time and money, I will ask any program I plan to register for whether they accept CLEP test requires to cover mathematics requirements. I was curious if others have had success using CLEP test results to fulfill math prerequisites.","['Ah ok.  I never took the calculus clep, but it is one of the more difficult exams.  It has a DOD pass rate of about 34% - but there are some caveats.  DOD offers free exams to members of the military.  Many take the exams for promotion considerations, often without studying.  The DOD is the only place to find reliable pass rates, because the college board does not release this info.\n\nThe real pass rate is probably closer to ~45%.', 'Will the clep for calculus fulfill all your requirements?\n\nIt depends on the school policy and program requirements, but my understanding is that the calculus clep will likely only give you credit for calculus I.\n\n…I’m the mod of r/clep and also a stats major.', ""That would at least save me at least $700, and time. Then I'll take Calculus II and III online at a community college. Some programs also require linear algebra credit, and some like Penn State only require the prerequisite knowledge and not necessarily formal credit.""]",1,3,https://www.reddit.com/r/statistics/comments/13o6g1m/e_for_graduate_programs_does_anyone_have/
59,2023-05-22 04:11:55,"[Q] Trying to learn imprecise probability theory, most content seems too vague. Recommendations?","I've been trying to learn imprecise probability theory. However, most of the literature is focused on gambles, games, rational decisions, and I am just a poor statistician, trying to see how it relates to estimation theory. Are any such resources available?",[],1,0,https://www.reddit.com/r/statistics/comments/13o4yzj/q_trying_to_learn_imprecise_probability_theory/
60,2023-05-22 04:00:53,[Q] What is the best online course/series of lectures to go along with Casella & Berger/Hogg & Craig?,Would like it at a fairly high level. But can’t seem to find good quality lectures. Thanks.,"[""There is a YouTuber called StatisticsMatt, he's also a redditor! u/statisticsmatt. He has excellent material for inference. He's really the only guy who puts out high level statistics videos that hit the sweet spot for explanations and rigour.\n\nI got through Inference 1 and 2 by using C&B, the solution manual for it, and that YouTube channel.\n\nEdit: tagged Matt"", 'Had the same question, thanks. Following here.', 'Just replying to say me too (and since I think the Reddit algorithm shows content with more engagement to more people).', 'Is there a playlist for C&B? thanks', 'This, lol']",33,17,https://www.reddit.com/r/statistics/comments/13o4pbn/q_what_is_the_best_online_courseseries_of/
61,2023-05-22 03:42:33,[Q] What kind of rasch Model should I use for a scale with likert scaled items? Dichotomous or Polytomous.,"I’ve never heard of the method or IRT for that matter and gotta use it now so I’m pretty uninformed :/
Also I’d be happy if you could give me any sources to research using rasch models for item analysis… thanks and excuse my english.","['Why use rasch and not something like grm?', 'For polytomous data, you can use a Graded Response Model (Samejima, 1969).\n\nFor dichotomous, you can use either a Rasch, 2 PL, or 3 PL. depending on your research question and how much data you have available. \n\nI suggest the book “The Theory and Practice of Item Response Theory” by R.J. de Ayala. It has some good chapters on these.', 'My professor mentioned the rasch model so I kinda focused on that but I’ll look into grm ^^\n(Sorry for being that uninformed)']",2,3,https://www.reddit.com/r/statistics/comments/13o49gp/q_what_kind_of_rasch_model_should_i_use_for_a/
62,2023-05-22 00:42:59,[Q] What normality test could be used for a sample size n=2? Any advice will be greatly appreciated 🙏,I need to analyze the behavior of concretes using ACI 214. But haven’t been able to find a normality for sample size n=2.,"['Its not possible to test general normality at n=2. \n\nI bet the response is not normal, though it might not matter', ""I don't think there is any way to statistically test normality when n = 2. You don't have enough information and the power to detect departures from normality would be basically zero. But if you look at your distribution, I think you can see that it's definitely not normal. The mean of your distribution equals the midpoint between the two data points. Unlike what you would find in a normal distribution, this mean value is not a peak but rather a valley."", 'You’re high bro', ""Look dude, it's Sunday afternoon and I'm not going to spend it arguing over a sample distribution with an n of 2."", ""What are you measuring? If you have a device that measures something like deformation over time under increasing load, you can compare those measurements if you're able to gather a sufficient sample of measurements... n = 30 is the traditional recommendation. However if you only have static measures of two samples, your hands are probably tied and you can't really say more than one is less or greater than the other""]",0,10,https://www.reddit.com/r/statistics/comments/13nzqjt/q_what_normality_test_could_be_used_for_a_sample/
63,2023-05-21 23:01:40,[career] Stats Consulting - How to get fast enough,"I started as a statistician at a small consulting firm about 6 months ago using SAS. I will be asked to run stats for a project that is typically a mixed design with a handful of outcome measures taken pre and post some manipulation with an experimental and control group. I'll be asked to generate all the results that would be needed for a paper - breakdowns of demographics, testing group differences at baseline, and testing the effect of the manipulation. Usually my company estimates these projects at around 30 or 40 hours. I'm finding it impossible to stay within the estimated time.  
  
I'm honestly not sure what is taking me longer than the other statisticians. I was new to SAS when starting, so my coding has been getting faster, and I have developed some macros for outputting tables that have sped things up a bit. The data typically needs some cleaning and it does take some time to for me to feel comfortable with understanding the data, and that it is being treated appropriately. It feels like there is always one more thing to check, normality of data, which particpants are missing data and why, does this measure have a range that is plausible, am I sure that merge was handled correctly, etc.  
  
Does anyone have tips for being more efficient?","['You are new at the job. Previous  statisticians would possibly have worked on similar problems before either in your company or elsewhere. Hence it becomes a pattern recognition problem rather than thinking and resolving problem. With time and experience you will get quicker.', 'Build up a searchable code base of your previous projects, or at least save your work in such a way that it is easy for you to reference. That way each project you do you can copy paste and modify some previous code instead of starting fresh.\n\nEdit: autocorrect had me ""modify some previous coffee""', 'My guess is that the line items you describe are not of equal importance.\n\nProbably your seniors understand the relative strategic priority of each and can either instinctively or through client engagement know what actually ""matters"" and they\'re letting the rest of it fall through the cracks.\n\nMy advice? I\'m thinking the structure of the data won\'t change much, so you can build functional reporting/analysis structures on top of dirty data. Build the stuff that will put out client-visible and client-valued stuff first. Run the dirty data through it to see if it ""works."" THEN start trimming and cleaning up your data as much as you can, feeding it through the model/exploratory code as you go.', 'Believe me. When I was new to the job it took me more than a week to do all that. This is normal don\'t panic. You will develop your own framework that will speed up your analysis, you already have macros which is great. I use R software and I use ""gtsummary"" and ""tableone"" packages to help automate the tables. This cuts lots of time. But most importantly make sure that data is completely clean. In clinical research most projects have similar analysis plans, reuse the codes everytime and change what is nesseary.', 'Find out the code that others have used, and reuse it.\n\nCommonize it.  Possibly write macros.']",25,5,https://www.reddit.com/r/statistics/comments/13nx945/career_stats_consulting_how_to_get_fast_enough/
64,2023-05-21 21:53:24,[Q] Sum of values of a sample from normally distributed data,"Hello all,

I have a sample from normally distributed data. I would like to know the probability of the sum of the values of that sample being equal or greater than the mean of the normally distributed data.

How can I calculate that probability ? 

Thank you","[""> The sum of Normals is Normal \n\nIf they're *jointly* normal, sure. OP's post seems to be unclear on that point. In fact the conclusion of normality seems to be based on looking at a histogram of a realization of (X1, X2, ..., Xn) taken together, which doesn't necessarily tell us that it's jointly normal."", 'Why is this information not in the posted question? Edit it', 'So you have 35 independent, identically distributed normal random variables. Their sum is also normal with the mean and variance described in the link.', ""You are almost certainly having trouble with terminology. That's common, statistics has a lot of terms with meanings which can seem a bit arcane to the uninitiated.\n\nAs yodenaneda is saying, I'm pretty sure that the correct terminology here is that you have a bunch of independent and identically distributed random variables which you are summing. Keep in mind that whether you've _seen_ the value does not change whether or not it was a random variable. Consider rolling dice. We can ask what the probability that the sum of four six-sided dice is greater than 14 and figure out that it's 50%. Then we can roll four dice, sum them up, and see that we got 10. That's still a _realization_ of a random variable which is the sum of several (independent and identically distributed) random variables.\n\nAnd keep in mind, if there's no randomness, it doesn't make sense to ask about probabilities."", 'I am not summing variables, I am summing the values of different instances of the same variable. Let’s say X is a normally distributed variable. What is the probability of the sum of say, 35 randomly selected instances, being equal to or greater than the mean ? I am doing a risk analysis, this is why I would like to know']",0,10,https://www.reddit.com/r/statistics/comments/13nvhjz/q_sum_of_values_of_a_sample_from_normally/
65,2023-05-21 20:19:22,[[Q] Can I use studies with different confidence interval in a meta analysis?,"I have been collecting data for a meta analysis project. I found 4 studies with same PICO. But mean difference from 2 studies is calculated with 80% CI, 1 with 90% and last one with 95%. How do I move forward?","['Do you know the method the confidence intervals are calculated with?\n\nIf you know the method you can maybe reconstruct standard errors if the statistics are asymptotically normal or you know the assumed model.', 'Hopefully you had a protocol describing your statistical methods in detail before conducting a meta-analysis? What is the outcome? What meta-analysis method are you using?', 'Meta-analysis of what? What is your model?', 'I dont have access to raw data. I have mean differences and intervals. So, I dont know if I would be able to that.', 'outcome is continuous. intervention vs placebo. DerSimonian-Laird method.']",1,10,https://www.reddit.com/r/statistics/comments/13nrkf0/q_can_i_use_studies_with_different_confidence/
66,2023-05-21 19:44:30,[Q] Can someone explain to me how a one-way ANCOVA makes sense here?,"First, some information about the [study](https://doi.org/10.1080/15213269.2016.1257392):

> An online experiment was conducted to investigate the effect of manipulated Instagram photos on girls' body image. **The experiment has a 2 (Instagram photos: original vs. manipulated) × 2 (social comparison tendency: lower vs. higher) between-subjects design.**

Because covariates were collected alongside this, I expected a two-factor ANCOVA.

> To test the hypotheses, **a one-way analysis of covariance** was performed with
body image as dependent variable, Instagram photo manipulation and tendency to make social comparisons as between-subjects factors, and level of
education as covariate. Participant age was excluded as additional covariate
because preliminary analyses showed no effects of age on the dependent
variable body image. Hypotheses were tested at the alpha = .05 level (onetailed).

Did they calculate *two* one-way analyses of covariance and just not write it down explicitly, or is there another possible trick that goes beyond my admittedly limited mathematical understanding?","[""Reading the results, it appears that they did a two-way. Maybe it's a typo?"", 'It definitely *should* be a two-way analysis, and unless there were other covariates, they didn’t even covary out participants’ ages, so it looks like just a two-way ANOVA ended up being their final statistical treatment. Best case scenario, they’re bad writers. Worst case scenario, they’re bad writers and their stats are bass-ackwards.\n\nYou’d know which is which from the results; if they discuss multiple main effects or an interaction, it wasn’t a one-way analysis. If they discuss any actually implemented covariates, it’s an ANCOVA. If they somehow did an ANCOVA with no covariates, that’s just an ANOVA.', 'thank you for the answer - Probably. It has to be.\n\nSomeone over in /r/AcademicPsychology suggested that the authors may also have confused ""one-way"" with ""univariate""', 'Thank you for your answer!\n\n\nThey included ""education level"" as a covariate', 'How do the paper get published lol. Probably one of those publish anything journals.']",6,5,https://www.reddit.com/r/statistics/comments/13nquth/q_can_someone_explain_to_me_how_a_oneway_ancova/
67,2023-05-21 17:32:47,[Question] Calculating the effect size from repeated measure ANOVA values,"\[Q\] Currently I am gathering data for a meta-analysis, but during my data collection I encountered really serious troubles with the data reported. Sadly asking authors directly was to no avail, as they are not answering my emails (most of them). I have the following issue: In a paper the results from reported as a repeated measure ANOVA, but no means or SD's are reported. Is it possible that based on the Degrees of freedom, F-value and p-value to calculate the effect size if I know the sizes of both study groups:  
E.G: (F6.4,105.8=5.1; p<0.001) n-active=12 n-placebo=12  
Can I calculate based on those values the effect size or should I rather try to contact the authors for the n\^k time.  
I would really appreciate a well argumented answer.",[],1,0,https://www.reddit.com/r/statistics/comments/13nofbo/question_calculating_the_effect_size_from/
68,2023-05-21 12:59:21,What is the chance? [Q],"Honestly, I was gonna do this myself, but I forgot everything in AP Stats right after I took the class.

Question:

If I buy 30 rings, what is my chance of getting 2 Tri's?

The chance of getting Pri (First one) is 75%

The chance of getting Duo (Second one) is 45%

The chance of getting Tri (Third one) is 30%

The condition is that you have to move up the ladder, you have to meet the first requirement (getting Pri) then the second and then finally the 3rd. 

&#x200B;

Thank you!","[""Please edit your question so it is comprehensible to someone who doesn't already know what you're talking about."", ""I can't figure out what you mean."", 'What do you mean by ""the chance of getting Tri is 30%""?', 'Oh, then you can get that from binomial cdf, x>=2, p= 0.75x0.45x0.3, n=30.', 'Thank you :)']",0,14,https://www.reddit.com/r/statistics/comments/13nje76/what_is_the_chance_q/
69,2023-05-21 11:19:19,[Q]is this degree course good for my goal,"I work at a market research company as a product manager for a data analysis product.

I would like to establish a new role as head of data products and feel like being the in-house expert or at least having some formal training background would be useful.

I was looking for courses I could do online, is this a good course? 

https://www.nottingham.ac.uk/pgstudy/course/taught/statistical-science-distance-learning-msc#courseContent

(I meet the edu background needed no problem there)",[],2,0,https://www.reddit.com/r/statistics/comments/13nhfei/qis_this_degree_course_good_for_my_goal/
70,2023-05-21 08:45:49,[Software] We've Built an AI-Powered SQL Query Builder - Looking for Feedback and Suggestions!,"Hello, fellow Redditors!  
  
As a software engineer, I've had my fair share of encounters with SQL queries. And let's be honest, they can be a bit daunting for beginners or cumbersome for the pros when they get too complex. That's why my team and I have been working on something we think could be a game-changer.  
  
We're excited to share with you [Loofi](https://loofi.dev/), an AI-powered SQL Query Builder we've built from scratch. This tool not only simplifies query building, but also provides real-time insights and recommendations, thanks to our AI algorithms.  
  
We're eager to get your thoughts on it and would appreciate it if you could try it out. Any feedback or suggestions are highly valuable as we continue refining our tool.  
  
Also, if you have any questions or need help, feel free to ask. We're here to support and learn from this wonderful community.  
  
Thanks in advance!",['So I hate to be that guy but… how does this perform against GPT-4?\n\n(I’m sure you’ve put a lot of work into this and I don’t want to diminish that. I’m just asking the obvious question most readers of this post are going to have.)'],0,1,https://www.reddit.com/r/statistics/comments/13ne6gp/software_weve_built_an_aipowered_sql_query/
71,2023-05-21 07:47:13,[Q] Job opportunities,"Hello everyone I'm currently a second year undergrad major in statistics. What job opportunities do I have once I graduate? All I know rn is either Data Analyst or Data science, anything else where I can use what I'm studying and pays well?",[],4,0,https://www.reddit.com/r/statistics/comments/13ncwdd/q_job_opportunities/
72,2023-05-21 07:30:00,[R] How do I estimate the parameters for this model,"I'm quite lost understanding how to produce the a.b and c parameters in these models. A typical regression model is something like y= intercept + bx (b is the coefficient using x as independent and y as dependent), now [these models](https://imgur.com/a/ASgQ46H) also should have just 1 independent and 1 dependent variable, yet the models should produce 3 parameters (a.b and c). Is anyone familiar with this, please? How do I achieve something like this in R.

Here's the paper link: [https://academic.oup.com/njaf/article/18/3/87/4788527](https://academic.oup.com/njaf/article/18/3/87/4788527). You can click on pdf at the bottom of the page to view the entire thing. I would really appreciate any help!","['These are non-linear regression models. If the conditional variance is assumed constant (which makes no sense for this application but it never seems to stop anyone) then see\n\n   ?nls\n\nfor fitting such models in R\n\nHowever, I strongly advise against trying to learn the ins and outs of nonlinear regression modelling just from that.', ""If you have a small set of data I can try to show you an example.\n\nEDIT:\n\nlooking at the paper, they talk about *growth models* or *growth functions*; if the data are following individual trees over time, a plain nonlinear regression is unsuitable because of the time dependence within-tree; it's more complicated. If the data are all from separate trees across a variety of ages (no tree measured at two different times), dependence issues probably won't be so much of a problem (there can still be spatial dependence, but it would be less strong and the extent will depend on the situation).\n\nEither way, it looks like they did nonlinear least squares regardless. \n\nA warning in relation to the paper -- R^(2) is not a particularly meaningful quantity for nonlinear models."", ""I could probably be more insightful if I knew anything about your area, but I don't.\n\nOn the data -- it's okay, I ended up making myself a small data set that seems to be slightly more realistic; I attempted to make it roughly consistent with the JP summary in the paper.\n\nMy problem right now is finding a good way to automate starting values for my example; this can be a bit of a bugbear if you don't have expertise in the area (where you might well *know* good starting values for one or more parameters already, greatly simplifying things)"", 'Thanks for your response. I’m really not trying to understand non linear regression based off this. I just want to know how to replicate this on my end and get those parameters\nReading through, I believe what they did was get height and diameter measurements of trees and fit these models, the fitting provided the a, b and c parameters which they can then use to predict heights using the same model, basically just a typical linear regression process.\nMy confusion is that for typical linear regression, if I run this with lm in r, I’d get an intercept and beta, I can the use that to predict height for any diameter in the data range. For this case, it still uses just height and diameter but someone gets 3 parameters which will be applied in the models.\n\nI’m looking through the documentation for nls now.', ""Oh wow, this is very insightful. \n\nBased on their description, the data collection wasn't over time but over the same period. You're correct, the major variation in this type of data collection is a spatial one, basically variations at the forest stand level. \n\nI just created this with chatgpt  - [https://github.com/brian-o-mars/height-diameter-sim](https://github.com/brian-o-mars/height-diameter-sim). Not a very clear non-linear relationship but it may suffice. I really appreciate your help.""]",0,12,https://www.reddit.com/r/statistics/comments/13ncile/r_how_do_i_estimate_the_parameters_for_this_model/
73,2023-05-21 07:04:30,[Q] Business analyst vs Data analyst,"I really don’t know what business analyst means on job postings. Just graduated with my bachelors in statistics and I recently saw a job for a business analyst position (contracted) even though I am mainly looking for a data analyst position. The job description doesn’t list their preferred data analyst tool (SQL or SAS, etc), and the responsibilities section looks like it’s data analysis with more responsibilities. 

I’m very curious from those in the industry, what is the difference between a business analyst and data analyst? Is this a good stepping stone for a data analytics role later on that might fit more what I did at university? 

This question is a little outside the scope of this Reddit but idk where else to ask like-minded people.","['You’ll come to find out that job titles mean nothing. At the end of the day you just need to look at what the job reqs are. If you have 4 years of sql experience, should you apply to a job that requires 1? Probably not, you’ll be fiercely underpaid', 'Or when data analysts are actually data scientists (or data engineers) but get labeled data analyst to be underpaid. Looking at you, United Airlines', ""I've always understood product managers to be experts in a single thing, i.e. their product.  And they kind of control the direction of that product.  While BAs are typically very fluent in cross functional scenarios.  At least again this is my experience in dealing with the product managers I've dealt with."", 'This might be bad advice as I\'ve never worked as a business analyst.  But I have worked with a bunch of business analysts and what they seem to do is act as the interface between software teams or other teams.  And they are usually expected to be ""experts"" on each of those subjects.  So for example in manufacturing, usually there will be a customer facing software and a manufacturing software system.  There needs to be a lot of communication between the two systems.  Or if your customer wants something special then the analyst will help to initiate the process of making sure stuff talks correctly.  Could it require tools you use as a data analyst?  Possibly.  Imagine production needs a list of items processed at certain stages so they can prepare the machines for them.  That could be a business analyst job to ensure all the requirements are met and that production has the list when they need it.\n\nI had a lot of these situations where we would allow some of our higher volume customers to do crazy special items.  Even though our software doesn\'t allow them to be input into the system, our BA would have to do a lot of testing and stuff to figure out how we enter it in our customer software so that our production team can ensure the materials are pulled with the correct sizes and amounts.\n\nAlso speaking of requirements, imagine a team wants to start using new software.  Well how will they software interact with other software already used?  What will be the limitations of this new software that might be unforeseen by the team wanting to use it.', ""I have met business analysts who act as really good data analysts and coders, and I've met data analysts whose role may involve serious quantitative understanding but little implementation of deep analytical methods and little digging upstream into source data.  All of those roles and duties are great but you gotta drill down to what's really likely to be asked of you by asking lots of questions about the team's niche and the role's niche within that.""]",19,14,https://www.reddit.com/r/statistics/comments/13nby3z/q_business_analyst_vs_data_analyst/
74,2023-05-21 05:12:27,[Q] Can someone explain the statistical result in this paper? Something is technically wrong but idk what.,"Paper:
https://epigeneticsandchromatin.biomedcentral.com/articles/10.1186/s13072-020-00378-0

I hope this is ok for me to post. I'm giving a talk on this paper and I am just damn certain that the findings are non-sense and that they only found something due to how the researchers applied statistics. Unfortunately, I'm not good with statistics and cannot exactly explain what's happened. Your help would be much appreciated! 


Even without looking at the science, the researchers tested occurrence of 3 events (DMR/DHR/ncRNA) and it seems there's no significant pattern across generations. 

Then, they looked at overlap of the location of these events and found generally 20-30% overlap across some events, especially with F3 events (makes sense given that there are SO many more incidences of all events in F3). However, then they did further analysis on the same dataset and suddenly end up with 80%+ overlap...?! And their permutation analysis came out as significant. 

I'm pretty sure this is a case of doing so much testing until something appears significant, though I don't understand the mechanism behind this. Also I thought permutation tests were kind of solid? 

Anyway, tldr; what's going on here?",[],1,0,https://www.reddit.com/r/statistics/comments/13n99yh/q_can_someone_explain_the_statistical_result_in/
75,2023-05-21 03:37:17,[Q] Variation of multinomial distribution,"As an illustrative example, imagine a trial where you roll a six-sided die. A multinomial distribution would treat each possible value separately, and describe the likelihood after *n* trials of rolling a 1 *a* times, rolling a 2 *b* times, etc. However, if all possible values are numbers, then you could instead reasonably ask ""what is the probability that, after *n* trials, the sum of the results is at least/exactly/at most *k*?"" and this corresponds to rolling *n* dice totaling *k*. Is there a probability distribution that answers this question, for dice of arbitrary fairness, side count, side values, etc?

edit: the side values would only need to be integers, if that helps simplify things.","[""You're just asking about the general case of sums of discrete random variables with finite support. The arbitrary face values part means that the number of distinct values in the support of the sum grows very rapidly (as the product of the number of values in the support of each of the components in the sum). For example, consider if the first die has integers, the second multiples of √2, the third multiples of √3, the fourth multiples of √5, ... \n\nThe answer is no, there's no *general* formula that simplifies anything over computing the whole thing out term by term.\n\nNaturally in many special cases there are simplifications (e.g. if the face values are all on lattices with the same spacing things simplify a little because the support of the sum is on a lattice with that spacing -- the number of values in the support grows roughly as the sum rather than the product of that of the components)."", '[Sums of random variables theory.](https://www.statlect.com/fundamentals-of-probability/sums-of-independent-random-variables) There might be better resources, just an example walkthrough I found.  \n  \nSimulation python code showing an approximately normal(ish) distribution for adding 20 rolls of a 6-sided dice:   \n      \n    import matplotlib.pyplot as plt  \n    import numpy as np  \n    sums = []  \n    rolls = 20  \n    dice_sides = 6  \n    for rep in range(10000):  \n        s = sum([np.random.randint(1, dice_sides+1) for n in range(rolls)])  \n        sums.append(s)  \n    plt.hist(sums, bins=range(dice_sides*rolls))  \n    plt.show()', ""> For example, consider if the first die has integers, the second multiples of √2, the third multiples of √3, the fourth multiples of √5, ... \n\nI guess I was a little unclear, because this isn't an example of what I presented. Each trial would be exactly the same. I should also clarify that the faces only need to be integers, if that simplifies things."", 'When you say ""each trial"" do you mean each die is the same?', 'Yes']",2,8,https://www.reddit.com/r/statistics/comments/13n46lb/q_variation_of_multinomial_distribution/
76,2023-05-21 02:34:33,[research] crime stats on transit,"gathering crime stats on transit by location, gender, race, type of crime, neighborhood, time of day 

let's say 2 guys robbed someone at X train station and in this ""north"" neighborhood  in the morning 

I'd report...2 counts of robbery (type of crime ), 2 males (gender)  towards the total 

but would you count 2 crimes for being in ""north"" neighborhood, 2 crimes in the morning , 2 crimes at X station ?  doesn't sound right to me ....so I'm thinking I'd ""strip"" one out in these cases 

now if one person committed 2 crimes on 2 victims then I would consider 2 crimes being in ""north"" etc 

thoughts ?","['I’d probably change the 2 counts of robbery to 1 instance of robbery with 2 males involved. It doesn’t make sense to me to attribute the number of crimes to the number of those involved but then not treat those as separate for the other variables as well.', 'You have multilevel data with offender-incidents nested within incidents and incidents nested within locations. \n\nYou need an incident ID# so you can let the computer know that rows 1 and 2 both refer to incident A.', 'to give an an extreme example, let\'s say 9 Asian  men were involved in a robbery on one person ...\n\nanother assault happens in South area at night ,\n committed by a black  female \n\nto me , it sound ""unfair"" to say 90 percent of the crimes occured in morning and in the north...I\'m inclined to treat it as one count \n\nbut makes sense to me to say 90 percent of crimes were committed by males, Asian, and type of crime is robbery ...10 percent by female, black, assault \n\nor maybe I\'m just being retarded', 'thank you', 'and that\'s what I\'ve kinda done \n\n3 worksheets\n1) main\n2) same crime, multiplex perpetrators\n3) chart data \n\nusing an extreme example , the main data tab  would have 9 rows of that robbery crime, so a count of 9 Asians, 9 north, 9 morning, 9 males\n\nin the ""same crime, multiple perpetrator"" tab, I\'d have EIGHT rows of that event \n\nthen, in the chart data tab, I would do a countif of gender,  location, ethnicity, neighborhood, time of day, etc based on the main tab ...then , depending on the category, I would do a countif to strip away  counts based on the ""same  crime, multiple perpetrator ""\n\nagain let\'s suppose there was another crime committed at night, in the south, by a black woman, and was an assault \n\nmy final counts\n\ngender: 9 males, 1 female (90 % , 10%)\n\nneighborhood: 1 north, 1 south  (50%, 50%)\n\ntime of day: 1 morning, 1 night\n\nethnicity: 9 Asian , 1 black \n\ntype of crime: 1 robbery  , 1 assault \n\nsome of these I\'m representing as a pie chat and thus expressing in terms of percentages']",1,5,https://www.reddit.com/r/statistics/comments/13n2hn6/research_crime_stats_on_transit/
77,2023-05-21 01:59:47,[Q] Intern/Career Options for Grad Students,"Hello, 

I just finished my first year of a Stats MS. I've taken Probability and Statistics I & II, Statistical Computing, and GLMs. By the end of July I'll have also completed Time Series Analysis and Multivariate Regression. My program is focused on the theoretical side of machine learning/prediction. Building ML algos from scratch but also learning relevant APIs. I'm just curious what companies/industries hire stats folk. 

Other than data analysis/science, I'm not sure what my career options are. My program boasts most alumni placement is in data science, but It's been a struggle to get analyst roles so far so Im looking to expand what I'm applying to. Any advice would be appreciated. Thanks!","['Finance because anything that impacts a customer’s financing has to be explainable. ML algos are difficult (not impossible) to explain, so a lot of financial institutions use traditional statistical models. That leads you to a statistician. You wouldn’t believe the number of people coming out of ms in ds who can’t explain a logistic regression.', ""Nope, I'm in finance and I studied computational solid state physics for my PhD, there are many others like me who studied either math / stats / physics / or have a CS degree focused heavily on the math side of things for their grad program. While I wish I would have studied business formally, it's not a requirement. Most finance firms have a research division that does machine learning and predictive analysis, which is where you would want to look for positions. Most large tech firms actually have similar divisions though, so if finance isn't your cup of tea you have many other options like healthcare (predictive analysis for diseases), telecommunications (multivariate analysis of consumer data, or if you have a good enough background in counter science the really fun problems involve reducing latency in streaming video apps), or anywhere that does marketing research analysis. If you can't find anything, you can always do your PhD while you look for something, that's what I did anyway until i could decide what to do with myself."", 'Interesting. I always thought the finance industry preferred people with accounting and business degrees out of tradition. What title does finance hire statisticians under?', '>\t Most finance firms have a research division that does machine learning and predictive analysis, which is where you would want to look for positions. \n\nFor “math / stats / physics” PhD students who are uncertain about whether they want to pursue academia or industry, what are your thoughts on spending a summer or two in an internship in such a research division (in finance or healthcare or telecommunications, as you suggest), especially as a first or second year? I ask because I’m not sure how I should make such a trade off between making progress on my degree requirements and preparing for post-PhD things, e.g. by doing such an internship.', 'Typically it’s broken up into teams so you can see model validator, data scientist, or even just modeler. There are teams for accounting and finance but they don’t build predictive models, the statisticians and data scientists do.']",9,7,https://www.reddit.com/r/statistics/comments/13n1m2e/q_interncareer_options_for_grad_students/
78,2023-05-21 00:25:15,What area of research describes these key words [Q],"Can someone help me identify what area of research this is in statistics? Basically some sort of area which includes topics such as “high dimensional statistical learning” + “shrinkage estimators/priors and penalized high dimensional regression” + “sparse linear models”+ “optimization algorithms”. 

Basically, an area of research which focuses on the development of statistical methodology for high dimensional learning problems. Involved custom shrinkage estimators, or certain algorithms for high dimensional learning, or topics from ESL that kinda go more in depth than lasso/ridge. 

Or any kind of research involving developing penalized regression or algorithms for high dimensional learning. 

I’m trying to find what this area of statistics is called but I can’t quite find it. I’d appreciate if anyone can point me in the right direction.

If there is also a Bayesian flavor to this, I’d like to know this as well. Thanks!","['Machine learning?', 'High-dimensional statistics', 'Yes yes yes! This is it! Thanks!', 'That’s too broad, wouldn’t you think?']",1,4,https://www.reddit.com/r/statistics/comments/13mzcg7/what_area_of_research_describes_these_key_words_q/
79,2023-05-20 21:52:52,[Question] Odds of three people born the same day of the week?,"Team, please help out a guy who has made every effort to stay away from math over the past 20 years.

Trying to figure out the odds of three people being born on a Saturday.  I'm thinking it'd be 1/343 since I did 7x7x7.  Or I could just as easily be wrong. 

Thanks for your help.","['Well, we can ignore the day of the week for the first person, all we care is that the second person matches the first person, and the third matches the first. \n\nYour probability would be correct if your were asking three people born on a Monday, for instance', 'Awesome, thanks for the help!', 'In the US, not sure about other countries, not all days have the same number of births. Weekends account for much less than 2/7 of births', ""But it still ends up a fairly close approximation. \n\nFor OP, if you don't want to assume a uniform distribution... You \n\nUsing US 2010 CDC data, with proportion for each day as:\n- Sunday: .0927\n- Monday: .1521\n- Tuesday: .1672\n- Wednesday: .1647\n- Thursday: .1629\n- Friday: .1560\n- Saturday: .1044\n\nYou end up with 1/44 rather than 1/49 that 3 random people share the same birth day of week.\n\nYou get there by doing the calculation for each day of the week, e.g., Sunday: .0927 * .0927 * .0927 = .000797, and then summing the result for each day of the week.\n\nThis is very similar to if you assume it's uniform: e.g., Sunday: .142857 * .142857 * .142857 = .002915, and then summing the result for each day of the week, which is equivalent to ignoring the first persons day of the week.""]",1,4,https://www.reddit.com/r/statistics/comments/13msv02/question_odds_of_three_people_born_the_same_day/
80,2023-05-20 16:02:49,[E] A little statistics help required please - assessing predictions,"If I conduct a study where I get a single group of people to guess what their IQ is, then get them to sit the test, how do I assess how accurate their guesses are? I’m planning a study but don’t know enough about statistical methods to know which ones to use.

Thank you so much in advance","['You plan on giving people IQ tests? Specifically which IQ test?\n\nDo you have IRB approval to conduct this study?', ""Hmm not sure what the exact objective is but i see three ways of doing this:\n\n1. Test the difference in distributions of the predictions and actual score for significance using a t-test or an equivalent non parametric test \n2. Compute a new variable as the difference between the predictions and actual scores and test whether it's mean is statistically different from zero.\n3. Use a regression model with the actual score as the dependent variable and prediction as  the independent variable and see of the coefficient is statistically significant as well as the direction of the relationship. Though in the absence of other variables, your estimate will most likely be biased.\n\nYou could also visually explore the data with a scatter plot. This might show you patterns . Like maybe people with high IQs predict lower scores for themselves  or something like that"", 'I.e. not applied, only theoretical? \n\nBut then the mention of ""I\'m not planning on taking too much of their time"" is a sorta strange statement to make...', ""Studies done for a course don't require IRB approval. They aren't considered research activity. This changes if they intend on publishing but I doubt OP is considering that."", 'In 1) and 2) if your model predicts perfectly, your test will be not significant.\n\n3) Makes more sense, but is very similar to just calculating the Pearson correlation between predicted score and actual score. I think this is pretty reasonable and will test the statistical significance if this correlation is different from 0. If it is not expected that there is a linear realtionship, but merely a monotonic one, one could use the Spearman correlation instead.']",6,24,https://www.reddit.com/r/statistics/comments/13mlq2m/e_a_little_statistics_help_required_please/
81,2023-05-20 05:46:18,[Q] ITT Estimation,,[],1,0,/r/econometrics/comments/13lc63c/itt_estimation/
82,2023-05-20 04:31:59,[Q] Confounders and colliders haven't been accounted for,"I'm exploring the causal relationship between 10 different variables, and I noticed there are some confounders and colliders.

Although I don't have data, I do have other peoples equations (independent sources) when they looked at some of the parameters in groups of 2 or 3.

Although I can combine all their equations into one, is there a way of accounting for confounders and colliders (such as cancelling certain variables out, or ignoring the equations of certain relationships) or would I need to get my own data and start from scratch?","[""Confounders and colliders are theoretically determined, with input from the data. Try to create a DAG with the input from the other sources you reference. Figure out an equation from there. Keep in mind that DAGs aren't perfect. There are too many cyclical relationships, misunderstood relationships, and unobserved variables for it to ever be perfect."", 'You can use a bayesian network and account for colliders and cofounders. Basically, if properly set up, you can ‘control’ for all variables and also unobserved variables', 'Can you recommend any examples or walkthroughs of this method?', 'Look up Judea Pearl’s work. The Book of Why is a easy read, non-technical intro to DAGs and causality.', ""Although the sources with the equations only conscider the variables in groups of 2 or 3, from other sources that I was able to create a DAG to understand to identify the colliders and confounders (though no quantification of these relationships are available). The problem is that these aren't conscidered in the individual research where analysis was done\nGiven that I have a DAG, and I have the equations for the relationships individually, what is the next step to combine it all together? For instancd, do I combine them all and re-arrange to remove colliders, or do I just ignore the equations for the colliders?""]",10,6,https://www.reddit.com/r/statistics/comments/13m6woz/q_confounders_and_colliders_havent_been_accounted/
83,2023-05-20 03:15:43,[Question] Signal detection theory question for memory task,"I'm a stats novice seeking some advice on what extent d-prime is appropriate here. My lab is running an experiment where participants are exposed to sounds in an initial encoding phase, and then in the test phase they are given a subset of the old sounds (targets),intermingled with two types of new sounds: lures (very similar sounds to targets) and foils (clearly distinctly different sounds). Participants here rate each response as either old or new.  
  
Right now I've been calculating two different types of d-primes: 1) d-prime for target-lures, z(""old"" to targets - ""old"" to lures), and 2) d-prime for target-foils, z(""old"" to targets - ""old"" to foils). The reason for doing so is that these two types of d-primes represent different constructs in the field of memory (target-lure = mnemonic discrimination; target-foil = the conventional ""old-new' effect).  
  
My question is whether there is any issue of inflating or deflating d-prime values when there are three signals (targets, lures, foils) in the probability space? How would one best correct for response bias here or would the use of d-prime (or a-prime or A) be adequate? What other measures have been described in the literature?  
  
Thank you for your responses in advance, would appreciate any advice!",[],1,0,https://www.reddit.com/r/statistics/comments/13m4xzd/question_signal_detection_theory_question_for/
84,2023-05-20 00:59:15,[Q] How can I test to see if two different groups answered multiple choice survey questions differently?,"I have quite a lot of experience with continuous data analysis, but have never worked with survey data before. I have the results of a quality of life survey where each question is multiple choice, and survey respondents can only give one answer per question. Some questions are scored 1-10, and some are categories e.g. [never, sometimes, quite often, very often]. I have some additional data that groups survey respondents according to a few other categories. What test can I use to see if different groups have answered the questions differently?

EDIT: I should say, I'm primarily interested in one group in particular, so it might be right that I want to compare that group to all the rest of the respondents pooled together. For other categories (e.g. age brackets) I'm interested in comparing across all of them (e.g. are some age brackets more likely to report problems sleeping as a result of a certain lifestyle intervention).","[""Immediately, Chi-squared test for independence or Fisher's Exact come to mind. \n\nI'm not sure how you're conducting this, but it would be a fun project for a DS reference.\n\nCheers"", 'For the ordinal responses, consider Kurskal-Wallis tests', '> more likely to report problems sleeping\n\nIf the issue youre interested in there is looking for *more* problems sleeping, with an ordered categorical response you should not use an analysis that throws away the ordering (such as plain chi squared homogeneity of proportions). Possibly a Wilcoxon-Mann-Whitney (for a two group comparison),  or a Kruskal-Wallis for more than 2 groups. There are other possibilities.\n\nWith your 1-10 variable youd have to explain more about what the score numbers represent.']",6,4,https://www.reddit.com/r/statistics/comments/13m19pf/q_how_can_i_test_to_see_if_two_different_groups/
85,2023-05-20 00:22:44,[R] Comparing variances for sales purposes,"I have sales data for ten items (SKUs) across 24 months. The data is in revenue (e.g. $10,345) on a month by month basis. 

My goal is to determine how consistent sales are of each SKU relative to one another, with the idea being that SKUs with lower variability in sales are more reliable/consistent revenue streams for our company. 

My intention is to measure the variation of each SKU individually across 24 months and compare those values to one another. Is this the right way to go about it? Also, would it be a population variance or a sample variance? This is all the data I have for two years, but I suppose it could also be a sample of ten years of sales for the company. I’m not sure which version to use.","['In your scenario, if the 24 months of sales data is considered a representative sample of the overall sales pattern for each SKU, you can use the sample variance formula. However, if the data represents the complete sales history for the SKUs, you can calculate the population variance.']",1,1,https://www.reddit.com/r/statistics/comments/13m0aib/r_comparing_variances_for_sales_purposes/
86,2023-05-19 20:39:26,"[Q] I want to make the claim that if you have no idea/prior knowledge that either event A or event B would occur, it would make the most sense to ascribe a 50% probability for each event. If true, is there some kind of rule I can refer to?",Thanks in advance :),"[""One of the early and famous ways of ascribing prior probability was the [Principle of Insufficient Reason](https://en.wikipedia.org/wiki/Principle_of_indifference), which I've usually seen ascribed to Laplace (but which wikipedia attributes to a number of people). \n\nThis leads to giving a coin flip 50/50 probability, or a dice roll 1/6 probability, which makes perfect sense, but often gives conceptual and empirical problems when applied to common continuous variable situations (e.g., a uniform prior on a logistic regression coefficient is probably a bad idea).\n\nMore recently, Edwin Jaynes proposed something called [maximum entropy priors](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy), which are a generalization of the principle of insufficient reason, and let you incorporate more complex constraints and additional info into the prior."", ""You would just be assuming a uniform prior, essentially. I'm pretty sure there's no, like, named rule or anything (like Occam's Razor or whatever)."", ""Assuming both events are mutually exclusive, your experiment can be modelled with Bernoulli distribution.   \n\n\nBernoulli distribution only has one parameter (we'll denote it as *p*), in your case that would be the probability of event A (or event B) taking place. The variance is maximised when *p = 0.5*. Since variance is essentially a measure of uncertainty about a parameter, we may argue that, by ascribing the probability of 50% to both events, we have maximised the uncertainty about the true probability.  \n\n\nThis is how I would defend you decision to assume *p(A) = p(B) = 0.5*"", 'If you set it at anything other than 50/50, it means you have some idea of which one is more likely. So no prior knowledge and equal probability are tautological, which is even better than a rule.', '""It would make the most sense"" is doing a lot of heavy lifting.... let\'s break that down a bit.\n\n- you have no idea what the probability of event A is ( that of B is 1 - Pr(A)), but you\'re insisting that you have to pick a number.\n\n- to justify a choice, you could define a loss function L(p, true p) and try to minimize expected loss E( L(p, true p)) wrt p. \n\n- Even if you do that, you still have to take a stand on (1) what kind of loss function to use (are overestimates as costly as underestimates? Does loss increase linearly with the error? its square? a higher power?), and (2) the distribution of ""true p"", about which you claim to have no information.\n\nIn general, you can think of many situations where this approach will give answers that are *not* p=0.5']",8,9,https://www.reddit.com/r/statistics/comments/13lubha/q_i_want_to_make_the_claim_that_if_you_have_no/
87,2023-05-19 20:23:16,[E] Generating samples from custom CDF,"To my great shame, I must admit I was today's year old when I realized than I could use the uniform distribution and empirical cumulative density functions to simulate sampling for a distribution I do not have a closed form for.

Context: I've been digging a bit deeper into simulation, in particular Monte Carlo-type simulations where I need a probability function or cumulative density function to generate meaningfully random samples. Sometimes, the normal or lognormal distribution is sufficient, but sometimes the distribution is weird (not normal, multi-modal, etc.).

Generating an empiric CDF from data and using calculating values from it using the uniform distribution actual works to overcome the closed-form or not-normal function.

As example:

(Can't actually include the image on this subreddit, but the distribution overlap to a great extent, as can also be seen from the calculated mean and sd values.)

&#x200B;

    library(tidyverse)

n <- 1000

mn <- 25

sd <- 10

rnd <- rnorm(n, mn, sd)

cdf <- tibble(

x = seq(min(rnd), max(rnd), length.out = 100),

y = map\_dbl(x, \~ sum(rnd < .x) / length(rnd))

)

new\_rnd <- runif(n)

new\_rnd\_dist <- map\_dbl(

new\_rnd,

\~ approx(cdf$y, cdf$x, xout = .x)$y

)

mean(new\_rnd\_dist, na.rm = TRUE)

sd(new\_rnd\_dist, na.rm = TRUE)

ggplot(data = tibble(x = new\_rnd\_dist), aes(x)) +

geom\_density(fill = ""red"", alpha = .1, linetype = ""dashed"") +

Geom\_density(data = tibble(x = rnd), fill = ""green"", alpha = .1)

Note: markdown won't let me comment code with hash-tag, some some extraneous comments

Generating normal values with set mean and sdGenerating the CDF from the samplesGenerating uniform distribution sample with `runif()`.Matching the uniform samples onto the empiric CDF.Calculating mean and sd from newly generated sample transposed to normal

The cool things start to happen when you exchange the initial sampling with e.g. a bimodal:

`rnd <- c(rnorm(n, mn, sd), rnorm(2*n, mn/2, sd))`","['Just a note, you want to be careful when you talk about _which_ distribution you are sampling from -- in these scheme, you are most certainly not sampling from the underlying distribution that generated the data because of the fixed sample size. This is more or less just the procedure used in bootstrapping. In information terms, this does not contain any extra information than your original sample.\n\nThat being said, this scheme _asymptotically_ will sample from the correct underlying distribution of the data as the eCDF converges to the CDF almost sure (Glivenko-Cantelli).\n\nHowever, it is certainly very useful to use this sort of method.', 'Just be careful in the tails. The approximation can get really crude near, and especially past, the minimum and maximum data points.', 'Google ""Gaussian Copula"" - there are general frameworks for this kind of thing.', 'Live and learn, I too only learned about the probability integral transformation 9 months ago.', 'For the other problem, quantile estimation, I might use a fully empirical method in the bulk of the distribution, but fit a parametric model for the tails.  This would apply here as well, like above and below 1% (or whatever you choose) empirical CDF points, draw from a parametric model (possibly fat tailed if you expect that).']",14,6,https://www.reddit.com/r/statistics/comments/13lty5h/e_generating_samples_from_custom_cdf/
88,2023-05-19 00:41:54,[Q] How to add trend back to the forecasted time series data?,"I am making forecasts on the quarterly Ontario population. It is not stationary, so, to make it stationary, I remove the moving average and divide by the moving standard deviation and train the model on that.

'''

rolling\_mean = data.rolling(window=4).mean()  
rolling\_std = data.rolling(window=4).std()  
  
y\_detrend =  (data - rolling\_mean) / rolling\_std

'''

To get forecasts in the original format, how do I add trend back to the forecasts?

Thanks",[],2,0,https://www.reddit.com/r/statistics/comments/13l3zsc/q_how_to_add_trend_back_to_the_forecasted_time/
89,2023-05-18 22:15:39,[Q] When should I add sql to my resume when I’m teaching myself,"So I just graduated not even a week ago with my bachelors degree in stats, but I was never taught anything besides R and a tiny bit of SAS. Because so many jobs love to see sql, I figured it’s best if I teach myself through YouTube videos. 

Considering how long these videos are and how simple sql seems at first glance, I figured I’d be able to learn it within 3 or 4 days give or take if I truely dedicate myself to those videos and following along. Iv got nothing better to do. But people online say it takes 2 to 3 WEEKS, and even then it’s just the basics. This wouldn’t be that big of a deal, but I really don’t want to sit around for that long getting denied jobs because I don’t have sql on my resume. 

So when can I comfortably put sql on my resume? Should I couch it a bit by saying “basic sql” or am I hurting myself by doing that? Is it unnecessary to couch it at all since the employer knows I’m fresh out of college with no experience in the real world anyway? 

Thanks everyone!","['Just put SQL in and be prepared to fake it until you make it. Nobody expects you to know complex SQL formulas.', 'If you’re interviewing for analytics or data science roles you 100% will be asked to code in SQL.', ""I think you're worrying about a non-issue. Put SQL on there as its a common keyword employers search for. Learn SQL the best you can in the interim, if you get an interview requiring some degree of SQL knowledge, do your best.\n\nThe job market for DS jobs is extremely tough. As a new grad you need to give yourself any and all opportunities just to get in the door."", 'You can put ""familiar with SQL"" in right away, but, unless you\'re getting a certification, employers are going to want to see professional experience. I\'m too lazy, but you might google ""what do employers think of self-study"" (no quotes) for some tips', 'You have to get the interview first, you can work on learning SQL while you hunt for jobs.']",31,35,https://www.reddit.com/r/statistics/comments/13l0d6n/q_when_should_i_add_sql_to_my_resume_when_im/
90,2023-05-18 21:45:31,[Q] What is the best source/program to do power analysis?,"I have to perform a power analysis for my chi square and kruskal one-way anova tests to know the minimum sample size needed detect a difference in proportion and mean, respectively.

On spss (version 28) it seems more complicated than I anticipated (looked up YouTube videos but still was lost). 

Is there a good program that statisticians use and recommend? Or are any of the online calculators on google suffice? Using for a manuscript so want to ensure I do it correctly","['Check out G*Power', 'I would go for JASP, has a simple GUI and powerful, you can also add R code for more complex cases.', 'As another person recommended, for simple power analyses GPower is the best resource', ""There's a handy set of calculators here: https://select-statistics.co.uk/calculators/"", '""best"" is subjective and depends on your needs, interests and preferences.\n\nG\\*power seems to be very popular among social scientists. I\'ve never used it but as far as I am aware it\'s pretty comprehensive.\n\n> Is there a good program that statisticians use and recommend?\n\nTBH I\'ve not seen a statistician use one (though I am sure it happens). On the rare occasions I want power calculations, I generally use simulation (outside the cases simple enough to do algebraically -- and sometimes even then I\'ll use simulation). I have occasionally used the t-test power function that comes with R, though, such as to draw a power curve to compare with some other test on some set of alternatives.']",1,10,https://www.reddit.com/r/statistics/comments/13kznbv/q_what_is_the_best_sourceprogram_to_do_power/
91,2023-05-18 20:57:33,[Q] Creating a Excel Reward Program,"Hello All,

  
I'm new to this sub, I don't have any formal training in stats. I am an enthusiast looking to build a reward system for my sales territory. Attached is a copy of an excel sheet that I built but it seems to have a little snag in it, I was hoping to get some help on it.

  
Overall I was trying to create a score system for all of my accounts based off of 3 metrics. Each account receives a ""score"" based off of their actual data compared to the median of my sales territory. Essentially; If my median sales is $10 then a store that did $30 will receive a score of 3, a store that did $5 will receive a score of -0.5.

  
The snag I'm running into is the highlighted section, the function I have in the cell currently reads as =IF(C17/$C$2<1,-1\*(C17/$C$2),1\*(C17/$C$2)) Where my confusion lies is that this store is well below the median sales number, however they are receiving a better score than accounts who are still below the median but are higher than them.

  
Does anyone have any insight, videos I can go watch, terms that I need to study in order for me to fix this and improve upon it?

  
TIA!  
\*Picture will be in comments","['You might want to rethink your scoring system. Using the score for other calculations will be simpler if your scoring distribution is continuous, but by using `if()` and two different functions, you’ve created a discontinuous distribution, and you’ll always have to handle positive and negative values differently. \n\nThree simple, continuous approaches to scoring that I can think of:\n\nDifference: `score_sales - mean_sales`. This gives you 0 when the store is on the mean, for every dollar over (under), the score increases (decreases) by one. You $30 store gets a score of 20, your $5 score gets a score of -5, and your $1 store gets a score of -9. The downside to this is that it’s not symmetric; the lower end is bounded by `-mean_sales` while the upper end is unbounded. \n\nRatio: `store_sales/mean_sales`. This gives you a factor that can be multiplied to give a result proportional to the store sales. a $10 store gets a score of 1, a $30 store gets a score of 3, a $5 store gets a score of 0.5, and a $1 store gets a score of 0.1. \n\nScaled Ratio: `(store_sales - mean_sales)/mean_sales`. This kind of preserves much of the scoring characteristics in your attempt, with stored below the mean receiving negative score and this above receiving positive scores. A $10 store gets a score of 0; $30 gets 2; $5 gets -0.5; $1 gets -0.9. This scoring system is bound by -1 on the low side and is inbound on the high side.', '(C17-C2)/C2 or equivalently C17/C2 - 1 leads to a score of -1 if there are no sales and 0 if the sales approach the median. There is a big step between the two cases, however - as soon as you reach the median the score jumps from 0 to 1. You can use C17/C2-1 everywhere, then 30 has a score of 2, 20 has a score of 1, 10 has a score of 0, and 0 has a score of -1. No jump in the score anywhere. That means your formula is just `=C17/$C$2-1`.', 'I think that you are not calculating what you want. With your formula if the sales would be $1, then you would have a score of -0.1 (which would imply that is doing better than a store that sold $5...)', 'This was perfect! Thank you for taking the time to sum all of this up. \n\nI believe what I’m looking for is your scaled ratio definition. I will try that in a new version of my document', 'Is there a particular term of what I am trying to accomplish? It seems like that formula accomplishes the appropriate reward for all my accounts above the median. So, now I would just need to accomplish a function/equation to account for just how little stores do.']",0,7,https://www.reddit.com/r/statistics/comments/13kyhku/q_creating_a_excel_reward_program/
92,2023-05-18 20:36:58,[Q] Linear regression of RCT data with multiple groups,"We are currently running analyses on data from an RCT. The primary outcome is continuous. We perform linear regression with adjustment for site and according to the group allocation/treatment (A/B/C). One statistician is performing the analysis on data subset by the exact comparison (i.e, A v B, A v C and B v C). I perform the analysis without subsetting the data. The results are almost identical. Is there any advantage or disadvantage of subsetting or performing the analysis without subsetting first?","['You did a multi site study without discussing this in a data analysis plan?\n\nAlso was treatment randomized by individual or by site location (cluster randomized trial)? How many treatments and sites are there? Also comparing the three pair wise treatments sounds like a post hoc test rather than the primary testing that should be conducted.', 'It is not clear what the two different analyses are. You have 3 treatment groups, and the statistician you mention performs all pairwise comparisons between those groups.. But what are you doing that is not this?', ""If I understand correctly, your colleague is subsetting the data by site, and then evaluating every treatment contrast in each subset. If that's the case, this should provide equivalent effect estimates to fitting a model to the full data that adjusts for treatment group, site, and the treatment group-site interaction. However, the standard error associated with the estimates will differ across the two methods, because the model on the full data has more information (and makes a homoskedasticity assumption!). Based on your other comments you did not include the treatment-site interaction in your model, so that the two approaches provide similar results is likely because the interaction effect estimate is small.\n\nPersonally, I would adjust for site in a model rather than subsetting the data, if only because subsetting the data and computing the contrasts is a lot more work than just fitting the model!"", 'I have used mixed models before, I am just not sure why it would be a better alternative to linear regression in this case. Were you thinking about nesting the participants on their respective site?', 'Yes, I would think the partial pooling across `site` would be preferable to ""full pooling"" by not including `site` at all, or the ""no pooling"" by using dummy variables. The subsets are honestly a weird approach but it sits in the ""no pooling"" side.\n\nAt the least, it should be in your decision set, especially given you are debating the ""poles"" and not the mixed model sitting between them.\n\nEdit: clarified after a re-read of the whole thread']",14,20,https://www.reddit.com/r/statistics/comments/13kxz3p/q_linear_regression_of_rct_data_with_multiple/
93,2023-05-18 11:06:59,[Q] Correlated binary outcomes — modeling approach?,"I’m an Assistant Professor (not in statistics or mathematics) and I’ve been getting differing responses from biostatisticians and I think I’m overthinking the problem. 

I have two binary outcomes, Y1 and Y2. Y1 is the strongest predictor of Y2, both theoretically and as identified by meta-analyses. 

X is also binary, and based on group membership (Group 1 v Group 2). — for now, let’s ignore covariates. 

Y1 happens in the life course before Y2, by approximately 9-12 months. 

The data are collected through a cross sectional survey. I have approximately 625 cases, which are approximately equal split for group membership (X). 

How should I take into account the correlation between Y1 and Y2? Putting this into a multilevel modeling framework doesn’t make complete sense. I wasn’t sure if a joint modeling approach was more appropriate…thoughts?","['What model to use depends heavily on the question you are trying to answer. Not saying I will be able to help you with that but it usually helps tremendously when the researcher states the question they want to answer and/or states what kind of statements they want to be able to make after the analysis. So, I humbly suggest you start with what.', 'To echo this sentiment, what relationship are you most concerned with?  Are you more concerned with how X is related to Y2, but you need to take Y1 into account?  A mixed effects model still may be the best solution, but the relationship of interest and question at hand need to be more clearly defined.']",2,2,https://www.reddit.com/r/statistics/comments/13kmq62/q_correlated_binary_outcomes_modeling_approach/
94,2023-05-18 06:40:20,[Q] how do I find the frequency of a term in british newspapers for certain years?,"
Trying to find out how many reports included the word ""obesity"" and/or ""smoking"" in 2011 and 2021 in British tabloids. It would be good to either have this as across top 5 british tabloids, or just compare the frequency of the term within 1 tabloid between the years stated. 

I've been trying to figure out how to do this in an accurate way but im struggling. Will need to source this for a project for Uni. 

Anyone have any idea? Thanks :)","[""Do they have accessible online archives?\n\nAnyway, that's not a statistics question, but a question about British tabloids."", 'Try the British Newspaper Archive, which may be accessible from the reference section of your local library, or at least the bigger ones.\n\nhttps://en.m.wikipedia.org/wiki/British_Newspaper_Archive', 'No really a statistics question but you might need some web scraping skills and then the ability to form a table from the data.\n\nMaybe there is a python  or R subreddit that can point you in the right direction.']",1,3,https://www.reddit.com/r/statistics/comments/13kgl2y/q_how_do_i_find_the_frequency_of_a_term_in/
95,2023-05-18 02:13:50,[C] Can an accounting graduate do a master's in statistics and actually find a job within the field?,"Hello. As the title implies, I am a recent accounting graduate. I am currently working on the role. But the problem is, I don't really like being an accountant. So I decided to do a master's degree because I love research, and I would love to even do a PhD right after if I could. I am looking for a master's degree that will allow me to switch careers, and I was thinking about statistics. I really like it, and it allows me to go into data analysis fields, which I adore. The question here is: can an accounting graduate finish a master's in statistics, and even if I do, will I be able to find a job with only a masters in statistics ?  
  
PS: Another option I have is a master's in business analysis and data analysis. It's mainly practical stuff like R, Python, SQL, etc., but it doesn't have statistics classes.","[""imo even if you have a bachelor's degree in fish psychology, if you get a MSc Statistics from a good uni and learn a lot, you'll be fine - you can also leverage it as you finally finding your path or something like that\n\nabout MSc Business Analytics, I am biased against it - often business schools offer it to get some extra $$$\n\nymmv"", 'You would need a solid math background to do a masters. Did you take stats classes before? How is your knowledge of linear algebra and calculus and probability? These questions will determine the answer to your question', 'Probably pretty hard to do a masters in stats from a degree in accounting. You’d be missing a lot of foundational knowledge. \n\nKinda like going from a bs in maths to a ms in engineering.', "">imo even if you have a bachelor's degree in fish psychology, if you get a MSc Statistics from a good uni and learn a lot\n\nI did this exact thing, but the masters was from a middle of the road uni. Still got a great job."", 'Which field?']",3,9,https://www.reddit.com/r/statistics/comments/13k9jw5/c_can_an_accounting_graduate_do_a_masters_in/
96,2023-05-18 02:02:50,[Q] Applied Statistics vs Data Science?,"Are these two terms for fields that are generally the same? I'm currently interested in both but I'm graduating with a bs in stats next year and was trying to figure out which of these two more align with my interests for grad school and work, or if they're the same. I really enjoy both coding and statistics.","['Anyone telling you ""it depends"" is wrong.\n\nStatistics is one of the most robust skillsets around, and University programs are mature. Most data science programs are poorly done -- frankly, they are turning into MBA programs (the programs draw lots of donors and hype and administration to waste money on, they fail no one, etc). Get the domain specific degree, not the new polished up turd.\n\nThere\'s almost nobody who knows less about what the job market wants than those at universities designing curriculum. Data Science programs are invariably poorly focused and crappy.', 'Data Science tends to teach a bit of everything - computer science, cloud technologies, data structures, ml, but not great depth.\nApplies Math will likely concentrate on stats and you will have much-much deeper understanding of how ml works. \nI see the difference being generalist vs specialist.', 'Data science masters programs are “tools” based degrees. Teach you tools. How to use xyz software, how to use xyz analytics tools, get you up to speed on the up to date technologies. And yeah sure sprinkle in your 1 or 2 stats classes and a class on ML and then a class on DL. \n\nSure, it teaches you tools. But tools change. The MSDS curriculums will have to change every 5 years due to the change in industry. But the stats programs are robust, because theory rarely changes and is still applicable. \n\nUltimately it’s up to you, but I’d say applied stats give you core skills that are robust ti change than a tools based DS curriculum.', ""I agree with this. Plus 'data science' because of the hype around it, may be more expensive as compared to statistics. At least in my country."", "">  what makes them distinct from statisticians is often knowing how to work with data\n\nAnd not really understanding the statistics they are using... I agree with the notion you put forward, but it's very much an explicit trade-off. Better ability to work in practice, but as a result less focus on the real statistics. \n\nI would never trade a good statistician for a good data scientist.""]",35,28,https://www.reddit.com/r/statistics/comments/13k999f/q_applied_statistics_vs_data_science/
97,2023-05-18 01:53:08,[Q] Can you use the Simpsons diversity index for ethnicities within schools?,"Hi guys, I’m working on a project for school where I’m using this dataset to compare charter schools and public schools and it includes some data for the amount of students of each race within a school. Is the Simpsons diversity index and appropriate metric to use when comparing diversity? Thanks 

https://catalog.data.gov/dataset/public-school-characteristics-2020-21","[""You *can*, sure. Who could stop you, it's not like there's a Statistical Weapons and Tests team going to come knocking.\n\nIt's a perfectly reasonable measure and has been used (*under various names*) in multiple application areas.\n\nThe main issues would be  - \n\n*Does it serve your needs?* (there's multiple sub-issues you may need to consider, depending on your needs), and \n\n*Will your intended audience be happy to accept it?*"", '> I’ve never seen it used (edit: in the context of social science data)\n\nYou almost certainly have, but called a Herfindahl index or an ""effective number of"" somethings which is just 1/herfindahl.  Same thing.\n\nFor your data source, it\'s fine.  With data sources that follow the Census approach of separating race and hispanic ethnicity, it might get unwieldy.  With data sources that allow respondents to select more than one racial category, you\'d have to figure out what to do.\n\nThe real answer tho is to stop and look to see how people studying school populations measure racial and ethnic diversity, and do that for now.  It might be they just use percent who aren\'t nonhispanic white.  In which case do that for now and start arguing for a diversity/entropy index after you get tenure.', 'That’s a tricky question. I’ve never seen it used (edit: in the context of social science data). It’s not impossible, but there are some things you’d need to consider.\n\n1. Will it be read as problematic to use an index for species on the somewhat blurry human racial/ethnic divisions?\n\n2. How would you handle multiracial folks? The intersection between race and ethnicity (Black Hispanic, white hispanic, etc)?\n\n3. Are historically underrepresented minorities (Black and Hispanic) considered?\n\n4. How many subdivisions of race and ethnicity will you use? Pacific Islander & Asian as one category (AAPI) or two? Do you subdivide further by which Asian country someone comes from? How does that impact the index? \n\nHaving typed that out, I’d say Simpson’s index isn’t well suited to understanding human diversity. You want to figure out what kind of diversity you care about, and then use a metric for that kind of diversity.', ""I don't think any of those concerns are specifically issues with Simpson's index but rather with classifying humans by ethnicity or binning literally anything into any discrete categories really. For example, you could raise the same sort of concerns for clarifying the diversity of chairs types. What counts as a recliner? Is a lazyboy the same thing? Is a seat in a car also a chair? Etc.\n\nSimpson's index is literally par for the course in ecology, so I think it's exactly the sort of statistic to use for the OP's study. There will be no other statistics that measure diversity with a categorical variable that don't have the exact and issues that you raise, and trying to find a way to classify people on some sort of continuous numerical scale is not only unfeasible but would erase the reality that social categories are in fact things, leading to results that by contrast have no meaning.""]",2,4,https://www.reddit.com/r/statistics/comments/13k8zve/q_can_you_use_the_simpsons_diversity_index_for/
98,2023-05-17 23:54:41,[Q] How to calculate the coefficient of variation in case of data unavailability?,"Hi all, I have an excel file with historical data, for which I wish to calculate the Coefficient's of variation to assess the volatility. However for one year there is no data available.  
  
Is it still OK to calculate it by using **STDEVA/Mean** in excel?  
  
Thanks in advance!","['There\'s two answers here:\n\nBasic: Yes, just go for it, it won\'t make a difference.\n\nAdvanced: You can impute the missing data. This is just a fancy word for ""add in an extra value"". For instance, day you had the following data set:\n\n2, 4, Missing, 1, 5\n\nYou could impute the missing value. If the data is a time series, take the average of the 2nd and 4th entries (so 2.5) to give\n\n2, 4, 2.5, 1, 5\n\nIf not a time series, take the average of the whole set to get\n\n2, 4, 3, 1, 5\n\nThere are other methods but I\'m just giving the basics here :)']",2,1,https://www.reddit.com/r/statistics/comments/13k5q0j/q_how_to_calculate_the_coefficient_of_variation/
99,2023-05-17 23:30:23,[R] [Q] Measure development-construct validity. Comparing correlation coefficients /how to measure effect size?,"I have average scores across items for all measures for each participant. All scales are interval level of measurement. My hypothesis is that my measure will be positively correlated with both other measures, but that the correlation will be stronger towards the convergent measure than to the discriminant measure, to a significant degree.

My measure:
N=185
Skewness is -1.69
Kurtosis is 4.158


Convergent:
N=183
Skewness-0.652
kurtosis=0.085.

Discriminant:
N=185
Skewness-0.178
kurtosis=0.496.

I think it's safe to say my distribution isn't normal. I ran Pearson 1-tailed because someone on the internet said if was okay (so if not oops I guess)

Convergent:
N=183
R=0.524**

Discriminant:
N=185
R=0.298**
Where **= correlation significant at 0.01 level.

Specific questions: 
Even though my distribution isn't normal, is it problematic that I used pearson correlation?

Do I need to do some sort of logarithmic transformation?

What analysis/tool/approach would be best to compare the correlations to each other to determine if the difference is significant? (It seems evident with one being moderate and one being very weak, but is that all I need to say in the results section?)

How do I measure the effect size of the correlations (or the difference between them) with any of the tools I have or others?

Any help or resources are appreciated. It's been years since I took any stats classes. I'm almost done with this program though. 😪

I have access to g*power, SPSS, R (no experience using R) and any online free tools (i.e. danielsoper.com).","['You might want to test the difference in r’s for significance.', 'The link in my earlier post has a calculator that does that test.', 'You can use [this calculator](http://vassarstats.net/rdiff.html) which gives a p<.01. I don’t know how robust this test is to violating normality (the assumption may be bivariate normality). A log transformation will make your skews more negative. You could look at Tukey’s ladder of transformations or Box-Cox.', ""Thanks for the response and the stats tool! I did an inverted reflection (I'm not sure if that's the official name?) transformation and it worked. The correlations were only marginally lower (Convergent=0.458, Discriminant= 0.282) after transformation. All significant to p<0.01. \n\nI did run a power analysis in G*Power (exact bivariate normal model) and got 0.999 for convergent and 0.988 for discriminant. Honestly thought I broke the calculator but my dissertation chair just confirmed that I entered the right numbers in the right places."", 'I definitely do. I have no idea how to do that and my googling has been unsuccessful. Any specific tests and or programs/websites?']",3,6,https://www.reddit.com/r/statistics/comments/13k51rr/r_q_measure_developmentconstruct_validity/
100,2023-05-17 23:22:39,[Q] How to perform prewhitening on the input data using Python if the error terms are crosscorrelated?,R has filter(). How to do this in Python?,[],3,0,https://www.reddit.com/r/statistics/comments/13k4txs/q_how_to_perform_prewhitening_on_the_input_data/
101,2023-05-17 21:26:49,[Q] central limit theory - ANOVA with exponentially distributed data?,"Hi all, 

I'm not too experienced with statistics, therefore apologies in case this question is too basic. 

[This page](https://support.minitab.com/en-us/minitab/20/help-and-how-to/statistics/basic-statistics/supporting-topics/data-concepts/about-the-central-limit-theorem/) describes that even with a non-normal distribution I could use statistical tests that rely on normally distributed data (such as ANOVA), as long as my sample size is ""large enough"" (""*The central limit theorem lets you apply these useful procedures to populations that are strongly nonnormal*""). Now I am a bit confused, and can essentially think of two interpretations for what the site describes: 

**I**. No matter what the underlying (""true"") distribution is, if you sample often enough your population will be normally distributed in the end (and therefore you can use a test like ANOVA) 

**or** 

**II**. If your sample population is large enough, you can use a test like ANOVA, even if the sample population is not normally distributed. 

This is relevant for me because I would like to compare distributions which are exponentially distributed, and my sample size is around 150 samples per population. I included two example histograms of my populations [here](https://imgur.com/a/VqAk7dT).   
If my interpretation **I** is correct, then I could not use ANOVA, as my sample population is actually not normally distributed, despite it having a large sample size. If **II** is correct, then I could use ANOVA. 

Would be great if someone could help me understand, thanks alot!","['The Central Limit Theorem says that the sample means of sufficiently large and random samples will be normally distributed. That means even if your population is exponential, when you sample from it, the mean of your sample will be drawn from a normal distribution. That means you can apply all the tests and tools to your sample mean that statistics has developed for normal variables, even though the underlying population is exponential. Hope this helps!', 'Normality assumption on the ANOVA is a conditional one: the variable on each factor/treatment should be normal, not the marginal variable. This assumption is evaluated through residuals, not on the original variable.\n\n  \nThe assumption is needed to guarantee accurate p-values and confidence intervals on things like mean differences derived from the parameters of the model.\n\n  \nIf you really know that your data has an exponential distribution, you can use a glm with the gamma distribution (exponential distribution is just a particular case of a gamma distribution) and the log link to correctly handle that.\n\n  \nBut also, it is known that, under certain circumstances, the ANOVA is kind of robust to deviations from the (conditional) normality assumption, more if the sample size is big enough, so you could just apply that model as well.\n\n  \nFinally, to be completely free of any doubts, you always can use a non-parametric approach like the Kruskall-Wallis test or use bootstrap to derive accurate p-values (and confidence intervals only for bootstrap) which does not rely on normality assumptions.', 'Edit: Note that my responses are initially to the statements in your post; the later part of my comments then come after I look at your plots. (However, choosing your model based on what you find in your data and then performing tests on that same data is problematic.)\n\n\n> No matter what the underlying (""true"") distribution is, if you sample often enough your population will be normally distributed in the end (and therefore you can use a test like ANOVA) \n\nThis is nonsensical; the distribution of the ""population"" you\'re drawing from doesn\'t change when you sample from it. The set of values in the population will remain the same.\n\nLook at the title of the linked page: *The* ***means*** *of large, random samples are approximately normal*(emphasis mine).\n\nThat bolded word matters!\n\n>  If your sample population is large enough, you can use a test like ANOVA, even if the sample population is not normally distributed. \n\nThat\'s literally what  the part that you quoted just above I and II claims.\n\nThe claim is broadly correct - you could use ANOVA with population distributions that were not normally distributed, including something close to exponential. However, the page\'s claim is not quite accurate and it\'s necessary to understand what you\'re getting. \n\nWhat is really being claimed when they cite the CLT: That the type I error rate will be close to your chosen alpha for large enough n.\n\nThe inaccuracies -- \n\n(i) the null distribution of the ANOVA statistic doesn\'t just rely on the sample means being approximately normal; a CLT-like argument is not sufficient - but the conclusion is often reasonable;   \n(ii) the CLT doesn\'t really \'work\' with just any distribution, and the claim about accurate ANOVA type I error rates doesn\'t really hold for just any distribution;  \n(iii) it\'s not just accurate rejection rate when H0 is true that you need to worry about -- you should also care about power; and  \n(iv) non-normality is not the only thing to worry about and it\'s usually not the most important thing to worry about; \n\nFirstly if what you\'re worried about is type I error rates and the other assumptions of ANOVA hold, you can get accurate type I error rates in other ways, without any parametric assumption (e.g. permutation test based on an ANOVA statistic and at smaller sample size -- not a rank-based test, though, since it would no longer be a comparison of means without additional assumptions -- why worry about something that doesn\'t compare means if you can compare means about as easily?).  \n\n[Note that if you *are* focused on type I error rates, the data may not be especially informative for some of the assumptions, since the null is likely false. You need to worry about the circumstances that you believe would apply when H0 is true. That\'s not going to make exponential distributions normal but it might save worrying over-much about heteroskedasticity, for example, since if you would only see that under H1 - a perfectly reasonable assumption under some situations - it won\'t affect type I error rates.]\n\nSecondly for exponential populations, there\'s more powerful (asymptotically, most powerful) approaches readily available - you can do an ANOVA-like comparison of means using a generalized linear model instead of a linear model; you can literally fit the exponential model you have declared. If you care to detect small effects, it may pay you to also worry about power.\n\n\n> and my sample size is around 150 samples per population\n\nThat\'s large enough that if you *did* have exponential populations, the distribution of a mean would be pretty close to normal and the type I error rate in ANOVA would be reasonably close to accurate. But see the comments below!\n\n---\n\nLooking at your data:\n\n1. You appear to only have two groups. Why ANOVA in the first place?? Why not just a straight two-group comparison of means?\n\n2. Those aren\'t exponentially distributed as the term is usually interpreted. Exponential distributions are on (0, ∞), not (1, ∞). \n\n   https://en.wikipedia.org/wiki/Exponential_distribution\n\n   You could subtract 1 from both and have regular one-parameter exponentials, Depending on what your data are, subtracting 1 may make perfect sense (be interpretable in its own right), or it might not.\n\n   If it is interpretable - you haven\'t said what you\'re measuring - then I would encourage you to do so, though the analysis could be done even if it wasn\'t interpretable, since it would still be a comparison of means if you shift everything by 1.\n\n   To rephrase -- you *might* perhaps have a shifted exponential with a known shift parameter (1). If that were the case\n\n3. Regarding the claim of exponentiality (after taking that shift into account). \n\n   One thing that worries me there is the spike at 1 in both groups. If you have many values exactly at 1 (or indeed, many values tied at *any* number) then you *don\'t* have (shifted) exponentials. \n\n   The second thing that worries me in the histograms is that second group looks too skew to be exponential. In some specific circumstances this might not matter very much. For example if group 1 is like a control and group 2 is like a treatment, then if *under H0* you assume the treatment has no effect at all, the situation under H0 would be that both groups should behave like group 1. In that case a non-exponential group 2 would not be a problem for significance levels, since likely the alternative is true -- if the shape is only becoming more skew as the mean grows, that wouldn\'t hurt type I error rates.\n\n---\n\nIt would help to understand what your response variable is.', 'No matter your underlying distribution, the distribution of *sample means* will approach normality. Your standard error is essentially the standard deviation of the sample mean distribution.\n\nYou can then run your usual tests on this new distribution, such as a hypothesis test. This test evaluates the question of how likely your sample mean is under different hypotheses.', '> The Central Limit Theorem says that the sample means of sufficiently large and random samples will be normally distributed\n\nwell, no it doesn\'t say that, though that\'s  *approximately* true in a wide range of situations. It follows from Berry-Esseen. This is not my main issue, though.\n\nThe bigger issue is how you leap from approximate normality of sample means to the claim that in effect  *ANOVA works as if you had sampled from normal populations* (""That means..."").  F statistics are not simply functions of means,  but a ratio of two variance estimates. The numerator might eventually  get close to the right distribution via a CLT like argument (since it is just a function of means), but the denominator is also random and for the F statistic to have an F distribution under H0 you need a couple of other things to be true. \n\nThe CLT doesnt provide any direct help there. It takes other results to get you to \'ANOVA works\' as n goes to infinity.']",12,11,https://www.reddit.com/r/statistics/comments/13k1qkm/q_central_limit_theory_anova_with_exponentially/
102,2023-05-17 20:11:05,[Q] Need an explanation for the probability of a plane crashing,"According to a very widely quoted statistic on the internet, the probability of a plane crashing is about 1 in 11 million. But, I can't seem to find out how exactly this figure is calculated and what the logic behind it would be.

If someone could cite a research paper that reports this, that would be a big help.","['Not sure of the source, but you could do a crude calculation yourself. I assume you’re only talking about airliners because general aviation is actually fairly dangerous. If you’re risk-averse, don’t ever get in a plane that isn’t being flown by a commercial pilot and maintained by a professional crew.\n\nIf you’re talking airliners only then you can find some kind of record for nearly all crashes going back several decades. That would be your numerator. Then you’d need to get some estimate of all flights, which might be a bit harder but you should be able to get a ballpark estimate without too much trouble (especially if you keep your time window small like the last 5 or 10 years). This would be your denominator. \n\nFormulated this way, the probability you end up with would be the probability of any random flight on a commercial airline crashing. One could argue that the better stat to calculate is something like “crashes per 100k miles flown” which would the allow you to differentiate between people who only make short flights and people who tend to do long-hauls, but that’s an extra layer of complexity.', 'The figure of 1:11,000,000 is not a published figure, it\'s one of the ""old wive\'s tales"" numbers to demonstrate the relative safety of air travel.', 'It only depends if OP cares about those dependencies. The probability of any random flight crashing is absolutely an interpretable and potentially interesting stat, even if it does combine commercial and general aviation. In fact, it gets easier if OP just wants to look at commercial flights because records are more reliable.', 'The best source seems to be [this link from PBS,](https://www.pbs.org/wgbh/nova/planecrash/risky.html) which is a journalistically well-regarded source.\n\nThe numbers are from 20 years ago, but it gives enough clues as to where the numbers come from.\n\nThe most important part to understanding the numbers is to realize that out of all Americans, most fly only one or two times per year, if that.  Others fly for work multiple times per week.\n\nIt seems that for all Americans, the odds of dying as a commercial airline passenger are 1 in 2 million per year.  For the typical American (ie, a non-frequent-flyer) the odds are 1 in 11 million per year.', 'The fraction of all flights crashing might be possible to estimate (it would still come with various caveats, like what counts as a flight and what counts as a crash), but it\'s a useless number because it would combine completely different things. You cannot fly on an ""average flight"" because there is no such thing.']",4,13,https://www.reddit.com/r/statistics/comments/13jzxv8/q_need_an_explanation_for_the_probability_of_a/
103,2023-05-17 17:12:57,[Q] measurement error on binary dependent variable,"(I'm working with Stan but could switch to something else if needs be) 

I have some data with a binary dependent variable *y_1*. *y_i* comes from an annotator annotating a series of data points, say 100. An experiment showed that if multiple annotators tried to annotate the same data point, about 90% of them would agree for that data point. I would like to include this uncertainty into a model:

    y   ~ y_i (???)
    y   ~ bernoulli_logit(mu)
    mu  = ...

I've looked into measurement error models, but they all seem to assume normally distributed variables, and an error term in terms of the standard deviation. I thought maybe with an explicit probit model on y and an sd that approximates 10% error, but I'm not sure this is sound.

Any suggestions are welcome.","[""My recollection is you can't do anything about measurement error in a binary dependent variable. That's from a frequentest perspective though I'm not sure about bayesian modeling if it\n\nAs you suggest, the error can't be normally distributed. It also can't be independent of the true binary, because if the true binary is 0 the measurement error is either 0 or 1, and if the true binary is 1 the error is either 0 or -1."", ""I would've thought that in Stan you could parameterise your outcome variable as a latent variable and have that be drawn from any distribution you fancy (with some known error). \n\nIt might be worth simulating some data, parameterising your model with a normally-distributed outcome, using the `brms` `mi()` syntax for [measurement error models](https://bookdown.org/content/4857/missing-data-and-other-opportunities.html#measurement-error), and then bastardising the Stan code it generates to get to where you want for your own model."", 'If you have a validation sub sample, there are lots of options for binary data measurement error, but I suspect you don’t have one. In that case, your best bet is something called a probabilistic sensitivity analysis. Basic idea is you have a justifiable distribution for the sensitivity and specificity of your outcome. Randomly sample values from that distribution, randomly reclassify your outcome based on those values and re-run the analysis. Rinse and repeat 10,000 times and rank order the results. Now you have a distribution of results that you can use to define interval estimates that account for the measurement error.', 'They’re called misclassification models. I don’t know anything other than the terminology, but it might give you a starting place to look.', 'I haven’t used Stan in quite some time, but this may help. 90% of annotations agreeing essentially means that your label yi is equal to the true value of the dependent variable zi 90% of the time. Normally you assume the label is correct 100% of the time, so P(zi) = (1)(yi) + (1 -1)(1 - yi) = yi where P(zi) is the probability that zi = 1, i.e. that zi is the true class label.\n\nIn your case, there is a 10% chance that the true label is the opposite of the one given by the annotator, so P(zi) = (0.9)(yi) + (1 - 0.9)(1 - yi).']",10,9,https://www.reddit.com/r/statistics/comments/13jwgp7/q_measurement_error_on_binary_dependent_variable/
104,2023-05-17 10:38:57,[Q] Sports Tournament Question (don't have to solve it for me I'm just curious how you'd go about solving it),"There's a tournament. If everything's fair, what are the odds of a specific set of 7 teams that does not change never win this tournament? The tournament spans 30 years. These 7 teams are in it the entire time. However the # of teams in the tournament (besides them) changes over time  
  
26 teams for first 5 years  
  
27 teams for 1 year  
  
28 teams for 1 year  
  
30 teams for 17 years  
  
31 teams for 4 yeras  
  
32 teams for 2 years  
  
  
  
Would it be like 7/26 (5 times) x 7/27 (1 time) x 7/28 (1 time) Is that how you do it?","['You calculated the probability of these 7 teams winning ALL tournaments. Your basic formula is correct though. Just be aware that this formula calculates the probability, not the odds. \n\nP(these 7 teams never win) = (1-7/26)^5 * (1-7/27) * ... * (1-7/32)^2 = 0.000277 = 0.03%', 'so its of them winning every single tournament, not them winning just one? how would i find the odds of one of those 7 teams winning once?\n\nthanks!', ""The probability of these 7 teams winning exactly 0 tournaments is, following the formula I posted in my previous post, 0.03%. \n\nWhen you want to calculate the probability of them winning 1 tournament, I suggest to approximate the answer by assuming each year had exactly 30 teams. Makes calculations a lot easier while the answer won't differ much from the exact answer, and I assume you don't need an exact answer. \n\nIf we assume that the probability of these teams winning in a given year is 7/30 (there are 7 teams out of a possible 30 teams each year) and they compete for 30 consecutive years, the number of wins follows a binomial distribution with p = 7/30 = 0.23333 and n = 30. You can use an online calculator such as https://stattrek.com/online-calculator/binomial to see that the probability of them winning once equals 0.00315 = 0.315%.""]",2,3,https://www.reddit.com/r/statistics/comments/13jozj1/q_sports_tournament_question_dont_have_to_solve/
105,2023-05-17 10:37:19,"[Q] In regards to sampling in statistics, if I want to analyze carbohydrate content of a sample of 5 apples from a sample of 5 trees from each subplot (10 in total), which of all these is my sampling unit? Why?","I am having a hard time to tell which of all these is my sampling unit to define in my experimental design. I would be thankful if you could help me with this.

5 apples from 5 apple trees from each subplot. Each plot has an area of 4m by 20m (4 lines with 40 apple trees). and each plot is replicated 3 times.","[""You have a two-stage design here, if I'm reading it correctly. Your first-stage sampling unit is trees, and your second-stage sampling unit is apples."", 'so apples are ""nested"" in trees (one tree have multiple apples, one apple cannot belong to multiple trees). Does the same happen for trees and subplots? How many trees are there per subplot?', 'Yes, the apples are nested in the trees and one apple cant belong to multiple trees.\n\n5 apples from 5 apple trees from each subplot. Each plot has an area of 4m by 20m (4 lines with 40 apple trees). and each plot is replicated 3 times.', 'Would be good if you describe your ""population"" as clear as you can. How many plots are there, how many trees per plot and if possible and approximate of how many apples per tree.\n\n>each plot is replicated 3 times.\n\nDoes that mean that you have just 3 plots?\n\n  \nOnce those things are defined, the next thing you should have clear is the purpose of the experiment: why are you doing this? Are you interested on the apples yield, irrespective of the tree, are you interested on the yield of the tree or rather on the yield for the plot?\n\nThis will determine your sample unit']",1,5,https://www.reddit.com/r/statistics/comments/13joy8n/q_in_regards_to_sampling_in_statistics_if_i_want/
106,2023-05-17 08:47:22,[Q] Forcing a maximum entropy distribution on normally distributed variables for causal analysis via do-caluclus?,"I have two normally distributed variables X and Y. I am interested in a ""causal"" analysis of X → Y. I've been reading up on Judea Pearls do-calculus, and found some papers by that show that the mutual information I(do(X) ; Y) is equivalent to computing I(X ; Y) after forcing a maximum entropy distribution on X. 

For discrete data, this would be easy with resampling, since the maxent distribution on a finite set is the uniform distribution. But the Gaussian distribution is already the maximum entropy distribution given μ and σ - how would I compute I(do(X) ; Y) in this case?",[],14,1,https://www.reddit.com/r/statistics/comments/13jmip5/q_forcing_a_maximum_entropy_distribution_on/
107,2023-05-17 06:30:49,how to chose G or R side random effects [Q],"i have a data with 150 patients and 30 readers. let's say the readers are from two different groups, old reader and young reader. each reader exam each of the 150 patients and make a binary judgement (positive or negative). now i want to compare the rate of positive between the two reader groups how should I specify the random effect in a mixed model? following sas codes give very different lsmeans and I assume are from whether to specify on the G side or the R side. what's the difference in interpreting the lsmeans as well as the differences? any comments are welcomed.

    proc glimmix data=data;
    class group patient reader;
    model y(event='1') = group/ dist=binary;
    lsmeans group/ilink cl diff;
    random patient ;  #only random patient effect on the G side
    \#random reader;  #only random reader effect on the G side
    \#random case\\\_number reader; #random patient and reader effect on the G side
    \#random residual / subject = case\\\_number;  #only random patient effect on R side
    \#random residual / subject = reader;  #only random reader effect on R side
    run;",[],0,0,https://www.reddit.com/r/statistics/comments/13jjdcr/how_to_chose_g_or_r_side_random_effects_q/
108,2023-05-17 05:41:50,[Question] Where to start modelling my workplace power consumption based on past data,"Context: I’m a mechanical engineer and stats is not my strongpoint.
I’m trying to build a basic model of our power consumption in order to estimate the required fuel storage capacity for a new generator.
I have half-hourly power readings (kVAh) from the main incomer, and would like to generalise the peak/off peak consumption, and then be able to fiddle with things like shift start/end, and scale the capacity. 
The site has pretty obvious daily cycles in consumption as it is day shift only.

What tools/analyses/approaches would actually be useful for extracting reasonably reliable data here?","[""I'd suggest a look at the structure of Rob Hyndman's models in the electricity area, which should give you an idea of a good approach to model components of various kinds like those you mention. I'll see if I can locate a relevant paper or talk when I'm near my laptop\n\n\nEdit:\n\nTalk slides and two references here:\nhttps://robjhyndman.com/seminars/electricity-forecasting/\n\n\nYou may be comfortable with somewhat simpler models (and also may wish to omit some of the things he had to take account of) but it's a useful framework. Note in particular the use of additive models with multiple components for the log of the demand.\n\nWhile he's modelling aggregate demand rather than a single purchaser's usage, the general approach should be helpful. You may also need some additional terms for the special characteristics of your workplace, but they will tend to be like terms already in these models.\n\nIt might be possible to use Kalman filtering to do estimation and forecasting of components within such a framework via something akin to Andrew Harvey's Basic Structural Model (but more general, to account for some of the terms).  However, if you look at the Hyndman talk you will see that he describes using the bootstrap for some parts of the approach; that may well prove necessary, particularly if you're interested in reasonably accurate modelling of quantiles rather than just conditional expectations and variances (e.g. if you're looking to give prediction intervals, you might not be able to rely on a simple parametric model for the error term)"", 'Thanks - that looks very interesting but we’ll beyond my comprehension, let alone ability!\n\nBut it’s certainly opened my eyes to the challenges of electricity demand prediction']",1,2,https://www.reddit.com/r/statistics/comments/13ji5da/question_where_to_start_modelling_my_workplace/
109,2023-05-17 03:55:06,[Q] GARCH or ARIMA,"I’ve been working on understanding these two models and am going to perform a Value at Risk analysis. 

So I have two interest rates and the variable «diff» which is the difference between them at any given day. Easy put: it’s the risk premium between the interest rates. 

The diff is mean reverting and has autocorrelation. Edit: worth mentioning is that it’s a significantly positivt autocorrelation.

I’m hoping some of you can help me understand which one I could use and Why. As of per now I have used GARCH to predict the VaR and gotten a result which can make sense.

Hope you can give me some input, thank you! :)","['Given you have used GARCH to predict VaR, it suggests that the GARCH model  is well-suited for your analysis.', 'Further expansion:\n\nThe suggestion that the GARCH model is well-suited for your VaR analysis is based on the fact that you used GARCH to predict VaR and obtained meaningful results. The fact that the VaR you found can be argued to make sense further supports the suitability of the GARCH model for your analysis.\n\nAn aside:\nI too am working on a trading system using GARCH and ML.', 'IF data stationary:\n\nThen ARIMA\n\nELSE:\n\nGARCH\n\nEDIT: If I remember correctly, if the residuals are autocorrelated, you might need to prewhiten your data. Also, is your data stationary (Constant mean and variance)? To add to that, are your coefficients significant? Are your correlogram values within the bounds? Are your q-q plot values along the normal line? Does distribution of residuals follow the normal plot? Is the plot of your residuals random?', ""Thank you. I've done an Augmented Dickey-Fuller test and gotten a p-value of 0.000, so from my understanding I can conclude the data is stationary.\n\nFor the added part: My coefficients are significant. I don’t think the correlogram values are within the bounds. The q-q plot shows that pretty much all my data except some extreme values at the tips are on the line. I don’t think the distribution follow a normal distribution entirely. I’ve done a Shapiro-France test and it suggests non-normality."", 'GARCH and ARIMA are models, whereas Bayesian methods can be used to make inferences about these models. apples and oranges.']",5,12,https://www.reddit.com/r/statistics/comments/13jfdse/q_garch_or_arima/
110,2023-05-17 01:55:16,[Question] Testing normality for RM ANOVA,"I know that normality is an assumption repeated-measures ANOVA, but I’m confused about how it works. If I have two within-subjects factors, each with two levels, for a total of 4 measures, how to I find out if the DV is normally distributed or not? Do I need to test for normality for all 4 of those measures? Or is there some way to assess the overall normality of the DV?   
I’m sorry if this is a stupid question. I’ve looked at so many different websites and sources to try to get an answer but they all just tell me what the assumptions are, then direct me to a standard page describing how to check them for a regular ANOVA, but I don’t understand how it applies to repeated measures.","['> how to I find out if the DV is normally distributed or not?\n\nThere is no assumption that the CV is normally distributed; the assumption is about the *errors*. \n\nThis question is asked so often that we really need a pinned post, and a search of the subreddit will give you dozens of similar questions with in-depth answers. In short, [you should not be testing for normality, ever](https://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless).', 'It is an assumption, yes. You should not verify this assumption by *testing* it, for all the reasons outlined in e.g. [this](https://old.reddit.com/r/AskStatistics/comments/13gy1xk/what_do_i_do_with_data_that_failed_shapirowilk/) thread.\n\n> So, even if I\'m looking at the normality of the errors, is it the normality of the errors for each of the four measures of the dependent variable?\n\nWhat do you mean by ""each of the four measures"" of the DV? Are you measuring one DV at four timepoints? What is your model, exactly?', 'You have 3 effects (2 main effects and an interaction). You compute a score for each subject for each effect. The assumption is that for the effect of A, scores computed as (a1b1+a1b2)-(a2b1+a2b2) are normally distributed. It’s the same idea for the main effect of B. For the interaction, (a1b1 + (a2b2) - (a1b2+a2b1) is assumed to be normal. Basically, apply coefficients 1,1,-1,-1) for A, 1,-1,1,-1) for B, and (1,-1,-1,1) for the interaction to the 4 measures: a1b1, a1b2,a2b1, a2b2. The variances of these differences are the error terms.', ""I'm a bit confused. I've always learned to check for normality by looking at histogram, checking skewness/kurtosis, etc. and every source I've looked at lists this as an assumption. I'll have to look into this more as I clearly am misunderstanding something.\n\n&#x200B;\n\nWhat I am specifically asking about though is how assumption checks apply to repeated measures. So, even if I'm looking at the normality of the errors, is it the normality of the errors for each of the four measures of the dependent variable? This is what I do not understand.\n\n  \nI have searched the subreddit and didn't find anything that answered my question."", 'Oh I see. I\'m sorry, that was poor wording on my part. I don\'t intend to ""test"" the assumption using the Shapiro-Wilk\'s test. Check would have been a better way of putting it. I just want to be able to look at the data and see if there are any clear violations of normality that might alter my interpretation of the results. Thank you for providing that link, it is very helpful! \n\n&#x200B;\n\nEssentially, yes, I am measuring one DV at four different time points. All participants are asked to provide a rating of four different advertisements that vary on 2 different characteristics (each with 2 levels). So the dependent variable, rating, is measured four different times.']",2,5,https://www.reddit.com/r/statistics/comments/13jc8fp/question_testing_normality_for_rm_anova/
111,2023-05-16 22:02:02,[Q] Linear mixed model for a repeated measures design: Is it correct to model an independent regression line for each level of the repeated-measure?,"Hi all,  
  
I did an experiment in which 30 people listened to 3 songs (""A"", ""B"", and ""C"") in a random order. Also, for each participant and song, I got the mean ""body temperature"" and the mean ""heart rate"" during the listening task. This is, I have a repeated-measures design with 1 factor (song) and 3 levels. Also, for each participant I have a measure of ""years of musical training"".  
  
I am interested in knowing the degree of association between ""body temperature"" and ""heart rate"" across songs. Specifically, I want to know how well can the measures of ""heart rate"" predict ""body temperature"" and whether this association changes depending on the song the participants were listening to. Additionally, I want to add ""years of musical training"" as a covariate that might explain some of the variance in the model.  
  
I tried to make a mixed model with R by using lme(), but without much success, because the summary of the ouput did not give me statistics of the associations between ""body temperature"" and ""heart rate"" for each song level. I used: lme(BodyTemperature \~ HeartRate, random = \~1|Participant/Song, data = data, method = ""ML"") , and I suspect this is not correct because I should create a regression line with a different intercept and slope for each song. Additionally, to know the ""degree of association"" between the two variables, which coefficient should I look? The mean square of the model?  
  
At this point, my questions are: (1) would it be correct to make a linear model for each song independently and look at the mean square of the model? Something tells me this would not be correct but I quite not grasp why, (2) If it is not correct, how can I model different intercepts and slopes for a repeated-measures design? I cannot find anything similar on the internet.  
  
Thank you in advance!","[""> [...] I should create a regression line with a different intercept and slope for each song. \n\nA line requires at least 2 points but you only have 3 points per participant, one for each song. As you have only one measurement per song, you can't assess the relationship between heart rate and body temperature *within* participants and songs. I think what you could do is this (btw: It is considered bad practice to name your dataset `data`):\n\n    lmer(BodyTemperature~HeartRate + Song + (HeartRate|Participant), data = data, REML = FALSE)\n\nThis uses the `lme4` package instead of `nlme`. This model would fit different relationships between BodyTemperature and HeartRate for each participant based on 3 measurements each (may be dubious due to small number of points). It also contains a fixed effect for Song allowing the mean BodyTemperature to be different in each Song."", 'Hi!\n\nThanks for the help. There is something I do not quite understand about your answer. I did not intend to assess the relationship between heart rate and body temperature *within* participants, I am not interested in that. Rather, I am interested in the ""treatment"" effect of the songs, this is, whether there is more association between heart rate and temperature for song A compared to song B. In this case, the model would be built with 30 data points (the 30 participants) for each song. Am I correct?Would your model still be useful for this? \n\nSorry if I did not explain myself properly before :)', '> Rather, I am interested in the ""treatment"" effect of the songs, this is, whether there is more association between heart rate and temperature for song A compared to song B.\n\nThen include an interaction between Song and HeartRate:\n\n    lmer(BodyTemperature~HeartRate*Song + (1|Participant), data = data)\n\nThis will allow the relationship between HeartRate and BodyTemperature to differ between the three songs.', 'Thanks a lot!! Two last questions:\n\n(1) By building this interaction model am I fitting one line with one intercept and slope for the three songs? Or am I doing three lines with the same intercept and different slopes? Or...? \n\n(2) To determine the degree of association between the two variables (how well we can predict body temperature based on heart rate depending on the song) what should I look at in the output of the model? Sorry for the naïve question but I am trully lost here :)', '1) You\'re fitting a seperate line for each Song while allowing each participant to have a separate intercept (but not a separate slope).\n2) This is difficult to answer because ""degree of association"" can mean many things. For example: the slope coefficients tell you how much the BodyTemperature changes if HeartRate increases by one unit, separately for each song. But the coefficients do not quantify ""how well"" the predictions are. To determine this, you could compare a model without HeartRate to a model that contains it and look, for example at the R^(2). But this is not visible in the default model output.']",2,7,https://www.reddit.com/r/statistics/comments/13j642w/q_linear_mixed_model_for_a_repeated_measures/
112,2023-05-16 21:06:05,[Q] Wealth distribution deciles per country,"Is there any website (or multiple of them) which present recent data on the wealth distribution deciles per country? In every US related research, it seems there are only the top 1%, then 5%, then 10%, then 50% (or something like this). The OECD also has publicly available results with a similar scale. 

After much effort, I found the French INSEE's [""Patrimoine net des ménages""](https://www.insee.fr/fr/statistiques/5371259?sommaire=5371304) (Households' net wealth) which gives the deciles in the downloadable data sheet, and this is exactly what I'm searching.

Thanks for your time and ideas!","['Have you tried computing this from census.gov data?', 'Have you checked the [World Inequality Database](https://wid.world/)?', ""I tried looking for the data, though I must say I haven't found anything satisfactory as of yet, if you have any link more precise, I'd be absolutely grateful.""]",1,3,https://www.reddit.com/r/statistics/comments/13j4pjh/q_wealth_distribution_deciles_per_country/
113,2023-05-16 20:59:42,[Q] Which forecasting model should I use?,"I am currently taking a basic statistics course at a univerisity and writing my final assignment.

The assignment requires me to:

Collect datas of Food imports of Malaysia and UK from 1997 - 2019.

Then, it asked me to use Excel's Analysis Toolpak to build three time series models (Linear, Quadratic and Exponential for each countries) and include the Summary Outputs of the significant models (which is determined by the p-value of the coefficients) in my paper.

Then, it asks me to recommend one model for each countries by calculating and comparing the SSE and MAD errors. Then calculating the predicted Food Imports in 2024, 2025, 2026 for both countries.

After finishing my calculation, I noticed that the Quadratic trend is not significant (the p-value of T^(2) is less than the significance level of 0.05), however, it has the lowest errors (both SSE and MAD) compared to the other models.

I asked my professor about it, he told me that it is a rare case in this assignment. Usually, the model has the lowest SSE and MAD errors is also significant. He refused to answer directly and told me that he wanted to hear my argument. I did some research, but no result could be found.

I would really appreciate it if someone could give me some arguments or guidances to start with. Thank you.","[""Adding more columns can _only_ improve fit in-sample. So it is trivially the case that the quadratic fit would have a lower in-sample SSR than linear.\n\nSince for **ax^2 + bx +c**, the coefficient **a** will be = 0 _if and only if_ it reduces the SSR. And in that case it's just linear. This implies **a** is not equal to 0 when it improves the SSR to do so.\n\nAssuming your professor isn't pulling your leg and isn't doing anything wrong, this to me implies that the exponential model must be the best fit (since it does not strictly overlap the way quadratic does with linear), and it's possible you didn't implement it properly."", ""Exponential fit, I'm assuming, is semi-log form log(y)=mx+b. This is totally different from linear fit, no guarantee it fits better or worse than a linear model. Quadratic fits better than linear because linear feature space is a strict subset and the y vector is the same.\n\nAlso, no guarantees at all that MAD is lower, even with quadratic vs linear; OLS minimizes SSR, not MAD."", 'Thank you for providing these insight. Have an upvote.', ""The traditional way to compare models is AIC.  Unlike R2 and RMSE, AIC is penalized by model complexity.  A model that gives marginally better R2 but with more terms will have worse AIC.  It might be in an undergrad-level textbook.  It's definitely in graduate-level textbooks on regression. \n\nA more complex option is cross validation.  This is a way to test the model performance on data held out from the model fitting.  This is the most rigorous way to test model accuracy.   \n However, it requires writing code."", 'This is the kind of question LLMs answer really well.  I just pasted your entire question into ChatGPT and the answer is excellent.']",4,9,https://www.reddit.com/r/statistics/comments/13j4jkd/q_which_forecasting_model_should_i_use/
114,2023-05-16 19:51:19,[S] Python package for the synthetic control method,"Out of frustration at not being able to find a small, simple and verifiably correct Python package for the synthetic control method, over the last few months I've worked at making one, and it's now mostly in a ready state available [here](https://github.com/sdfordham/pysyncon) and on Pypi.

You can do the usual synthetic control method with it, or several of the variations that have appeared since (augmented, robust and penalized). It also has methods for graphing and placebo tests.

There's worked examples from several sources worked out in notebooks [here](https://github.com/sdfordham/pysyncon/tree/main/examples) that reproduce the weights correctly, namely from

* The Economic Costs of Conflict: A Case Study of the Basque Country,  Alberto Abadie and Javier Gardeazabal; The American Economic Review Vol.  93, No. 1 (Mar., 2003), pp. 113-132, ([notebook here](https://github.com/sdfordham/pysyncon/blob/main/examples/basque.ipynb)).
* The worked example 'Prison construction and Black male incarceration'  from the last chapter of 'Causal Inference: The Mixtape' by Scott  Cunningham, ([notebook here](https://github.com/sdfordham/pysyncon/blob/main/examples/texas.ipynb)).
* Comparative Politics and the Synthetic Control Method, Alberto Abadie,  Alexis Diamond and Jens Hainmueller; American Journal of Political  Science Vol. 59, No. 2 (April 2015), pp. 495-510, ([notebook here](https://github.com/sdfordham/pysyncon/blob/main/examples/germany.ipynb)).

I'd appreciate any feedback and also thoughts on what else may useful in such a package 🙂.","[""\nI see you've posted GitHub links to Jupyter Notebooks! GitHub doesn't \nrender large Jupyter Notebooks, so just in case here are \n[nbviewer](https://nbviewer.jupyter.org/) links to the notebooks:\n\nhttps://nbviewer.jupyter.org/url/github.com/sdfordham/pysyncon/blob/main/examples/basque.ipynb\n\nhttps://nbviewer.jupyter.org/url/github.com/sdfordham/pysyncon/blob/main/examples/texas.ipynb\n\nhttps://nbviewer.jupyter.org/url/github.com/sdfordham/pysyncon/blob/main/examples/germany.ipynb\n\nWant to run the code yourself? Here are [binder](https://mybinder.org/) \nlinks to start your own Jupyter server!\n\nhttps://mybinder.org/v2/gh/sdfordham/pysyncon/main?filepath=examples%2Fbasque.ipynb\n\nhttps://mybinder.org/v2/gh/sdfordham/pysyncon/main?filepath=examples%2Ftexas.ipynb\n\nhttps://mybinder.org/v2/gh/sdfordham/pysyncon/main?filepath=examples%2Fgermany.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) \n[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) \n[^(Author)](https://johnpaton.net/)""]",30,1,https://www.reddit.com/r/statistics/comments/13j2yqx/s_python_package_for_the_synthetic_control_method/
115,2023-05-16 12:27:31,[Q] Anyone know how to create these standard deviation cones in excel?,https://imgur.com/a/ErzKNWD,"['I guess calculate the standard deviation for each point and +- the number from average. Then, plot the line using the newly created columns.', ""Cumulative alpha? What type of modelling is this. I don't immediately recognise the type. Is it theta modelling forcing only trend?""]",0,2,https://www.reddit.com/r/statistics/comments/13iuw64/q_anyone_know_how_to_create_these_standard/
116,2023-05-16 05:09:39,[Question] Alternatives to a Chi-Squared test when independence assumption is violated?,"I’m trying to compare rates of an event occurring between two groups. The issue I’m facing is that the groups are not independent- the same individual might contribute an observation to each group. 

For illustrative purposes, let’s say that I’m trying to compare rates of people crossing the street (yes vs. no) at a red light vs. a green light. I have some subjects who arrived at the light twice - once when it was red and once when it was green - so they’re included in both groups. 

Is there a good alternative to a chi-squared test that allows for subjects to be counted in both groups?","['If there aren’t so many such cases that you’d loss too much data, you might randomly choose one incident for subjects that have two or more.', 'Can you identify the responses that belong to those same individuals?\n\nIs it possible those individuals will have different responses (e.g do you feel happy about traffic lights right now)? Or should they be the same both times (e.g if you collect both groups on the same day, demographic responses wont change)?\n\nMight the one individual appear multiple times at *each* light?', 'Thanks, it’s a good option. I’ll definitely share with my team. I think my preference would be to keep all of the observations if possible - this is just one part of a larger analysis and I’d prefer for the cohort to be consistent if there’s a way to do so.']",2,3,https://www.reddit.com/r/statistics/comments/13ikmge/question_alternatives_to_a_chisquared_test_when/
117,2023-05-16 00:27:42,[Education] Fully- Funded MS Statistics,Does anyone know of any fully or partially funded MS in statistics?,"['Miami university in Oxford oh, wake forest are two that I had applied to. Ohio state and UGA do as well.', ""Some will offer a bursary or scholarship if you've done particularly well at undergrad""]",3,2,https://www.reddit.com/r/statistics/comments/13icven/education_fully_funded_ms_statistics/
118,2023-05-16 00:04:16,[Q] Multi score RDD: missing a score,"Hello!

I am writing my master thesis and I am examining the impact of needs based scholarships on drop-out and other things. To do so I wanted to use use a fuzzy regression discontinuity.

The problem is that the scholarship is given based on two things, an income and a wealth measure.

However, I have data on both only for one university in my sample. I was wondering if anyone knew whether there are papers stating that it's ""okay"" or anyway somewhat justified to run a fuzzy RDD on the single threshold I have (or telling me I can't).
I am struggling to find literature on this specific problem (there isn't much to begin with on multi score rdd)

Thanks a lot!","['As long as the probability of treatment function only has one discontinuity of the forcing variable, it shouldn’t be a problem.\n\nBut I have a hard time understanding how that could be the case here.', 'I have already checked and there is two of them. I have seen a few papers talking about smth like ""fuzzy frontier estimation"", but it\'s not entirely clear what they are talking about. That\'s why I was looking for additional research', 'The thing is you don’t know if the discontinuity your estimating is actually the effect of the income or the wealth variable, since wealth and income are tightly correlated. If you simulate this I think it would be clearer']",5,3,https://www.reddit.com/r/statistics/comments/13icas4/q_multi_score_rdd_missing_a_score/
119,2023-05-15 22:17:43,[Research] Exploring data Vs Dredging,"I'm just wondering if what I've done is ok?

  
I've based my study on a publicly available dataset. It is a cross-sectional design.

  
I have a main aim of 'investigating' my theory, with secondary aims also described as 'investigations', and have then stated explicit hypotheses about the variables.

  
I've then computed the proposed statistical analysis on the hypotheses, using supplementary statistics to further investigate the aims which are linked to those hypotheses' results.

  
In a supplementary calculation, I used step-wise regression to investigate one hypothesis further, which threw up specific variables as predictors, which were then discussed in terms of conceptualisation.

  
I am told I am guilty of dredging, but I do not understand how this can be the case when I am simply exploring the aims as I had outlined - clearly any findings would require replication.

  
How or where would I need to make explicit I am exploring? Wouldn't stating that be sufficient?","[""I assume their main qualm is the use of stepwise regression. If so they might have a point. If you are using a hypothesis driven approach, you shouldn't need to use stepwise. This method will test the model you had in mind, and also iterate over a bunch of models you probably didn't hypothesize *a priori*. This tends to uncover a lot of overfit models and spurious p-values."", ""If you turn this into an exploratory analysis and look at all the models, you should do the appropriate alpha p-value Bonferroni correction. That is: p / every_single_model_tested during the stepwise iteration. If you are using the standard alpha of p < .05 and the iteration tested 100 models then your crit cutoff would be p<.0005 for all the models including the ones you originally hypothesized. That might be a dealbreaker for some people, and really shouldn't be a judgment call you make after looking at the outcome."", 'I\'m sorry that you were told you were doing something bad without any explanation of how. That wasn\'t fair at all.  \n\n\nIt appears that your critic may be onto something though. Here is a quote on the topic of multiple inference. tldr; you can\'t just hit a data source with a bunch of hypotheses and then claim victory when one succeeds, because your likelihood of finding something increases as with the number of hypotheses.\n\n>""Recognize that any frequentist statistical test has a random chance of indicating significance when it is not really present. Running multiple tests on the same data set at the same stage of an analysis increases the chance of obtaining at least one invalid result. Selecting the one ""significant"" result from a multiplicity of parallel tests poses a grave risk of an incorrect conclusion. Failure to disclose the full extent of tests and their results in such a case would be highly misleading.""  \nProfessionalism Guideline 8, Ethical Guidelines for Statistical Practice, American Statistical Association, 1997\n\n  \nMultiple inference is also baked-into stepwise regression inherently unfortunately, and is one of the approach\'s many documented flaws. In essence, the approach runs through countless models, then selecting the ""best"" model it\'s observed. Then that final model is presented as if it came about a-priori, which is the way that it\'s supposed to work. Doing all of that violates the principle above in a massive way however. From my understanding stepwise regression is generally regarded as a horrible practice among most sincere and informed practitioners.', 'Idk about you but my advisors have been up my ass because I’m awful at explaining why all my methods are used, “if the audience doesn’t understand why you’re doing what you’re doing and how you’re doing it then it’s not a very thorough explanation” is what I’ve had to tell myself.', ""You just need to make it clear that your supplementary analysis was a post hoc analysis.  Nothing wrong with that.  They're helpful for further investigating an association of interest, even if they're not considered to be a definitive proof of any identified result.\n\nMake sure that your methods clearly state what you did, and in your discussion/limitations section, just reiterate that this was a post hoc analysis and that studies specifically assessing the relevant associations are needed to verify and further clarify these results.""]",46,53,https://www.reddit.com/r/statistics/comments/13i9nb4/research_exploring_data_vs_dredging/
120,2023-05-15 21:31:01,Bounding difference of expectations [Q],"Let X and Y be two discrete random variables. Assume sup_a |P(X = a) - P(Y = a)| < b for some b in (0,1). That is, the difference in distributions is bounded under the l-infinity norm. Is there some way to bound the distance between the expectation of X and the expectation of Y?

It makes sense intuitively that as distributions get closer, their expectations will also. But I can’t quite figure out how to do that.","['No. A simple example would be to pick any n>2 and let \n\n> P(X=0) = P(X=n^(2)) = 1/2 \n\nand \n\n> P(Y=0) = 1/2 - 1/n, P(Y=n^(2)) = 1/2 + 1/n\n\nThen for any given b n can be chosen large enough to satisfy your condition, but E(X)=n^(2)/2 whereas E(Y)=n^(2)/2+n, meaning E(Y)-E(X)=n. You can make the difference in expectations arbitrarily large, despite the difference in probabilities being arbitrarily small.', 'EX = sum_a a P(x = a) and EY = sum_a a P(Y = a), then we can consider | EX  - EY | = | sum_a a (P(X = a) - P(Y = a)) | <= sum_a |a| |P(X = a) - P(Y = a) | <= b sum_a |a|.\n\nWithout further restrictions, I believe this bound is sharp, e.g. consider two Rademacher random variables with parameters p and q, so P(X = 1) = p, P(Y = 1) = q, then we have EX = 2p - 1, EY = 2q - 1. b = | p - q |, and our bound gives  2| p - q |, which is exactly | EX - EY |.\n\nEdit to clarify: This just reframes a bound on the absolute difference in expectation as a bound on the sum over the absolute value of elements in the image of the random variables. There is no guarantee that this sum is bounded. If, for your particular random variable, it is bounded, then you get a bound on the absolute difference of expectations.', ""So we've got a proof of a bound and a counterexample to any bound in the answers. The counterexample looks very convincing to me."", 'I might be wrong, but in the ""proof"" wouldn\'t the upper bound be an infinite, divergent sum if it wasn\'t a finite sample space?', 'Probably better to write ""difference in pmf"" rather than ""difference in distribution"" since the later can readily be taken to mean ""difference in distribution function"", (i.e. cdf).\n\n---\n\nNo, the expectation is clearly not bounded if the support of either X or Y (or both) is not bounded.\n\nTake the distribution of X as given; assume for simplicity of exposition that X has bounded support (this is not necessary but makes it easier to see the counterexample).\n\nChoose some m up beyond the support of X (i.e. larger than the largest value X can take)\n\nJust put P(Y=m) = b/2 (say). Otherwise, take the pmf of Y to be the same as for X but scaled down equally everywhere ... which should be P(Y=t) = P(X=t) * (1-b/2) for all t in the support of X.\n\nIf I didn\'t screw up somewhere we have a valid pmf for Y, and it\'s one that satisfies the stated bound on difference in pmf everywhere in the union of support for X and Y\n\nNow push m up past any finite bound. Clearly the contribution to the difference of expectation within the support of X is fixed and finite, but the contribution to the difference of expectation from the pmf at m is not bounded. \n\nSo the absolute difference in expectation is not bounded.']",3,8,https://www.reddit.com/r/statistics/comments/13i8ewx/bounding_difference_of_expectations_q/
121,2023-05-15 17:35:19,[Q] Currently getting to know about Machine Learning. Got to Know Statistics plays a Pivotal role in this field. Can you recommend some of the best books to Learn Statistics properly?,,"[""I'm just at the end of my statistics degree, I was recommended a book during my thesis which I thought was better at explaining core topics I learnt in my first couple of years it has a nice foundation then builds into generalised additive models which are very common in statistics \n\nGeneralised additive models by Simon n wood"", 'Data Scientist here.  Only took a few stats courses in mathematics undergrad but taught myself probability theory and applied statistics on the job to learn how they were the same/different from the ML side of the fence.  I can say that most of the best learning I had came from reviewing Shalizis course notes and his book.  For most bread and butter stuff its pretty good as a starting point into deeper topics. :-)\n\nhttps://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/', 'I used this to learn about stats and machine learning with R: http://www.statlearning.com', 'depends on what you mean by ""learn statistics"" (it\'s a huge subject; what do you want to learn?) and by ""properly"" (at what depth are we talking?)\n\n\nA lot of people coming from outside stats seem to imagine they can just pick up one book and read it and they know all there is to know. That\'s an error of scale of a similar order to Douglas Adams\' *the entire battle fleet that was accidentally swallowed by a small dog*. If you want to be done in less than a few lifetimes, you\'re going to have to be more specific about what you want to learn about and what you want to understand about it -- what you want to be able to do.\n\nIf that (read one book) is what you imagined, maybe start with Wasserman\'s *All of Statistics* (it it pretty broad but isn\'t remotely close to \'all\'; indeed it\'s not even his only ""All of"" book on statistics, and there\'s a ton of stuff outside anything Wasserman has written about). It is aimed at ML people but it\'s very much on the theory side; if you want practical stuff you\'ll need a very different kind of thing. If you want to know about topics it doesn\'t cover, you\'ll need more.\n\nIf you\'re seeking something more basic, the usual books used for mathematical statistics subjects subjects aimed at undergrads might be a starting point, since that will get you basic inference, and you can branch out from there to whatever topics you need.', ""It's an excellent book, to be sure.""]",2,5,https://www.reddit.com/r/statistics/comments/13i3bnb/q_currently_getting_to_know_about_machine/
122,2023-05-15 13:47:19,"[Q] Can someone critique my shiny dashboard? I'm a new grad looking for a job, did this for a capstone project. Uses data from the Armed Conflict Location and Event Data project and exploratory point pattern methods. I didn't really get any critiques from my professor. The dashboard isn't hosted.","Would you hire me? Rate dashboard 1-10? It has kernel estimation, measures of interpoint interaction, a lot of plots and tries at explanations. 

[https://www.youtube.com/watch?v=rkb\_C4Q0X20](https://www.youtube.com/watch?v=rkb_C4Q0X20)","[""This video isn't available anymore"", 'It is for me?']",1,2,https://www.reddit.com/r/statistics/comments/13hz3r9/q_can_someone_critique_my_shiny_dashboard_im_a/
123,2023-05-15 13:12:01,[Q] Cows are 20x deadlier than sharks: what are some commonly quoted but wrong/stupid/manipulative stats?,"I'm looking into writing a character for a short story, and the character quotes these sorts of statistics. Like ones that are either wrong, or don't make sense when compared with whatever else, etc. I imagine you'll know what i'm talking about. What are some other examples of these?","['Freakonomics.\n\nI said what I said.', '76.5% of statistics are made up.', 'dihydrogenoxide is by far the most harmful chemical on earth and is commonly found in all aquatic enviroments. All cancer cells require dihydrogenoxide oxide to survive. It is estimated that dihydrogenoxide directly kills (number here) people yearly etc etc', 'People only use 10% of their brain at a time', 'There’s also people lose half of their heat through their head. Think it was measured with people not wearing a hat but wearing other warm clothing']",6,15,https://www.reddit.com/r/statistics/comments/13hyfaz/q_cows_are_20x_deadlier_than_sharks_what_are_some/
124,2023-05-15 06:01:39,[Q] basic stats question,"Apologies for the basic question but I just can’t find a clear answer anywhere.

I am doing my dissertation and I need to test for significant (either via T-Test or Mann Whitney), but for some of the data I’ve encountered group A would be parametric and then group B would be non-parametric. Do i then go with Mann Whitney? T-Test? Something else? I have no idea.","['> , but for some of the data I’ve encountered group A would be parametric and then group B would be non-parametric\n\nData cannot be parametric nor nonparametric. Those are descriptions that apply to *models*, and from there can be applied to tests, estimates, intervals or other inferential procedures.\n\nDo you mean ""you tested normality and rejected, concluding that some B samples may be non-normal""? That might not be particularly important nor particularly relevant. I expect that none of the populations you sampled can actually be normal (what are these measuring?) so a test that rejects is only telling you what you should already be able to tell for sure by other means (and the rest will be type I errors). What you need to know is how much impact whatever non-normality there is in the populations (not the samples) could have on the properties of your test (a test-dependent effect size, not significance). If you\'re mainly focused on correctness of significance levels then you need to worry about the impact *when H0 is true*. The data may be pretty much irrelevant to that consideration (because H0 is very probably strictly false).', ""Just use the non-parametric test. Is the safest one if any one is going to judge your statistical analysis. Don't forget to inspect and present a boxplot comparing both groups for interpretation"", 'Failure to reject normality doesn\'t mean you have normality; it just means you didn\'t detect the non-normality you have. A rejections wouldn\'t tell you whether your test will work as you want, and rejecting normality does not tell you that the test will not work as you want. The normality test is not answering the questions you need answers to.\n\n>  My supervisor said to me if normally distributed use T-test and if not use Shapiro Wilks\n\nYou don\'t mean what you wrote there. I presume you mean ""Mann-Whitney"". \n\nBriefly, this is fairly poor advice for substantial list of reasons.\n\n> What do I do\n\nAre you asking \n\n(i) ""what makes sense to do?"" (for which the answer is mostly ""uh, don\'t do what you were told, you really need to focus on different things than what you\'re focusing on""), or\n\n(ii) ""what would my supervisor want me to do?"" \n\n for which the answer is ""don\'t ask me, I don\'t know them, nor what other unhelpful things they might believe or expect"". I might *guess* that they\'d expect a Mann Whitney, but if they didn\'t tell you already and you don\'t ask them, there\'s no way to know for certain. Maybe they imagine a t-test is okay. Maybe they would consider still other criteria -- e.g. maybe they expect you to test still other things first, like whether the distributions are the same shape, as I sometimes see people doing before a Mann-Whitney? Maybe they\'re aware of more than two tests? I cannot guess what mix of sense and nonsense they\'re carrying around. Based on what you\'ve said so far, I don\'t anticipate that their expectations will make a lot of statistical sense and I cannot be sure what they might want.\n\nI could t seems like you missed a lot of the information in what I wrote before, so perhaps more details on what it would make sense to do would be more or less moot. On the other hand, if you want to do what your supervisor would want, that\'s easy -- just ask them what that is.', 'I have never heard about such a restrictive assumption on this test. Would you mind to provide a source? This is one of those assumptions that should hold in order to make valid some type ""shift"" interpretation?', 'We sampled 2 populations (group A and group B) and measured a bunch of variables. For one or two of the variables Group A would be normally distributed (via Shapiro wilks) and Group B would not be normally distributed (via Shapiro Wilks). My supervisor said to me if normally distributed use T-test and if not use Shapiro Wilks. What do I do if one is normally distributed and one not?']",11,12,https://www.reddit.com/r/statistics/comments/13hortk/q_basic_stats_question/
125,2023-05-14 21:06:24,[Question] Forest Plot save as Word,"Is there any way to generate Forest Plots and save them as word so that the text is searchable?
I couldn't find the option for that in RevMan, and am searching in R but with no results. 
Sometimes I save them as pdf and convert it to word but that basically destroys a lot of the text and makes me end up with something unpresentable.
Thank you.","['Set.wd\n\nLibrary(meta)\n\nMeta-function\nPdf(file = name.pdf, width = your.width, height = your height)\nForest.meta(meta-function object, code for your desired forest plot)\nDev.off()\n\nI know you asked for word, but you can search in a pdf, so I am not sure why you require word.', ""Talk to a senior editor about it. That's not a reasonable demand by a reviewer. I'd submit it with the pdf and if it's rejected on that basis then move to a different journal."", 'Have you tried using Markdown and Knit to word?', ""If you use the emf device from the devEMF library then you get a figure which Word understands as vector graphics. The text will be like text boxes.\n\nThat said, just having the text there as text doesn't necessarily mean that Word will include it in searches. The internal structure of a Word file is quite different from that of a pdf, and neither even remotely resembles what is displayed."", ""I am submitting a paper to a journal and one of the reviewers had that exact request.\nWe went back and forth trying to convince them that you can search in pdf but they still returned the paper with that request.\nIt's the only remaining comment on the paper.\nAcademia!!""]",1,11,https://www.reddit.com/r/statistics/comments/13hbro9/question_forest_plot_save_as_word/
126,2023-05-14 19:15:23,[Q] What is the correct way to write t-test ?,"Okay so this is a very niche and spefific question but what is the correct way to type t-test. 

I have seen it typed as ""T-Test"" ""T Test"" ""t-test"" ""t Test"" ""*t* Test"" etc ect 

what is seen as the ""correct"" way to write this ?","['It may depend in part on who is judging the correctness, but since Fisher invented the thing we call the t-test\\* (in the form that gets the name ""t""), if we\'re going to argue that someone gets to claim that one form is more correct, I think he gets priority about how to name it.\n\nIf you accept that basis, I offer this quote from *Statistical Methods for Research Workers* (the edition I looked at was published in 1934; this quote is from page 122 of that edition):\n\n> The validity of the *t*-test, as a test of this hypothesis, \n\nNote the italicization of *t* in *t*-test and the hyphen. When I write the name of the test, I don\'t typically reproduce the italicization, but I use the lower-case ""t"" and usually include the hyphen; which is to say, I often write ""t-test"". Sometimes I just write ""t test"".\n\nSorry, I don\'t seem to have an earlier mention of the test written in that form ready to hand (though I think his 1927 Metron paper would be a good candidate for an earlier mention of it).\n\n\n\n---\n\n\\* yes, I definitely mean *Fisher*; the statistic for the test that Gosset (Student) invented was *not* called ""t"" and used a different form of statistic from what we do call t. Take a look at his 1908 paper, it\'s not so hard to get hold of. Fisher (rightly) gives Student priority on the test (since he invented an equivalent test and correctly identified the distribution - albeit without formal proof), but the t-statistic in the form we use it seems to be due to Fisher.', ""The Student's t-test."", 'A capital T is sometimes used to refer to other distributions, so I always use a small t for clarity.', 'There may be guidelines following a particular style guide.  For example, I believe it was APA guidelines I was following for one particular article I was writing, and the editor was very particular about what Greek letters and statistical quantities were were italicized or not.  It didn\'t make much sense coming at it cold. *\n\n***EDIT, addition***: In APA ""t-test"" doesn\'t have a hyphen, and the t is italicized: *t* test. \n\nSo you might look at what style guide is used for journals in your particular field.  If you can get used to those conventions, it may save you some mental distress in the future.\n\n______\n`*` For example, Greek letters were italicized, but not if they were commonly used in English, so some were and some weren\'t.', 'I dont think it is too strictly when it comes to the question of how to write it correctly. You just need to know that the concept come from a scientist who during the time working for the Guiness brewery had found it. Because the company dont want others competitor to know that they have a statistician as worker, he have to publish it under a code name which is Student T.']",8,8,https://www.reddit.com/r/statistics/comments/13h9jmu/q_what_is_the_correct_way_to_write_ttest/
127,2023-05-14 17:20:29,[Q] book recommendation,"Hi,

Looking for a book on use of statistics in real world applications. Not too academic, more focusing on real world data collections and modelling. Fun to read.

Thanks,","['Larry Gonick - Cartoon Guide To Statistic\nhttps://amzn.asia/d/hya77zP', '- The Art of Statistics\n- Mastering Metrics\n- Statistical Rethinking', 'I like Applied Predictive Modeling by Kuhn and Johnson!', ""Tim Harford or Nate silver's book would do."", ""I have a book coming out in the fall that fits this description, called *Probably Overthinking It*:\n\n[https://www.goodreads.com/book/show/123226972-probably-overthinking-it](https://www.goodreads.com/book/show/123226972-probably-overthinking-it?from_search=true&from_srp=true&qid=cudMcFIYGB&rank=1)\n\nYou can get a sense of what it's like from this talk:\n\n[https://www.allendowney.com/blog/2022/11/09/chasing-the-overton-window/](https://www.allendowney.com/blog/2022/11/09/chasing-the-overton-window/)""]",31,15,https://www.reddit.com/r/statistics/comments/13h7jpz/q_book_recommendation/
128,2023-05-14 16:51:03,[S][Q] Excel Polynomial Regression Confidence Level,How can I use the Linest function in excel (hopefully some data from the additional statistics output option in this function) to compute the two sided confidence interval curves for a polynomial regression?,"[""I can't answer your question, but I have to ask, why are you doing this sort of analysis in excel?\n\nPresumably you could recover the confidence intervals the old way with Pe+-1.96*SE"", 'Fair point! Only a portion of overall the calculation is statistical in nature and the rest needs to be accessible to my coworkers. Python, matlab, R are options but not ideal in this case.']",1,2,https://www.reddit.com/r/statistics/comments/13h719c/sq_excel_polynomial_regression_confidence_level/
129,2023-05-14 16:47:21,[Q] Dual Form of class weighted soft-margin SVM,"I am familiar with the dual form of the soft margin SVM when there is only one parameter C, but I cannot find the dual form of the class-weighted soft margin SVM which has the following objective with parameters C\_1 and C\_2: 

[https://i.stack.imgur.com/BZCGB.png](https://i.stack.imgur.com/BZCGB.png)

Can the dual form of this problem be derived?","['It essentially only adds up the slack variables belonging to that class', ""I think there's a nice derivation of the Lagrangian Dual SVM in the Boyd/Vandenberghe Convex Optimization textbook"", 'What does the \\xi_i : y_i notation mean?', 'I see, but is it for a class weighted SVM?']",1,4,https://www.reddit.com/r/statistics/comments/13h6z15/q_dual_form_of_class_weighted_softmargin_svm/
130,2023-05-14 14:17:16,[D] Shapley values as a collection of experiments,"I have written about causal Shapley values on this blog:

https://mkffl.github.io/2023/04/20/causal-shapley.html

I look at à Shapley value as a collection of experiments that measure the effect of adding a feature on the final prediction. The effect can be total or incremental, or a combination of. This perspective helps understand asymmetric Shapley values, which add constraints from causal assumptions.

Any feedback appreciated!","[""Looks like a great read, I'm saving it to read tomorrow.\n\nConcerning the use of shapley values for causal inference, this part of the Shap documentation gives advice on what to be careful about, as shapley values are state of the art for model explanation, but come with some caveats in causal inference.\n\nhttps://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html"", 'interesting, thanks. the problems raised broadly overlap, though the solutions are different. The examples of observed confondouded variables are similar: ad spend is a non-interventional variable and interactions is a mediated variable. The CausalML solution with doubleML looks great and makes me wan my to try this library. It is interesting that lundberg wouldn’t even mention solutions that directly improve the logic of SHAP like the one I used.']",14,2,https://www.reddit.com/r/statistics/comments/13h4fbz/d_shapley_values_as_a_collection_of_experiments/
131,2023-05-14 09:38:32,[D] floats in statistical computing,"I was reading comments in this [post](https://www.reddit.com/r/ProgrammerHumor/comments/13gt6co/standagainstfloats/) and I was wondering why I never had any issues with floats when doing statistics. Is it important for statistical computing? in which cases one should be worry about this?  
  
I code mostly in R and the most complex thing I had done in terms of programming was to implement a Bayesian estimation algorithm using metropolis Hastings and Gibbs sampling for a complex nonlinear model with mixed effects. I just didn't worry at all about floats and precision.","['>  Is it important for statistical computing?\n\nYes, absolutely.\n\n> in which cases one should be worry about this?\n\nmostly when writing (designing or coding) algorithms; its important to consider loss of significant figures when doing so. Catastrophic loss of accuracy can be a real problem and *has led to problems* many times in the past.\n\nOne thing to be particularly aware of (because it comes up pretty often in stats) is the cancellation that results from subtracting *almost-equal* quantities, and why it\'s important to avoid that.\n\nAnother thing that does come up (this is one that\'s pretty common in MCMC work) is the issue of underflow and overflow (a pretty extreme version of accuracy loss) when dealing with likelihoods of different models, such as when performing Bayesian model averaging for example. It\'s important to know the log-sum-exp trick for dealing with that (I had to rediscover it on my own; I wish someone had mentioned its existence to me before having to figure it out myself).\n\nhttps://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n\n\n>  was wondering why I never had any issues with floats when doing statistics\n\nHow do you know that you didn\'t? Just because you didn\'t *notice* an accuracy problem doesn\'t mean there wasn\'t one. MCMC in particular has a tendency to happily chug on even when its answers are poor and it can be hard to spot any problem if you don\'t already know what the results should have been; how would you know the difference between an inaccurate calculation (say one with barely a single digit of accuracy) and an accurate one?\n\n>  the most complex thing I had done in terms of programming was to implement a Bayesian estimation algorithm using metropolis Hastings and Gibbs sampling for a complex nonlinear model with mixed effects. \n\nYou can easily hit it just coding up a sample variance.\n\nhttps://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n\nThings are way better than they once were; the IEEE 754 (1985; most recent revision is 2019) standard for floating point calculations sets out a number of things that really do help floating point calculations work better, and the use of ""math chips"" for calculation where the calculations are done to considerably more precision than the final result needs to be stored in both help quite a bit (e.g. On generations of Intel machines doing mathematical calculations in 80 bits when the double precision floats store 64 bit words, though of course the digits of the fractional part - the mantissa or *normed significand* - will be less than the storage, because you lose bits for exponent and sign).\n\nWhile important, neither of those helpful things can fix an algorithm that causes a catastrophic loss of accuracy.\n\nsome reading related to floating point numbers:\n\nhttps://floating-point-gui.de/ -- light, lacks details\n\nand  (though you don\'t have to read further if you don\'t wish to, the first one does mostly cover the main issues)\n\nhttps://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html  -- not so light, lots more detail\n\nhttps://en.wikipedia.org/wiki/IEEE_754\n\nhttps://pages.cs.wisc.edu/~markhill/cs354/Fall2008/notes/flpt.apprec.html  (has a detailed discussion of guard, round and sticky bits if you want to get into why 754 helps preserve accuracy over what came before)', 'There are so, so many things in computation that can bite you that you don\'t find out about until you\'re bitten. For many, this is one of them. For R users, many other topics are covered in [The R Inferno](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf). The deeper you look, the more you realize that you\'re skating on a very thin sheet of ice over a very deep ocean filled with unspeakable horrors. \n\nThe first time I became aware of the fuckery that is floating point was coding numerical comparisons, things that look like `if (x == y)`. Depending on what you\'re doing, this can lead to a world of pain (or it can be, somewhat shockingly, okay in particular circumstances). If `x` and `y` are some quantity computed two different ways, you\'re in for a world of hurt. More recently I was made aware that floating point [breaks some of the rules of math](https://stackoverflow.com/questions/10371857/is-floating-point-addition-and-multiplication-associative) we\'re all used to. Is this a problem in many contexts? No. But it can be! Though practically for me it most often means that I need to be sure whether I should be using `if (x == y)` or `if (abs(x - y) < threshold)`.\n\nAs efrique has pointed out, there are plenty of other places it can become very important to understand what\'s going on under the hood. I\'ve found some of this is a bit easier to wrap one\'s head around than others. The implications of finite precision for numbers makes ""work with log-likelihoods"" and ""log-sum-exp will save your life"" pretty clear consequences. Catastrophic cancellation took a bit more to get through my skull.', '[These notes](https://www.stat.umn.edu/geyer/3701/notes/arithmetic.html) are the minimum a statistician should know about computer arithmetic.', 'That post was a bit extreme (maybe a bit satirical?). The basic idea is that computers work in binary, and some numbers can be perfectly represented in binary numbers, just like some numbers can’t be perfectly represented in decimal. Float and double are two standardized methods of storing such numbers with finite precision. The difference is that doubles use double the size of floats, and as such, can represent numbers be represented with greater precision.\n\nIn statistical computing, computations often come down to differences in the 3rd, 4th or even higher decimal digit, or else need precision to a high level for intermediate calculations. In lots of practical applications, float precision is good enough that and loss of precision is inconsequential. However, computers are dumb and it’s easy to forget this notion so we may end up making these precision errors when programming. Some languages abstract way some of this difficulty, but none are immune.', 'Very few statistical computing courses cover this material.  IMHO all should.']",4,15,https://www.reddit.com/r/statistics/comments/13gyzef/d_floats_in_statistical_computing/
132,2023-05-14 03:26:44,[Q]Tails distribution question,"Probably this is a dumb question but i want to know if this is correct, till so far, all the statistical test i have seen for rejecting hypothesis focus on the tails following some threshold to see if we reject or fail to reject the null (this in a very supercial way).

But i do not understand why the tail if in distributon like the normal most of the data is in its center, my main thought is because of the outliers that focus on the tails, I am also not sure if this is true, but if it is, then why we should focus the test on the outliers to check an hypothesis would not it make more sense to the where most of the data is gathered?","['The hypothesis test is testing how likely it is we would see scores as extreme or more extreme (further from the mean / median) if the null hypothesis is true.', ""We're trying to decide  if a sample tells us information that's not consistent with H0.\n\nSomething that would behave like what we typically expect to see under H0 would not lead us to doubt H0.\n\nIf I  want to see if I have an unfair coin, I might toss it a bunch of times and look at the proportion of heads. \n\n If H0 is true, then that sample proportion should be close to 0.5, but will vary from it a bit due to the sampling variation of the sample proportion. The result of the experiment will be a single draw from the sampling distribution of the proportion of heads under H0. \n\nIf I see a proportion that's far into the tails of that \ndistribution, it becomes hard to maintain the null position - that its a fair coin.\n\nE.g \n\n- if I toss 30 times and get 18 heads , the explanation 'fair coin with random variation' is pretty plausible. I don't have strong reason to doubt it\n\n- if I toss 30 times and get 25 heads, 'fair coin with random variation' is pretty implausible (in that we should almost never see a result as or more extreme if H0 were true). That explanation based on the null is not tenable.\n\nhttps://i.stack.imgur.com/wfedw.png\n\nThe situation shown on the left is not something that would surprise us if H0 were true; we'd see something at least as weird as that (at least as far from 50% heads) *pretty often*.\n\nThe situation depicted on the right would be quite surprising if H0 were true; we should almost never see something like that if H0 were true."", ""The tails aren't actual data. They're the extremes of the theoretical distribution of results if the null is true (and all assumptions are met).\n\nThe observed result is compared to this theoretical distribution of results we'd see purely by chance to help decide if what was observed is consistent with chance, or might be evidence of a real underlying difference.""]",1,3,https://www.reddit.com/r/statistics/comments/13gqdpq/qtails_distribution_question/
133,2023-05-14 01:39:46,[Q] The use of detrended correspondence analysis and canonical correlation analysis?,"Hi everyone! So bear with me here, statistics is my worst subject! Is there anyone here who could help explain detrended correspondence analysis (DCA) and canonical correspondence analysis (CCA)? I've tried looking it up but I still can't understad if these are two different analyses as in they are always used separately (sort of like t-test and ANOVA are two different analysis) or do you commonly use them together, as in one is usually followed by the other?  On what type of data do you usually use these for and is one analysis better than the other or does it wholly depend on what kind of data you have/question you want answered? Thanks for any and all help!",[],3,0,https://www.reddit.com/r/statistics/comments/13gnrjb/q_the_use_of_detrended_correspondence_analysis/
134,2023-05-13 22:21:01,[Q] Questionable statistics in medical research,"I have noticed a trend within the covid literature. This post is not about the decision to vaccinate/not vaccinate, I am using it as an example to ask if I am wrong about the statistics, because this is what stood out to me. So I am not stating what I am saying about the vaccine/virus are factual, that is not the point here, the point is, am I making a mistake with the statistical analysis? Because I find it bizarre that I would be right and 100s of PhDs who publish in reputable academic journals don't understand basic statistics.

We keep hearing that vaccination reduces long covid. From what I have read, these studies typically range from around 15-30% reduction in long covid after vaccination. But none of the studies control for this variable: severity of illness.

From my own research (again, the point is not whether or not these are facts, focus on the statistics), there appear to be 4 generally agreed upon unique causes/mechanisms of long covid, which appears to be a heterogeneous condition:

* damage from severe acute covid itself
* autoimmune issues
* persistent viral load
* clotting/inflammation from the spike protein of the virus

So, if ""damage from severe acute covid"" is not controlled for, I am unsure how it makes sense to say ""vaccination reduces long covid by 15-30%"". Wouldn't this mean that that this is likely because that 15-30% comes from reduction in rates of severe acute covid itself, given that we know vaccines do a good job at preventing severe acute covid in the first place?

But if it is a heterogeneous condition, and if the majority of people don't get severe acute regardless (and instead get mild/asymptomatic infection, in which you are still susceptible to long covid), and if the vaccine does not prevent the other 3 mechanisms, and thus the majority of people who get long covid get it from one of the other 3 causes, doesn't that mean the statement ""vaccination reduces rates of long covid"" or ""vaccination reduces the chances of getting long covid by 15-30%"" is meaningless for people who don't get severe acute covid, aka the vast majority of people? Isn't that 15-30% only applying to those who get severe acute covid? Shouldn't we keep in mind the base rate? Wouldn't saying ""vaccination reduces the risk of long covid by 15-30%"" assuming that all people have the same base rate risk of severe acute covid? (which is definitely not the case)? So is it statistically correct to give the blanket statement ""vaccination reduces the chances of long covid"" to people as a whole? How much practical/statistical utility does it have?

Am I missing something here?

EDIT: I am being downvoted: can anybody please specifically cite which part I am wrong about? I posted here asking if I made a mistake with the statistics. I am not sure why I am getting downvoted. If you think I am wrong, I don't know why you would downvote me, that is why I posted. If I am wrong, please show me, that is why I posted this. I am not sure why you would downvote + not show how I am wrong.

**EDIT: I understand this can be confusing. I am adding this example I made in a post to clarify things: It is basically like telling 100% of the population of a country that winter tires will reduce the risk of a car accident, yet 90%+ of the population lives/drives in a place where it does not snow. And you said this because you took a sample of 1000 people, and divided them into 2 groups, 500 people with winter tires, and 500 people without winter tires, but DID NOT CONTROL FOR THE VARIABLE ""snow on the road"", and found an OVERALL 15-30% reduction in accidents in the group that had winter tires on their car. Well obviously that is because SOME people in both groups live in areas with snow, and those in the winter tire group would be less likely to get into an accident compared to those in the no winter tire group who live in snowy areas, meaning that the OVERALL rate of accidents is lower in the ""winter tire"" group. Then based on that concluding ""winter tires reduce the chances of accidents"". TECHNICALLY you would be correct, but this statement would be MEANINGLESS for those who live/drive in an area where it does not snow.**

&#x200B;","['Consider this analogous example (if I understand your point correctly):\n\nIn an analysis of whether parachutes (vaccines) prevent death from hitting the ground (long covid) after falling out of an airplane, would you want to adjust for speed at ground-impact (severity of covid infection)? You’d quickly prove that parachutes don’t _directly_ provide protection against dying from the fall.\n\n(the data might even suggest that parachutes are dangerous if it contained some poor fellas who tripped and fell out of stationary, grounded aircrafts and some of those with parachutes got tangled up in the cords and suffocated, while those without parachutes had nothing to choke on and survived).\n\nSeverity of covid infection is on the causal pathway to long covid, and vaccination causes less severe infections. As another poster stated, by adjusting for severity of covid, you could isolate the direct effect of vaccination beyond the effect through severity of infection - but that would not be useful or representative of their effect from a public health planning perspective.', ""Wouldn't damage from severe covid be a mediator in this setting (meaning, it's one of the mechanisms through which vaccines reduce long covid)? In that case, you don't want to control for it if you're interested in the total effect of vaccination on long covid."", 'Vaccination almost certainly has an effect on severity of disease, which means controlling for it is almost certainly a bad idea. This will give an inappropriate effect estimate that cannot be interpreted causally and does not have any real policy relevance.', "">I am not sure how it wouldn't solve the problem? If we control for severity of acute illness, it would indeed tell you the effect of vaccines on the other mechanisms/causes of long covid, and yes that would be interesting: that is what we are looking for/that is what would be relevant for approximately 90% of the population (given that roughly 90% of the population didn't get severe acute covid in the first place without vaccination). So I am not sure why you are downplaying this or saying it will likely not tell you anything useful at all.\n\nThe problem is that severity of the acute illness is itself an outcome of the vaccine, and controlling for an outcome is very tricky. As I said, at best it gets you the partial treatment effect through other channels, but that require additional assumptions.\n\nWhat you want to do instead is to condition on pre-determined variables, such as age or physical fitness. That is both uncontroversial and useful, although as I said you need to be careful to get it right statistically.\n\n>So isn't my example similar?\n\nNo, because your example is about a variable which is itself an outcome of the treatment. Furthermore, the base rate fallacy is about failing to consider the full population. This issue is different - a population ATE of vaccines on long covid is a well defined statistical object, and requires no adjustment of the sort you're talking about. Again, it can be more or less interesting, but it's not wrong or bad statistics."", ""I don't know the studies you're talking about.\n\nHowever, there are in general a number of different causal effects that might be interesting in any given setting. The most common ones are the average treatment effect (ATE) and the average treatment effect on the treated (ATET). The former tells you the average treatment effect in the population, i.e., the expected effect for a randomly drawn individual. The latter is similar, but for those who actually took treatment. This might be different if there is selection into treatment.\n\nIf I understand you correctly, you're saying that the ATE is not the expected treatment effect for any particular individual, because the effect is heterogeneous and will be much larger for some, and much smaller for others. This might well be the case, but the ATE is still informative, because it tells you the expected reduction in long covid if everyone took the vaccine compared to if nobody took it. \n\nThat said, it's always interesting to estimate heterogeneous treatment effects for different groups, but it is often challenging (for reasons ranging from statistical power to study design). \n\nFinally, controlling for severe covid would not solve this problem. At best, it would tell you the effect of vaccines through the other mechanisms, which might or might not be interesting (but it's more likely to tell you nothing useful at all).""]",10,74,https://www.reddit.com/r/statistics/comments/13giv5p/q_questionable_statistics_in_medical_research/
135,2023-05-13 22:17:19,[Q] What type of statistical test should I use to compare between time variables?,"Hello! I am analyzing some survival data using Kaplan-Meier Curves and log rank tests. This part was easy enough to do in SPSS but I now want to compare the overall survival of some patients with the prognostic time they first received. I can't do this with K-M and log rank

My question is which statistical test should I use for this nonparametric data: basically comparing ""time A"" variable with ""time B"" variable. 

I'm not a statistician sorry if this question may seem dumb to y'all I'm still learning this stuff","[""There are parametric survival methods that exist.  I don't use spss so I don't know the specifics of what it is capable of.  The link I am sharing is an overview of those models, but using Stata instead of SPSS.  I imagine SPSS has similar capabilities.\n\nhttps://stats.oarc.ucla.edu/stata/examples/asa2/applied-survival-analysis-by-hosmer-lemeshow-and-maychapter-8-parametric-regression-models/"", 'Cox model?  https://www.r-bloggers.com/2016/12/cox-proportional-hazards-model/', ""Thank you I'll look into it""]",4,3,https://www.reddit.com/r/statistics/comments/13giryz/q_what_type_of_statistical_test_should_i_use_to/
136,2023-05-13 19:10:46,[Q] Is this enough info to calculate the sample size?,"All I am given is the list of variables, their estimates, variances and covariances.  I swear this is not enough information, and am stuck.","['Those are definitely not going to reveal the sample size. Unless you have the actual sample, or the reported sample size, it’s going to be difficult to determine the sample size', ""The output most likely contains a confidence interval, based on a standard error, and the corresponding variance(s) or standard deviation(s) from which that standard error was derived.\n\nFind the values you can link to some algebra and solve for sample size. If you think the output is missing something, explain what you were looking for and why you can't find it."", ""I give homework exercises and quiz questions where I give the mean & std error and ask for n. Or % and MoE and ask for N. It's not particularly clever, but just another way to make sure they know the formulas. Is this homework, or how the heck do you end up with this question?"", 'No but you could guess 🤔', 'Are you trying to do a prospective, planned sample size calculation, or back calculate what the sample size must have been?']",7,8,https://www.reddit.com/r/statistics/comments/13gepbu/q_is_this_enough_info_to_calculate_the_sample_size/
137,2023-05-13 17:09:58,[Q] How to conclude if multiple slightly different trendlines can be described by one overal trendline for the data?,"Hello,

for my research i gather data and for every series i can make a trendline (it's just a simple linear regression so excel trendline is fine). This results in me having about 5 trendlines that are nearly identical. how can i prove that fitting one trendline through all the data is correct (opposed to having 5 slightly different trendlines)?

i have calculated the prediction interval which says something about what the spread at a certain x value is. i also calculated the confidence interval which says something about what the mean y is at a certiain x. 

yet i don't think these are the correct calculation to conclude fitting one overal trendline is correct. what can i use to conclude this?

thank you!","[""Pool the data, and create an indicator variable for each series. Regress the outcome variable on the time variable, along with the set of indicators and a set of interactions between time and the series indicators (excluding one reference group to avoid perfect collinearity). Now each interaction term estimate the difference in slope between the reference series and the others. To test if they're all the same, you can test all of the interactions jointly."", 'well in both cases you have residuals, either with respect to the five curves or in the latter the latter case the one curve. You can look at the distribution of residuals in both cases and if you have some prior expectations about how measurement error should look like, you can see if having more parameters just make the residuals unrealistically small compared to expected noise or whether if they are justified. you can also do some sort of cross-validation (leave %30 out fit to the rest look at how well the your model predicts, do for many times for both models and compare to each other and see if extra parameters give ""significant"" improvements). Since this is linear regression I think you can also directly set some sort of chi2 test.', 'The model with one trend line is nested within the model with 5 trend lines.  You can test that one is sufficient vs. 5 with an F-test.', 'This sounds nice, is this much different than doing 5 seperate linear regresions, one pooled regression and comparing where the slopes from 5 regressions stand with respect to the parameter estimate of the pooled regression and its standard error', 'I recommend the same approach as standard_error. This is probably the outcome you are looking for']",3,20,https://www.reddit.com/r/statistics/comments/13gcfwj/q_how_to_conclude_if_multiple_slightly_different/
138,2023-05-13 11:33:30,[Q] What’s the term to describe low relationship strength due to low population variability?,Is there a specific term that describes the phenomenon of sample variability being too low to detect relationships?,"[""It's called [range restriction](https://stats.stackexchange.com/questions/404954/significance-of-correlation-with-range-restriction-in-x)."", 'Thanks a ton!', 'If I’m understanding your question correctly, you might be referring to [power](https://www.scribbr.com/statistics/statistical-power/)']",1,3,https://www.reddit.com/r/statistics/comments/13g610x/q_whats_the_term_to_describe_low_relationship/
139,2023-05-13 09:34:26,Stats roadmap for CS student [E],"

The title says it all. I'm currently in bachelor degree in CS, and already doing an internship in data engineering. What is the path in statistics that I should take in order to understand ""the data science"" better? I can fit curves, do regressions and etc, but I genuinely want to learn all the statistics around this stuff. Thank you very much.

TLDR: CS student wants to learn all the basic of statistics to work with data.","['When it comes to statistics, alot of the basic starting points revolve around probability distributions (discrete and continuous) and what you can do with them. \n\nThen when you get comfortable with that you can move along with point estimates and finding efficient/sufficient/consistent estimates. \n\nI dont know if hypotheses testing would be of benefit for you, however it is critical to the statistical processes in which you want to test the probability  of a parameter is true or not. Typically by calculating most powerful test(MPT) for type 1 and type 2 errors.\n\nIf anyone else wants to fill in any gaps I missed please feel free. Things like confidence intervals, sample size, etc.', 'I’m also a CS student going into ML. Starting out w Ross is great, I loved his probability book. The next step would be mathematical statistics, the most common undergrad recommendation is Wackerley. If your math is good - for ex a first course in real analysis, you can go to a more rigorous math stats book, such as Casella & Berger or Hogg. \n\nFor more applied modeling stuff, I’ve heard really good things abt Elements of Statistical Learning. There’s also great ML books too, such as Bishop’s PRML.', ""Thank you very much and noted. It's good to have a place to start. Currently studying Ross's A First Course in Probability. Trying to get the grasp.""]",1,3,https://www.reddit.com/r/statistics/comments/13g3iez/stats_roadmap_for_cs_student_e/
140,2023-05-13 07:30:20,[E] - ANOVA on multicenter clinical trial - Exercise on 3 models,"I'm doing an exercise on a multicenter clnical trial to train when to apply specific types of ANOVA or mixed models.  
  
The exercise goes as follows:  
  
""  
  
A clinical trial on hypertension was randomized with double-blind to compare three different treatments:  
  
New drug (A-Carvedilol) Existing drug 1 (B-Nifedipine) Existing drug 2 (C-Atenolol). 29 centers were selected and patients were randomized into treatments. The study consisted of 6 visits: 2 considered pre-treatment and 4 during treatment:  
  
Visit 1 (week 0): measurements (including diastolic blood pressure, dbp) were taken to verify if patients met inclusion criteria, and a placebo was administered for hypertension treatment during one week. Visit 2 (week 1): measurements were repeated, and patients who met the inclusion criteria were randomized into one of the 3 treatments. Visits 3-6 (weeks 3, 5, 7 and 9): dbp measurements were obtained in 2-week periods. 311 patients were recruited, 288 were considered eligible for randomization, and 30 patients dropped out of the study until visit 6. The primary endpoint was diastolic blood pressure (dbp).  
  
The proposed analysis consists of analyzing the dbp value at visit 6 with the following models:  
  
Model 1 (fixed effects only): 𝐷𝐵𝑃i =mu + dpbBase + treat + centre Model 2 (with center as a random effects factor) Model 3 (with center and center\*treatment interaction as random effects)""  
  
Given that we're ""fixing"" at visit 6, we could see this as a ANOVA of one block, right? In this case, we'd have 1 quantitative variable (dpbBase) + 1 categorical as independent factor (treat) + 1 categorical as independent factor (centre), correct?  
  
Thus:  
  
Model 1 - We'd have a model of type y = u + bi + tau\_j + error where b\_i would be the effect of block i (in this case 6), and thau\_j represents eht effect of treatment j. Would this logic be correct? Here we'd apply a randomized block model with fixed blocks (of fixed effects).  
  
Model 2 - Now, we make center as a random effects. Here, since we now have a mix of fixed + random (fixed would be treatment and random the center), we start to get in ""mixed models"" territory, right? From my understanding, this would be an hierachical model, right? Since we could start from the Center factor and then branch out onto the treatment factor. I don't think this would be a random coefficients model since we have no repeated measures in this analysis (from what I think at least) and we're fixing on visit 6 (we don't have a ""time"" factor here)  
  
Model 3 - We now have interaction + fixed and random effects here. Which type of model would be applied here? Would it also be an hierarchical model? Can we even evaluate this interaction in an hiearrchical model? I believe it would be DBP = mu + dpbBase + Treat\_k + center\_j + center\*Treat\_(j)\_k + error, correct?  
  
Thank you in advance!",[],1,0,https://www.reddit.com/r/statistics/comments/13g0nvt/e_anova_on_multicenter_clinical_trial_exercise_on/
141,2023-05-13 03:06:04,[E] Motivating Example to (Benevolently!) Trick People into Understanding Hypothesis Testing,"I'm a PhD student in statistics and wanted to share a motivating example of the general logic behind hypothesis testing that has gotten more ""oh my god... I get it"" responses from undergraduates than anything else I've tried.

My hunch - almost everyone understands the idea of a hypothesis test inherently, without ever thinking about it or identifying it as such in their own heads. I tell my students hypothesis testing is basically just ""calling bullshit on the null"" (e.g., you wake up from a coma and notice it's snowing... do you think it's the summertime? No, because if it were summertime, there's almost no chance it would be snowing... I call bullshit on the null). The example I give below, I think, also makes clear to students why a null and alternative hypothesis are actually necessary.

The Example: Let's say you want to know if a coin is fair. So you flip it 10 times, and get 10 heads. After explaining the p-value is the probability, under the null, of a result as / more unlikely than the one we observed, most students can calculate it in this case. It's p(10 heads) + p(10 tails) = 2\*\[(0.5)\^10\] = (0.5)\^9. This is a tiny number that students know means they should ""reject the null"" at any reasonable alpha level, even if they don't really understand the procedure they are performing.

I then ask: ""Do you think this is a fair coin?"" To which they say, of course not! When I ask why, most people, after some thought, will say, ""because if it were fair, there's no way we would have gotten 10 heads"". I write this on the board. I then strike out ""because if it were fair"", and replace it with ""if the null hypothesis were true"", and similarly replace ""there's no way we would have gotten 10 heads"" with ""we'd see ten heads/tails only (0.5)\^9 percent of the time"". Hence, calling bullshit.

This is usually enough for them to realize that they use this thinking all the time. But, the final step in getting them to understand the role of the different hypotheses is by asking them how they got their p-value of (0.5)\^9. Why didn't you use P(heads) = 0.4 instead of 0.5? The reason is because the null hypothesis is that the coin is fair, meaning P(heads) = 0.5! This is the ""aha"" moment for most people, in my experience - **by getting them to convince themselves they HAD to choose a certain P(heads) to calculate the odds of getting 10 heads, they realize the role of the null hypothesis. You can't calculate how likely/unlikely your observed statistic is without it!**","['I love this sort of thing.\n\nA huge problem with understanding hypothesis testing is just the absolutely bizarre language that it uses.\n\nAn intuitive notion of the null hypothesis IMO is the ""devil\'s advocate"" who\'s job is to always argue. ""Nope, nothing to see here folks. Be on your way!"" This devil\'s advocate however, can only make arguments based upon shared knowledge that both they and you have about observed likelihoods. So they\'re always limited to marking arguments in the form of something like:   \n\n\n""Common, you\'re telling me that these are two different groups? There\'s a 15% chance of seeing what you saw if they were a single group. You can\'t honestly tell me that\'s good enough"".\n\nor \n\n""Common, you think that this observation didn\'t come from that group? There\'s a 2% chance that it did. That\'s 1/50. Are you willing to risk that?""\n\nIt\'s up to us to consider the devil\'s advocate\'s argument and decide whether or not we\'re persuaded by them, or we think that they\'re being overly cautious.', 'Maybe a daft question on my end but if you observe ""10 heads"", why then is the following calculation made:\n\n> p(10 heads) + p(10 tails)\n\nIs that not saying ""10 of the same is observed"", i.e. ""10 heads or 10 tails"", not just ""10 heads""?', 'I love this! Nothing against ""Active learning"" but I love a good, clear walkthrough of a concept.', 'Here is an OLD video of mine dong something similar-- it is a much better thing to do in person, where I trick a student into thinking they got 10 guesses in a row correct... though after 5, 6, or 7 flips almost everyone thinks that something is up. https://youtu.be/Y5UPmUN1w94', ""It's great that you've independently discovered this approach - here are some slight wrinkles on it with [weighted dice](https://www.researchgate.net/publication/230192798_A_Classroom_Demonstration_of_Hypothesis_Testing) and [playing cards](https://www.tandfonline.com/doi/full/10.1080/10691898.1994.11910464).""]",105,35,https://www.reddit.com/r/statistics/comments/13fu17a/e_motivating_example_to_benevolently_trick_people/
142,2023-05-13 02:34:44,[Q] SPSS Custom Table (Crosstab) Macros?,"The firm I work for uses SPSS to create crosstabs of each question. We use the ""custom table"" option and manually drag each question and then export each question to excel. Is there a macro, or a way to automate this? It would save a lot of time.","['Firstly, use syntax instead of manually dragging things. Typing it out once and then changing variables is already much faster\nSecond step is then to standardize your tables, so you can create a macro and only feed it the variables. Thirdly, you can use the Output Management System (OMS), or simply export all viewer content into a specified excel document.', ""Dm me. It's past midnight where I live, but i can show you the details another time."", 'May I add an SPSS question here as well…\n\nWhat test should i do after chi-square? I have one data set and two questions with multiple responses: silly example; A) Do you own fish (y/n)?, and, B) How happy do the fish make you (scale of 1-4)?', 'This is helpful, if you’re willing to hop on a call next week I’m happy to pay for an hour of your time. This will save me a lot of time in my workflow, I am unfamiliar with how to write the syntax but I am sure with a little guide I could figure it out.', 'You can look at the standardized residual for each cell to see what is generating the sign effect.']",0,5,https://www.reddit.com/r/statistics/comments/13ft8i3/q_spss_custom_table_crosstab_macros/
143,2023-05-13 01:59:00,[Q] How can I calculate/present this data most accurately? Surface area and volume ratios.,"I'm a doctor analysing some data from patient scans to try to gain some insight into various parts of the disease process I'm interested in.

  
I am analysing some data using 3D modelling files. The software is a commercial medical tech company's, so it's proprietary and only certain parameters are available for me to use.  
Lets say I have a number of 3D models of various objects, and I have the volume of each model.  
On the surface of these objects are abnormal patches, for which I have calculated the total surface area abnormal patches on each model.

  
I do NOT have the total surface area of each model, only the volume.

  
I wish to perform some comparisons of different objects in two groups, by comparing how much abnormal patches they have. However, because each object has a different size (volume), these comparisons are meaningless unless I find some way to standardise or index them.  
I thought about doing a very crude surface area in cm2/volume in cc or ml

  
However the problem with this is that at lower volumes, the ratio becomes exaggerated (due to high surface area to volume ratio) and at higher volumes, the opposite occurs.

  
Can anyone think of a better way to measure/compare these data with the given parameters I have? I appreciate there may not be a good answer to this, but I'd be willing to listen to bad answers, ones with lots of potential inaccuracies, as long as I can justify that those were the best way to do these calculations given the circumstances, when I present them to my colleagues in the hope that we can collect some preliminary data in order to potentially come up with some research studies that may add to our knowledge in this disease area.","[""If the models are all approximately similar shapes - doesn't matter what shape as long as they're all more or less comparable - then the square root of the volume will be roughly proportional to the surface area. If the shapes are quite different that's not going to be very accurate but it'd still be a justifiable way to make the volume: surface area ratio a little more consistent across sizes, and fully consistent for any given shape.\n\nE2A: if the models are organs, you could probably find some data on typical volume:surface area for each organ and tighten it up that way."", ""> square root of the volume will be roughly proportional to the surface area. \n\nI'd have thought 2/3 not 1/2 since surface area would be proportional to a linear dimension squared and volume to its cube, (unless we're dealing with fractals)""]",3,2,https://www.reddit.com/r/statistics/comments/13fsbgv/q_how_can_i_calculatepresent_this_data_most/
144,2023-05-13 00:38:44,[Q] Need help finding research papers published between 1967-1968 that neglected their effect size,"Based on research published in the early 2000s, older psychotherapy research papers commonly had those mistakes and especially in the 60s Would love some help from anyone who can come up with a few names, even the later ones from the 70s and 80s could do :) Thanks","[""Presumably the 'research' cited examples. Start there."", ""I looked at one examples and went to the research, but then the research takes me to other research that analysis similar mistakes, yet they never explicitly say which papers they used, because they disproved them or something\nThis should've been an easier task but it's taken me way too much time and effort..."", ""If you don't like time and effort, don't do research. No one here is going to have a mental list they can handily download straight into your brain.\n\nFocus on a topic area and type of research, do a literature search, and plough through the results."", ""I literally just need a resource or something to help me find those old research papers to get an unaddressed statistical error from it to use on my course work.. I thought I found it when I found articles about psychotherapy. I am very inexperienced and I am tried really hard but I still don't know how to properly read research papers I guess..\nEven the retractiondatabase website is no help"", ""Are you saying that you have the reference but you can't find the paper?\n\nTry scihub.se. You may need a VPN to acces it, depending on where you live.""]",0,5,https://www.reddit.com/r/statistics/comments/13fq5py/q_need_help_finding_research_papers_published/
145,2023-05-13 00:24:51,[D] Linear Regression for Beginners,"Linear regression is a statistical method used to determine the relationship between two or more variables. In particular, it is used to find the best-fit straight line that describes the relationship between the dependent variable and one or more independent variables. The dependent variable is the variable that is being predicted or explained, while the independent variables are the ones that are used to explain the dependent variable.
https://link.medium.com/DNDBCdzfKzb","['This is not a statistical description of linear regression.  “Using a statistical software package, we obtain the following output”…. This needs to be explained.  How do you use such software to perform a linear regression? When you do so, what is the computer doing?']",0,1,https://www.reddit.com/r/statistics/comments/13fps9y/d_linear_regression_for_beginners/
146,2023-05-13 00:09:50,[D] Please share any career or educational advice for rising senior,"I am a rising senior, double majoring in economics and statistics. I do not plan on attending graduate school right after I graduate, so I understand the following year is crucial as I will be going into the job market. Can y'all please share any general advice you have to help me finish strong and put myself in a good position upon graduation next year? For example, when and how I should search for jobs, specific skills I should pick up within the next year, and anything you wish you did before your senior year of college. Thank you for your time.","['If you have not had any formal public speaking courses, take them now. The professional world, and doubly so any technical field, is over run with weak communicators. Having both economics and statistics degrees (which I happen to also have, btw) you are expected to both be capable of complex analysis as well as explain your analysis to those without your specialist education. You will find many of your educational and professional peers are not formally trained presenters, they are learning on the fly, and not presenting their best selves. Presenting is a seriously valued skill for audiences as small as one and as large as a televised broadcast. Formal communications also includes learning how to listen, learning how to debate for solutions and not to ""win"", as well as learning how to moderate a group conversation for equality of voice and inclusion of all participants - for the purpose of the communication and the purpose the group exists at all. \n\nI expect you\'re aware that economists typically work for major corporations or governments. While statistics is the foundation of artificial intelligence. You could parlay your education to become an advisor of AI technologies at the economic level, for a major corporation or government agency. I recommend getting gregarious, as you could end up at some quite interesting places, which you may not be aware even exist yet. Start a personal program of talking to anyone you can find with a similar set of degrees and 7-10+ years post graduation to get an idea of what\'s out there. It is vast.']",2,1,https://www.reddit.com/r/statistics/comments/13fpdsv/d_please_share_any_career_or_educational_advice/
147,2023-05-12 23:47:56,[Q] Working with weighted survey data,"I'm working with survey data about how customers are satisfied with a service, they can answer from 1 to 10. I have a weight vector for each respondent to represent the actual population share. I want to create a box plot with the satisfaction scores, but if I use the weighted score (the score multiplied by the weight) then my scale goes outside the original 1 to 10. What is the best way to rescale the weighted score to build the box plot? Even further, does this process I'm doing make sense? Or the weighting only works when summarizing results?  
  
  
  
Thanks for the help","['You could transform the weighted data to fall within a range of 1 and 10 while maintaining the weighted distribution. You could plot the unweighted data but this becomes disjointed with the summary statistics calculated on the weighted variables.', ""I used a MinMaxScaler from scikit learn to bring it back, from 1 to 10. But, does really high outliers kind of messed up the central values. I'm not sure the weighted distribution is kept after the rescaling.""]",2,2,https://www.reddit.com/r/statistics/comments/13fosdt/q_working_with_weighted_survey_data/
148,2023-05-12 23:36:25,[Q] Can someone identify this regression algorithm?,,"[""We're sorry, your account hasn't gathered enough karma to post in /r/Statistics yet. You might try first posting in /r/AskStatistics, /r/homeworkhelp, or asking a question at [CrossValidated](https://stats.stackexchange.com/questions). \n\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/statistics) if you have any questions or concerns.*""]",1,1,/r/AskStatistics/comments/13dwplr/can_someone_identify_this_regression_algorithm/
149,2023-05-12 22:56:52,[Q] Statistics World Data Cache _ Website?,"I'd like to look up some peer review general knowledge data on stuff like mobile phone ownership but unsure where to start?

Pew maybe?","['Try r/datasets', 'Thanks so much!']",3,2,https://www.reddit.com/r/statistics/comments/13fnh0b/q_statistics_world_data_cache_website/
150,2023-05-12 19:41:04,[Q] Should I use Bayesian or Frequentist approach to network meta-analysis? Good walkthrough resources for network meta-analysis in R for non-statisticians?,"Hi everyone, thank you for taking the time to read my post!

I'm trying to conduct a network meta-analysis (NMA) in order to eventually perform a cost-effectiveness analysis (CEA). I'm trying to decide between Bayesian and Frequentist approaches for the NMA. My research question is basically to assess different treatment options (five total) for a specific type of pain. For all but two of these treatment options (three of the ""established"" treatments), there's about 4-5 randomized controlled trials (RCT) with common comparators (namely placebo) that total about a thousand patients. However, for two novel treatment options that we're interested in, there's only one RCT for each that is suitable for the NMA (i.e., there's only one RCT with a common comparator); this totals about 50 patients per study per novel treatment.

Therefore, there are few data points for the two novel treatments, but there are many for the other, more-established treatments. Would the Bayesian or Frequentist approach be more suitable for NMA?

Could you refer me to a good walkthrough resource for conducting this NMA in R? I'm looking for a resource that would be able to explain (as succinctly as possible) the different variables that can/should be optimized or changed and the rationale behind them, if possible.

Thank you so much for any help with this--very much appreciated!","[""use netmeta library and review the package documentation. If you're still having trouble, I mean, there's a post in /r/rstats that Google Bard and ChatGPT are both pretty good at walking through R code."", ""I'd add that it also depends on what you're doing with the output. \n\nFrequentists will give you an estimated value (+ eventually confidence intervals). \n\nBayesians will give you a distribution, which can be useful for specific use cases. Are there assumptions I want to make? e.g. Strictly positive parameters, specific target distribution,.. Is there an overlap between 2 parameters distributions? Etc...\n\n\nI think a good practice is to first quickly reach a model using a Frequentists approach, then switch to Bayesian and compare the results."", 'There are many differences between the Bayesian and frequentist framework. Most are ""philosophical"". \n\nYour choice should depend on your data imo. If it is very limited (small) and unbalanced (missings, truncations) I would consider looking more in to Bayesian techniques. If the data is quite neat and complete, then stick with frequentistic methods. \n\n(This is the ""Reddit"" version of my answer, short and clear, we could write several pages about it ;) )', 'In my experience, non-statisticians might be better served by Bayesian analysis. Although it may seem a little more difficult to understand at first, once you’ve gotten past the initial learning curve, everything becomes much more intuitive. With frequentist approaches, it’s easier to think you understand something when, in truth, you’re mistaken. Consider how confusing p-values are, for example.', 'With meta-analysis, the big choice is not so much Bayes vs frequentist, but fixed vs random effects. \n\nWith a random effects model, you allow for ""shrinkage"" of your effect sizes towards some common mean (which ideally should lead to estimates for the lower-power studies that are a bit more realistic/generalizable), and a mean whose confidence/credibility intervals more accurately accounts for relative uncertainties of the individual studies.\n\nIn practice, Bayesians will nearly always go for a random effects model (it\'s very natural to write and estimate hierarchical models, like random effects models, in Bayesian statistics), but you can do them just fine in frequentist approaches as well.\n\nIf you actually have meaningful (i.e., ""informative"" or ""mildy informative"") priors you can elicit (e.g., maybe you know there is a very low likelihood of effect sizes greater than X or less than Y), then it\'s best to incorporate that via a Bayesian analysis. If not, then I would just choose whichever is simpler to run.']",15,5,https://www.reddit.com/r/statistics/comments/13fipca/q_should_i_use_bayesian_or_frequentist_approach/
151,2023-05-12 06:12:55,[Q] Excel Help!,"Hello,  
  
I have bullshitted my way to a statistical analysis promotion, although I am very comfortable with SPSS. I am pretty much illiterate in everything else. I am currently assigned to organize a excel spreadsheet by matching codes with criminal charge descriptions. It wouldn't be that bad of a manual task buuuuuut there's about 5k different Texas criminal code charges. An additional issue is that there are duplicates in the charges but there will be slight differences such as misspellings and added spaces. Any suggestions for formulas or ways to facilitate the process? I have tried the find and replace and it's working but it's a little slow. Any help would be appreciated! Thanks in advance fam!!",['Dm me I think I can help. This isn’t really statistics though lol'],0,1,https://www.reddit.com/r/statistics/comments/13f1uut/q_excel_help/
152,2023-05-12 06:09:52,[Q] Explaining a Mixed Effect Model to a Non Statistician/Mathematician,"I'm not a statistician, but I do have a basic understanding of biostatistics in the context of Medicine and Clinical trials. However, recently I came across a trial that is using a Statistical Method that I am very unfamiliar with and was hoping someone could help.

Here is the study. It's in the ""Statistical Analysis"" section: [https://www.ajo.com/article/S0002-9394(21)00222-1/fulltext](https://www.ajo.com/article/S0002-9394(21)00222-1/fulltext)

Where I'm getting very confused is with this sentence: ""Reproxalap was compared to vehicle via a **MIXED EFFECT MODEL for Repeated Measures with baseline area under the curve as a covariate** and **treatment group and minutes post-challenge as factors**. A generalized estimating equation procedure, with baseline area under the curve as a covariate and treatment group and minutes post-challenge as factors, was used to compare responder proportions for the key secondary endpoint.""

I'm trying to understand what the terms in this entire paragraph actually mean and why this would be a VALID statistical method to use for this given trial.

If someone could explain, non-mathematically, the actual intuition and reasoning behind what a Mixed Effect Model for Repeated Measures is(and what the significance of the covariate and factors are in this model) with some examples or point to some sources that explain it well to someone with a basic understanding of stats, and even perhaps then proceed to explain it mathematically, I'd be very grateful. Thank You.","[""I think the other answers are better on the details, but my big picture advice is, mixed effects models are useful when you observe a sample of instances from the true population (like people, if you ran the experiment again, you wouldn't use the same exact people for it) and you think individuals have some special differences that modify that general relationship.\n\nTo statisticians, mixed effects means there are fixed effects (the same for every observation) and random effects (different for every observation). It might be more intuitive to think about them as *varying* instead of random. \n\nSo maybe you have a bunch of children and you're measuring their heights as they age. You might have a fixed effect for the average rate of growth, but varying effects for each child, cause they start at different heights and grow at different rates.\n\nOften we model those varying effects as if they're random draws from a normal distribution and we estimate the (possibly very complicated) covariance structures based on how individuals are related to each other in nested groups or hierarchies (which is why you'll sometimes hear these called hierarchical or multilevel models) to make statements about those effects."", 'The mixed effect model for repeated measures (MMRM for short) is a very common model used in clinical trials and often supported or even ordered by regulatory agencies for registration trials. The basic regression model treats all of the repeated measures of the outcome variable as a multivariate outcome, for example from baseline all the way till the stated primary outcome timepoint. This allows for the use of all available data and is more efficient than just looking at a change score or the final time point, where missing data can really impact the available data. Next, the model assumes no structure on the time points, in effect modeling them all as individual factors. (I couldn’t open your link, and the description didn’t explicitly state this, but this is the most common setup.). Technically, there should be an interaction between treatment and time, so that you are in effect modeling each groups mean value at each time point. Because this model doesn’t impose any structure on the time trajectory or profile of how the means change over time, it allows for a great among of flexibility to let the data represent themselves without making possibly unjustifiable or unrealistic assumptions, such as both groups changing in a linear or quadratic manner over time. The mixed effect refers to allowing the residuals for each mean to be modelled differently (though they don’t specify the structure). This adds to the flexibility of the mean modeling and doesn’t impose further restrictions, such as homogenous variance over time, as outcomes tend to get more variable over time. \n\nLastly they also include a couple of covariates, presumably to increase statistical efficiency.', 'The ""mixed"" refers to including both fixed and random effects in the model. The link is broken so I can\'t help you about the specifics. \n\nThese terms ""fixed"" and ""random"" are used differently depending on who you ask or what literature you\'re reading. \n\nIn my field of psychology, we typically call models that include both between-group and within-group effects ""mixed"". This makes sense in repeated measures data because you have variables that vary between persons but not within an individual (fixed; e.g., biological sex) and you have variables that vary within persons (variable; e.g., reaction time).', 'mind trying again with that link?\n\nEDIT: NM, found it: https://www.ajo.com/article/S0002-9394(21)00222-1/fulltext', 'Fixed effects are variables for which the researcher decide which value it is going to take. For example, I want to test the effect of pizzas on mood. I’m giving a pizza to one group and no pizza to the other. The variable pizza is fixed. They also are variables for which the degree of change is constant. Time can be a fixed effect (I know, time is relative according to physics) but the rate to which time advances can be seen as constant. \n\nRandom effects are variables we can’t control. For example, the weather is something that we can’t control (I think). If you want to study the effect of weather on mood, you can’t decide that the weather is going to be rainy for a group and sunny for another group at the same time and place. Therefore, you have to measure weather. \n\nNow, time can be a random effect even if it is constant. However, it is not the time that is random but the measure of another variable at different points in time. It is a simplified way of saying that the weather is different when measured at two different points in time (even if slightly). The weather is not constant so if you you measure it more than once (repeated measures), you have a random variable.\n\nI hope that my explanation is clear and correct! :)']",37,7,https://www.reddit.com/r/statistics/comments/13f1rud/q_explaining_a_mixed_effect_model_to_a_non/
153,2023-05-12 03:28:58,[Q] Help with guidance on exercise problem using r studio,"Guidance on solving an exercise problem using RStudio

The question is as follows, “using a stimulation approach, approximate the chance that Bob will pass his 25 question test. 
The rules are:
1. create at least 10,000 sets of 25 single digit numbers. 
2. Each number is the answer to each question. 
3. An even number is a correct answer, while an odd number dictates a wrong answer. 

Going back to the question, it is asking for the relative frequency of 17 or more correct answers from the 25 questions. 

This is to be done on RStudio software. 
Guidance is appreciated and not sure if this is the right post as my options are being exhausted","['given the way the instructions are stated, `sample` (with the default value of `replace` altered) would also work', 'Start with this function:\n\n    runif()']",0,2,https://www.reddit.com/r/statistics/comments/13excc3/q_help_with_guidance_on_exercise_problem_using_r/
154,2023-05-12 01:04:37,[Q] mixed models - parameterization,,[],1,0,/r/rstats/comments/13et720/mixed_models_parameterization/
155,2023-05-11 23:23:01,[Q] Can i perform a post-hoc test to identify on which day the samples become significantly diiferent if i have two samples only (control and experimental)?,"Hi. Im working on my undergrad thesis and we're having a hard time analyzing our data as were also not good with statistics. In our study, we wanted to know whether the two samples only (control and experimental) are significantly different in the parameter chosen. We used kruskal wallis test (we used SPSS) as the data are not normally distributed and we only have two samples, hence Anova cannot be carried out. However, we have a total of 6 different analysis period (storage time/ day 0-21). The result says that significant difference exist within the samples but we cannot perform post hoc test as we only have two samples. Is there any way we can identify on which day the samples have significant difference?","['I think the short answer is you can\'t without replicates. You can plot the data and point it out qualitatively by saying ""the experimental set appeared to diverge from the control at day X"". \n\nAs a side note you it\'s the residuals (how far the real response deviates from the models predicted value) that need to be normally distributed. This is not necessarily linked to whether the actual data appears to follow a normal distribution', 'hi, thank you for the detailed response. We conducted the experiment with three replicates each sample. Right now, I was thinking of setting the data per storage time as different treatments and compare each to the control and run a post hoc...but im not sure if that would be okay though.', 'If you have three replicates for both time points across 6 time periods you have a two factor design - time X treatment. Therefore, I think you could possibly use a mixed model in SPSS, or just a two way ANOVA. You would not be able to say that the treatment had an effect at a specific time period unless there was a significant interaction between time and treatment.\n\nThe version of SPSS I used you (pre-2020) had to mess around input a tiny bit of script. Hopefully, they changed this in the version you are using.']",2,3,https://www.reddit.com/r/statistics/comments/13eqhzl/q_can_i_perform_a_posthoc_test_to_identify_on/
156,2023-05-11 22:18:46,"A test for the ""jaggedness"" of a graph [Q]","I made a big ol' excel sheet where I counted up all my favorite songs of all time and found that 1966-1975 had a very clear bias towards the odd years. Are there any tests I can do to test the significance of the ""jaggedness"" of the graph.  The first thing I thought of was just to add up the odd total vs the even total and assume a p=0.5 and test how unlikely it is to see such a bias. Are there any methods more oriented towards testing the jaggedness of a graph. I attached a zoomed in graph and zoomed out graph","['With e.g. Monte Carlo simulation on some model (random walk for example) you might find that this particular type of weirdness is unlikely. However, there are uncountable types of weirdness, and the probability that at least one of them will occur in a data set just by chance is very high. So testing for a particular pattern spotted a posteriori is a dubious excercise (cf. the ""Texas sharpshooter"", who fired at random, hit some random objects, and afterwards claimed he had aimed at these in particular).', ""If it's just odd-even bias, why would the consecutive jaggedness be interesting?\n\nHowever, whatever you do, you have a problem, in that you're testing hypotheses *generated by the same data* that you're testing on. Your p-values will not be meaningful.\n\nhttps://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data"", 'It is possible to measure how ""noisy"" a distribution is with kernel density estimation - https://en.wikipedia.org/wiki/Kernel\\_density\\_estimation.', 'You could probably do something along the lines of testing for a seasonal trend']",1,4,https://www.reddit.com/r/statistics/comments/13eor0w/a_test_for_the_jaggedness_of_a_graph_q/
157,2023-05-11 21:05:08,[Q] Example of collider bias? regression of wage of years of education,"I saw an example online of regressing wages on years of edu to find the causal effect. They suggest experience is an omitted variable correlated with both, which makes sense.

But as I have been learning about colliders, I wonder if this is an example of collider bias, where we wouldnt want to control for experience.

My thought process is education may cause more work experience, but wages may also cause someone to work for more years (or health /social connections or some omitted factor may be related to both experience and wages)

In this case, controlling for experience would introduce bias, no?

If someone with more edu got more experience, then comparing rhat person to someone else who already has that level of experience without the edu would then no longer be a good 'control' because the latter was able to get that experience without more education (work ethic, discpline etc.)

Does that logic hold or is this not a good example of a bad control?","['At this point, experience being a confounder is a reflex, from the famous mincerian equation. \n\nI think what you say about experience being caused by education and wages is plausible but  the timing is important, if you think wages and education today are affected by experience today but affect experience tomorrow,  there is no collider bias, as you are looking to explain wages today and you are not including experience tomorrow as the control.\n\nIf you think the relation is simultaneous, meaning, experience today causes and is caused by wages and education, then yes, you are in a pickle. I think this perfectly ilustrates why modern economics is abandoning controls altogether, relying instead in pseudo-randomization to obtain causal estimates, as relations between socioeconomic variables are extremely complex and more often than not, controls are endogenous themselves.', 'Build a DAG and experiment with it. Try daggity or dagitty, can’t remember how it’s spelled haha.', ""Yes, I think that's a bad control. The standard example of an omitted variable in this setting is some form of innate ability, which impacts both education and wage."", 'how you define the causal structure, makes experience indeed a collider. And indeed controlling for experience introduces bias between education and wage, and you will likely overestimate the true causal effect.', 'These thing are always difficult to answer. While you explanation on why salary has a causal effect on experience makes sense, I wouldn\'t say that\'s the case in any real data set. Your data is collected at specific time points. Let\'s say it\'s collected today and I have started working 42 years ago, so I have 42 years of experience. If my salary was (counterfactually) lower than it currently is, I might need to work longer, but you can\'t see that in my current years of experience. If my salary was counterfactually higher, I might retire earlier, but again, we don\'t see that in my current (counterfactual) experience. No matter how high my salary might be, I will always have 42 years of experience right now. \n\nNow, I\'m not smart enough to think about the consequences of ""counterfactual censoring"", in the case where I\'d already be retired by now thanks to a high salary, and not included in the data at all. So that maybe destroys my whole argument. If somebody wants to destroy my argument, I\'d be happy to hear that.']",17,6,https://www.reddit.com/r/statistics/comments/13emv0x/q_example_of_collider_bias_regression_of_wage_of/
158,2023-05-11 21:03:27,[Q] Method for forecasting a one step ahead time series based on another time series that has a strong relationship and causality with it and has been observed already?,"Hello,

I have 2 time series that have a strong relationship and causality to one another - one is ""total invoiced in a given month"" and the other is ""value of purchases paid by credit card in a given month"".

Let's say that I have the observed value for the last month for the second series, but I don't have the information for the last month of the first series yet. 

What are good methods to predict the total invoiced last month based on the payments of the last month that I already have the information?

I've been doing some research and I've come across Dynamic Regression Models, but I'm trying to find other methods that can be good as well and to compare their performances in my case.

Do you guys have any suggestions? Thanks!","[""You want the transfer entropy here, probably. For two time series X, Y, the TE(X -> Y) = I(X\\_{past};Y\\_{future}|Y_{past})\n\nWhere I( | ) is the conditional Shannon mutual information. If TE(X -> Y) is significantly greater than 0, then you can just use the maximally likely estimate for Y\\_{future} given X\\_{past}. \n\nIf your data are Gaussian, use a parametric estimator, if it's not, use a nearest-neighbors based KSG estimator."", 'The simplest is also the best. The Bayesian posterior predictive distribution minimizes the K-L Divergence between nature and the model. It isn’t possible to build a closer model. \n\nIt will give a probability distribution for each value. It also has a simple model selection mechanism, so if you are unsure of how much information may be in lags, it will provide a probability for each potential model. \n\nBecause accounting data is highly correlated by the design of accounting systems, you can readily add other information streams that improve predictive quality such as changes in GDP, local unemployment and so on. It takes very few accounting variables to estimate the others. Again, model selection methods allow you to determine if it is an improvement. \n\nThere are also a set of scoring rules to determine if your predictive model is good. Likewise, there are validation methods in the literature. \n\nThe only thing that might be an issue is creating what is called the prior distribution. The prior encompasses information about the location of parameters that comes from sources that are outside the data itself.', 'regArima or VAR.']",5,3,https://www.reddit.com/r/statistics/comments/13emti5/q_method_for_forecasting_a_one_step_ahead_time/
159,2023-05-11 12:42:10,[Q] Probability of at least one of two different rolls given X amount of tries.,"Hi, is there a formula to do something like this? For example:

Say I have a three sided weighted dice with 1 and 3 having a 10% chance with 2 having a 80% chance. I want to calculate the probability of getting at least one 1 and one 3. I calculated it using a tree diagram seen here

https://imgur.com/a/A8LNz3g

Binomial distribution does not work because the events are different?","[""Binomial doesn't work because binomial only allows for two different states. Since there are three sides to the die, binomial won't be an appropriate distribution. \n\nWhat you want is multinomial. https://online.stat.psu.edu/stat504/book/export/html/667"", 'You can calculate the chance as sum of two groups:\n\n1, 2, 3, in any order (6 options)\n\n1, 1, 3 or 1, 3, 3, in any order (2*3 options)\n\n=> 6\\*0.1\\*0.8\\*0.1 + 6\\*0.1^3 = 0.054', 'Thanks, this does help but is there an easy way to use this if I want one of the outcomes at least an x amount of times? It looks like I would have to do it for multiple cases then add up the probabilities together', 'Is there a formula that extends this to larger number of trials and options? Seems like it would take long to do this if say the dice had 6 sides and I did 10 rolls', 'In some cases you can use [inclusion-exclusion](https://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle), but in the most general case you only have the options to go through many cases (in one way or another), to simulate it for an approximation, or make some approximations for a direct (but not exact) calculation.']",0,5,https://www.reddit.com/r/statistics/comments/13ed6df/q_probability_of_at_least_one_of_two_different/
160,2023-05-11 11:43:33,[Question] Probability of surviving?,"[Question] I don’t know where to ask this so I figured some of you could help me with this. I have a genetic condition that only ten people in the world (including me) have been diagnosed with. Three of them have passed away, all between the ages of 20-22. I am now the oldest living diagnosed person with my condition. I am 22. So that’s 7 of us still alive today. Now since there could be others out there who have it but aren’t diagnosed, I have no idea how to figure this out. But so far only ten cases have been discovered worldwide. What is the probability of me making it to 23? 

(I know not to base any judgement on the probability of this outcome; however I do realize that it is a possibility and I have to be aware of that. Yes it does scare me, but anything could happen. I just am curious what the probability of it would be)","['I hope it\'s good, but we cannot tell with the given information. Even if we make the (absurd) assumption that all cases behave in the same way and they are indistinguishable except for deaths you are in an age range where we don\'t have data.\n\nHere are three extreme scenarios that are all consistent with the given information but lead to completely different outlooks:\n\n* 1000 people have this genetic condition, in over 99% of the cases it doesn\'t lead to notable symptoms ever, in the remaining cases it\'s often deadly before the age of 25. Some of the 10 diagnosed people showed symptoms and died or will die young, for others it was discovered while looking for something else. If you are in the second group, you have an over 99% chance to live a normal life, just like the 990 people who never get diagnosed.\n* The genetic condition always* leads to death at some point before the age of 25.\n* The genetic condition is most dangerous in the age range of 18-22, people who survive that range have a good chance to survive to old age.\n\n\\* there is always the chance of outliers. Stephen Hawking survived a ""0% survival chance"" disease for over 50 years.']",3,1,https://www.reddit.com/r/statistics/comments/13ec4cu/question_probability_of_surviving/
161,2023-05-11 09:21:25,[Q] [C] What kind of careers do a statistics degree come with?,"What career should I consider with a statistics degree?

Very curious what kind of career fields that comes with statistics. I know statistics is very broad so if anyone wants to share their experience with their jobs that uses statistics, I would be grateful! Currently a stats major and super curious about what I could get into :)

I was thinking maybe getting into public health and be a biostatistician? Idk, still early in my degree so I still have a lot of time to think about it.","['A friend of mine has become theater actor after stat degree but Idk if this is statistically significant', ""I am a statistician who works in global health as a scientific researcher. That's the best part of being a statistician - you get to play in everyone's backyard.\n\nIt's a rewarding career with a lot of interesting problems."", 'Public health biostatistician sounds like an awesome career! Although just a heads up, most of the jobs I\'ve seen with a ""Biostatistician"" title require at least a masters degree.\n\nI\'m a data analyst in the healthcare industry (private). I don\'t do a lot of statistics on a day to day basis but I build datasets and spend a lot of time in SQL and Python. I also have an undergrad in statistics, and I\'m finishing up my masters in statistics right now.', 'If I could go back, I’d do a lot more programming courses and do my masters in stats not math. Algebra did me no favors 🤡', ""Depending on the day, my work may consist of a combination of literature reviews, meetings with my peers and other teams who would like to use my models, writing Python and R code to build pipelines from raw data to forecasted outputs (and the usual debugging sessions), Bash scripting to run big tasks on our cluster computer, and creating presentations to share our findings. There's also a periodic crunch for publication deadlines. \n\nI'm currently working on methods to obtain population health and poverty data from areas that aren't easy to survey via satellite data.\n\nIt's a great sector because you get to work on big problems that feel meaningful.""]",27,59,https://www.reddit.com/r/statistics/comments/13e99fr/q_c_what_kind_of_careers_do_a_statistics_degree/
162,2023-05-11 05:53:47,[R] I think I've made a huge mistake in my systematic review,"\-So, my dissertation is due in 5 days. I've left it late but everything is written except for the results and discussion. Maybe too late for the issue that I've just run into.  
I am writing a systematic review on infanticide (killing of infants) in primates. In all of the cases I have included, infanticide has occurred. I have made certain predictions on what I think I will find in my research.

&#x200B;

  
\-My original predictions were that infanticide would most occur on unweaned infants, most after a takeover event and most in one male group as seen in previous research.  
Of course, only after I've collected all the data and placed it in SPSS have i realised- if infanticide occurred in all my cases, its going to be very hard to run a statistical test that actually says anything about the influences on infanticide. Yes I can be descriptive but in terms of saying anything significant- its going to be difficult. i feel like I've backed myself into a corner. I spoke to the statistics tutor at my school who told me I could shift my prediction to be ""Unweaned infants are more at risk than weaned infants after a take over event or in a one male group"" and do a logistic regression analysis on my data. But she told me she was unsure and google is saying that this is maybe not possible. I think it would be worse to do a test that means nothing and base my whole paper around it. My dissertation leader is away and unavailable.

&#x200B;

  
\-I have data on: date, species, region/site, sex of attacker, age of infant  
I also have binary yes/no data on if infanticide occurred in one male group, an unweaned/weaned infant and after a takeover.  
I also have a lot of data that looks at the study design of each of the studies.  


&#x200B;

My question is: How can i use this data the best I can ? Obviously its looking like its going to be extremely descriptive. Is logistic regression analysis possible?  
Luckily I've done a cluster network analysis on the authors so I at least have one complex method but It would be great to have more.  


I tried to be as clear as I can but I am freaking out a bit, please be kind ! Thanks in advance","[""Most cases or *all* cases? A variable is only a variable if it has variation... Does yours have variation? And if so, what is the variation?\n\nAlso - you said dissertation. Are you doing a PhD? It's a little strange to have a deadline sneak up on you for a PhD dissertation."", 'If infanticide occurred in each of your data points, is there variation in the intensity (?) of infanticide (maybe # of offspring killed)? If so, you could for example make your predictions be like infanticide of unweaned more likely at higher intensities.', 'Don\'t beat yourself up about your thesis. Getting this far is a huge accomplishment that the majority of undergrad students do not reach. I\'ve seen students in worse shape than you 5 days before scrape together a passable thesis. And don\'t blame yourself for having a fatal flaw in your analysis plan, either.- honestly, whichever faculty member is advising your thesis should have signed off on your analysis plan. \n\nYou\'re right that you can\'t do logistic regression predicting infanticide from your predictors, since all of your studies involve an occurrence of infanticide. I think it\'s hard to predict much about infanticide without also having some information about when it doesn\'t occur. It might be that the ""right"" statistical approach here isn\'t complicated, and it\'s just reporting a summary table of the infanticide studies, and describing trends (e.g., 40% of documented infanticide cases occurred with weaned infants, and in species A, 80% of documented infanticide cases occurred with weaned infants) or something. Unless you believe that your cases represent a random sample of all instances of infanticide, it wouldn\'t be appropriate to run statistical tests. And that\'s totally okay for a systematic review.', 'Maybe you could do a time to event analysis / survival analysis. So for example, with a Kaplan-Meier curve you’d have a chart with time on the x axis and percent remaining alive on y. And you’d draw a line for males and a line for females, and you could say : among cases of infanticide, infanticide tended to occur earlier (or later if that’s the case) among males, compared to females. And a log-rank test would tell you if the difference is statistically significant. \n\nI’m not at all sure what such a study design would be called, I usually work with cohort data, not cases-only data. \n\nUpdate: I don’t see much from a google search about case only study designs. It is a thing, but it seems to be used in studies about genetics, so I’m not sure how much would translate to what you’re doing.', 'I think this is probably the best way to go!']",1,12,https://www.reddit.com/r/statistics/comments/13e4jd6/r_i_think_ive_made_a_huge_mistake_in_my/
163,2023-05-11 05:48:25,[Q] Interval-censoring in the context of marginal structural models and g-estimation,"The causal inference literature is full of models that handle time-to-event data and time-varying confounding, like g-estimation for time-to event data and marginal structural Cox models.

However, I have not come across any papers discussing interval-censored data in the context of time-varying confounding; such as when you don't know the exact time an event has occurred, but only know that it lies within a specific interval.

Could anyone point me in the right direction on how to handle such data for time-varying exposures?","['When an exposure varies over time, the confounders also change over time. So you have time-varying confounding or different confounders at different time points', ""I can't say I have any direction to point you in, but I'm also interested in any answers! Interval-censoring generally seems like a hard problem to get around, especially with non- and semi-parametric models..."", 'Interval censoring always makes me think of survival models. what does “time-varying confounding” mean?', 'I’m not really aware of any canonical solutions to dealing with interval censoring. Most that I see of it is always in reference to very specific problems in published papers.\n\nI would say even that dealing with interval censoring is sufficiently under-explored that exploration of it via simulation in the time varying confounding domain could be worthwhile for a master’s thesis, or in greater depth with theoretical contribution could probably be a substantial portion of a PhD dissertation.', ""Wow, glad to see someone else is struggling with this issue. My team has a projects we're working on with interval-censored exposures we expect to causally impact later outcomes which are also sometimes interval-censored. At the moment, all I've been able to come up with for a practical solution is using conventional survival models for interval-censored outcomes to perform multiple imputation of the exposures. I doubt this is a robust solution, but I haven't tested it in simulation yet. As u/Kroutoner notes, I suspect this would make for a good thesis or dissertation topic.\n\nSince I'm actively searching for solutions, I'll make a note of this thread and let you know if I encounter work on the topic. Would love to know if you turn anything up as well.""]",11,6,https://www.reddit.com/r/statistics/comments/13e4e6c/q_intervalcensoring_in_the_context_of_marginal/
164,2023-05-11 04:37:11,[Q] Imputation of missing data not at random - using highest value?,"I have a dataset where one variable is ""time taken to perform a task (sit - to stand 5 times)"", such that longer it takes, the worse the performance. I have noticed 25% is missing data, and the reason is ""not able to perform"" so this not random missing.  
I get very statistically significant correlation with other variables if I impute the mean time taken. However, in reality, the imputation should be closer to ""infinity"" as it is 'unable to perform'. Someone suggested I could actually use the ""highest value and add 1 second"", which makes the correlation even more significant.  
  
Is this a legitimate strategy in this situation? What is this method called?","['You get half-credit for trying to do something reasonable rather than blithely assuming your missing data was MCAR.\n\nBut I think you\'d do better to create a 2-stage model, with ""did the person complete the task"" and ""if so, how fast"" as two DVs.', ""...no it's not, it's just moving the data from the time domain to the frequency domain."", 'You can’t impute this since it’s an actual data entry. You either have to drop these observations in their respective time categories such that sample size becomes less and less as time goes on, or you need to treat this as some kind of survival analysis instead where p(failure) increases over time.', 'A trick I just learned from some biostatisticians on a similar problem: invert the values to a velocity.  Take 5/time to get a variable whose units are sit-to-stand per second.  Then all the missing values that are ""not able to perform"" can be set to zero.', 'Isn’t this technically censored? So you might interpret the missing as actually indicating that those individual would have taken at least X-s to perform the task… then you could model such right censoring. But without more details on how the assessment was performed it’s hard to say']",2,13,https://www.reddit.com/r/statistics/comments/13e2gdk/q_imputation_of_missing_data_not_at_random_using/
165,2023-05-11 02:50:38,[Q] Need help calculating statistical probability,I'm trying to figure out what the probability would be if there's a 10% chance of something occurring and that thing doesn't occur for 100 straight instances.  I'm not sure what formula I would need to use but if you could provide the formula that needs to be applied I'd highly appreciate it!,"['You are looking for the binomial distribution formula. The chance of this event happening once is 90%, the chance of it happening n times without another outcome is (.9)^n.\nMore information and full formula here https://byjus.com/binomial-distribution-formula/', 'To compute the probability of an event (with a 10% chance of occurring) NOT occurring for 100 times, that\'s just a 90% chance a hundred times.   \n\n\nFor the ""100 times"" condition, you have to also know that if we want to know the probability of several outcomes as a result, we multiply them together (assuming they are not correlated). Our formula says that we need to take the chance of the thing not happening (which is 90% in this case), and multiply it by itself 100 times.   \n\n\nTo generalize, the probability of an event not happening over some number of instances:\n\n(1 - probability of the event occurring) \\^ number of instances\n\nIn our case, (1 - 0.1) \\^ 100 = 0.9 \\^ 100. This is approximately 4e-87, which is approximately a 0% chance of happening. That is, again, assuming that the event outcomes are completely independent of one-another. (The result of one success/fail does not influence any of the other successes/failures.)', 'Thank you so much, super helpful!']",0,3,https://www.reddit.com/r/statistics/comments/13dznmr/q_need_help_calculating_statistical_probability/
166,2023-05-11 02:06:33,[Q] Biostats texts for applied mathematician,"Hello r/statistics,

I am looking for a book (or several) that I can study certain elements of biostatistics from. I'm making this post in the hope that someone can recommend something appropriate for my background: I am a PhD in mathematics and I have a masters in applied and computational mathematics. I once took a course in the foundations of (measure-theoretic) probability - mostly about understanding random variables and distributions. I also once took a course in SAS programming as part of my masters but I found that there was little to not statistical content; I mostly learned about manipulating simple datasets.

Here's the topics I am interested in:

* A short, general overview of biostatistics, especially as it is used in the pharmaceutical industry
* Regression analysis, especially function transformed (i.e. log) regression
* An overview of ANOVA, RMAN(C)OVA, etc
* An introduction to categorical data analysis, especially Mantel-Haenszel
* how to do these tests in SAS

Thanks for your time,  
/u/mattlink92","['Perhaps the Statistical Inference by Casella and Berger would be a good starting point from the mathematical statistics side.\n\nThe last chapters are about linear regression and may be useful\n\nStatistical Inference https://mybiostats.files.wordpress.com/2015/03/casella-berger.pdf', 'Ah, a kindred spirit.  I did my masters in applied math before doing a PhD in Biostats.\n\n* Jumping right into pharmaceutical stuff would probably see you using mixed effect models.  Might I recommend learning more about regression first?  I liked [this](https://www.wiley.com/en-ca/Biostatistical+Methods:+The+Assessment+of+Relative+Risks,+2nd+Edition-p-9780470508220) book as an intro to some basic statistical approachs in biostats.  It has some SAS code.\n* Frank Harrell\'s ""Regression Modelling Strategies"" and ""Biostatistics for Biomedical Researchers is also good"".\n* If you learn about regression, you don\'t need to know about ANCOVA and the variations thereof.\n* Agresti\'s ""Categorical Data Analysis"" is another seminal text in GLMs and categorical data analysis.', 'You can check the following books on Biostatistics and Clinical Trail Management\n\n1.Clinical Trial Management – an Overview \n\nISBN-13 \u200f : \u200e 978-1393386179\n\n2. Essentials of Bio-Statistics: An overview with the help of Software - R Programming \n\nISBN-13 \u200f : \u200e 978-1723712074\n\n3. Essential of Biostatistics: An Overview with the help of software- 2nd in Series: Pocket Guide - SAS\n\n[www.ijsmi.com/book.php](https://www.ijsmi.com/book.php)', 'Following!', ""Thank you! I'll check it out""]",14,7,https://www.reddit.com/r/statistics/comments/13dyh8t/q_biostats_texts_for_applied_mathematician/
167,2023-05-10 22:48:40,[Q] Standardization with multimodal data,"I'm attempting to apply PCA to metabolomics data, but am running into a gap in my understanding.

I need to standardize the data so that PCA will work right (ie, not prioritize variables with high variance/intensities). The most straightforward explanation of the methods I got from here: [https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02#:\~:text=turn%20to%20StandardScaler.-,StandardScaler,values%20by%20the%20standard%20deviation](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02#:~:text=turn%20to%20StandardScaler.-,StandardScaler,values%20by%20the%20standard%20deviation).

However, I am technically expecting my control set and test set to have different means and probably variances, which would make my base data multimodal. Do some standardization methods (such ""StandardScaler"" which affects the variance of the dataset) distort multimodal data by treating it as part of a single distribution? Do other people who work in bioinformatics use the StandardScaler (0 mean 0 stdev), or the RobustScaler (0 median, divide by interquartile range), which I imagine leaves the original distribution relatively intact? I understand that using the RobustScaler would probably result in higher variance features being prioritized, but is that a bad thing for metabolomics data (ie, wouldn't features affected by the treatment look like they have higher variance)?

For context, my goal for using the PCA is to discover a smaller set of features with explanatory power in the data before directly inserting class information.","['Most implementations of pca standardize the features by doing the eigenvalue decomposition of the correlation matrix, rather than the covariance matrix as most people are commonly taught pca.', ""The PCA loadings can span orders of magnitude.  Different strategies for scaling probably only differ by a factor of, say, 2.  Therefore, I don't think the results will be overly sensitive to the scaling strategy.  But you might want to read a few papers doing similar work to see if there is a standard practice in this specific field."", 'Thanks this is really helpful!']",1,3,https://www.reddit.com/r/statistics/comments/13dswcm/q_standardization_with_multimodal_data/
168,2023-05-10 21:20:16,[Question] question about confidence levels.,"Is it possible to find the confidence level while only knowing the binomial confidence interval, the margin of error, and the sample size?","['Confidence levels are chosen prior to constructing the confidence interval.\n\nIf you have the binomial confidence interval constructed, then you should have chosen alpha already. Your confidence level is 1-alpha.', 'You can determine the confidence in the following way:\n\n1). Assuming you are using the normal approximation here you have a interval of the form\nHat(p) ± z(alpha)*sqrt(hat(p)(1-hat(p))/n)\n\n2). As you know the interval and the margin of error you can solve for hat(n) and know n you can solve for z(alpha) \n\n3). Using z(alpha) you can plug into the inverse normal CDF to solve for alpha', 'i’m sorry, i’m new to notations, what do those mean?\n\nHat(p), Hat(n), z(alpha) all flew over my head haha', 'i’m sorry, i’m new to notations, what do those mean?\n\nHat(p), Hat(n), z(alpha) all flew over my head haha\n\nmy current question that is confusing me is given the binomial confidence interval of 0.591-0.749 and a sample size of 25 trials, what is the confidence level that they used? i’m so confused. it’s also asking me what the success rate is within the sample first which is also confusing, thanks for any help, you look like you know what you’re talking about haha', ""Here are the definitions for the notation:\n\nhat(p) = # of success / # of samples (think proportion of heads with coin flips)\n\nn = the number of samples\n\nz(α/2) = The 1-α/2 quantile of the normal distribution.\n\nI miss-typed previously there is no hat(n).\n\nYou can read about the confidence interval here: [https://en.wikipedia.org/wiki/Binomial\\_proportion\\_confidence\\_interval](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval)\n\n&#x200B;\n\nRegardless, here is plan to find the alpha level in more detail.\n\n1). Find the center of the interval: (.749 + .591)/2 = .6685, This corresponds to hat(p).\n\n2). We can then find z(α/2)\\*√(hat(p)(1-hat(p))/25) = .749-.6685 = .0805, and solve for z(α). In this case (as we can find √(hat(p)(1-hat(p))/25) = .09415), we have z(α/2) = .85.\n\n3). Using a Z-table we can find the coverage of this interval to be about 60% (I'll leave that part to you)""]",2,8,https://www.reddit.com/r/statistics/comments/13dqgn3/question_question_about_confidence_levels/
169,2023-05-10 19:49:03,[Q] Does Anyone have access to Statista and help out a poor college student? Their yearly rate is egregious and I need data for the research thesis I’m working on,"I cannot afford the yearly subscription and they do not have any monthly plans which I was willing to pay for but they don't exist.

Any chance someone can help please?

 [https://www.statista.com/outlook/dmo/eservices/dating-services/online-dating/united-kingdom](https://www.statista.com/outlook/dmo/eservices/dating-services/online-dating/united-kingdom)

Comment if you can help and I can DM you my email.

Thank you so much","['Yes, dm me', ""Done\n\nYou're a lifesaver thank you"", 'Can I please email you too ?I’ll pay', 'Go ahead!', 'FACE THE LEAD!']",0,7,https://www.reddit.com/r/statistics/comments/13do8pp/q_does_anyone_have_access_to_statista_and_help/
170,2023-05-10 16:28:33,[Q] How do you compute how many times (based on statistical probability?) four straight heads would show up in a coin toss of 100 times?,"Assuming a fair coin toss of 50% heads and tails showing up.


Five to seven straight heads is still considered one count, but eight straight heads is considered two count.


Given these information, what is the chance that four straight heads show up once, twice, 3x, 4x, and so on in 100 coin toss? Is there a formula for it?


Thank you.","['There are two ways to go about this. \n\nThe first way would be to just simulate it. Flip 100 coins (hopefully through the use of a computer) count the number of 4 in a row. Repeat many times (for some reason 10000 is a commonly chosen number of repeats) and then see the empirical pmf. \n\n&#x200B;\n\nThe second way is to just calculate the probabilities. There is some issues with this because you have defined things in an awkward way (I would probably think that 5 straight heads counts a two sets of 4, the first 4 and the last 4). Do you need the exact value?', 'I’m no expert, but the probability asked is for “out of 100 tosses” and your reply is “for any given toss” and subsequent throws in succession. I believe using your formula for any toss as *P* in the binomial equation will the right answer.', 'Consider your group of 4 heads a unit and calculate the combinations', 'Not 100% sure on this, but scrapped together a simulation based on some of the comments here and made a plot of it. Will include the code here in case anyone else sees an error (especially considering how you group by the final cumulative distribution since not all trials will have the same lengths).\n\nBut I think the answer is 3% of the time.\n\nIn R of course. \n\n`# [Q] How do you compute how many times (based on statistical probability?) four straight heads would show up in a coin toss of 100 times?`  \n`n_simulations <- 10000`  \n`simulations <- c()`  \n`for (i in 1:n_simulations) {`  \n  `simulations[[i]] <- rbinom(n = 100,size = 1,prob = .5)`  \n  `}`  \n`# Clean Up for dataframe`  \n`sim_name_vector <- c()`  \n`for (i in 1:length(simulations)) {`  \n `sim_name_vector[[i]] <- paste(""Simulation"", i)`  \n`}`  \n`names(simulations) <- sim_name_vector`  \n`p <- lapply(simulations, rle)`   \n`t <- do.call(rbind.data.frame, p)`  \n`t$sim <- row.names(t)`  \n`t$sim <- gsub(pattern = ""\\\\.[0-9]+"",replacement = """",x = t$sim)`  \n`# Plot It`   \n`#simulation_data`   \n`simulation_data <- t |>`  \n  `tibble() |>`  \n  `arrange(values, lengths) |>`  \n  `group_by(values, lengths) |>`  \n  `count(lengths) |>`  \n  `ungroup(values, lengths) |>`  \n  `mutate(tot = sum(n),`  \n`freq = n/tot)`   \n`# Find how frequently sequences of four sets of heads happen`   \n`simulation_data |>`  \n  `filter(values == 1) |>`  \n  `filter(lengths == 4 | lengths == 8 | lengths == 12 | lengths == 16 | lengths == 20) |>`  \n  `mutate(final_sum = sum(freq)) |>`  \n  `select(final_sum) |>`  \n  `distinct()`  \n`# Plot it`   \n`reddit_plot <- simulation_data |>`  \n  `ggplot(aes(x = lengths, y = freq)) +`  \n  `geom_point(shape = 4, size = 4) +`  \n  `facet_wrap(~values) +`  \n  `theme_bw() +`  \n  `#stat_summary(fun = ""mean"", colour = ""red"", size = 1, geom = ""point"") +`  \n  `scale_y_continuous(limits = c(0,1), seq(0,1,.1),name = ""Probability"") +`  \n  `scale_x_continuous(breaks = seq(0,20,1)) +`  \n  `labs(title = ""Consecutive Coin Flips"",`   \n`subtitle = ""10,000 Simulations"",`   \n`x = ""Number of Consecutive Flips"",`   \n`y = ""Probability"")`  \n`reddit_plot`  \n`ggsave(filename = ""~/Desktop/reddit-plot.png"", height = 3, width = 6, dpi = 300)`', ""If I were going to try to calculate this seriously I'd probably just simulate, but I think there are multiple other avenues of attack. Either way you do need to be a little more precise - how many sequential runs of 4 are contained in a sequence of 100 heads?\n\nPending a more precise definition, two things come to mind:\n\n1. If you think about the sequence of tosses as a Markov Chain, then there are some nice results you can use. [Here's an example](https://stats.stackexchange.com/questions/264811/coin-toss-probability-of-a-run-of-certain-length-out-of-a-longer-sequence) of trying to calculate the probability of at least one run of a given length in a fixed number of tosses, which is not quite what you're asking but related. \n\n2. Another general strategy that comes to mind is brute force - you'd still need to use a computer but it seems feasible to calculate P(one run of length 4), P(two runs of length 4), .... etc.  via careful enumeration.""]",9,10,https://www.reddit.com/r/statistics/comments/13dkcok/q_how_do_you_compute_how_many_times_based_on/
171,2023-05-10 11:41:09,[Q]What are skills you wanna master at work to make your life easier?,,"['File management', 'Coding to automate tasks like file organization, running forecast generation over the weekend, and scraping the web for data.', 'Convincing people to care about data quality, management, and pipelines', 'Office politics']",1,4,https://www.reddit.com/r/statistics/comments/13df9eb/qwhat_are_skills_you_wanna_master_at_work_to_make/
172,2023-05-10 08:32:19,[Q] plot confidence interval of a bivariate,"I have a question regarding the confidence interval of a bivariate.
I have found the confidence interval of each variable, and now I want to plot the confidence interval in dimension 2.
How is this usually done ? 
Thank you.","['confidence interval for what parameters? of what distribution?\n\n>  How is this usually done ?\n\nWell, not from the marginal CIs, but jointly. Specific details will depend on stuff you left out.', 'It depends on what estimates the confidence intervals are based on. There are some times where you can do this (for example with sample means and variance) when you can have two marginal of coverage sqrt(1-alpha), which by independence will give 1-alpha coverage. You could also to a bonferroni region where you have the marginals with 1-alpha/2 coverage to give a Cartesian product of the sets 1-alpha coverage. Without more information the bonferroni correction is the way to go.']",1,2,https://www.reddit.com/r/statistics/comments/13db8ur/q_plot_confidence_interval_of_a_bivariate/
173,2023-05-10 07:34:30,[Q] Applying for jobs while feeling unqualified. How was it like for you?,"I’m graduating soon with my bachelors in statistics, but I still feel super unprepared for the real world. I have found some jobs online, but writing cover letters for them makes me nervous because that’s basically me overhyping myself. 

My worst nightmare is getting a job and being expected to do something that I either did not learn, or I forgot how to do. I’m very curious how you guys got started, how was it. Also recommendations on where to apply as an entry level data analysis/scientist would be helpful as well. 

Also lots of jobs online require masters degrees, but I still want to apply to them. Landing a job that requires such a degree would be cool, but what might be expected of me is worrying me. All my statistical analysis experience comes from my courses.","['There\'s a saying that ""If you have everything described in the job description you\'re probably overqualified"". That helped me with applying', 'Your first focus should be  get a job. You can’t get experience without a job. \n\nIf you just graduated, your manager will know you need training and they also know you’ll be cheaper to hire than someone with a lot of experience, that’s why there are entry level jobs.\n\nGood managers will prefer someone with good values and a good attitude and willingness to learn but no experience, than someone with a lot of knowledge and bad attitude.\n\nWhen you get a job, ask why they chose you, you’ll see that they focous on your potentiential, and please ask anything that you don’t know. If you are in a good company with a good manager, they’ll be glad you ask instead of hiding what you don’t know.', ""I do sometimes wonder if you ever feel really qualified. \n\n>  being expected to do something that I either did not learn, or I forgot how to do\n\nThis is very likely to happen. You need to keep in mind that you'll rarely have to do something instantaneously and that you're able to *go look things up* or *ask people questions*. \n\nGood training can't teach you how to solve every problem, but it should equip you with the tools to be able find some way to do it most of the time. You have many resources, life is not an exam.\n\n> All my statistical analysis experience comes from my courses.\n\nThat's less good; you can choose to do things outside your coursework, right? But once you have a job, the job itself should help give you plenty of things to be getting experience from."", ""I would recommend doing something else during this time besides just looking for jobs. The most obvious thing is to continue learning things that the industry requires (especially technical stuff such as coding). Online courses are useful (they always have discount for students). But doing an online project from Kaggle or DataCamp is something I *strongly recommend,* since it will give you a taste of the real world and something nice to put on your resume.\n\nWorking part-time or volunteering are not bad ideas also. Expertise and skills are important, but companies also desperately need people who can work well with others. I saw some talented people who ended up nowhere because they are too stubborn and uncooperative.\n\nEither way, keep the wheels running and don't let any gap. Your statistics degree means you possess the fundamentals for the most trendy things nowadays. There are a ton of things for you to do, and all you need is to take an extra step."", ""> If you just graduated, your manager will know you need training and they also know you’ll be cheaper to hire than someone with a lot of experience, that’s why there are entry level jobs. \n\nI'm not sure this is true everywhere. At least where I am, every entry level role gets hundreds of applicants and the ads generally have a lot of requirements for a low pay. A lot of employers want experienced candidates for entry level pay, which they're sadly able to get.""]",47,16,https://www.reddit.com/r/statistics/comments/13d9wv4/q_applying_for_jobs_while_feeling_unqualified_how/
174,2023-05-10 04:41:27,[Q] how would you visuallise my data?,Hi everyone! I wanted to create a clustered boxplot but encountered a problem. The sample sizes of the subgroups on the left side all are all 2 and thereby do not properly form boxplots. The subgroups on the right side all form proper boxplots. How would you visually compare this? Should i just display my individual data in a table? Thank you in advance,"['Compare what? What features of the data are you trying to visualize?', ""You really can't do a lot of meaningful statistics with a sample size of two. Even if you could draw a box plot it wouldn't tell you very much. You can plot the mean and the range but... It's not going to be very meaningful information.""]",0,2,https://www.reddit.com/r/statistics/comments/13d5ctr/q_how_would_you_visuallise_my_data/
175,2023-05-10 03:26:11,[D] Is time series analysis much better than just using common sense?,"I did some poking around about sales forecasting.

It should be possible to predict monthly or seasonal sales of an item by looking at previous sales of the item. However, you can do this just by looking at a list of sales or taking average sales per month. However, the number of items available seems to be conspicuously absent from these computations. Obviously, you can't sell more than 24 widgets if you only have 24 widgets.

Similarly, it should be possible to predict what hour of the day people go shopping or other similar data.

My plan was to just cram data into a neural network and hope for the best. There's a limit to much data you can cram into a neural network in a reasonable amount of time, and neural networks are kind of nonsense anyways.

I noticed that other people use more advanced mathematical tools for this. It looks like they have different mathematical models, and they attempt to see which model is most accurate for the data that they have. What that says to me is that the mathematical models are wrong. When they're right, it could just be noise. It looks like they're using math to connect the dots on the graph and produce some sort of function for predicting sales.

I assume this is better than having a 5-year-old draw on a graph with crayons, but is it really that effective? Shouldn't inflation data, production costs, and the competitive landscape be factored into this somehow?

There seems to be a lot of sales forecasting going on, and it sounds to me like a bunch of people are making a bunch of overhyped sales forecasting products. It's just that no one can tell because the math is complex.","['> I assume this is better than having a 5-year-old draw on a graph with crayons, but is it really that effective? Shouldn\'t inflation data, production costs, and the competitive landscape be factored into this somehow?\n\nNoisy, highly seasonal, and poor quality data is the norm at my work, so ""looks like a 5 year old drew this extrapolation curve with crayons"" is the gold standard I work towards', 'Time series analysis can indeed be a better approach than relying solely on common sense when it comes to sales forecasting. Common sense can certainly be helpful in understanding the underlying factors that may affect sales, but it is usually limited by biases and incomplete (and potentially inaccurate) information.\r  \n\r  \nUsing past sales data to predict future sales can be a good starting point, but it is important to use appropriate statistical techniques to account for factors that may influence sales, such as changes in the competitive landscape, inflation, or production costs. Simply taking averages or looking at raw data can oversimplify the complexities of the data and result in inaccurate predictions.\r These math tools in statistics can help account for these factors and provide more accurate predictions. These tools do not necessarily produce a ""function for predicting sales"" but rather provide a statistical model that can be used to make predictions based on historical data and other relevant variables.\r  \n\r  \nNeural networks can be a useful tool for sales forecasting, but they require appropriate data preparation and parameter tuning to produce reliable results. Additionally, as you noted, there are limits to how much data can be processed in a reasonable amount of time, and neural networks can be prone to overfitting if not used correctly.\r  \n\r  \nSo, while there may be overhyped sales forecasting products out there, using appropriate statistical techniques can provide more accurate predictions and help businesses make more informed decisions.', '""Time series forecasting"" can include a whole slew of models, of which you seem to be considering only a particular kind.\n\nIt also sounds like you have information that\'s not in your models; relevant external information should inform your models whenever possible, for all that it might make things more complicated to do so.\n\n>  Shouldn\'t inflation data, production costs, and the competitive landscape be factored into this somehow?\n\nCertainly, if they\'re relevant, they should come into the modelling of the variable, and into the forecasting.\n\nSometimes ""obvious"" factors are already fairly well covered by fairly naive models like ARIMA (because they\'re also strongly related to previous values in the series which are being used in the predictions), but sometimes not. \n\n>  it sounds to me like a bunch of people are making a bunch of overhyped sales forecasting products\n\nOh, no doubt. What people sell and what people use may both fall far short of what should reasonably be done.', ""It's actually a mash-up from a recent review of Springer's Applied Predictive Modeling and my experience with time series analysis. On mobile, it's easier to copy and paste the original post and reply to the concerns in-order. I'm assuming that's the ChatGPT flavoring you had noted. \n\nOr maybe I'm a ChatGPT bot pretending to be human. In that case, who knows?"", ""My expectation is more items that there were a statistically insignificant number of or that weren't consistently in stock with no indication that they weren't in stock.\n\nI don't know what to do with it, but that presupposes that I ever actually have the data.""]",0,8,https://www.reddit.com/r/statistics/comments/13d3bib/d_is_time_series_analysis_much_better_than_just/
176,2023-05-10 01:33:15,[meta] statistically determining the best way to become happy,"We have a data set with millions of data points. Each single data point represents one method to become more happy, like meditation, working out, relationships, watching youtube, etc. Alternatively, a data point can also be a combination of other data points, for example a religious teaching containing multiple methods. Each data point has a value of  -100% (leading to 100% despair) to +100% (leading to maximum happiness).

The problem is: The value of most of the data points is hidden to us and we don't have the time to check every single of these millions of data points by our own.

How do we find the data point that leads to the highest happiness? I have a candidate, but how can I be sure that candidate is the right one as there are millions of data points with hidden values? Any tips narrow down the list? 

This question might seem technical, but actually, isn't this the only game that we as humans are playing all the time? Constantly trying to find happiness? Therefore, I think it's highly important to thing strategically about the right approach.","['Some questions cannot be answered by statistics.', 'where should I ask then?', 'Why did you post essentially the same post twice?', ""Latent variable models might be applicable here. A web search for that term will find some resources.\n\nYou'll have to be careful about cause and effect here. It's hard to gauge the effect of some intervention when people choose the intervention for themselves, because there's some correlation between the choice and the effect. Maybe there's some kind of proxy for a random selection -- I saw a reference to a study which compared people who had been drafted into the army with non-draftees. Dunno what else there might be.\n\nIn general, I don't see why a statistical approach to happiness couldn't work at least as well as the multitude of other approaches that have been proposed, so have at it, I would say."", 'lul']",0,13,https://www.reddit.com/r/statistics/comments/13d07rk/meta_statistically_determining_the_best_way_to/
177,2023-05-10 00:40:37,[Meta] Statistically vetting a spiritual teaching?,"I have come across a spiritual teaching that claims to lead to ultimate happiness, highest bliss.Now I need your help: Please go through the following statistical reasoning and tell me if this reasoning is true or false:

Now, there are millions of ways or let's call them strategies that claim to lead to happiness, like:  
\- Buying certain products or services (millions)  
\- Reading and following the teaching of certain books (roughly 100k-1million)  
\- working on relationships  
\- working out  
\- following certain religions (roughly 10000)  
\- etc

Therefore, statistically, the probability that the teaching I have come across is true, is one in millions. Do you agree or did I overlook something?

**EDIT 1**: I think statistically we have a distribution with millions of data points. Each data point represents one strategy and it's effectiveness in increasing happiness, on a scale from -100% (leading to 100% despair) to 100% (leading to complete happiness). That's what we know. Now, how probable is it that the spiritual teaching I have found is actually the data point with the highest effectiveness?

**EDIT 2:** The underlying assumption here is, that each strategy has the same probability. This places an arbitrary, excessive overweight on the materialistic strategies like buying products or services, simply because there are more of them available. How do we correct this overweight? ","['How high are you right now', '""the probability that the teaching I have come across is true, is one in millions"" - for that to be true, you would need to have randomly sampled that one teaching from a pool of all of the other millions of alternatives. If you heard about this teaching in a non-random way, a friend told you about it, or you found a book about it - that is not a random sample. It may be that more effective teachings are more likely to be repeated and written about.', 'I think this is a bit far from statistics. I will just say your assuming several things: there is one state of happiness that is the same for everyone and also that you are as likely yo get there with any possible action. The first point seems really not true in a logical way since not everyone gets happy the same way, and the second one is also linked to this and not true. \n\nI think this is not really a stats question, but I\'ll try my best to explain the second point (you can\'t always assume equal probability to all possible events) as I think this is a common way of thinking if you\'ve never seen prob distributions. Let us forget the philosophy behind it and let\'s assume there is a right ""way to happiness"", then we can\'t say anything about how likely it is to find it without knowing more. Imagine it is turning 20 yrs old for example. Then the probability of you finding it is 100%, as you will eventually age. If the way of happiness is to visit hawaii, then the probability of it happening to you is affected by your environment. If it is earning 1000000€ in a casino, then it is very unlikely. I could go on, but the point is that the event changes completely the probability.\n\nThe point is, events a lot of times do not have the same probability of happening, and thus we need to asign different weights to different events.', 'Hi - how are you.', 'Consider a scenario where people are randomly assigned to strategies for happiness, and then each group is surveyed with the question ""how happy are you, on a scale from 0-10?"". For data we would report the mean and standard deviation of each group.  Now we want to know, how do groups associated with your teaching rank among the others? \n\nIf you feel this sufficiently matches your scenario, I suggest using it instead because you\'ll get better help from us.']",0,14,https://www.reddit.com/r/statistics/comments/13cyqsr/meta_statistically_vetting_a_spiritual_teaching/
178,2023-05-10 00:02:09,[Q] Is Akaike Information Criterion a reasonable metric to use when choosing an appropriate Time Series model?,"So, theory states that the Time Series model with the lowest AIC technically the best model. But, does this always hold true? I am building time series on the quarterly Ontario population for the past 23 years. However, when I make predictions on the past 19 quarters, the data has a significant vertical zig zag behaviour which is counter intuitive since there was no historical drop in population. Also, the correlogram values out of the lower bounds at lag 5. Is an issue perhaps is that my grid space is too small? My range for p, d, q goes from 0 to 5. Should  I perhaps increase it?","[""AIC and BIC are good metrics that tells you how much information you've extracted from the series. Diebold (2007) suggests to follow BIC if AIC and BIC disagree, opting for the more parsimonious model. This is good enough for modeling. \n\nBut for forecasting, I think most important is the model's root mean square error when backtested against historical data the model has not yet observed.\n\nEdit: if the residuals of your model exceed the bounds, you still have unaccounted seasonality in your residuals, which is not good. In general, a good forecast/model should have unforecastable/un-model-able residuals.\n\nHave you considered using time trends, breaks, or data transformations like taking logs? How about introducing exogenous variables into the data?"", 'It\'s impossible to specify the ""best"" model selection strategy without specifying what you\'re trying to use the model for, and what properties you want the model to have.', 'I think Minimum Description Length is the superior model selection principle. There are various versions and implementations in specific models, but if you can find an appropriate one already computed for you I would use it.', '> theory states that the Time Series model with the lowest AIC technically the best model\n\nI don\'t think that\'s quite what ""theory states"".\n\nWhere are you seeing that?', ""Still insufficiently specified. Like the person you responded to said, you have to consider not just what you're trying to fit, but *why*. How will be model be used? Is it more costly to over-estimate or under-estimate? Is it more important to correctly estimate the trend, the variance, the seasonality?""]",16,16,https://www.reddit.com/r/statistics/comments/13cxm9v/q_is_akaike_information_criterion_a_reasonable/
179,2023-05-09 23:56:12,[Q] Comparing two unequal groups,"I need to compare two groups of unequal group sizes to determine if the results are equivalent or comparable. The groups are two different populations that are from different manufacturing processes, and I’m trying to determine if the in-process testing results for certain parameters are the same. 

Problem is, one of the groups has a very small sample size (n=8), while the other group has around 30 samples. 

I considered performing a Kruskal-Wallis test, but my understanding is that distributions of the data need to have a similar shape and spread, which they do not. The group with with the larger samples size has a much wider spread than the group with the smaller sample size. Does it really matter, or should I consider a different test?","['For two groups, Kruskal-Wallis and Mann-Whitney are identical.', ""Hey there! So, you want to compare two groups that have different sizes, right? The two groups come from different manufacturing processes, and you want to see if their in-process testing results are pretty much the same.\n\nThe thing is, one group has only 8 samples, while the other group has about 30 samples. That's a bit tricky!\n\nYou mentioned the Kruskal-Wallis test, but you're right that it might not be the best choice here. That test works best when the two groups you're comparing have a similar shape and spread, and it sounds like that's not the case for your groups.\n\nInstead, you could try using a test called the Mann-Whitney U test. This test is useful when you have two groups with different sizes and different shapes/spreads. It helps you figure out if the two groups are likely to come from the same population, or if their differences are significant.\n\nJust remember that when working with small sample sizes, like your group with only 8 samples, the results might be less reliable. But the Mann-Whitney U test is still a good place to start for your situation. Good luck with your comparison!"", '> I considered performing a Kruskal-Wallis test, but my understanding is that distributions of the data need to have a similar shape and spread, which they do not. \n\nYou can find this claim in numerous places but it\'s not at all true.\n\nThe need to have similar shape and spread is not in the data, but in the populations, and specifically *when H0 is true*, in order to get correct significance levels, and hence p-values. \n\nThe conditions you need under H1 can be considerably weaker, unless you feel the need to impose conditions beyond those needed for the test to have the usual things people expect of a test (an actual  significance level close to that selected from the available significance levels, consistency against some set of alternatives, etc). Since a pure-equality null is almost certainly false, the data aren\'t necessarily even relevant to this consideration.\n\n> Problem is, one of the groups has a very small sample size (n=8), while the other group has around 30 samples.\n\nHuh? Why are you using *Kruskal-Wallis* if you only have two groups? \n\n>  The group with with the larger samples size has a much wider spread than the group with the smaller sample size. Does it really matter\n\nIt might or it might not. It depends on what the situation would be *if H0 were true*. For one example imagine a case where spread and mean increase together (e.g. the sort of thing you might see with a gamma distribution with fixed shape parameter). You would have that the shape and spread are the same when H0 is true (so the required exchangeability holds), but not when H1 is true, so if H1 is false and you see different spread in the data, that might be of little consequence. What matters is whether it would be reasonable to think they\'d be more alike when H0 were actually true (e.g. one example where it makes sense is where it makes sense to thing that a treatment has no effect at all, so that when H0 is true, nothing happens; what you see under H1 may well allow both shape and spread to change as you move away from H0)\n\n>  determine if the results are equivalent or comparable\n\nThis ""equivalent or comparable"" phrasing sounds more like you need an equivalence test, not a test with an equality null.\n\nWhat is your response variable?', 'And neither test actually necessitates the two distributions have similar shape and spread, unless the test is being used as a test of the median.', ""You are right!\n\nSince we are comparing two groups, the Mann-Whitney U test is still the more appropriate choice for the situation. The Kruskal-Wallis test is designed for comparing three or more groups. The Mann-Whitney U test will help determine if there's a significant difference in the rankings of the two groups, regardless of their shape and spread.\n\nRight?""]",2,5,https://www.reddit.com/r/statistics/comments/13cxdlc/q_comparing_two_unequal_groups/
180,2023-05-09 23:05:26,[Q] How to calculate rate disparity using cars,"Let's assume that there are 200 cars on the road in an isolated town that comprise of 5 different makes. We'll go Tesla, Ford, Chevy, Nissan, and Maserati. 

Of the 200 cars, 130 are Tesla. 26 are Ford. 26 are Chevy. 11 are Nissan, and 7 are Maserati. 

There's only one mechanic in town who's constantly busy. He's always working on a car, but he notices a significant difference in the rate of the make of cars coming in compared to how many there are on the road in town. 

Because there are so many Tesla's (65% of the total cars) you would think that he would be working on those the most, but he's not. In fact, just slightly more than half of the cars that come in are Chevys.

Trying to solve for the disparity, the mech sees that there's no noticeable difference in the roads the Chevys drive on compared to the others. Mileage of each car hovers near average. If anything, the Chevy dealership in town offers the Chevy owners free tire rotations and oil changes, while other makes do not have such luxury. He just can't put his finger on it.

Given the Chevys only account for 13% of the cars, but account for 52% of all trips to the shop, what is their likelihood of visiting the mechanic more often than say, the Tesla, who represents 65% of the cars, but only 40% of the shop visits.","['Quick thoughts:\n\nThis sounds like set-up for a *chi-square goodness-of-fit test*.  Looking at examples may give you some insight.\n\nYou might be interested in the *odds ratio*.  Perhaps starting by just thinking about Chevys and Teslas.', ""Thanks Sal. I'll see if I can find a tutorial for that""]",2,2,https://www.reddit.com/r/statistics/comments/13cua9o/q_how_to_calculate_rate_disparity_using_cars/
181,2023-05-09 22:36:14,[Q] Textbook recommendations - comprehensive exams,"I'm at the end of my first year as a PhD student in Biostatistics and will be writing the comprehensive exams for my program at the end of the summer. One of the exams will be purely math stats, and one applied stats. I'm planning to use Casella & Berger mostly, also have read most of Keith Knight's book, Mathematical Statistics,  and All of Statistics by Larry Wasserman so will be using those ones as well. Are there any other good texts that this group would recommend? Does not need to be an intro text.

Sorry if this has already been asked, used the search bar but results didn't match what Im looking for.","[""Will the pure math stats exam use decision theory? I haven't heard of the Knight book but if you will be tested on it. The two Lehmann books (TPE, TSH) are classics in that regard. Although It is worth mentioning they are a level above Casella and Berger IMO. For the applied side there are nearly an infinite number of books. Does your department focus on experimental design, causal inference, bayesian methods, etc. Knowing this will help me to recommend some more.""]",2,1,https://www.reddit.com/r/statistics/comments/13cs64u/q_textbook_recommendations_comprehensive_exams/
182,2023-05-09 19:34:49,Are F- and t-tests for regression coefficients always meaningful? [Q],"For example we might predict body fat using abdominal circumference as our predictor. 

y = b\_0 + b\_1(circumference)

It doesn't make sense to have a person with zero abdominal circumference so do we care if its coefficient is statistically significantly different than zero?","[""The coefficient b\\_1 is about the *effect* of circumference on body fat. The coefficient being 0 wouldn't mean that there's somebody with 0 circumference, but rather that circumference has 0 effect on bodyfat. That would be an interesting conclusion.\n\nFor the intercept term b\\_0 it's true that it's often not interesting whether it's different from 0. Note that in regression we only claim that the model is correct within the range of observed data. We're not trying to extrapolate outside of that range and predict that the bodyfat of someone with 0 circumference would be b\\_0, that would be bad statistics. The intercept is simply positioned so that the model properly fits the observed data."", ""While it is often ignored, the intercept can still give some insights about the data and/or the correctness of the model.\n\n\nIf you Center the predictor, which is often a good idea, the intercept will give you the average of the predicted value after correcting for the effects of the predictors.\n\n\nIf you don't center it, a significant intercept could also be a hint that your model is either missing some additional predictors (e. G. Non-linear terms) or that you would need a transformation on your dependent variable or a different error distribution etc. You might also want to force the intercept to be zero in some cases. However, most often the intercept is indeed ignored, as the model might still make sense in most cases and the errors only lead to minor improvements but require complicated methods (e. G. Cross validation). It could also tell you the model is only applicable to certain cases. \n\n\nYou would need an expert in the domain to tell if the model makes sense. In the case of body fat predicted by circumference, I would not worry about a negative intercept, as I intuitively expect the body to almost use up all available fat at some (unhealthy) point. Those people would still have a circumference but close to zero body fat. If I get a significantly positive intercept, I would either expect the model to be applicable to normal or obese people but not to anorexic people."", 'Are they always significant? Yes, if the test statistic exceeds the critical value.\n\nAre they always meaningful? No. And you just gave one of 1,000,000,000,000+ reasons why not.', 'thank you! This is where I was confused.  :)', 'That wasn\'t my interpretation. It reads to me like a very nitpicking ""response for the sake of having a response"" sort of comment.']",5,8,https://www.reddit.com/r/statistics/comments/13cn2he/are_f_and_ttests_for_regression_coefficients/
183,2023-05-09 18:50:22,[Q] Which evaluation procedure for nine variables?,"Hello everyone!

&#x200B;

As part of my master's thesis, I have conducted a quantitative survey. In total, the dataset contains 9 different variables that are to be compared. The variables break down as follows:

&#x200B;

Socialization Tactics:

1. collective vs. individual

2. formal vs. informal

3. sequential vs. random

4. fixed vs. variable

5. serial vs. disjunctive

6. constructive vs. destructive

&#x200B;

Newcomer Adjustment:

7. role clarity

8. social acceptance

9. self-efficacy

&#x200B;

All of the above variables were collected within the questionnaire using a 5-point Likert scale and were rated by the subjects.

&#x200B;

Since I unfortunately have little contact with statistical procedures and their evaluation during my entire studies, I now turn to you. The question now arises for me, which statistical evaluation procedures I have to use. The descriptive procedure is clear to me so far. Current problems I see with the inferential statistics.

&#x200B;

Which procedure(s) do I have to use within the inferential statistics in order to be able to make statements regarding the interaction of the variables?",['Have you looked at Item Response Theory (IRT) yet? Have you analysed the validity of questions using Cronbach alpha?'],1,1,https://www.reddit.com/r/statistics/comments/13cm4bb/q_which_evaluation_procedure_for_nine_variables/
184,2023-05-09 17:31:16,"[Q] When you toss a coin 100 times, are each toss considered an individual event? So would chance of 3 straight heads be 0.5 x 0.5 x 0.5 ?","So if I toss a coin a million times, would chance of 3 straight heads still be 0.5 x 0.5 x 0.5 after getting 2 straight heads in last two toss? Because I am thinking each toss does not affect future toss. Or am I mistaken?


(Let us assume both heads and tails have equal 50% chance to appear)","['The answer to your question in the title is, yes.\n\nThe question posed in the body of your text is a bit confusing. I think you might mean to ask ""what is the probability that my next toss will be heads, given that the previous two were also heads"". As these events are independent, the answer is 0.5. I\'m not sure what tossing the coin a million times has to do with it .', 'They are all independent events. However, if you toss the coin 100 times, the probability that a streak of 3 heads will appear *somewhere* in that series is quite high. Much higher than 0.125.', '\\*Assuming a fair coin.\n\nThe chance of throwing 3 straight heads is 0.5x0.5x0.5 before you toss the first coin.\n\nThe chance of throwing an H after already throwing 2 straight heads is 0.5.\n\nThe chance of throwing an H after throwing 1,000,000 straight heads is 0.5. But if you threw 1,000,000 straight heads you would probably not have a fair coin.', ""> Yeah what's the name of that one fallacy\n\nIt's called the [Gambler's Fallacy](https://en.wikipedia.org/wiki/Gambler's_fallacy)."", 'What color did you add? Personally a nice seafoam green is always welcome.']",13,37,https://www.reddit.com/r/statistics/comments/13ckogr/q_when_you_toss_a_coin_100_times_are_each_toss/
185,2023-05-09 17:30:45,[Q] Can anyone help me understand the stats in this study?,"I keep seeing references to this paper to the effect ""dancing reduces risk of developing dementia by 76%"".  I can't figure out how they got that conclusion from the 'results' tables included in the original study. Can anyone help me understand the stats? Thanks!

Example reference of the 76% figure: https://socialdance.stanford.edu/syllabi/smarter.htm

Original study: https://www.nejm.org/doi/full/10.1056/NEJMoa022252","['The pertinent table is [table 2](https://www.nejm.org/doi/full/10.1056/NEJMoa022252), and looks like folks that are statistically naive pulled the “76%” as a misinterpretation of the 0.24 hazard ratio. Unfortunately hazard ratios are notoriously misinterpreted like this. First, the study only looks at relationships between behaviours and dementia, and VERY importantly, this study can’t assert causality from any relationship it observes; there are many stories consistent with the existence of a relationship, and “dancing reducing risk” is only one such story. Second, when verbally describing association-with-risk data like this, it’s always best to describe things in multiple ways. Hazard ratio is more useful for folks that are very familiar with statistical models and the base rate of the particular risk of interest. For most people, it’s better to talk about the rates of the identified groups, so here it would be that dementia occurs in about 30% (99/339) of people generally, but in only 20% (25/130) of people who dance frequently. One way of describing those numbers together is to say the % has dropped by a third, but I tend to always prefer fully showing the two rates. The 76% is an outright misinterpretation; presumably the misinterpreter thought one could subtract then hazard ratio from one to get a meaningful number, which you can’t.\n\nNote also: the data from that study are such that while the one-off best-estimate it can achieve for the association between dancing and dementia risk reflects the reduction mentioned above, given the number of people observed, that reduction is not particularly inconsistent with a reality where there’s no true association at all, and our observation of a best-estimate reflecting a reduction is merely a product of random sampling. A Bayesian analysis would provide a proper description of the relative credibility of proposals for what the true association is, and I’ll post a result from one when I’m at my computer next. But from the “frequentist” confidence interval shown in the table, the confidence interval for the hazard ratio barely excludes 1. And such barely-excluding-one intervals should be considered particularly non-diagnostic in the context of an exploration of many different associations, as present in this paper. (When you look at many things, you should expect to see at least some that appear to have non-zero associations, even when reality is such that they’re all zero)', 'They get the number from the hazard ratio of 0.24 that was found for dancing. That’s a measure of how often dancers will get dementia compared to non dancers over time. \nHope that helps!', 'Brilliant, thanks so much for this, makes a lot of sense!']",2,3,https://www.reddit.com/r/statistics/comments/13cko2b/q_can_anyone_help_me_understand_the_stats_in_this/
186,2023-05-09 07:50:36,[Q] How can there be only one log-likelihood value for the OLS.summary() output?,"Imagine that we are doing a linear regression on multiple variables and I use the statsmodels OLS function to do so. When I print the OLS.summary() output, there will only be one log-likelihood value which doesn't make sense to me. This implies that there is only one parameter in the parameter for which we are finding the log likelihood value. If so, what is this parameter?

I ask this because what if we are finding the log likelihood of a normal distribution, then we can get two different values - one for \\mu and one for \\sigma\^2.

Am I missing something here?","[""> This implies that there is only one parameter in the parameter for which we are finding the log likelihood value\n\nIt's is the value of the (log) likelihood function at the estimated coefficients. The likelihood functions accepts (all of) the parameters as inputs, and outputs a single real number.\n\n> I ask this because what if we are finding the log likelihood of a normal distribution, then we can get two different values - one for \\mu and one for \\sigma^2.\n\nNo, the likelihood accepts mu and sigma as arguments, and outputs a single value, which is the value of the joint density at the observed data when the parameters are set to mu and sigma."", 'You’re not stupid. You’re learning!', ""Ah wait I'm stupid. I'm sorry for asking such a silly question. Thinking of deleting this post but it may help some other people so I'll leave it up.\n\nI confused MLE and Log-Likelihood and thought that we would would want to take the derivative w.r.t each parameter and this would give us a log-likelihood.\n\nThanks!"", ""It's the log-likelihood for the model. Indeed the documentation says as much. \n\n> Am I missing something here?\n\nPresumably the log-likelihood value it offers will be the log-likelihood for both parameters at their ML estimates, rather than some marginal/profile likelihood or conditional likelihood.""]",1,6,https://www.reddit.com/r/statistics/comments/13c93ei/q_how_can_there_be_only_one_loglikelihood_value/
187,2023-05-09 04:04:14,[Q] How to generate synthetic dataset for anomaly detection?,"Hi, I'm implementing an anomaly detection algorithm and I need to generate some synthetic data to test it. My current setup for generation is as follows:


* define a multivariate normal distribution from a random mean σ and random covariance matrix
* sample N points from this distribution
* generate anomalies by picking a percentage of the N points and adding k*σ to these points
* varying k to see how my algorithm behaves for ""bigger"" anomalies

I am not very knowledgeable in statistics so I don't really know if this method of generating anomalies is correct. Do you suggest any improvements on it? Thanks!","['What could be correct would depend on what you want power to detect...', ""Which data are you using to train your model? I'm trying to understand why you need synthetic data to test it..."", 'Maybe a T1 distribution for one or more of the marginals?\n\nContaminated normal?\n\nJust my $0.02.', ""I will be using MNIST images with corrupted images as anomalies. It's for a final project and our assignment requires to first test our algorithms on synthetic data."", ""Maybe you can use a synthetic data generator and use your current dataset as input? I believe there are a lot of GAN-based models for this purpose out there.  \nThe ones listed on [https://github.com/Data-Centric-AI-Community/awesome-data-centric-ai](https://github.com/Data-Centric-AI-Community/awesome-data-centric-ai) are mostly focused on structured data, but I'm sure there are similar packages for images.""]",1,6,https://www.reddit.com/r/statistics/comments/13c32iy/q_how_to_generate_synthetic_dataset_for_anomaly/
188,2023-05-09 03:56:58,"[Q] If I built the Time Series model on the detrended data and forecasts are detrended values themselves, it is possible to get the actual value forecasts (detrend + some previous data equals actual population)?","So, detrended data can be explained as the one that has the moving average removed. For example:

\- Original data (od) = \[y\_1, y\_2, ..., y\_n\]

\- Detrended data = (od - 4 quarted moving average of od) / 4 quarter moving standard deviation of od

So, just trying to understand how to reconcile the forecasted data to get the original data. 

Thanks","['data = detrended-data + fitted-trend    ->   \nforecast = detrended-data-forecast + trend-forecast', 'Would OD = DD * SD + MA get you there?\n\nI don’t work with time series often so I’m probably missing something. But if that’s the formula used couldn’t you just “solve for OD” and plug the observations to get the original data?']",5,2,https://www.reddit.com/r/statistics/comments/13c2v3v/q_if_i_built_the_time_series_model_on_the/
189,2023-05-09 02:48:34,Where to start with Causal Inference [Q]?,"
Hello, i am an incoming MS student in Statistics. I am looking to self learn causal inference to the point where I can be comfortable using the methodology in practice on real world datasets. My background is a BS in statistics with a mathematics minor up to real analysis, and I’ve taken inference, regression and a lot of the fundamental statistical inference courses. I was wondering what some good papers/books would be good starting points to get me up to speed in causal inference and the various methodologies used in it. Thanks.","[""I like this one (free online): https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/\n\nPearl's is good too, as i rubin and imbens book."", ""My favourite book on the subject is The Effect by Huntington-Klein.  Very accessible, great for practitioners, stats undergrads, or maybe graduate students in something like a psychology masters.  If you're looking for a deep dive into causal inference then maybe not the best choice, but if you're looking for a better intuitive understanding, or wondering how to answer some research question you have, then this is a great choice"", 'I am using a combination of the Mixtape book by Scott Cunningham, and The Effect by Nick Huntington-Klein. The former is quite technical and deep, while the latter is good for intuitive understanding.', '[Imbens and Rubin](https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/71126BE90C58F1A431FE9B2DD07938AB)\n\n[Morgan and Winship](https://www.cambridge.org/core/books/counterfactuals-and-causal-inference/5CC81E6DF63C5E5A8B88F79D45E1D1B7)\n\n[Angrist and Pischke](https://press.princeton.edu/books/paperback/9780691120355/mostly-harmless-econometrics)\n\nAll very solid books which should be very accessible given your background.', 'Interesting timing. I just bought two books (arrived today) by Pearl.\n\nCausal Inference in Statistics. A primer and The Book Of Why. I have 2 MScs, one of them in stats.\n\nGood luck on your journey.']",65,45,https://www.reddit.com/r/statistics/comments/13c0xv4/where_to_start_with_causal_inference_q/
190,2023-05-09 01:01:04,"[Q] What test to use? Ordinal DV, multiple IVs","Hello!

I need to find out which of my IVs (if any) have an effect on the dependent variable. The dependent is ordinal, it's measured on a 10-point rating scale, and doesn't follow normal distribution. My three independent variables are ordinal and categorical (binary). At first I though I should use ordered logit regression but since it requires normal distribution it doesn't seem to fit with my needs.

What test could do what I need it to do? Or should I choose a test that only allows for one IV, test all my three IVs separately and see if any of the results are significant? I'm using SPSS, if that matters.

&#x200B;

Edit. thanks for the responses! I think I read too much about different tests and their requirements and ended mixing things up in my head lol","[""Ordered Logit (aka Proportional Odds) seem like a good fit here. And no, it does not require your data to be normally distributed (even conditionally). Maybe you've read something about a latent normal distribution ? But that's not an assumption per se, more like a modeling choice. You're simply assuming that your likert scale captures discrete values from an underlying (latent, unobserved) continuous opinion/ability/etc distribution accross your population. Take a look at [this paper](https://psyarxiv.com/x8swp/) for a good explanation of the various types of ordinal models and how they model that latent scale from the observed ordinal data.\n\nIts hardest assumption to meet is its namesake, aka the proportional odds assumption."", 'I’m not exactly sure what you mean by “ordered” logistic regression, but unless it’s something weird, I don’t see why there would be a “normality” assumption.', ""> The dependent is ordinal, it's measured on a 10-point rating scale, and doesn't follow normal distribution\n\nIf it's ordinal, it cannot be normal (which is continuous,  necessarily numeric/interval). So the first 4 words cover it.\n\n> What test to use? Ordinal DV, multiple IVs\n\nStart with what model:\n\nOrdinal logit models, ordinal probit models, etc...\n\nAs for what test, it depends what you're testing, exactly, but the usual tests for glms apply.\n\n> At first I though I should use ordered logit regression but since it requires normal distribution\n\nNo, that's not the case. \n\nWhat exactly led you to think that? Did you read it in some book or paper? If so, which one, and what did it say, exactly?\n\nThere's some assumptions about how the levels of the ordinal variable relate to each other, depending on which ordered model you use, but it's not critical that those hold exactly."", 'The key question is how much the residuals deviate from normality. Don’t do significance tests to test whether they are exactly normal. You basically have an ANOVA design, and ANOVA is robust to non-normality in many situations. There are other issues with ordinal data but in many practical applications they are not serious.']",2,4,https://www.reddit.com/r/statistics/comments/13bxy9p/q_what_test_to_use_ordinal_dv_multiple_ivs/
191,2023-05-08 23:24:41,Statistical Learning as a Research Area [Q],"I have seen this topic, statistical learning, show up as a research area across a broad range of departments, even outside of statistics. For example, I see the standard statistics departments with a few faculty doing work in statistical learning (my dept has folks working on Bayesian regression trees). However, I’ve noticed that this field also shows up in CS departments (statistical learning theory) as a research area. My question for anyone here who is a statistician who researches statistical learning, how do both of these departments approach research within the area of statistical learning? 

For example, in a statistics departments, is statistical learning research essentially a spin-off of nonparametrics? Where they work on developing new statistical learning methods with theoretical properties of these existing estimators? Ie. The advancements of ensemble learning, do people just try and find new ensemble methods? 

Are CS departments using statistical learning as a computational problem? Ie. How can we make x method faster, a more efficient algorithm for y etc. how can we make ensemble methods faster?


This would help me decide if I ever did a phd which departments statistical learning is worth researching in. A statistics department or a CS department. If anyone can give some example of questions people may pose in statistical learning that would give me some examples between how the two departments think.","[""Don't base your PhD decision on a Reddit post. Talk to people in the department."", ""In adjacent fields like CS, OR, IS, I've seen both developing new statistical learning methods, and attempted applications to existing methods in industry domains, and often a blend of both. For a more specific look at pushing statistical learning boundaries, one place to start might be Mike Jordan, who is joint faculty between CS and Stats, his publication history is heavily skewed to nonparametrics. [https://people.eecs.berkeley.edu/\\~jordan/publications.html](https://people.eecs.berkeley.edu/~jordan/publications.html) ; he also has a great AMA from a while back that gives insight into how he approaches stats in CS: [https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama\\_michael\\_i\\_jordan/](https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)"", 'That’s true. I guess what I really wanted to know is what type of research is in statistical learning, which I could ask profs about too', 'Ah okay. I’ll check this out, thanks! I really haven’t ever been able to pinpoint what stat learning research really is. It’s very cool, and introduction to stat learning made me want to learn more. I am reading elements right now, and was always interested in knowing what exactly research within this space is. I’ll read some of these. Thanks']",6,4,https://www.reddit.com/r/statistics/comments/13bvari/statistical_learning_as_a_research_area_q/
192,2023-05-08 23:23:56,[Q] Stupid question about M&Ms.,"This genuinely happened, and I want to know what are the odds that it happened because it struck me but I don't know anything about probabilty beyond, maybe, one die. Two dice? I'm putting my money on boxcars at the casino because it pays better. That's my level of understanding, so Dunning Kruger is at the wheel. Right?

This is what happened: My wife bought a four pound jar of pastel M&M's on sale after Easter. 5 colors, let's assume equal amounts, random distribution in the jar.

I reach in, grab a handful, and put them on my desk to eat while I work. As I did, I noticed I had taken 10 M&Ms, and exactly two of each color. Weird.

I could have grabbed more or less, but let's say less than seven and I would have taken more, and more than 13, I would have dropped some back. Random chance, between seven and 13. I picked 10 and I got two of each color, there being five colors in the jar. (If it affects the math, the jar was half full, so about 2 lbs, which is about 800 M&Ms. If that even matters.)

What are the odds against me doing that again with another random handful? Did I hit a lottery-level freak occurence, or was it like rolling boxcars in a craps game - unusual but not exceptional?","['Since things happen all the time, unusual things happen all the time. Trouble is we only pay attention to them if they are meaningful to us. So ""after timing"" probabilities like this and marveling at the tiny numbers doesn\'t really take this into account.', 'Let\'s call the types (colors) A B C D E and assume the glass has 20% each, and it\'s so large that drawing 10 doesn\'t change the contents notably. We draw 10. The chance that one M&M is ""A"" or ""B"" is 2/5, the chance that all 10 are is (2/5)^(10). We don\'t need A and B in particular, however, any combination of two colors will do, and there are 10 of them. If we multiply 10\\*(2/5)^(10) we over-count cases with only a single color, however: If all are ""A"" then it\'s in ""A and B"", ""A and C"" and so on, i.e. we count it 4 times. We need to subtract 3 times the chance to get only a single color, or 3\\*5\\*(1/5)^(10), which means our total chance is 10\\*(2/5)^(10) - 3\\*5\\*(1/5)^(10) = 0.0010 = 0.1% or 1 in 1000.\n\nDrawing 7 increases the chance to 1.6% or about 1 in 60, with 8 it\'s still 0.65%.\n\nIf you first randomly select a number of M&Ms to draw then we should average all the cases (7, 8, ...), which leads to an overall probability of ~0.4% or 1 in 250.\n\nNot that common, but also nothing super unlikely, and you only noticed it because it happened. Our daily life is full of things that can show in 1000 coincidences - we notice these, but not the 999 times more common occurrences where they don\'t happen.', 'The probabilities in this case are given by the multinomial distribution.', ""I'm not great at statistics, but my job sucks and I'd rather do this.\n\nI'll just assume you are grabbing 10 M&Ms\n\nTo make things easier, let's first figure out the probability of choosing 1 M&M and it being red or green. This would be:\n\n(# of M&Ms that are red or green) ÷\n(# of M&Ms that are in the jar)\n\nBut for simplicity, lets just assume there are an equal amount of each color in the jar. Notice as well that everytime you choose an M&M, the probability would change. However, since there are so many, the probably won't change much, so I'll assume it doesn't change. With these assumptions, the probability of choosing an M&M that is red or green is 2/5.\n\nThat means the probability of choosing an M&M that is red or green 10 times will be (2/5)^10 .\n\nCoincidentally, there are 10 different ways to choose 2 colors from a little of 5 colors. So to generalize from thinking about red and green to any two arbitrary colors, we just multiply the probability by 10:\n\n 10 * (2/5)^10 = 0.001 or about .1%\n\nSo not terribly unlucky, definitely not as unlucky as the lottery. To be honest if the ratios of the amount of each color is off, the probability could be much higher."", 'My novice statistical knowledge would say that if the bag has equal numbers of each color, central limit theorem says this is going to be the most common outcome over time']",9,7,https://www.reddit.com/r/statistics/comments/13bva41/q_stupid_question_about_mms/
193,2023-05-08 14:54:17,"[Q] Paired T test, data shows difference in mean but p value says it is not significant.","Hello everyone sorry if this is a beginner question but I keep confusing myself. So my data is as follows

||Muscle Recovery Time (mins)|Muscle Recovery Time (mins)||
|:-|:-|:-|:-|
|Participants |Weight|Without Weight|% Change|
|1|6|3|0.50|
|2|4|3|0.25|
|3|4|3|0.25|

Running a paired t test of weight and without weight I get a p value of 0.13 which is > 0.05 and signifies that the mean difference is not significantly more than 0. The average % change from without weight to weight is 36%. I feel as if with the % change the p value should support the change. Could it be because of the small sample size or am I doing something wrong? Any help or clarification would be greatly appreciated.","[""This is literally why you do this test so that you don't fall into the trap of considering nominal differences meaningful when they are outweighed by the uncertainty.\n\nNow you come to us and ask for ways to yet fall into the trap.\n\nYour differences are large but your number of participants tiny. I'm not sure if I should interpret 3 or 6 total, in any case a tiny amount, to the point that you probably cannot even use a t-test there."", 'Consider the following scenario:  \n\n\nYou have a class of students with an equal number of boys and girls. You want to find out whether generally speaking, boys are taller than girls in the population (remember, when doing inferential statistics, we are always interested in population-level differences, based on the sample differences). You take a single boy and a single girl, and measure their height. The boy measures 180 centimetres, and the girl measures 150. There is a 30 centimetre difference. Can you conclude that boys are taller in the population? No, because you only have a single subject from both groups, and therefore a lot of uncertainty. It is perfectly possible that you just happened to randomly pick a taller-than-average boy and a shorter-than-average girl, and in fact, if you were to measure all boys and girls, girls would be taller.   \n\n\nThis is reflected in the formula for the paired-sample t-test: The standard error in the denominator contains the square root of N in its own denominator. The larger N (sample size) is, the smaller the standard error, and consequently, the larger the t-statistic. Your result simply indicates that based on the available evidence, you cannot say that in the population, there is a difference between the two means.', 'I know this is meant to be colloquial, but describing the results of a hypothesis test as identifying a ""meaningful"" difference, as opposed to a ""nominal"" difference, probably just adds to the confusion.', 'Why is a p value < 0.05 significant to you specifically?\n\nWhy an alpha of 95% instead of 99 or 85?\n\nOther commentators have already brought up the the importance of considering sample size and effect size. It would also be worth looking into p-values and the how/why for selecting your cutoff point. \n\nThe fact that we use 95%  for almost everything is something Fisher is probably rolling in his grave about. Especially since he pulled 95% out his a** and specially asked future statisticians to select their own numbers and not just blindly follow that example.', 'This is a good explanation.  The only way hypothesis tests make sense is to remember that they are tests on the population given the data that\'s been sampled from the population.\n\nIt\'s unfortunate that sometimes we say in shorthand, ""real difference"" or similar things.']",20,20,https://www.reddit.com/r/statistics/comments/13bgqvv/q_paired_t_test_data_shows_difference_in_mean_but/
194,2023-05-08 06:48:54,[Q] MaxDiff Analysis Packages in R?,"I'm trying to run a max diff analysis in RStudio for an assignment but I have never done this before so I'm struggling. Does anyone know of any packages I can use (that have detailed documentation preferably)? I was trying to use flipMaxDiff but it seems like it's no longer public. Or alternatively, is there a way I can do it without a package? Thanks for any help!","[""I think you don't need to use a package as per https://docs.displayr.com/wiki/MaxDiff_Analysis_Case_Study_Using_R"", ""I attempted to use it but I get an error that flipMaxDiff is not installed, so it's still dependent on a package. Apparently it used to be available in R but is not anymore? So now I'm trying to install it from here ([https://rdrr.io/github/erikerhardt/flipMaxDiff/src/R/max-diff.R](https://rdrr.io/github/erikerhardt/flipMaxDiff/src/R/max-diff.R)) but I'm messing up setting up my git PAT because I keep getting the error that I have exceeded my rate limit. I'll probably just try again tomorrow.""]",13,2,https://www.reddit.com/r/statistics/comments/13b5n93/q_maxdiff_analysis_packages_in_r/
195,2023-05-08 05:49:56,[Q] Perfect Collinearity,"Hi everyone,

I am coming across some difficulties for a regression model. I am deriving a variable from income by making a few manipulations to it. However, when I run a multiple linear regression and include income as a control, I see that there is perfect collinearity between income and the derived variable.

Now I understand that I can drop one of the two variables from the model. But the issue is that I see other papers including income as a control, and I don’t understand how other researchers are not coming across this same issue.

Can anyone tell me if it is possible to include a variable derived from income and then include income as a control in a model?

Thank you","['> I see that there is perfect collinearity between income and the derived variable.\n\nIs this other variable just a linear transformation of income? If so, why include it?', 'The important information about perfect collinearity is that all the information in one is fully in the other. The transformation you performed was non-informative.  You have to drop one. \n\nUnless there is a strong theoretically sound reason for the transformation, I would keep the raw data. \n\nThe transformation you performed will impact the location of the intercept but nothing else.', 'Stepping back from the algebra, consider that the model you are trying to fit is DOA, conceptually; it could only generate nonsense even if it were possible to fit it to data.\n\nA model that ""controls for income"" aims to understand the implications of varying the income-derived quantity while holding income itself fixed - i.e. to compare the expected outcome for two hypothetical individuals who differ on the derived variable but have the same income.\n\nBut notice: no such pair of individuals can exist, even in theory. If they differ on the derived variable, they must differ on income.\n\nIn short, it is nonsense to speak of the \'effect\' of the income-derived variable \'controlling for income.\'\n\nHow is it possible, then, that others have successfully fit the sort of model you are trying to construct? What may be happening is that they are using income in two different ways in concept, and generating non-collinear variables that match that concept.\n\nThey may, for example, generate the derived variable, and then separately generate a discrete analogue of income (e.g. classifying individuals into categories such as \'low income,\' \'middle income,\' and so on). They could then use these categories as proxies for socioeconomic status or social mobility in general, but call it \'income,\' and talk about how their models \'control for income.\'\n\nIt would then be possible to fit a multiple linear regression model with these two different versions of income as independent variables. But how to interpret the associated regression parameters, and whether the model that includes both even makes sense, would still be important considerations.', ""Choose one or the other. The derived variable didn't add any information. Tomato tomato"", 'Seems like you miscalculated the variable or did something wrong, I’d be very careful. If you summed up some differences with logical rules, this seems nonlinear, ie, not a linear transformation of income, and i would be very suspicious of perfect multicollinearity']",12,10,https://www.reddit.com/r/statistics/comments/13b42a0/q_perfect_collinearity/
196,2023-05-07 21:20:23,[Q] Surveying customers. Never get to n = <30. Can statistical relevance still be found?,"So, let’s say I have 100 customers come to an event and I send a survey asking them a series of questions, but only 20 respond. Can I still infer statistically sound meaning from the responses? For example, 18/20 respondents rank experience “A” 10/10 but only 13/20 of the same respondents rank experience “B” 10/10. I would think the deviation between the two is meaningful and that experience “B” should be evaluated and changes made. Is that true? And what would be the language I would use to explain the significance?","['With a response rate of only 20%, I would worry about non-response bias more than a small sample size.', ""You can only use what you have to use.\n\nAs, mentioned, just be cautious that you may have response bias.  In the example you mention, it doesn't sound like there would be an obvious response bias, but these things can be tricky.  For example, in American politics, you might suspect that an imbalanced sample for age or gender might bias the party preference results, but you might not think that there would be difference in political preference for those who would or would not answer the phone in the first place.\n\nBut also in a lot of situations, a 20% response rate isn't bad.  \n\nBTW, the number 30 doesn't have any special meaning."", 'Suppose your question is ""do you ignore online surveys?"". \n\nYou send it to 100 people. 20 (or 30, 50, whtever) return it awnsering \'No\'. \n\nYou conclude that 100% awnser online surveys.\n\nDo you see the problem?']",0,3,https://www.reddit.com/r/statistics/comments/13anj5y/q_surveying_customers_never_get_to_n_30_can/
197,2023-05-07 21:18:45,[Q] robust TOST in jamovi,"Hi there!
I am trying to calculate a robust TOST (Wilcoxon TOST) in jamovi, however I cannot find a way to do so. It is supposed to be included in the TOSTER add-on (as wilcox_TOST function), but I cannot find it there - I already clicked trough all options. Am I maybe just to stupid to find it?",[],2,0,https://www.reddit.com/r/statistics/comments/13anhrl/q_robust_tost_in_jamovi/
198,2023-05-07 17:10:07,[Q] What test to use for a single participant study?,Say you wanted to determine whether giving someone alcohol has a causation of making them dance with a lampshade on their head (the qualitative data would be how many times they dance vs how many times they don't). What variables would you need to consider and how would you analyze the sample (or would it count as a population)? Would I include data from times they're not drunk?,"[""Sounds like a time series to me, and of course you would have to use data from when they're not drunk, other wise you have no variation in your independent variable."", 'With a single participant, your inference would be to that individual. sampling that individual in any valid way would be very challenging', 'It means there is only 1 group of people with 1 person in it', 'You test that individual on multiple days, randomly making them drunk or not.', 'OP said one participant']",5,8,https://www.reddit.com/r/statistics/comments/13aibp6/q_what_test_to_use_for_a_single_participant_study/
199,2023-05-07 11:41:07,"[Q] What is better? To do 2 studies of N = 1000, or 1 study of N = 2000?","I recently read a study where scientists looked at 1000 people's brains and found some effect (brain activity locations predicted psychopathological symptoms). They then ""replicated"" the findings by looking at another 1000 people. This was not their data, but publically available brain data. 

Outside of the issues with fMRI, I'm wondering if there is a reason why it would be better to do one study of 1000 brains, then replicate it with 1000 other brains, vs just finding the effect with the first 1000 sample, and then adding the second sample to the first and re-running the analysis with all 2000 people. 

Thanks.","[""If they're both from the same population, then you have exactly the same amount of information in both cases."", 'Did they use the first 1000 brains to explore the data and identify possible effects, and the second 1000 brains to test whether the effects replicate on the new sample? If so, this is good science. The effects found in the first study are not very convincing since the researchers were effectively predicting the past (not difficult). The second study gives them a chance to try to predict the future (very difficult). This is much more convincing and suggests that they really know something about the brain.', ""If they're both from the same population and they're both random samples, then the split between them is arbitrary. You don't gain any additional information by splitting your 2k sample into two 1k samples.\n\nMy hunch is that the two studies' data are not from the same population due to either instrumentation or sample selection differences. Then maybe it simply wasn't possible to combine datasets. I've seen things like that happen before.\n\nOh! I also just realized: if they can't guarantee independence of the two datasets (eg one patient is in both) then they should analyze them separately."", 'Well, hypothetically you get *more info* as you can reproduce \n\nThis assumes that the n=1000 gives proper power, the sample is constructed well etc.', 'No, this ignores the Garden of Forking Paths/Experimenter Degrees of Freedom. Choices have to be made by the researcher in data processing and analysis. These are often (unwittingly) conditioned on the results they generate, which undermines the severity of the hypothesis test(s). If the authors developed their processing and analysis workflow and it’s code implementation on the first dataset, and pre registered it and applied it to the second, the second confirmatory analysis has higher evidential weight. Let’s not pretend that the replication crisis didn’t happen or that there are not things to be learned from it. \n\nThere is a strong analogy with training and testing datasets in machine learning. No one would take you seriously if you argued that results found purely in training datasets were valid, you always need strictly constrained confirmatory analyses/out of sample predictions.']",58,34,https://www.reddit.com/r/statistics/comments/13abpg3/q_what_is_better_to_do_2_studies_of_n_1000_or_1/
200,2023-05-07 09:44:57,[Q] What kind of logistic regression analysis is most suitable?,"Hi all,

What kinds of logistic regression are (most) suitable for a model with 1 within-subjects categorical factor, 1 categorical between-subjects factor and a binary nominal dependent variable? 

Thanks a lot","['What\'s the outcome like? Nominal as in no natural ordering e.g. ""male/female"" or nominal as in ""0 (didn\'t) or 1 (did) happen?"" The first would be more multinomial logit (that\'s the most common I think).', ""The dependent variable is a prediction on the outcome of a casino wheel (red/black). So multinomial then? I asked mainly because I'm not sure which types are inappropriate due to the mixed model with between/within factors, I'm not very familiar with logistic regression :)"", ""You can do binomial GLMM (generalized linear mixed modeling), just as you might a linear mixed model. Clay Ford has a [good tutorial](https://data.library.virginia.edu/getting-started-with-binomial-generalized-linear-mixed-models/) in R on it. With just two levels to your outcome you can convert them to 0/1 and use regular ole logistic regression (the binomial GLMM in this case). Sorry for not being clear/reading your OP closer; multinomial's more for 3+ outcome levels. At 2 levels multinomial's just binomial LR."", 'Awesome, thanks for your help!']",3,4,https://www.reddit.com/r/statistics/comments/13a90ls/q_what_kind_of_logistic_regression_analysis_is/
201,2023-05-07 01:10:09,[Q] Help with interpreting regression line,"Hi, absolute begginer here. 

Just a quick question. Can we say that XOM and VDE daily returns are highly  correlated ? 

[https://imgur.com/iiYvS2U](https://imgur.com/iiYvS2U)

Is there more to be said when explaining this graph ?

Thank you","['Yes, but an assumption is your model is correct. A model that also includes some other factor, say average blue-chip daily returns, could be more believable. After statistically controlling the correlation could be eliminate (or stronger, reversed, weaker, ...).', 'I don\'t know if this is for an assignment or personal use, but it looks to me like you\'re doing a Capital Asset Pricing Model analysis of XOM. In that model, the slope coefficient is called ""Beta"" and it measures the volatility of one stock to the wider market. \n\nPositive betas imply that the individual stock moves in the same direction as the wider market (pro-cyclical); Negative betas imply that the individual stock moves in the opposite direction as the wider market (counter-cyclical). Betas greater than 1 or -1 mean the individual stock is more volatile than the overall market.\n\nSo, if you have a beta of 1.5, what that means is that, on average, the individual stock moves in the same direction as the wider market but moves 1.5x as much. So if the market increases by 10%, the stock increases by 15%.', 'I will be very careful before coming to any conclusion with this chart and regression line.\n\nThe suitable kind of regression for your data is time series regression, i.e., the relationship of the daily returns of XOM and VDE should be considered in the chronological order. A vector auto regression (VAR) model is normally used in this case.\n\nHowever, what you are doing in here is regression on the cross sectional data. You put every available values of XOM and VDE together and run a regression, regardless of their order. It is easy to have a high correlation coefficient in that way, since daily returns of stocks are usually have similar distribution and centered around 0 (try to use histogram for both and you will see that).\n\nTo sum up, in this case, you need to read more about time series regression, because what you want to investigate in stocks data is how it changes over time.', 'I think the purpose of what OP’s doing isn’t predictive. It seems like it’s just a Capital Asset Pricing Model application. In that case, normal regression should be fine and using daily returns rather than stock price should make the day stationary (i.e. remove most autocorrelation).', ""Good thoughts already from Doc.\n\nThe residuals of your model seem to be dependent on your independent variable (that is, at low XOM, all points are below the regression line, and at high XOM, all points are above)\n\nMy guess is that you don't have an intercept, and including one would lead to a much better fit (as long as it makes sense for VDE to be zero when XOM is not, and I have no idea what they mean so that'd be up to you to consider)""]",7,10,https://www.reddit.com/r/statistics/comments/139vuo1/q_help_with_interpreting_regression_line/
202,2023-05-07 00:07:47,[Q] Getting LOR from stats processor,Going to apply for masters programs this fall which will be after I’ve graduated. I’m getting a LOR from my research professor and one from my job. But for some programs I’ll need 3 letters and I think it would help if one is from a professor who’s class I’ve talent. Problem is idk how well the professor should know you before you ask them? I don’t want to be rude and demand a huge favor from someone who doesn’t know me.,"[""I'm not in statistics, but I'm a professor. I think that asking for a favor is ok, but of course the answer can be no or something else negative. It's the professor's choice to write something or not, write something good or not.\n\nHow much I know the student will affect what I can write.\n\nIf I barely know the student, then I barely have anything to write. So, I'll probably refuse writing the letter. I would expect the student to first have a stronger connection with me. \n\nIf I know the student, but not much, I may write a letter but it won't be a strong letter. I can't strongly recommend someone that I don't know well. So, the letter would be one of the three letters required, but probably would not make much of a difference otherwise. \n\nIn your situation, my recommendation would be to talk to the professor first. Explain what you're doing, your plans, your goals. Ask for advice. And ask for a letter only after you had a good talk, so the professor would feel better about you, and about writing a letter of recommendation for you. You want a huge favor, put some effort into it.\n\nThe worst case I had was a student who didn't even ask. I got an email from a university saying I had been listed as one of the recommenders, but the student didn't even talk to me about that. Then I get an email from the student the day before the deadline, not even giving me time to write a letter if I decided to write one. Students sometimes feel they are entitled to a letter of recommendation (a positive and strong one), and that's the main thing to avoid. Be respectful, communicate, do your part before you ask for a favor like that.""]",2,1,https://www.reddit.com/r/statistics/comments/139u4bx/q_getting_lor_from_stats_processor/
203,2023-05-06 21:57:08,[D] The probability of Two raindrops hiting the ground at the same time is zero.,"The motivation for this idea comes from continious Random variables. The probability to observe any given value of a continious variable is zero. We can only assign non zero probabilities to Intervalls. Right?

So, time is mostly modeled as a continious variable, but is it really ? Would you then agree with the Statement above? 

And is there even a thing such as continuity or is it just our approximation to a discrete prozess with extremely short periods ?","[""Couldn't you generalise it to the probability of any event happening at the same time as another is zero?"", 'Bring measurement into your thinking. We only measure to some level of accuracy. So specifying a time is always plus or minus some amount. Then probability, as you say, is assigned only to intervals, but every measurement of time is actually an interval.', '> *Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful.*  \n-- George Box\n\n\n\nThere\'s a difference between models and the thing they\'re modelling. This only becomes an issue with any conceptual difficulty if you think a probability model is in any sense \'real\' rather than a *model* -- an abstraction and a simplification. The most that we can practically hope for is that it may be useful.\n\nThe Poisson process is a model. It may well be that *no process* is ever exactly a Poisson process. This is not particularly material to the purpose of the model, which it to enable probability calculations where the process is a good approximation -- and in many cases it is an excellent one.\n\nIndeed, for such probability models, the question of whether any physical process is ""actually"" continuous is almost in \'how many angels can dance on the head of a pin\' territory; there are more realistic aspects of the discrete/continuous dichotomy that come up at much less precise level of data.\n\nConsider the difference between any *recorded* time and some actual specific instantaneous *instant*, were it possible to define the event precisely enough to associate an instant with the event. As a practical matter, we deal with the first thing (the data we\'re modelling), rather than the second thing.\n\nIndeed, arguably any sense of continuity in either time or space may break down if you go fine enough, but this is not normally of any consequence for us. Practical ability to measure and record differences many, many orders of magnitude larger is already nonexistent.\n\nIt\'s not unusual to find that there\'s several different levels of abstraction where different stages of model flip between continuous and discrete -- at this level it\'s continuous, at that one discrete, and at another, again continuous, and so forth. Whether the lowest level is \'really\' discrete or continuous (indeed even  and the need to consider whether there is \'really\' a lowest level at all) is moot. \n\nWorry about the usefulness of your probability models, don\'t expect them to exactly represent reality, in practice they don\'t. Leave considerations of what\'s ""really"" happening at some sufficiently fine level (and whether that question of some lowest-level fundamental \'reality\' is even particularly meaningful) to the physicists and philosophers.', ""That is true, it's basic. But in my opinion the implications for the real world are interesting."", 'Good answer! That is true. Maybe i am just overthinking this stuff... Can we even understand what a ""real"" instant is.']",35,68,https://www.reddit.com/r/statistics/comments/139pc7o/d_the_probability_of_two_raindrops_hiting_the/
204,2023-05-06 02:34:34,[Q] Choosing between a count model and spatial error model,"Hi everyone, I hope this doesn't fall into the homework category: 

I working with a dataset consisting of spatial grid cells. After performing some spatial diagnostics (Moran's I, LaGrange multiplier tests), it seems that there is spatial dependency in the form of spatial error, so a spatial error model seemed fitting. However, my dependent variable is a count variable and is quite skewed (the standard deviation is much higher than the mean). From what I have gathered this violates the assumptions of the spatial error model (which assumes more continuous data) and also seems to lead to heteroskedasticity, biasing the results.

An alternative could be to run a negative binomial model, which seems to fit the data better but leaves out the spatial dependencies, also biasing the results. I know there are some ways to perform spatial count models, but these seem to complex to grasp for me at this stage (or to difficult to implement in R).

My question is: how do I choose between these two evils; unfortunately the results are quite different. Is there a good way to compare goodness-of-fit? The AIC and Log Likelihood are much better for the negative binomial model whereas the RMSE of the spatial model is better but I am not sure if I can compare either those metrics across the models.","[""Wouldn't it make more sense to model both aspects  together rather than choose?"", ""You should be able to use mgcv in R. Something like `gam(target_var ~ te(latitude, longitude), data = data, family=quasipoisson())`. Depending on where you're at in your spatial work or with ecology or whatever, you can check out Simon Wood's book on GAMs with R. \n\nYou should be able to compare models in R with `AIC(model1) < AIC(model2)` and you can also just roll with `anova(model1, model2)`.\n\n If you've got things to a train/test split to predict then you should just be able to use RMSE - something like `sqrt(mean((data$actual - model1$predicted)^2)) < sqrt(mean((data$actual - model2$predicted)^2))` \n\nOne package that deals with varying scales is gjam, so if you're just working with lm then that might be a non-spatial option that fits better. You could also try a transformation (boxcox is probably fastest, but something like log-log is more straightforward in its interpretation than dealing with the boxcox lambda).""]",16,2,https://www.reddit.com/r/statistics/comments/138xb6m/q_choosing_between_a_count_model_and_spatial/
205,2023-05-06 02:21:27,[Q] Studying regression and correlation - which resources to get a deep understanding?,"Years ago at university, I met all many of the usual statistics elements introduced to students. I was interested in the topic and had a good intuitive understanding, but I was also frustrated when we were, for instance, taught that the standard deviation is ""the mean deviation"". I could clearly see from the definition that it was not - and later saw Nassim Taleb talk about this where he also defines it rather as the "" root mean square deviation "".

I ended up dropping my statistics studies but want to get into it again. I'd like to understand things at a deep level and not just use an equation without knowing its logic. And in particular, I'd like to move on to multiple regression and be able to use data to forecast what values can be expected. As a bonus, I'm also very interested in the use of absolute deviations versus squared deviations.

Any great resources you want to recommend?","[""What area are you in? What mathematics have you covered?\n\n>  I'd like to understand things at a deep level and not just use an equation without knowing its logic.\n\nThen some basic mathematical statistics - enough to learn the theory in regression - is where I'd start.\n\nFor that you'll need some basic linear algebra and some calculus, then some probability. That will give you the background to pursue some standard mathematical statistics -  point estimation, interval estimation, hypothesis testing ...\n\nThen something with a decent coverage of regression. From there you can move out to whatever additional topics you need."", ""I have studied psychology, but done a but of linear algebra and such on my own. \nThis reminds me that I've often thought it would be effective with some 'placement test' to figure out which topics one hasn't covered.\n\nIt sounds very good, but do you also have a suggestion for a good ressource? Maybe starting from there I'll automatically find out what prerequisites I need"", ""I have seen the book *Applied Linear Statistical Models* by Neter recommended. But I think that uses least square regression. I don't know if the way to go is to learn least square first and then look at absolute deviation regression later.""]",3,3,https://www.reddit.com/r/statistics/comments/138wyey/q_studying_regression_and_correlation_which/
206,2023-05-06 00:18:07,[Q] Benjamini-Hochberg Correction,"Hi everyone, I am trying to run a Benjamini-hochberg correction on the results of spearman correlation analysis to correct for multiple tests. I am a bit confused on how to apply the correction. I have four separate treatment groups that I ran the correlation analysis on separately. Do I run the correction four times as well? Or do I pool the p values from all the groups and perform one correction? Thank you!","['If you want to consider the tests a ""family"" of tests, you pool the *p*\\-values, and apply the correction to them as a set.\n\nThis is implemented in software packages, often with the *p*-values themselves being reported as ""corrected"".  But if you are doing it by hand, you are setting a threshold to reject / not reject for each *p*-value.']",3,1,https://www.reddit.com/r/statistics/comments/138tje6/q_benjaminihochberg_correction/
207,2023-05-06 00:05:15,[Q][E] Lottery with multiple ball sets,"Supposing it is possible for a lottery to use different sets of balls, such that each set is biased (by weight presumably), such that the average frequency of numbers/balls drawn is nearly equal over time. How would one analyze the result for the past n drawings to discover the sets?

My thinking is that in an elementary case, where in 1-50 lottery, if there are two sets of balls; set A the balls from 1-25 are 1 gram less than the balls 26-50. In set B the opposite. One could simply sum the 5 numbers drawn, per drawing, and see the sets. Or at least have a good idea where the bias is.

When considering a more complex case of gaps of interleaved weightings is where it gets complicated (obviously). So, how would one analyze this situation? Thanks!","[""This is an example of [cluster analysis](https://en.wikipedia.org/wiki/Cluster_analysis). Balls with the same deviation in a set will be drawn together more often than we would expect from random chance. If you assign a distance to numbers based on how often they show up together you can look for clusters of numbers with a small distance. You'll probably need a large number of drawings before this produces reliable results.""]",2,1,https://www.reddit.com/r/statistics/comments/138t6gn/qe_lottery_with_multiple_ball_sets/
208,2023-05-05 23:03:05,"[Q][D] Predicting a binary result or regression using an ""uneven"" amount of data (potentially time-series data)","Hi all,

I'm working on building/selecting a model to predict the result of a sales lead: whether it's ""SOLD"" or ""NOT SOLD"".  

My dataset consists of past leads with the following data:

- sales rep
- product pitched
- lead source
- date of lead
- zipcode
- result

The issue I'm running into is that I have a various amount of leads per day per rep. Here is some sample data:

| ID | salesrep | product | leadsource | date | zipcode | result |
| -- | -------- | ------- | ---------- | ---- | ------- | ------ |
| 1 | Bob | A | Website | 5-1-2023 | 12345 | SOLD
| 2 | Bob | A | Call In | 5-1-2023 | 12344 | NOT SOLD
| 3 | Alice | A | Website | 5-1-2023 | 12343 | NOT SOLD
| 4 | Bob | A | Referral | 5-1-2023 | 12346 | NOT SOLD
| 5 | Alice | A | Call In | 5-2-2023 | 12345 | SOLD
| 6 | Bob | B | Referral | 5-2-2023 | 12344 | SOLD
| 7 | Alice | B | Website | 5-2-2023 | 12342 | NOT SOLD
| 8 | Alice | A | Call In | 5-3-2023 | 12333 | SOLD

In this example, I have three dates of data. 

- Day 1 (5-1-2023) has Bob with 3 leads, and Alice with 1. 
- Day 2 (5-2-2023) has Bob with 1 lead and Alice with 2 leads. 
- Day 3 (5-3-2023) has just Alice with 1 lead.

Now let's say I have information for a lead for day 4:

| ID | salesrep | product | leadsource | date | zipcode | result |
| -- | -------- | ------- | ---------- | ---- | ------- | ------ |
| 9 | Bob| A | Website | 5-4-2023 | 12345 | ???

I want to predict the result - specifically, the probability of the result being ""SOLD"".

Initially I was thinking about using something like a Decision Tree/Random Forest/XG Boost classifier, but realized that this data may have a time-series aspect to it.  The business is seasonal and has a ""busy season"" during the spring and summer months, and I'd also like to account for sales reps that are on ""hot streaks"".

For this I was thinking about making use of an LSTM RNN model, but I'm not sure if my data's ""shape"" is appropriate for this model type, as I have a various amount of data points per date, some dates have no data points for specific reps, and some dates have no data points at all (no leads on Sundays, federal holidays, etc).

Thoughts on this?  Which model would you use in this scenario?

And not to throw a wrench into it, but assuming that I'm able to somewhat accurately predict the result of the lead, I'd then start to look at trying to predict a range for the sale volume as well (using past data of course).

Appreciate anyone who takes the time to respond to this.","['What is your intended final use for this model for your business? I ask since the data you have seems to be mostly internal data (such as who made the sale + the product). Is the plan to funnel more business to the higher performing sales reps? \n\nI would start simply. It sounds like there is legitimate reason to believe that sales differ for your busy season relative to the rest of the year, so you can create a new indicator variable for your busy season (1 for busy and 0 for not) and use that in your model instead of the daily date column. You can model this with a logistic regression to get your probabilities.', 'Thanks for the response!\n\nSo the main idea is that every day we have a list of leads that we are set to run.  I\'d like to take that list of leads and optimize it so that we\'re sending out the best rep available for that specific lead (based on the limited amount of data we have available).\n\nSo if our best rep only closes on product A, if the customer is looking to buy product B, I\'d run my model and evaluate the probability of ""SOLD"" for each rep, and then provide a ranked order in return.  I\'m hoping for my model to pick up on some underlying trends that maybe most people wouldn\'t know to look for (Bob never closes on Mondays, or Bob has gone 0 for 100 in this specific zip code, etc).\n\nHopefully that makes sense!\n\nEDIT:  Sorry, one last thing!  I\'m also trying to take into account how well a rep has been performing recently moreso than their past performance.  A rep may have been doing well 5 months ago, but really the last 4 weeks are more indicative of their current performance.  And I\'m trying to capture that performance and hopefully combine it with the rest of the data (how well a rep does with a specific product, or with a specific lead source, etc)', 'One thing to think about with zip codes is they are so granular they are likely to be very unbalanced or very dispersed i.e. 90% of your sales might come from 4-5 zip codes and then you have 100 zip codes with only 1 or 2 sales or you may find that you have hundreds of zip codes with a few purchases in each giving you a lot of data to process and not a lot of return. \n\nI would again look at this through the lens of your business expertise - is there a reason why someone from one zip code is more likely to purchase from another? Do you focus advertising on a specific geographic area? Do you only serve customer base from a narrow zone. If your business is national, do you actually see a concentration within certain zip codes? If so, why? If not, it will likely not be useful for your model. \n\nMy personal strategy (which sometimes does not work) is to start at a more aggregated level and then start drilling into more detailed analyses in future models after some robust exploration. A simple model may give you exactly what you need without extraneous details.', '>So if our best rep only closes on product A, if the customer is looking to buy product B, I\'d run my model and evaluate the probability of ""SOLD"" for each rep, and then provide a ranked order in return.\n\nFor this sort of thing wouldn\'t a simple proportion do to rank salespeople?\n\nAlso keep in mind that once you\'re using this kind of model to direct salespersons, you will force specialisation.\n\nIf Bob is great at product X, he will always sell product X and forget about product Y.', 'Is this your first data mining project? You have a lot of hopes and dreams and most of it sounds like a nightmare to implement.']",6,8,https://www.reddit.com/r/statistics/comments/138rf6t/qd_predicting_a_binary_result_or_regression_using/
209,2023-05-05 17:23:56,[E] My solution to the Sleeping Beauty problem.,"[https://www.youtube.com/watch?v=XeSu9fBJ2sI](https://www.youtube.com/watch?v=XeSu9fBJ2sI)  


Obviously, the probabilities for the Sleeping Beauty problem are just = (0.5 for heads)\*(1 of token A) + (0.5 for tails)\*(2 of token B). The chance of the coin is 50-50, from Bayesian testing. The number of tokens is equivalent to the number of ""links"" on the decision chain. Sleeping Beauty's chance of correctly guessing a specific token is 1-3. There is more information in the question than expected.

The mistake people make, is assuming the payout probability is the same as the win probability.

Essentially it is an overpaid bet - this is how casinos make money. The roulette wheel has 33 slots, but one slot pays 2-1 to the casino. The calculation for the roulette wheel is = (32/33)\*(casino pays out 1 token)+(1/33)\*(casino wins 2 tokens). So the casino in the worst case does not lose money, and the casino in the good case wins 2 tokens (essentially the green slot is linked with a red or black slot).

After every slot has been hit, the casino always walks away with 1 extra token.

Halfers think the calculation is = (0.5 for heads)\*(1 token)+(0.5 for tails)\*(1 token). This is wrong because the links are not counted correctly. These people lose money by overpaying for bets at casinos.

Thirders think the calculation is = (1/3)\*(1 token \~ effectively for heads)+(2/3)\*(1 token \~ effectively for tails). This is practically more correct but backwards, but it misses key information.

The multiverse state is unknown because the bayesian probability is unknown and the number of links is not known, like a coin is tested to be.

This solution is obviously correct, once somebody points it out.","['Why would you write this (and tag it education) when you seem to have no idea how roulette works!?\n\n> The roulette wheel has 33 slots, \n\nRoulette wheels have [37 or 38 pockets](https://en.wikipedia.org/wiki/Roulette#Roulette_table_layout) not 33 and you omit that that in American (but not European) roulette there are *two* [green pockets  ""0"" and ""00""](https://www.eyeonannapolis.net/wp-content/uploads/2022/02/image1-2.jpg) that ""pay"" the casino. \n\n> but one slot pays 2-1 to the casino.\n\nA 0 or 00 pocket can\'t be said to pay 2-1 to the casino - you place your bet and it\'s either taken by the house, or you get paid out according to the odds. In American roulette you can bet on 0, on 00 or a split bet on either in which case the ball stopping on a zero would not pay the house.\n\nThe casino makes money any time someone places a bet that isn\'t paid out. Their edge comes from the difference between the returns they offer for a bet type and the long run average probability of the event in that bet type: https://en.wikipedia.org/wiki/Roulette#Bet_odds_table\n\n\n>  The calculation for the roulette wheel is = (32/33)*(casino pays out 1 token)+(1/33)*(casino wins 2 tokens).\n\nThe casino pays 35x your stake if you bet ""straight up"" on a single number (probability 1/38) on roulette. I don\'t think there would be many takers for roulette if you had to wager $10 on a 1/38 shot to win $10.', 'I am not quite sure what your resolution is here (thirders are practically more correct but backwards? What does this mean?).\n\nFor anyone interested in this problem, I recommend this CV thread: https://stats.stackexchange.com/questions/41208/the-sleeping-beauty-paradox', '>My solution\n\nYou mean your failed attempt...', '[Whuber](https://stats.stackexchange.com/users/919/whuber) is the GOAT.']",1,4,https://www.reddit.com/r/statistics/comments/138g9ez/e_my_solution_to_the_sleeping_beauty_problem/
210,2023-05-05 10:12:00,[Q] What is probability of 4 losing streak if I have a betting system with historical win-rate of 70%?,"Is it correct that since my chance of losing is 30%, then I should multiply 0.30 by itself 4 times to get 0.81% chance of getting 4 losses in a row?


Or am I doing probabilities wrong?","[""That's for a single instance of 4 games.  But realistically, I imagine that you're computing the probability of a 4 game losing streak in a long run of games, which makes the probability much higher."", ""0.81% is the chance to lose 4 specific games. If you play 1000 games in total then you have 997 possible places where such a losing streak can be. If all these places were independent then your chance to not have a losing streak would be (1-0.0081)^997 = 0.0003 or 0.03%. That means a 99.97% chance to have at least one losing streak in it.\n\nThe 997 places are not independent, however - if you won the first 4 games then there is no way to have a losing streak with games 2 to 5, for example. You can calculate the chances in a spreadsheet ([longer explanation here](https://www.reddit.com/r/DreamWasTaken2/comments/kkaysw/the_chances_of_lucky_streaks/)), and the result is a 99.7% chance to have at least one losing streak of length 4 if you play 1000 times.\n\nNote: This assumes each bet is independent. If that's not the case, then the chance can be different."", 'You’ve calculated the probability of exactly 4 successes in n tries, not 4 in a row.', ""So I just went down a deep rabbit hole of Markov chain and I want check with you mine understanding of it if it's alright.\n\nI created a 5 by 5 transition matrix with. first row 0.3,0.7,0,0,0, second row 0.3,0,0.7,0,0, etc.  and If I take it to the 1000th power (matrix multiply itself 1000 times), the probability of getting  at least 4 consecutive loss in a row is near 1, not to mention exactly 4.\n\nCan you give me some pointers as to what I did wrong?"", 'I can almost guarantee you’ll get a streak of 4 losses at some point in 1000 bets.\n\nThe exact probability is difficult to calculate, lots and lots of inclusion-exclusion. But via simulation, I believe the probability is something in the neighborhood of 0.996.']",19,23,https://www.reddit.com/r/statistics/comments/1387i9m/q_what_is_probability_of_4_losing_streak_if_i/
211,2023-05-05 03:47:18,"[Q] Is the bottom-middle part of this flowchart accurate? If so, how can you do a binomial or hypergeometric test of a hypothesis about a sample mean?","[Link to the flowchart is here](https://ibb.co/Mpqq2mt)

I could see using binary quantification, but that assumes symmetry, and that isn't mentioned in the flowchart. Is there some method here I'm unaware of?

This is just from a tutoring website, so it's nothing too official. I was thinking of using it in my class but that part of the chart looks a little iffy, so I thought I'd check here first before handing it out to my students.","['The chart goes wrong at the first split.\n\n(When I saw the mu on the first arm I was encouraged because I thought ""oh they\'re going to address hypotheses in the chart"" but they don\'t; merely mentioning some parameters. But that sense of encouragement was a rather brief moment.)\n\nThe ""question"" there at the top is *type of data*, but you can see that it has *population parameters* on the arms, which is not about data but about populations. (what it should probably start with is making it clear whether we\'re looking at one or two samples, but given where we end up in the part you\'re asking about, it looks like it must be one-sample)\n\nIt commits the usual error of ignoring the specific hypothesis; it sort of nearly gets there by mentioning parameters but not how they relate to any hypothesis (nor even how many there are), but then the mention of *data* is just nonsense. Whatever they meant to do here, they didn\'t do it right, but in any case there would be several changes to fix just the first split\n\nBut lets take it at its word and assume that the whole left arm is, as labelled, about *means*.\n\nThen putting the n>=30 split next .. that\'s just dumb organizationally in terms of the flowchart, which is why it ends up pointlessly duplicating the t/z test bit. It\'s also just bad statistically (n vs 30? That\'s just demonstrably wrong), but let\'s follow it along and assume n<30\n\nThen if we don\'t assume we have normality, we head right again.\n\nWhat\'s this, proportions tests? WTF?\n\nLook back up the top, at the first arm on the left. We\'re in an arm for *testing means*.  Neither of these tests are for testing means. They\'re presumably thinking about doing a *one sample* test for a *median*.  I can\'t really see how it makes sense otherwise, at least not without a lot of handwaving.\n\nBut to do that (go to a test of medians), they just changed the hypothesis, which was about means not medians. Sadly the flowchart doesn\'t explain the reasoning so we\'re left to guess what it\'s thinking.\n\nA flowchart does not get to tell me what my hypothesis is; while we didn\'t get to say what it was, we already established that the hypothesis was at least about means\n\nSuitable tests for means in this non-normal arm (if the previous splits had made sense) would be \n\n1. Choose some other distributional assumption and test means with that. e.g. via a generalized linear model\n\n2. Don\'t choose some other distributional assumption and test means nonparametrically, perhaps via a permutation test or maybe a bootstrap test (though if n<30 perhaps the sample size would be too small to get accurate alpha with a bootstrap).\n\n> that assumes symmetry\n\nDoes it? A binomial test for a median? Oh, unless you mean ""in order for a test of medians to be a test of means"" I guess.', '> I don\'t understand all of what you wrote\n\nMy apologies. By all means feel free to ask me to clarify anything that\'s insufficiently clear.\n\nBesides that issue of importance of being specific about hypotheses, I\'m especially concerned that so many of these charts fail to consider other parametric assumptions than normality\\*, especially when in many cases a reasonable parametric choice may be fairly natural (e.g. models for counts, durations, etc) and so many of them are readily accessible; not just what you get in a typical implementation of generalized linear models, either -- with hardly any effort parametric survival models should offer a host of additional non-exponential-family models, and beta regression\'s not hard to find either, all of which should allow for typical tests of equality of of one population mean to a hypothesized value or equality of two, three or more population means.\n\nHowever, I wouldn\'t just reserve such things for small samples, since it\'s not just significance level correctness that people need to worry about, but also power; if your sample size is not-tiny because you want power against small effects, relying on large sample approximate normality of sample means to take care of the significance level doesn\'t help with that; you really want the option of other parametric tests - or nonparametric tests - at any sample size.\n\n> Yeah, that is what I currently have in my recommendations chart, looks like I will be sticking with that.\n\nIf you have resampling tests (like permutation test or bootstrap test) of means in yours then it sounds like yours is considerably better.\n\nIf you get to being clear about what hypotheses you might want to test, so much the better. \n\n> >  ""in order for a test of medians to be a test of means"" I guess.\n\n> Yes, that\'s what I meant.\n\nOkay, then yes, symmetry\\*\\* would be a sufficient (but not necessary\\*\\*\\*) condition for a one sample test of medians to be a one sample test of means (as long as means are finite, at least). If they meant that, they really would have to state that assumption/restriction in the chart, because a naive user of it isn\'t going to be able to infer that from the chart.\n\n---\n\n\\* and fail to mention other nonparametric tests than rank tests, and typically fail to consider anything but location-tests at all\n\n\\*\\*  though for correctness of significance level you really only need it  under H0, as with most such assumptions; the constraints under the alternative could be milder and still preserve the same ordering under H1 (you\'d just need that the population mean-median difference grows substantially more slowly than the median grows from the hypothesized value to guarantee you were on the same side of the alternative)\n\n\n\\*\\*\\* it\'s possible for (a) population mean and median to be equal under asymmetry and (b) be in a situation where population mean and median *differ* but a change in one implies a proportional change in the other. Consider for example, an exponential distribution. The median is 69.3% of the mean, so if I assume an exponential I *could* convert my mean-hypothesis to a comparison of the median with 0.693 μ₀.  In that specific (exponential) case you wouldn\'t want to do it that way -- you\'d be much better off with a chi-squared test based on the test statistic nȲ/μ₀ (with n d.f.) -- or you could use a generalized linear model to achieve the same end, but it serves to illustrate that there are some simple exceptions to the requirement of symmetry if you have some suitable model.', 'I don\'t understand *all* of what you wrote, but I am going to address these two points:\n\n> Don\'t choose some other distributional assumption and test means nonparametrically, perhaps via a permutation test or maybe a bootstrap test (though if n<30 perhaps the sample size would be too small to get accurate alpha with a bootstrap).\n\nYeah, that is what I currently have in my recommendations chart, looks like I will be sticking with that.\n\n>>that assumes symmetry\n> \n> Does it? A binomial test for a median? Oh, unless you mean ""in order for a test of medians to be a test of means"" I guess.\n\nYes, that\'s what I meant.', 'Yuck. Teaching/learning stats is so much easier when one gets out of the shackles of compute-limited tradition that leads to flowcharts like this and instead embrace modern generative inference.', ""I'd say no. A binomial distribution would still be used if the samples were being replaced.""]",6,9,https://www.reddit.com/r/statistics/comments/137xspb/q_is_the_bottommiddle_part_of_this_flowchart/
212,2023-05-05 03:40:45,[Q] How to conduct a post-hoc power analysis for an (partial-)proprotional odds model?,"Hello!

I have a question about conducting a post-hoc power analysis for a (partial-)proportional odds model. I have some experience with G\*Power, but I was not able to conduct a post-hoc power analysis there. My Models each consist of one ordinal response (4-point scale), one predictor numeric and one predictor ordinal (4-point scale) variable, making it difficult to use logistic regression which is the only form that seems to match in G\*Power.

I have already conducted the necessary analyses, have the odds ratios and calculated the McFadden Pseudo R2.

Is there a way to conduct a post-hoc power analysis for my (P)PO models? (e.g. alternative software, a r-command)

Alternatively, would it be possible to use logistic regression in G\*Power instead? If so, how would I go about doing that?

Any help or advice would be greatly appreciated! Thank you very much in advance! (If you need any more information about the analysis, I'll gladly provide it.)  


*Edit: Missing Letter.*","['Post hoc power analysis is not a conceptually valid thing. See [this](https://gpsych.bmj.com/content/32/4/e100069) or [this](https://data.library.virginia.edu/post-hoc-power-calculations-are-not-useful/).', 'Thanks for the clarification, it helped me a lot!']",3,2,https://www.reddit.com/r/statistics/comments/137xma9/q_how_to_conduct_a_posthoc_power_analysis_for_an/
213,2023-05-05 03:17:25,[Q] How can forecasts be increasing if the coefficient is less than 1 in an AR1 model?,"In order to compare the forecasts of another model, I estimated a simple AR(1) model of log commodity prices with daily observations using *statsmodels* in Python. For crude oil I got an estimate of 0.9983339 for the lag and 4.21 for the constant.

 I wanted to calculate the forecasts generated by the simple AR(1) model, I do that with the code

    ar1_model = sm.tsa.ARIMA(endog=log_data, order=(1, 0, 0), trend='c') ar1_results = ar1_model.fit() forecasts = ar1_results.get_forecast(steps='2022-12-30') forecast_mean = forecasts.predicted_mean 

My problem is that the forecasts generated are an increasing function of time. I don't see why that is the case, as the autoregressive coefficient is less than 1, so shouldn't that mean that the forecasts ought to be decreasing?

I suspect that the confusion comes from the constant term and the inclusion of a trend, but I figured that this constant term here cannot be equal to the trend as that would mean that for each timestep logprices are estimated to be increasing by 4.21, which cannot be the case. However, if there is a trend, I don't see why its estimate wouldn't be given by the estimation results.

I would appreciate any clarification regarding how this works.","[""Each forecasted period is a simple linear addition of the previous day and a constant.\n\nForecast day 1 uses the last complete day from the AR(1). You basically multiply the coefficient (based off of history) of that with the result of that day, and add a constant based off of your trend.\n\nI would expect to see a different model if you just use auto ARIMA (it's the `fable` package, correct?) and I also suspect that your portmanteau test (Ljung-Box) shows an underfit"", ""What does the help say about 'trend=c'?"", 'The documentation basically just says that we can choose ""c"", ""n"", ""t"" to choose what type of trend term we want. However, I also tried estimating the model with the *SARIMAX* method (of which *ARIMA* should be a special case) and there for the results, instead of an estimate for the constant, I get the actual intercept, which does seem to work as the trend in prices.\n\nThe problem was probably just it is never really explained what the estimation results for ""constant"" mean and that the actual intercept estimate isn\'t given with the *ARIMA* method.']",3,3,https://www.reddit.com/r/statistics/comments/137wzmp/q_how_can_forecasts_be_increasing_if_the/
214,2023-05-04 22:50:58,"[Q] When using the SARIMAX model in Python, can I recreate the Confidence Interval for each of the forecasted value using the simulation?","I am trying to forecast quarterly population volume on the differenced population data. The issue is that the model.get\_prediction(start = start, end = end, dynamic = False).summary\_frame() calculates the Confidence Interval values on the differenced data. Therefore, I cannot reconcile the actual quarterly population forecasts with the model's calculated Confidence Interval. Was just wondering if I could use the simulation to circumvent that since I need to take cumulative sum to derive the actual forecasts.

Thanks!","['Have a look at the forecasting packages  written in R by Hyndman et al. You can find awesome books (either fpp2 or fpp3) on otext.com which accompany these R packages and explain in great detail how all the maths work. I would recommend you check that book out and have a go at this R package combined with your data so you can have something to validate your own Python script with', 'Alright, boys and girls, buckle up. Here is what you need to do.\n\n1) Fit your model on your data.\n\n2) Run diagnostics on it to make sure it makes sense.\n\n3) Create n samples over the next t periods using the simulate() method of TS model of your choice. \n\n4) Add the last observation from your original data to the first observations of the new data and then find the cumulative sum over then t periods for your simulated data.\n\n5) Then simply calculate the mean, upper bound, and lower bound for the 95% confidence interval for your cumulatively summed data over the t time periods.\n\nEasy peasy lemon squeezy.']",6,2,https://www.reddit.com/r/statistics/comments/137o73o/q_when_using_the_sarimax_model_in_python_can_i/
215,2023-05-04 22:40:55,[C] Is there a github link for new grad openings for full time or internship?,I was going through r/csmajors and saw a github link for internship and new graduate openings. I was wondering if Statistics also has something of this sort?,[],0,0,https://www.reddit.com/r/statistics/comments/137niq2/c_is_there_a_github_link_for_new_grad_openings/
216,2023-05-04 22:36:26,Are there any rules of thumb for sample size for hierarchical linear regression? [Q],"Hey all, 

I’m wondering if there are any literature-supported rules of thumb for sample sizes that are sufficient for hierarchical linear regression. I know that Green (1991) recommends N = 50 + 8m for multiple linear regression, but I’m not sure if that can be adapted for hierarchical linear regression with 2 blocks (2 predictors for the first block, and 1 predictor for the second block). Note that I cannot do an a-priori power analysis, as the sample has already been recruited (that was an oversight by me as this was the first study I’ve ever done).

If you know if any papers that offer some simple sample size recommendations/rules of thumb for hierarchical linear regression that would be amazing. Or if you can tell me whether I can just use green’s formula, that would also be helpful. Thank you!","[""If the sample is essentially set at this point, I don't think I would bother with this power calculation. Assuming you do this and find that your sample is too small, what are you going to do? Can you recruit more participants?"", ""This is tough to estimate, in my experience. I don't know of any rules of thumb. The recommendations I see call for simulations to make the power estimates. Jake Westfall has some webapps on his home page that might help with this.\n\n[https://jakewestfall.org/](https://jakewestfall.org/)\n\nEdit: For some reason, his webapp links don't appear on his home page. But they are on his resume page: http://jakewestfall.org/resume/"", 'Well, we used Green’s rule of thumb prior to recruitment because we didn’t have an a priori idea of what effect size we were expecting given that our construct of interest is a relatively new construct with very minimal research (like a few papers, with no regression analyses). Therefore we weren’t able to do an a priori power analysis, which requires expected effect sizes from what I understand. Green’s rule of thumb is good for medium effect sizes, but in our analyses we detected significant medium to small effect sizes. So that’s basically the situation.', 'I’ll not familiar with Green’s rule of thumb unfortunately, but I think that as long as the confidence intervals look reasonable, there’s no reason to do a post-hoc analysis. Sounds like you may have overestimated the variability initially and got better results than expected, which is a nice “problem” to have! Normally people do these post-hoc power analyses because they don’t get significant results and want to blame it on being underpowered (i.e. saying “this wasn’t statistically significant but it would have been with a larger sample”). \n\nBased on what you’ve said, I think you’re doing things correctly!']",1,4,https://www.reddit.com/r/statistics/comments/137n7yd/are_there_any_rules_of_thumb_for_sample_size_for/
217,2023-05-04 22:33:35,[Q] Change over time with (POSSIBLY) unpaired sample,"Currently working on a project. Wave 1 had a sample size of n=2000. We are currently collecting wave 2 sample n=2000. We did online data collection and will try to reconnect with the same sample as wave 1 but there's a good chance that Wave 2 will have some new people ranging anywhere from 1 new person to 2000 new people. not sure. 

My Question is:

1. at what sample size will a sample go from paired to unpaired? i.e. can we consider the sample paired if we have 1999 people from wave 1 and 1 new person, how about if we have 1000 new people and 1000 people from wave 1?  
2.  is it possible to look at change over time with a (potentially) unpaired sample?
   1. if so what stats test will that be?","['> at what sample size will a sample go from paired to unpaired\n\nIt\'s no longer strictly paired as soon as you have some unpaired people, and it\'s not longer strictly unpaired as soon as you have some paired people. In between you have *overlapping samples*.\n\n> i.e. can we consider the sample paired\n\nNo, this is *not* a matter of *id est* (""that is""); it\'s a different question with a distinct answer. The previous question is about what *is*, this one is about what you *can do*.\n\nClearly if you have a single unpaired person, there\'s little information loss in omitting them\\*. But if you had 1990 people unpaired in each sample and only 10 pairs, leaving out the unpaired people would be consequential. On the other hand, ignoring the pairing or even leaving out those 10 altogether would probably not be. (How consequential depends on how correlated the paired values are. Very high correlation means that the pairs have a lot of information, but near-zero correlation means there\'s little loss in treating them as unpaired.)\n\nNote that in between the extremes, there\'s methods for overlapping samples. If I had say 40% paired data and 60% unpaired values in each wave, unless the pair-correlation was unusually high or unusually low I\'d very much want to try to avoid the loss in either dropping the unpaired or ignoring the pairing.\n\n> is it possible to look at change over time with a (potentially) unpaired sample? \n\nPossible, yes, but the difficulty is that if there are no paired values (indeed, in many cases, even if there are), you have to find a way of attributing differences to the thing you\'re interested in rather than the alternate explanation that it\'s due only to other factors than what you want to look at, whether events that ""intervened"" or drivers of the response that changed over time. You would have to worry about issues like omitted-variable bias, taking steps to deal with such issues, or your conclusions would be suspect. \n\nThere\'s not enough information to say a lot, though (and I might not be the best person to offer a lot more detail if there were).\n\n(When you say \'online data collection\', is your sample self-selected?)\n\n---\n\n\\* however, the reason *why* they\'re missing can certainly impact *bias*: you can\'t just blithely assume there\'s no consequence to either omitting *or including* people only present in the first sample -- e.g. if they\'re missing in a way that relates to their observation, you\'ve got problems. https://en.wikipedia.org/wiki/Missing_data#Types', ""If by self-selected you mean participants choose to participate then yes. and thats the problem. we will TRY and get the same people as wave 1. BUT we have no way to guarantee that they are going to participate in wave 2.\n\nThe problem is we NEED 2000 people to participate in Wave 2. so its not a matter of excluding people who didn't participate in Wave 1 and just analyzing the people who did (i.e. only using the 1900 who participated in wave 1 AND wave 2). So it sounds like you are saying is that the sample is not going to be paired if we can't guarantee the same 2000 people as wave 1 (and we are not going to be). since the goal isn't to get as many of the same people who participated in wave 1 but to get 2000 people it sounds like we will be working with overlapping sample.\n\nWhat are my options then if we have overlapping sample and we want to look change over time. Am I just limited to a unpaired t-test and look at group differences. or is there a fancy way to look at change over time with overlapping sample?""]",1,2,https://www.reddit.com/r/statistics/comments/137n15z/q_change_over_time_with_possibly_unpaired_sample/
218,2023-05-04 22:32:25,[Q] Navigating Differences in Scale: A multivariate approach with standardization or separate component analysis?," I am conducting a research study on sex differences in the physical limitations of patients with ankylosing spondylitis (AS), using the **Bath Ankylosing Spondylitis Metrology Index (BASMI)**. The [BASMI](https://www.mdapp.co/basmi-score-bath-ankylosing-spondylitis-metrology-index-calculator-616/) is a composite score that consists of five components, including cervical rotation (measured in degrees), tragus-to-wall distance, intermalleolar distance, modified Schober's test, and lumbar side flexion. Each component is scored on a different scale, with most of them being distances measured in centimeters, but each having a distinct range. For example, intermalleolar distance can be up to 120 cm, whereas tragus-to-wall distance is usually between 5-15 cm.

 To account for the differences in scale, I am considering two approaches for my analysis: (1) standardizing each component by converting them to z-scores and performing a multivariate linear mixed model analysis, and (2) performing separate analyses for each component (i.e, a univariate analysis). My main research question is focused on analyzing sex differences in the physical limitations of the patients, and how these differences vary across the individual BASMI components over time after treatment.

However, I am concerned about how to interpret the regression coefficients if I choose to standardize the components, as well as how to explain the implications of this approach in my research report or paper. On the other hand, performing separate analyses for each component may result in a loss of statistical power due to smaller sample sizes for each component, and may not account for the correlations between the components.

I would appreciate any advice or guidance on the advantages and disadvantages of these approaches, and how to choose the most appropriate approach for my specific research question and data.",[],2,0,https://www.reddit.com/r/statistics/comments/137my9l/q_navigating_differences_in_scale_a_multivariate/
219,2023-05-04 22:28:43,[Q] HELP ! what test should I use ? Comparing test times before and after having caffine or decaf,"Desperate! , I was thinking independent samples t-test but I have 2 metric and 2 categorical variables so it’s confusing. 

1. My participants are either allocated Into caffeine group , or decaf. 
2. They complete there first test with without drinks. 
3. After completing, I record the times it took for each person to finish the test 
4. I give them their allocated drink , 
5. wait 30 mins 
6. they complete a second test. 
7. compare the test times of before and after - to see if caffeine will make them finish quicker","['You have a mixed design on your hand! A time predictor and a treatment predictor.\n\nYou can do this with a 2x2 repeated measures ANOVA. If you want to evaluate score and time on the tests, you could use a multivariate procedure (MANOVA), however I suspect that this would be something you need to spend time learning. So, I would suggest just doing a 2x2 repeated measures ANOVA for each outcome.\n\nThe 2x2 refers to having factor one with two groups and factor two with two groups (time & treatment)', ""One possibility would be to look at a regression model, where you have a model for the after times as the response (DV) with the before times is a predictor (IV), where you'd then fit treatment and look at interaction of before-time with treatment.  after = before + treatment + before x treatment + error\n\nThere's issues with times, however. They tend to be right skew and more variable as the mean increases (heteroskedastic). I'd probably want a generalized linear model rather than a regression (a gamma glm would be the most obvious way to deal with all those issues at once).\n\nYou'd probably want to do mixed models as well, to deal with subject effects as a random effect, so that would be glmm, but I expect we're getting some way beyond what you're able to deal with."", 'Thankyou so much ! You saved my life . I’ll will get on it asap', 'Repeated measures ANOVA', 'So it would be the test is the factor and the caf/decaf is the groups']",1,12,https://www.reddit.com/r/statistics/comments/137mouz/q_help_what_test_should_i_use_comparing_test/
220,2023-05-04 19:58:52,[Q] Linear mixed model with a variable that repeats itself as IV,"Hi all,

I'm running a LMM with the formula : `DV ~ audioStim + visualStim + x_Audio + (1 | participant)`

My dataset includes multiple measurements from each participant (DVs), audioStim has 3 levels (representing 3 audio stimuli) while visualStim has also 3 conditions (3 visual stimuli). x\_Audio, however, is continuous variable that measures the perception of the participant for each audio stimulus.

I am wondering whether there are any issues with including x\_Audio as a repeated variable in the LMM. Can you provide guidance on whether this approach is appropriate?

This is an example of my dataset:

|participant|audio\_stim|visual\_stim|x\_AudioStim|DV|
|:-|:-|:-|:-|:-|
|1|A|X|3|0.35|
|1|A|Y|3|\-0.14|
|1|A|Z|3|0.5|
|1|B|X|2|0.1|
|1|B|Y|2|\-0.05|

&#x200B;

Thanks in advance!","['> 5-point\n\n&#x200B;\n\n>continuous', 'You don’t need to assume anything about it, because you defined it ;)\n\nAnd it’s not continuous if it has discrete points. So you should model it as an ordinal variable.', 'Since x_Audio seems to be a measure of individual difference and you are not interested in it as part of your research question, you may consider coding it as a random effect. If you believe the relationship between x_Audio and your DV varies across participants, treating x_Audio as a random effect allows the effect to vary across participants. This provides a more accurate estimate of the effect(s) you are interested in per your answer. \n \nLikewise, this would also allow you to estimate the variability of x_Audio’s effect across the hierarchical levels in your dataset. \n \nJust my two cents. Be sure to ask your supervisor/prof/university stats consulting service (if available).', 'There should in principle not be a problem. \n\nHowever, if it is a continuous variable, how can it be identical across multiple measurements? That sounds to me like either it is not a continuous variable, or there is additional nesting of observations which you did not mention.', 'What is the question are you trying to answer, and how does the model syntax answer it?']",15,15,https://www.reddit.com/r/statistics/comments/137igbi/q_linear_mixed_model_with_a_variable_that_repeats/
221,2023-05-04 16:01:06,[Q] Chi square test or Mc Nemar?,"My data looks like this (contingency table). Two measurement approaches that should measure the same underlying behavior. 

I need a signifiance test in order to report coefficients that both measurements differ significantly. The behavior measured is quite complex and there is no ""true"" or validated measurement.  Measurement 2 was after Measurement 1. Which statistical procedure would be the most appropriate?  


&#x200B;

|||Measurement 2||
|:-|:-|:-|:-|
||| Behavior not identified | Behavior identified |
|Measurement 1| Behavior not identified | 349 | 116 |
|| Behavior identified | 60 | 126|","['*Perhaps* the way you\'ve described your hypothesis, this would be McNemar\'s test.  The test will compare the 116 count to the 60 count, concluding that more individuals switched to ""identified"" from ""unidentified"" than vice versa.\n\nYou might be asking a different hypothesis.', 'Thanks! I think you got my idea right.\n\nWe actually invented a measurement (measurement 1) to capture the behavior. We then directly asked them if they think that show the behavior (measurement 2). The behavior is quite seldom and a little bit complex and technical but there is a possibility that people think they may do it although they don\'t. So it\'s not easy to assess.  Thus, there is no ""real"" measurement, just two approaches compared and the results have to be discussed.\n\nChi square does not fit because the groups are not paired?', 'A chi-square test of association tests something different.  If *Measure 1* and *Measure 2* are correlated.', 'Ah okay.\n\nAnd for McNear: Is it a problem that the kind of measurement differs? As I described later, we had two measurement approaches. Further, I do not think that the two measurements are independent.', 'If you can arrange the table meaningfully as you did, then the test is appropriate.  (Assuming other assumptions and stuff).']",3,5,https://www.reddit.com/r/statistics/comments/137dudx/q_chi_square_test_or_mc_nemar/
222,2023-05-04 10:07:03,[Q] [C] Assessing exponential decay model against data with both random and systematic errors in the dependent variable data points.,"
I have a data set that I know would strictly follow an exponential decay function if it were free of error. I even know the decay constant with certainty. I do not know the initial value and there is a systematic error that will grow with time. I want to fit initial values to the data points on a rolling basis and then quantify how well the data fits the decay model with the know decay constant and the fitted initial values. This should allow me see when the systematic error becomes too high and truncate the data points past that so that I can make a good estimate of the initial value and project forward using the model without the systematic error. 

I have used the standard transformation to a linear regression and have calculated a rolling set of initial values using the data points and the known decay constant. I tried calculating R2 = 1 - RSS/TSS with the transformed data points ( z = lny = a+b*lamnda). None of them are very close to 1 and they eventually go negative as the systematic error gets out of hand. It is possible that there is no interval of independent variable for which a credible exponential model can be fitted. I am also concerned that R2 might not be an appropriate metric as it's assumptions only hold for linear regression. I do not know it can used for a nonlinear exponential model that has been transformed into a linear one. R2 might even be valid for the transformed model, but can't be back converted to the exponential one.

I am here to listen.","[""Can you describe the sort of variable you're observing? e.g. is it a concentration (say, one substance in another), a count, a proportion, an angle, a length, ...\n\n> there is a systematic error that will grow with time\n\nCan you describe how this systematic error operates? \n\nCan you also describe how the noise (unsystematic error) would operate?"", ""Variable is a rolling average of the count rate of a radioactive source. Systematic error is knuckleheads recording values based off of what the source has 'always' read rather than as independent measures that would reflect natural decay. Random error should be gaussian. \n\nThese details are less important than determining a valid metric for comparing the estimated initial values to see if there was an interval early in the data set in which the systematic error was not obscuring the natural decay rate."", ""> Random error should be gaussian. \n\nNot if it's averages of counts, no. That might be an adequate approximation, but obviously not the actual distribution \n\n> knuckleheads recording values based off of what the source has 'always' read\n\nSo a constant value for a given source?\n\n> These details are less important than determining a valid metric \n\nThe details are *critical* to a serviceable model, and thereby to a good way to approach the question.\n\n> I am here to listen.\n\nI detect a slight contradiction,"", ""The random error distribution may be worth poking at, but the systematic is what is giving me grief. \n\nThe Sr/Y-90 source is used in the field to check that a given instrument is reading normally with a 20% tolerance. The source check readings made recipt from the calibration facility are used to calculate the rolling average, which should be decreasing per the 28.79 yr half-life. Knuckleheads have been inadvertently using the variability of the needle movement to record readings closer to what they are used to and retarding the decay of the rolling average that was supposed to trend down to follow the source decay. After enough time passed, the rolling average had diverged so far from the true source response value that even knuckleheads couldn't fool themselves into recording readings that were within 20% of the corrupted rolling average. I am trying to tease out a defensible estimate of the initial true instrument response to the source from early data points for which the systematic error should be relatively small. I could then decay that value, trusting physics, and give them a good value to perform the field source checks against.\n\nWe have a plan B of manually checking all the source and instrument type combinations, but that would entail a lot of leg work. I want to make an objective determination as to if it is possible to straighten this out with math or be able state that we really do need to do it the hard way. Thus, I need a valid metric to assess how well my exponential regression model fits subsets of data points.  If I can get a good fit, I can say this subset is not excessively corrupted by systematic error and the fitted initial value from it can be used with confidence to seed the new decay correcting source check value that will replace the rolling average.""]",3,4,https://www.reddit.com/r/statistics/comments/1376rtt/q_c_assessing_exponential_decay_model_against/
223,2023-05-04 06:15:11,[Q] How to find the Confidence Interval values for each of the forecasted value if I trained the SARIMAX time series model on the differenced data?,"I have trained my model on the first differenced population volume for the last 93 quarters (Data is [https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901](https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901)). I have then trained the model on the differenced data. The issue is  that model forecasts the differences. The easy part is simply adding the last observation from the original data to the forecasted differences to get the actual forecasts. The issue is that the model.get\_prediction(start=start, end=end).summary\_frame() returns to the Standard Error and Confidence Interval bands for the differenced data. Therefore, how can I calculate the confidence interval values for each of the forecasted observations that were cumulatively added?","[""There's always bootstrapping"", 'I tried asking ChatGPT, but it does not understand.']",5,2,https://www.reddit.com/r/statistics/comments/1371czs/q_how_to_find_the_confidence_interval_values_for/
224,2023-05-04 06:07:49,[E] Which courses should I select? (MS degree) or other recommendations.,"I have to choose two of the following classes for my degree: 

Machine Learning (svm, neural nets, regression trees, bagging, boosting, etc, not sure what language will be used, focused on classification)

Advanced Data Analysis (covers time series, nlp, done in MATLAB (?)) 

Time Series (box-jenkins, seasonal and non-seasonal, done in SAS) 

 I honestly don't know what would be the most useful. I'm also allowed to take classes in other subjects as a grad student. Any recommendations would be greatly appreciated. Thank you. I think I'd mostly like to come out with some good projects that I can add to my portfolio. I am interested in becoming something of an analyst in the future.","[""Time Series is nice, my own class on that also used SAS. I wish we'd used R or Python, but it is what it is. The machine learning class would be good too, hopefully it uses R or Python. I'd probably not bother with the advanced data analysis class since you'll probably be able to pick up that stuff on your own after taking more ML and stats classes. Not very informative, but it's my two cents.  \n\nI don't know if you've had a class on Stochastic Processes or on Bayesian Data Analysis but those two courses are highly recommended for a MS degree too."", 'Don’t take any classes that teach stats/data science with matlab. Waste of time. I’ve never, ever heard of nlp being done in matlab. Wtf? \n\nJobs will want you to know R and/or python. SAS is mostly used in pharma industry (although my knowledge this fact may be out of date, so double check this)']",3,2,https://www.reddit.com/r/statistics/comments/137165o/e_which_courses_should_i_select_ms_degree_or/
225,2023-05-04 02:45:47,[Q] ANOVA versus MANOVA for answering research question? (GraphPad PRISM),"Hi everyone! I'm a researcher, but by no means a statistician, and I've run into a little statistical snag that neither my advisor nor committee member can help with. Thank you in advance to anyone who reads and responds!

In brief, I'm looking at whether Status and Context have an effect on Test Scores (measured in 5 domains: Communication, Gross Motor, Fine Motor, Problem Solving, and Personal Social). 

Originally, I analyzed the data from each domain using a 3-way ANOVA with sex, context, and status as the between-group variables. When a significant F statistic was identified, I ran post-hoc tests for planned comparisons using Holm-Šidák's multiple comparisons test.

However, my committee member is asking why I didn't run a MANOVA instead of the ANOVAs for each domain (as this inflates my risk of Type 1 error). 

From my understanding, there are a few things I am wondering:

1.  A MANOVA would allow me to assess the overall relationship between the independent variables (context and status) and the dependent variables (test domains) while controlling for the intercorrelations among the dependent variables. However, with regards to answering the research question, we were not looking for any intercorrelations among the dependent variables (e.g., whether the scores in one domain are related to the scores in another domain). Does it still make sense to run a MANOVA, then, if it will conduct more comparisons than we need? Doesn't this pose issues with inflating the Type 1 error as well?  (Again, not a statistician, so I apologize if this is a pretty dumb question).
2. If I had conducted a MANOVA, I would have had to conduct follow-up univariate ANOVAs afterwards to determine which domains were driving the significant differences, right? How is this different from simply starting with the univariate ANOVAs, finding that there are significant differences, and then adjusting the threshold for multiple comparisons in the post-hoc planned comparisons? 
3. As well... is it even possible to run a MANOVA on GraphPad PRISM? I can't find the option in the Analysis drop-down, and nothing online is giving me a clear answer.

Thank you again!","['>If I had conducted a MANOVA, I would have had to conduct follow-up univariate ANOVAs afterwards to determine which domains were driving the significant differences, right? How is this different from simply starting with the univariate ANOVAs, finding that there are significant differences, and then adjusting the threshold for multiple comparisons in the post-hoc planned comparisons?\n\n[Here\'s](https://stats.stackexchange.com/questions/133297/what-is-the-null-hypothesis-of-a-manova) a relevant nugget:\n\n>The answer is that running all univariate ANOVAs, even though would test the same null hypothesis, will have less power. See my answer here for an illustration: How can MANOVA report a significant difference when none of the univariate ANOVAs reaches significance? Naive method of ""combining"" (reject the global null if at least one ANOVA rejects the null) would also lead to a huge inflation of type I error rate; but even if one chooses some smart way of ""combining"" to maintain the correct error rate, one would lose in power.\n\nAnyways in the scenario where you start with a MANOVA, follow-up tests would be contingent on rejecting the null. You\'d be using it as an omnibus test to (weakly) control the type I error rate, and under certain circumstances pick up on differences that you otherwise wouldn\'t have with individual ANOVAs.', 'Sometimes there are interesting multivariate interpretations of MANOVA. See [this article.](https://psychology.okstate.edu/faculty/jgrice/personalitylab/Grice_Iwasaki_AMR.pdf)', 'Thank you for sharing this!', ""Thank you for sharing this article - I'm reading it right now!"", 'No problem!\n\nI\'d suggest digging deeper on the topic because there are [additional considerations to be made](https://digitalcommons.wayne.edu/cgi/viewcontent.cgi?article=2185&context=jmasm) in controlling error rates. The omnibus test may only have an X% change of being a false positive in the null hypothesis is true, but if you end up with a false positive you\'re exposing yourself to the risk of additional false positives when following up with univariate tests. Conversely, a significant MANOVA might only reflect a ""true"" difference in one of the follow-up tests, but won\'t protect against the rest being false positives.']",2,5,https://www.reddit.com/r/statistics/comments/136vt9e/q_anova_versus_manova_for_answering_research/
226,2023-05-04 02:30:38,[Q] kruskal test is significant but Wilcoxon rank sum test shows none are,"Hello everyone,

I was wondering if someone could please help me in that. I am trying to see whether habitats are microbes found in controls or influences the number of genes in a specific group (e.g. number of transporters or CADzymes or COGs).

More specifically is to compare whether different habitats have different number of genes. I was told to first do a kruskal test to see if there is significance difference between groups, followed by a Wilcoxon rank sum test to see which groups are different. 

Therefore the kruskal test has found significance (p-value = 0.0006427)  difference between habits and number of genes. However when I do Wilcoxon rank sum test all groups are highly insignificant (p > 0.25).

As a result could someone please help me in why this might be so or why this is occurring?","[""1. Don't use plain rank sum tests as post hocs for kruskal wallis. There's a number of closer-matching alternatives.\n\n   e.g. see the answer [here](https://stats.stackexchange.com/a/515697/805)\n\n2. Whatever post hoc you use, this sort of thing will still happen. It happens with anova and t tests for example\n\n  The joint acceptance region of pairwise tests is a rectangular (hyper-)prism. The joint acceptance region of a single omnibus test is not a prism. E.g. for anova its an ellipsoid. Even for the best possible choice  of pairwise post-hocs, parts of the ellipsoid stick out, outside the prism and vice versa. See this diagram https://i.stack.imgur.com/4woXw.png (from here: https://www.reddit.com/r/AskStatistics/comments/wygvj8/insignificant_ftest_but_significant_coefficient/ilxovzi/)\n\n   Kruskal Wallis has [a more complex boundary](https://stats.stackexchange.com/a/76080/805) but the story is quite similar in that parts of it can stick out of the pair-wise test's acceptance-region (still a rectangular prism).\n\n3. There's an additional issue with pairwise rank sum tests vs a Kruskal-Wallis test beyond what happens with t-tests and ANOVA (though it goes in the opposite direction); the ordering relation that the rank sum test looks at is *non-transitive* (A>B and B>C does not imply A>C). You can get a cycle of pairwise differences (A>B, B>C and C>A) that don't show up on the overall test. This is not what's going on here though, the problem is the other way around."", 'Did you do the wilcoxon test pairwise for each group? You should have different p values for each comparison.\n\nAlso, consider the Dunn test as well for your point estimates.', ""It doesn't seem right that the KW test result would be < 0.001 and all the pairwise rank sum tests would be > 0.25.  It sounds like there's an error there.\n\nBut don't use pairwise WMW rank sum tests as a post-hoc.  Use Dunn's (1964) or another appropriate post-hoc for KW."", 'what a great feedback you gave there,those resources are pretty interesting. Thanks for sharing', ""I did do a wilcoxon test pairwise for each group yes and in my output I get p-values for each combination of groups. However, when I look at the output all combination's p-values are over a p-value of 0.05.\n\nHowever a colleague recommended the Dunn's test of  the Kruskal test and it showed the combinations that were significant!""]",8,7,https://www.reddit.com/r/statistics/comments/136veo5/q_kruskal_test_is_significant_but_wilcoxon_rank/
227,2023-05-03 23:46:29,[Q] Why is the t-distribution said to be leptokurtic rather than platykurtic?,"Mesokurtic distributions are similar to the normal (Z) distribution, leptokurtic distributions are more narrow and high (less variability), and platykurtic distributions are more flat and wide (more variability).

So if the t-distribution (especially with n < 30) is flatter and the tails wider, then why isn't it regarded as platykurtic? Several sources say that it's leptokurtic (see link below to Rice University statistician):

*The 𝑡 distribution is very similar to the normal distribution when the estimate of variance is based on many degrees of freedom, but has relatively more scores in its tails when there are fewer degrees of freedom. Figure 10.8.1 shows 𝑡 distributions with 2 , 4 , and 10 degrees of freedom and the standard normal distribution.* ***Notice that the normal distribution has relatively more scores in the center of the distribution and the 𝑡 distribution has relatively more in the tails. The 𝑡 distribution is therefore leptokurtic.*** *The 𝑡 distribution approaches the normal distribution as the degrees of freedom increase.*

It would seem to me that fewer scores in the center and more in the tails is the definition of platykurtic.

[https://stats.libretexts.org/Bookshelves/Introductory\_Statistics/Book%3A\_Introductory\_Statistics\_(Lane)/10%3A\_Estimation/10.08%3A\_t\_Distribution#:\~:text=The%20t%20distribution%20is%20therefore,the%20degrees%20of%20freedom%20increase.&text=Since%20the%20t%20distribution%20is,Table%2010.8](https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Book%3A_Introductory_Statistics_(Lane)/10%3A_Estimation/10.08%3A_t_Distribution#:~:text=The%20t%20distribution%20is%20therefore,the%20degrees%20of%20freedom%20increase.&text=Since%20the%20t%20distribution%20is,Table%2010.8).","['It looks like you may have gotten yourself confused here by mixing up kurtosis and variability. You are probably mentally comparing distributions where the variance is changing at the same time as kurtosis is changing. \n\nKurtosis is about heaviness of tails primarily. If you are comparing kurtosis to the normal distribution you ideally should be considering distributions that vary in kurtosis while variance is held constant. \n\nIf you compare a t-distribution to a normal distribution with the same variance, you will find the t-distribution has a narrow point with high density, but decays rapidly from that point. The probability of being in a neighborhood around the high density point is less than the normal distribution. This is leptokurtic because there is reduced total probability in the central region with increased probability out in the tails.\n\nFurther, consider two normal distributions. One with high variance and one with low variance. The one with low variance is very “pointy” compared to the flatter high variance distribution. However they both have exactly zero excess kurtosis as they are both normal. The pointiness here is a measure of variability, not kurtosis. Kurtosis is only related to the pointiness after you have already fixed the variance.', 'The t distribution is leptokurtic because it has relatively more scores in the tails. Compare critical values of the z and t distributions to see you have to go out farther on the t distribution to include 95% of the scores. [This page](https://www.scribbr.com/statistics/kurtosis/) has a nice graph. Note that the platykurtic distribution has very few scores in the tails.', 'David Lane (u/dmlane) sometimes posts here.  Maybe he can offer a clarification.', 'Ahhhh that makes sense. Thanks very much.', ""1. Your link is not working in old.reddit; hopefully this would work for people who use old:\n\n   https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Book%3A_Introductory_Statistics_(Lane)/10%3A_Estimation/10.08%3A_t_Distribution#:~:text=The%20t%20distribution%20is%20therefore,the%20degrees%20of%20freedom%20increase.&text=Since%20the%20t%20distribution%20is,Table%2010.8\n\n2. What's wrong here is there doesn't seem to be correct accounting for the variance of the t-distribution. You have to make the t have unit variance to compare tails and heights.\n\n2. Beware, however, that sometimes the tails and the height in the center can give contradictory messages; *usually* (but not always) it's the tail that has the most impact, and it's normally the thing you'd focus on. \n\n   It will be fine in this case (after making the variance constant, at least as long as the variance is finite), but as a general rule if you want to compare kurtosis the best way to do it is to actually compute the standardized fourth moment either algebraically or via numerical methods.""]",9,7,https://www.reddit.com/r/statistics/comments/136r19x/q_why_is_the_tdistribution_said_to_be_leptokurtic/
228,2023-05-03 23:11:05,Difference between shrinkage estimator and shrinkage priors?[Q],"Hello, I’ll be working as a RA with a professor related to developing a modified shrinkage estimator for high dimensional omics data. More specifically, we are interested in recovering moderately strong signals, that would be zeroed by lasso. I was reading about horseshoe priors and found it interesting how you could essentially control the shrinkage by reformulating the feature selection problem in a Bayesian way.  I wanted to know however, what the difference is between a shrinkage estimator and these shrinkage priors. Are shrinkage priors a form of the estimator? For example there horseshoe prior is a scale mixture of gaussians, where the scale parameters are modeled by half cauchy priors. Is the specification of this shrinkage prior distribution , equivalent to calling it a shrinkage estimator?

Also, I was wondering what other books or literature may have more information on this area of statistics. Basically I’m interested in signal recovery in high dimensional data with shrinkage estimators. I was going to checkout tibshiranis statistical learning with sparsity book. But I was wondering what else there may be.","['A shrinkage estimator is any estimator that applies shrinkage in any way. In contrast, a shrinkage prior has to be explicitly a prior, used in a Bayesian context, that results in shrinkage. The use of a shrinkage prior usually comes with a more general Bayesian setup where you end up with a full posterior distribution. However we can use summaries of the posterior, like the mode or mean of the posterior, as estimators. These specific estimators generated via a shrinkage prior will then also be shrinkage estimators.\n\n\nShrinkage also goes by the name regularization and this keyword will also be helpful for looking for more information. One other book I would recommend is Simon Wood’s book “Generalized Additive Models.” Generalized additive models are usually based on regularization that penalizes curvature (or other aggregate characteristics) of functions in a dense basis rather than sparsity of the basis. \n\nRegularization as whole is a very active area of work, but I’m not aware of any singular treatments. You would be best off looking into journal articles after familiarizing yourself with some basics.', 'I see. So one thing I’ve been trying to wrap my head around is, the posterior distributions here are the posteriors of the coefficient estimates of parameters right? I would imagine some of these are centered around zero, and some centered non-zero (these are the strong signals). Posterior of the betas in regression that is. Is this correct?', 'You get posterior distributions for all model parameters you infer, which includes the coefficients, and potentially other parameters and hyperparameters as well. \n\nIn my experience using shrinkage priors, you get basically three classes of posteriors on parameters with shrinkage priors:\n\n* Rather unambiguously shrunk. Strong/sharp peaks at 0, often symmetric but sometimes you’ll get larger tails pulled one way or the other. These look a lot like the prior. \n\n* Rather unambiguously in the model. Non-zero modes, generally less spiky distributions. These don’t look like the prior.\n\n* Ambiguous support. Much like with spike and slab priors (which are in a sense discrete shrinkage) you can get a mixture distribution for a posterior. In this case, a mixture of the above two kinds of distributions. A multimodal distribution with a spike at 0 and another at a non-zero value.\n\nSomewhat related to all this is what counts as a shrinkage prior. You’ll see people calling g Laplace priors “Bayesian LASSOs” but the Laplace is only exerting a pull on the mode. To get the means, medians, and overall distributions shrunk, you need something more powerful. Hence things like the Horseshoe.', 'I see gotcha. It feels like people who try and come up with new methodological advancements in this area try to choose the priors on the scale so that the tails are fat and sharp peaks (ie. Cauchy). Is new methodological advancements really as simple as trying to find a new mode combination such that the distribution to scale prior/likelihood attains certain theoretical conditions? \n\nI’ve been trying to find a place to start doing a literature review in this stuff and been trying to find patterns. This is something I observed when I read the horseshoe paper. Different distributions for priors and likelihood combinations.', 'Yes and no. \n\nMost shrinkage priors are scale mixtures of Normals, and anyone can slap a distribution on the scale and see what happens. Actually saying something about what you get out of that, like proving that it has useful properties, can be a bit harder. (Plenty of interesting papers never leave the realm of models that have simple expressions for the maximum likelihood estimate so that they can either inform the prior or make statements about shrinkage.) There certainly is a cottage industry of people testing out new combinations, others working to prove properties of existing approaches, and yet others pushing for various refinements of existing distributions. Though those refinements can go many different directions. I’ve seen people argue that we need heavier tails and others argue that we need to calm the tails down a bit. I’ve seen reasons to agree with both sides of this.\n\nBut there’s also computational tractability to consider. I love me a Horseshoe, but the global scale doesn’t mix very well and that’s kind of a pain. Other distributions like the Bayesian Bridge (sometimes called the generalized normal) have global scales that mix better, in the case of the Bayesian bridge because there’s a closed-form density for the distribution conditioned on the global (but marginalized over the local). The trade off I’ve found are that this convenience requires a prior on a transformation of the global scale which is less intuitive to set and harder to think about. Then there are some modifications to the shrinkage priors (like tail-regularized versions) which can make sampling difficult (and the prior even harder to conceptualize and understand). I haven’t seen a ton of papers on all this, but [this one](https://projecteuclid.org/journals/bayesian-analysis/advance-publication/Shrinkage-with-Shrunken-Shoulders--Gibbs-Sampling-Shrinkage-Model-Posteriors/10.1214/22-BA1308.full) covers a decent bit of ground and has some good references. It’s a bit of a dense read.']",19,9,https://www.reddit.com/r/statistics/comments/136prxv/difference_between_shrinkage_estimator_and/
229,2023-05-03 21:23:20,[E]Does anyone have acess to statista? I need for my final project in collage,"Hi everyone I Found a document i've been looking for to use it on my thesis.however, it costs a huge amount of mony and i'm just a 3 rd world country student, can anyone provide it for me. Link:https://www.statista.com/statistics/1186820/co2-emissions-commercial-aviation-worldwide/ Email:segniziad@gmail.com",['Talk with your university IT people:\n\nIs there student licensing?\n\nis it on computer lab machines?'],0,1,https://www.reddit.com/r/statistics/comments/136kfw3/edoes_anyone_have_acess_to_statista_i_need_for_my/
230,2023-05-03 20:48:07,Which type of omega for scale reliability? [Question],Hi! I have a question about which type of omega I should be using to assess the reliability of a scale and its subscales. I have a scale which has two separate subscales derived from confirmatory factor analysis. Should I be using omega total or hierarchical for reliability? Would it be one for both the overall scale and its individual subscales or should it be omega hierarchical for the overall scale and omega total for the subscales (or vice versa)? Thanks!,[],0,0,https://www.reddit.com/r/statistics/comments/136jl9v/which_type_of_omega_for_scale_reliability_question/
231,2023-05-03 19:20:41,[Q] Ranked test result necessary fof testing significant difference,I have ranked test results from rank 1 (highest) and rank 6 (lowest). Is it necessary to use One-way MANOVA to determine significant difference to ranled results? The reason MANOVA was used it because there are two dependent variables but that's not the point. Just want to know that ranked data needed to test its significance.,"[""if you're wondering whether you need to use One-way MANOVA to test the significance of ranked data, I would say it's not always necessary. You could use other tests like the Kruskal-Wallis H test or the Friedman test, which are more appropriate for ranked data. These tests are nonparametric and are used to determine if there are significant differences between groups in your data.\n\nSo, if you're only concerned about the significance of ranked data, I'd suggest looking into these other tests instead of MANOVA. However, if you want to consider the two dependent variables, then using MANOVA might still be a good choice."", 'Sorry for the late reply. Thanks for the answer, I learned a lot!']",6,2,https://www.reddit.com/r/statistics/comments/136hmxx/q_ranked_test_result_necessary_fof_testing/
232,2023-05-03 10:32:37,[Q] Can I compare coefficients in the estimated multiple regression equation?,"Hi all,

If coefficient b1 is higher than b2, can I say the independent variable x1 has more influence on dependent y than independent variable x2?

For example:

y\_hat = b0 + 3\*x1 + 2\*x2

Is it safe to conclude that x1 has more influence on y\_hat than x2?

Assume the overall regression relationship and each independent variables are significant.

Thank you in advance!","[""Not if your b's aren't standardized coefficients.  Imagine x1 was measured in cm.  Now change it to being measured in m or mm."", 'Look up ""standardized coefficient"" ""beta"".', 'Thank you u/SalvatoreEggplant, I divided an independent variable by 1000 and its coef increased. As my understanding, unless all my independent variables have the same scale, comparing coef is wrong.', 'Adding to salvarore\'s excellent answer. Be really careful about the following (ordered by importance):\n1) p values: if the p value of beta2>beta1, and the second is not statistically significant, you cannot reliably state the ordering you are looking for\n2) how you state your results. Just a reminder that your regression model, unless otherwise specified, is not a casual one. Interpreting coefficients as ""influence"" sounds a pretty demanding task to put on a linear regression model\n3) linearities: the linear regression model assumes a certain shape of the coefficients (linear). If, for whatever reason, this assumption is violated (for instance if what links an input-outlut model is a sum of a sin+cosin), there may exist a transformation of your variables that has different coefficients. In this case your statement will still be true -> the linear coefficients are ordered, but saying that all the possible coefficients that can be used to express the relationship between dependent and independent variable in have an ordering may be incorrect.', 'Assuming that your basis functions for each coefficient are fixed, then yes. But careful. That will still be a model assumption, not a data assumption. The way you are framing the question, sounds like you are using your model as a replacement for a test, and most models are bullshit.']",8,13,https://www.reddit.com/r/statistics/comments/1367zj2/q_can_i_compare_coefficients_in_the_estimated/
233,2023-05-03 09:37:48,[Q] Is this left-censored data or something else?,"I have some longitudinal event data where a subject (potentially) repeatedly takes an action over the course of some observation interval, and basically want to figure out how the rate of this event occurring depends on the time that has elapsed since the previous event occurred (I suspect the rate is much lower immediately after an event occurs, then increases over time). Normally when I want to determine event rates I reach for survival analysis since it is able to handle censored observations in a robust manner. I am mostly familiar with how to handle right-censored data where the observation interval ends and all we know about the next event is the time that has elapsed since the previous one.

But I am a little perplexed about how to handle censoring where the time the previous event occurred is unknown, but the current event has been observed. As an example, say I observe a subject for one week, and they take the action once during that time. I know for a fact they also took the action at some previous time before the observation interval started, but don't know how much before. I initially thought this might be considered left-censored data since it is due to the effect of the beginning of the observation interval, but upon closer inspection I'm not so sure anymore because I do observe the time the event occurs.

So to summarize my questions:
1. What is this kind of censoring called?
2. How do I handle it appropriately? Specifically, if I want to write down a likelihood function, how do I do so taking both this type of censoring, and normal right-censoring into account?","['In survival analysis, the failure event is assumed to only happen once.  After you die, you stay dead forever. No resurrection is permitted, so no second death is possible.\n\nIn your situation, the fact that the event of interest is repeated means survival models are not the appropriate methodology. \n\nSince you are interested in the counts of these events over intervals of time, you might think about point process models, indexed by time. They will allow you to incorporate the dependence coming from the fact that the rate at which the event occurs depends on the time elapsed since the previous event.', 'I think you\'re fixating a bit too much on the ""survival"" moniker. Perhaps a more neutral name would be ""time to event model"". There\'s no reason you can\'t use SA techniques for recurring events; in fact [recurrent event analysis](https://en.m.wikipedia.org/wiki/Recurrent_event_analysis) is built on top of it.\n\nI will look into your suggestion of point process models though, that sounds like it could be relevant.', 'This problem reminds me of Poisson distribution (e.g., number of customers during next time interval) and the related exponential distribution (e.g., time between customers).', ""Sounds like interval censor. Say you make an observation at `t1` and `t2` and the actual time of event is unknown. \n\nIt's interval censor if **the only thing known is that it occurred somewhere ***between***** `t1` **and** `t2`.\n\nIt's left censor if **the only thing known is that the event occurred ***before***** `t1`."", ""I think I might have figured it out. In normal right-censored data the contribution to the likelihood of an unobserved event would be `\\int_T^\\infty dt\\, f(t-s) = 1 - F(T-s)` where `s` is the beginning of the interval (for the event), `T` is the end of the observation interval (the censoring time), and `f` is the event density (i.e. `f(t) = F'(t) = -S'(t)`). The interpretation of this is the total probability for the event to occur in the censored region of time.\n\nFor this start-time censored data (which I think is still technically considered right-censored because all we have is a lower bound on the time to event), the contribution to the likelihood is of a similar form `\\int_{-\\infty}^0 ds\\, f(t-s)`, which through suitable u-substitutions can be shown to be equivalent to `1 - F(t)` where `t` is the time elapsed from the beginning of the observation period to the occurrence of the first event.\n\nSo in summary, this is also right-censored, just in an unusual form.""]",3,6,https://www.reddit.com/r/statistics/comments/1366til/q_is_this_leftcensored_data_or_something_else/
234,2023-05-03 08:29:21,[Q] Can I create a PCA time series by using the eigenvectors for each date?,"Long story short, I have a set of data over each summer for four years. I'd like to show the PCA across each season, so see if there is a pattern/how the pattern changes (these are environmental variables).

I have done some reading and I'm wondering if I can take the eigenvector for each date/set of variables and plot it across the season. I've already completed the PCA in python, but it is not great for looking at changes across seasons because it produces ~56 plots per study area.","['> I have a set of data over each summer for four years\n\nWhat does the data represent? How many variables are you working with?\n\n> I\'d like to show the PCA across each season\n\n""Show the PCA"" doesn\'t really make sense. PCA is a technique, not an object or a visual. Do you want to project the data on the top 2 PCA components and plot it?\n\n> take the eigenvector for each date/set of variables\n\nSimilarly to above, ""take the eigenvector & plot it"" doesn\'t make any sense. The top eigenvector of the correlation matrix gives the first PCA component, which you could use to project the original variables. \n\nIf each row of the data represents a distinct time, then you could project this data into the PCA space and view its pattern over time. It\'s not clear exactly what this plot represents though, nor what question you\'re trying to answer with it. What do you want this plot to accomplish?', 'Thanks for the reply. As you can tell, this isn’t my area of expertise but I was asked by my advisor to do a PCA on my data (grad student here). \n\nThe data contains five variables (these are topographic variables derived from remote sensing data). For each date, I have a spreadsheet that contains about 400,000 points that represent a pixel in the remote sensing data. Each pixel contains a value for each of the five variables. There are five columns for each variable, and each row (1-400,000) is just a different pixel within a specific geographic area. I hope this makes sense \n\nSo, the rows of the data do not represent time, but represent a different pixel on the same date. So, I have many spreadsheets (for separate dates) that have a ton of data on them. \n\nThe goal would be to retain a value from each spreadsheet to plot on a time series. So far I haven’t figured out if there is a way to do this using PCA. The goal is to show, using PCA, how relationships between these variables change across season. I work in the Arctic where vegetation changes very rapidly, and is possibly affected by topographic variables like slope, elevation, solar insolation, and topographic wetness, but these relationships change depending on the time of the season. We would like to know how these relationships change across the season and will be also sampling stream chemistry for the corresponding dates. \n\nThanks again for your reply!', ""Based on what you describe, I'm questioning whether it make sense to lump all pixels together into a single PCA. Is it possible that pixels in one area may not have much in common with pixels in another area, or perhaps the patterns could be different? Specifically, could the correlation structure of pixels in one area be entirely different from pixels in another area? Where correlation structure essentially denotes the relationships between your 5 variables.\n\nIn general, I'd be weary of the sparse nature of information across 400,000 pixels. If there is a way to distill the pixels into something more meaningful, that'd probably be a useful step.\n\n> The goal would be to retain a value from each spreadsheet to plot on a time series\n\nSo you want to summarize each date's spreadsheet into a single value or small set of values? \n\nDepending on the nature of the dataset (e.g. discussed above, how much diversity we expect across pixels) this may be straightforward or it may be difficult.\n\nLet me phrase the question this way. Which difference do you expect to be more substantial: the variety of pixels within a single date's spreadsheet, or the variety in behaviors across dates for a single pixel?""]",1,3,https://www.reddit.com/r/statistics/comments/1365au0/q_can_i_create_a_pca_time_series_by_using_the/
235,2023-05-03 07:32:57,[Q] Any learning tools that incorporate sports stats?,"I’m a prospective data/business analyst and it’s my understanding I should develop a strong background in statistics and excel to get started. I’ve already taken some of Eddie Davila’s stats courses on LinkedIn so I’ve learned some of the fundamentals of stats, but I want to take it to the next level and begin applying my learning, specifically working more in excel. 

1. Are there any resources you suggest I look next?

2. Are there any resources that use sports statistics to teach these concepts? 

Thanks!","[""[David Robinson](http://varianceexplained.org/r/empirical_bayes_baseball/) has a nice blog, and a nice pay-what-you-want ebook, about using baseball statistics, specifically for a technique called Empirical Bayes Estimation (in effect, using the distribution of the events you've already observed as a prior for what you might observe in the future.) It is just one tool in your toolbox, not a whole way of doing statistics, but you may enjoy looking at it.""]",1,1,https://www.reddit.com/r/statistics/comments/136412s/q_any_learning_tools_that_incorporate_sports_stats/
236,2023-05-03 05:52:47,[Q] How to perform ordinary least squares with possibly correlated data points?,"Hello!

I'd like to present an example to illustrate my question.

I have a number of athletes and would like to test whether there is a relationship between their running time (measured on the first of Jan) and how much they trained in the previous year. I have multiple data points for each athlete, one for each year.

I can perform OLS and get back some statistics metrics such as p-value etc., but I'm wondering whether these are really accurate since the data points from a single athlete will each be correlated to each other? But at the same time, if the same athlete trained less and therefore got a faster time, is that really correlated to the previous year?

I was thinking I could perform PanelOLS, but just wanted to check here whether anyone has any other ideas.

Thanks!","['By you *specifying* to use the specific estimator ""ordinary least squares"", the correlation among the data points is simply irrelevant and would be ignored.\n\nSo ""how to perform ordinary least squares"" is simply to do it. The question would instead be whether such an algorithm makes sense.\n\nThe problem is that by talking about the algorithm, you\'re placing the cart before the horse.\n\nSo the better question would focus on a suitable *model*, not the estimation algorithm.  Then with a model in place, you might consider how best to estimate that model, and hence how to perform any additional statistical analysis (like say, tests, confidence intervals, prediction intervals or whatever else).', 'You’ll want to use a panel regression that estimates the variance-covariance matrix using the group structure of the data; the assumptions of the OLS estimator for the variance and derived statistics (i.e., standard errors, t-values, confidence intervals) will be incorrect in your setting.', 'Sure seems like panel data', 'Great, thanks, I did think so. Are there any out-of-the-box tools you can suggest I look into?']",3,4,https://www.reddit.com/r/statistics/comments/1361jmf/q_how_to_perform_ordinary_least_squares_with/
237,2023-05-03 05:37:30,[Q] what is the name for this quantity in the covariance matrix?,"I am trying to find out if there is a consistent name in the literature for a term in the calculation of the covariance matrix.

If you have N random variables in a N dimensional vector x, the covariance of x is given by:

Cx = E[ (x - E[x]) (x - E[x])^T ] = E[ x x^T ] - E[x] E[x]^T

where E[x] is the N-dimensional mean vector of x.

I am interested in this first term, **E[ x x^T ]**, and want to know if it has a name. It isn't called the [autocorrelation matrix](https://en.wikipedia.org/wiki/Autocorrelation#Matrix), since I am thinking of a general form where variables in x can be anything, not necessarily representing a signal and delayed copies of itself.

does anyone know if this particular quantity has a name? I looked up ""expected sample outer product"", but couldn't come up with anything meaningful.","['Maybe the second moment of multivariate gaussian sample?', 'the signal processing folks call that the autocorrelation, since signals generally have mean zero', 'thanks alot! I looked into what you said, and it seems like it could be called the second noncentral moment, or the second raw moment, based on this page (https://en.wikipedia.org/wiki/Moment_(mathematics)). regardless, I was hoping it had a simpler name :) guess not.', ""and I don't mean to be mean, here"", ""Your own name was sufficiently descriptive that people would figure out what was meant. \n\nI can think of other potential terms but none are simpler than your suggestion (nor the one by /u/vermillion50), so I don't see much point there.""]",2,5,https://www.reddit.com/r/statistics/comments/13614dx/q_what_is_the_name_for_this_quantity_in_the/
238,2023-05-03 04:58:01,[Q] Is this normal? Grade distrubtion in master level statistical inference?,"Hi,

I am currently a graduate student in biostatistics, who just finished my final for statistical inference (casella and berger chapter 6 to 10).

I got my ass whupped by the final (66), which left me with course average of 82. But I got the final grade of A.

Which got me thinking.. Is it normal for theory heavy classes to kick a lot of people's butt especially during finals? The last midterm average was somewhere around 70's to low 80's.

Did you class usually ace the finals for statistical inference?

I notice that this year's final is much more harder than the previous years?

I feel little dumb after taking that final.","[""Numeric grades in graduate school don't mean that much. Pay much more attention to letter grades. \n\nA= you are doing fine\n\nB= we will fail your ass on quals if you don't improve between now and then \n\nC= how the fuck did you get in here to start with?"", 'Might just be a graduate course thing. At my institution, generally the only grades given in graduate courses were A or B. The latter meant you were probably not ready to pass the quals.', 'I\'ve seen plenty of stats subjects (not just at masters level) that pitch the subject fairly hard (especially exams) but then have to push the grades up (so a seemingly very ordinary mark on the exam results in a high grade). It\'s not that unusual.\n\nI\'ve also seen plenty that don\'t do that.\n\nDon\'t worry overly much. While it\'s important to realize that there\'s a lot there that you haven\'t understood well enough to just do it in exam conditions (so you may find yourself relearning parts of it, if you ever need them),  you\'re in a relatively good position to be able to do that. You may not have all the knowledge firmly in hand but it sounds like you have the meta-knowledge (you have a decent coverage of what things you ""should"" know about, and the knowledge needed to mostly get the deeper knowledge when you need it). \n\nI see no reason to feel bad about it not all being at your fingertips. It sure as hell isn\'t all at mine. If it comes up that you need some of that stuff again, you\'ll need to do some work is all.', 'Depends a lot on the program, but usually somewhere in the handbook they’ll spell it out for you. And if not there, the departments I’ve been in cover this pretty thoroughly during orientation. The last place I worked, the “you’re going to need to improve for quals” grade was a C. When I was in grad school, we got simple pass/fail and a yearly written evaluation from our advisor (1st year) or thesis committee.', 'There were a few classes in my program that had a reputation for being really hard, and statistical inference I and II were at the top of the list. One was frequently repeated by students to get the b- or higher required by the program. These classes usually involved a heavily weighted final exam score to bring up the average letter grade to something acceptable for the program. I was pretty sure I was going to fail the class based on my mid term and final exam grades, but somehow I walked away with a B.  I graduated 15 years ago, and I was chatting with a current student taking that same course with a different professor and he said the course difficulty hadn’t changed.\n\nOn the plus side, aside from understanding the theoretical concepts taught, the course work hasn’t been relevant to my career.']",12,13,https://www.reddit.com/r/statistics/comments/13601yv/q_is_this_normal_grade_distrubtion_in_master/
239,2023-05-03 04:01:18,"[Q] When it comes to evaluating the Time Series model, do we choose the model with the lowest Akaike Information Criterion?","If yes, can we then choose the model that has negative AIC? For example, model 1 with AIC of -100 vs model 2 with AIC of 0 vs model 3 with AIC of 10. In this case, would I choose model 1?","['Yes, it can be negative. Just pick the lowest one, although BIC tends to be somewhat more popular in time series because it favours parsimonious models.', "">  do we choose\n\nThere's not enough context to be so normative; you *could* do all manner of things. *If* you decide to use AIC as the basis for choosing between models, then choosing the lowest AIC is generally what you'd want to do. just as you'd maximize likelihood for situations where that would make sense. Indeed that sense is quite directly related; in effect the AIC adjusts -2log L for the expected effect (asymptotically, at least) of maximizing L over some parameters.\n\n> If yes, can we then choose the model that has negative AIC?\n\nyes, -100 is smaller than 0 which is in turn smaller than 10""]",1,2,https://www.reddit.com/r/statistics/comments/135yjp3/q_when_it_comes_to_evaluating_the_time_series/
240,2023-05-03 02:05:56,[Q] Quality Control sample calculation,"I believe this is probably very simple, but it’s a been a long time since I’ve done any stats work and I’ve tried googling and haven’t found exactly what I need. So sorry if it’s kind of a dumb question. But here’s what I’m looking for. 

I have a product that should have unique serial numbers between 1 and N (probably greater than 100,000 for reference). I need to do QC on my orders to get some level of confidence that the numbers are within that range and are unique (say 95%). What is the equation for figuring out how many samples I need to take in order to get to that confidence level?","['This seems a very odd problem to solve with statistics. Your serial numbers should be assigned deterministically. You should have 100% confidence in their uniqueness, they should not be randomly assigned; and it should not be possible, in theory, to have unexpected duplicates. If duplicates are suspected, an exhaustive search to identify to problem is probably going to be a better approach.', 'What software are you using for this calculation? For most, you could easily import all \\~100,000 serial numbers.', ""You're looking for the confidence interval on a binomial distribution.  Basically, 95% confidence that the failure rate is below X% requires a sample size of N with 0 observed failures.\n\nhttps://www.weibull.com/hotwire/issue118/relbasics118.htm"", ""So, you're looking to check for the absence of an event, rather than the occurrence of it? Or do you want to measure the proportion of dodgy numbers given that you know some will be dodgy?\n\nIf you're looking to check for the absence of an event, the simplest way is to use the rule of three, which says that if you observe zero events in N tries then an approximate 95% CI for the risk of the event is (0, 3/N), as long as your sample is at least 20-30 (because the maths relies on the Central Limit Theorem).\n\nSo you just need to decide how much of a real underlying prevalence of dodgy numbers is too small to care about. If it's 1 in 100 then a sample of 300 with no events observed would be required to be reasonably confident that the prevalance is no more than 1%.\n\nIf you expect to observe one or more dodgy numbers then you may prefer to do the sample size calculation on that basis. [Here's a sample size calculator](https://select-statistics.co.uk/calculators/sample-size-calculator-population-proportion/)."", 'Overall I agree. It’s actually RFID numbers and I’m using a new supplier. The numbers should be in a range that we determine, but I am trying to figure out a way to sample test them to make sure I have a reasonable expectation that they are correct.']",2,8,https://www.reddit.com/r/statistics/comments/135vjce/q_quality_control_sample_calculation/
241,2023-05-03 00:31:06,[R] [E] [Q] Violating Assumption of Logistic Regression,"Hello, I built a dataset that includes repeated observations of student retention through each term of a student's enrollment.  One observation for each student in each term of enrollment.

My question is: one of the assumptions of logistic regression is that observations need to be independent of one another: ""Logistic regression assumes that the observations in the dataset are independent of each other. That is, the observations should not come from repeated measurements of the same individual or be related to each other in any way.""

I think I'm violating this assumption, but when I run the same dataset through both regression based models and tree-based models, I get very similar results. I would expect that the LR model would preform worse?","['If you violate them, I think you’re mainly inflating variances and covariances inside the model. Therefore your inference on the inputs might draw you to incorrect conclusions. Would you mind sharing why you think you’re violating this assumption?', ""Those repeated observations may not affect the quality of fit or the predictions, but they will make the coefficient standard errors and p-values invalid.  It's like artificially increasing the size of the data set, which has the effect of decreasing standard errors."", ""I haven't checked the residuals yet, which I know is how to check this assumption.  I'll do that this afternoon If I have time.  I was assuming that I would violate it based on the reading of the assumption, which I know isn't definitive.  Does that answer your question?"", 'Not yet. Was there something on the sample selection or is there a particularity in the data that says observations are not independent? Like time dependence or something? Were individuals measured twice?', 'Many students were measured twice at different times.  \n\nFor example, I started the dataset with the 2019 Fall file, and then added the 2020 Spring file (the next term).  So, students who were in the 2019 file would be duplicated (with different values for some of the variables) if they were also present in the 2020 Spring.']",0,7,https://www.reddit.com/r/statistics/comments/135swwd/r_e_q_violating_assumption_of_logistic_regression/
242,2023-05-02 23:14:37,[Q] How to interpret non-significant results?," Hello,

I´m working on my diploma thesis whether airplane crashes have an impact on the stock price of the airplane manufacturers. The thing is, my results are not statistically significant. How do I interpret these results.

I´m sure I can say: ""I didn´t prove that airplane crashes have statistically significant effect,..."", but that is lame, it sounds like I didn´t find out anything. So can I also say that: ""Airplane crashes don´t have statistically significant effect,...""?

Because I don´t think this is true, that no significance means automatically no effect, but my teacher who helps me with my thesis told me this.

Thank you","['Don’t say prove. Say something like, according to the model the estimated effect is this, and is not significant at this alpha level. That’s all you can say.', 'Okay here is more context. Interpreting the magnitude of p values was first done by Fisher. Neyman-Pearson proposed the whole hypothesis testing thing (with all the Type-X error and all) as an alternative to that. Doing hypothesis testing *and* interpreting the magnitude of p-values, although not rare, is a malpractice. You can be Fisherian or not. But mixing the two was never the intention of the people who created these methods.', 'No I’m saying that the value p value of a hypothesis test should not be used to inform whether more or less research is justified.', 'Your intuition is broadly correct. The oversimplified way of explaining this, is that your effect estimate still represents the ""best guess"" for the ""true"" effect (if you study is otherwise valid), just with higher than usual uncertainty. For the result we would report the estimate with the confidence interval/SE and discuss the limits to the study. \n\nHowever, I would also caution against going into this discussion if (1) your teacher specifically told you otherwise and (2) if you need to be asking this question. P-values are the first thing we typically teach because it\'s simple and easy(er) to understand. Moving beyond them before you are ready is mostly just risking making a lot of mistakes.', 'You can’t interpret a p value in a way that adjudicates between whether more or less research is warranted.']",2,32,https://www.reddit.com/r/statistics/comments/135oyfa/q_how_to_interpret_nonsignificant_results/
243,2023-05-02 23:03:13,[Q] How to combine four highly correlated variables into one?,"Hi everyone, I'm fairly new to econometrics and have to build a model that predicts the price of maize, rice, soybeans and wheat.

The thing is that their prices are extremely correlated with each other. The coefficient matrix shows values higher than 90% for every combination.

How can I transform the four commodities into one, in order to have their price as one Y variable to be predicted? My main concern with aggregating them would be unnacounting for substitution effects between some of them, given that, for example, both maize and wheat and probably soybeans are fed to livestock.

Is there a way to combine them and ignore possible substitution effects? I have a paper that transformed them into caloric equivalents and aggregated them, but I don't know the proportion of each one.

What other techniques can I use to combine them? Is it even logical to do that? Can't I just predict the value of just one of the four?

Thank you in advance.","['Principal Component Analysis or Factor Analysis might be exactly what you are looking for. :)', 'I agree, PCA is the way to go', 'What variables are you using to predict the prices? Are the prices your independent variables or your depend variables? Could you describe the problem in a little more detail?', 'Throwing my hat in with the first component from PCA responses.', ""The first principal component would be one typical way to combine such variables.\n\nIf they have similar variances, you typically end up with something very close to simply adding (/averaging) the components (up to a linear rescaling of the result).\n\n> I don't know the proportion of each one.\n\nThat's unfortunate; if they're being fed to livestock, presumably the relative totals are available.""]",0,10,https://www.reddit.com/r/statistics/comments/135o7ml/q_how_to_combine_four_highly_correlated_variables/
244,2023-05-02 23:00:28,[Q] How to calculate r^2 with missing data,"Hey guys.

I have been trying to solve this problem for more than 2 weeks now and I just can't understand it properly. Can you guys give me a hand with it? 

I need to solve the r\^2 and the f-test of an estimated simple linear regression model using OLS. But the only info that I have is the estimated model itself, with the standar errors of every coefficient. (b0 and b1) since it has a single independent variable. And as an ""additional"" information, I know the n (total observations) and the sum of squares of the observations minus their mean. That's all I have, I don't have access to the observations, anything else.

What do you think i could do to solve it?

thanks in advance","['Write down the formulas for each of the pieces of information you have, along with the formulas for R^2 and the F statistic. See if they have anything in common.', "">is it true that you can use the standard error of the regression to estimate SSE as:\nSSE ≈ s^2 * (n-2) ?\n\nThat's exactly the type of questions you should be able to answer by following my advice above."", 'bruh you left me in the same doubt hahaha I thought u meant the normal thing about R\\^2 = ssr/sst and f-test = r\\^2/k/1-r\\^2/n-k-1 lol.\n\nAny idea where I can find sources to understand better this? im struggling 😅 no joke I couldnt find anything about what I mentioned in my previous comment. Every video or website tells the same thing but none break down the formulas', 'You need to go deeper - what are the formulas for SSR and SST? You should be able to find everything you need in your textbook - if not, Wikipedia has detailed formulas for all of these things.', 'whats your point? I already know that relationship between the formulas, the problems is to get the SSE or SST without the additional info... btw since ur username checks out lmao. \n\nis it true that you can use the standard error of the regression to estimate SSE as:  \nSSE ≈ s\\^2 \\* (n-2) ? or that you can use the sum of squares of X as a proxy for SST, since they are equal in a simple linear regression model. That is: SST = ∑(Xi - Xmean)\\^2 ??']",1,5,https://www.reddit.com/r/statistics/comments/135o0qu/q_how_to_calculate_r2_with_missing_data/
245,2023-05-02 20:34:33,[Q] Are there any situations in which a smaller sample size is a good thing?,"Hi all,

I’m a Data Analyst with a Physics background, and just wanted to get thoughts on my question above.

Typically, I would think that a larger sample size means you’ve got more statistical power, a smaller confidence interval, and ultimately more evidence to back up any conclusions that can be drawn from the data.

Is there literally any situation where it’s better to have a smaller sample size, as opposed to a larger one? (Other than less data perhaps being easier to work with)","['Easier to work with, and also cheaper to collect.\n\nOtherwise, there are edge cases like when it is still a sample but tending towards the population, then it is likely that your sample is not randomly selected but introduces specific biases. For example you have data from almost all students in a district because they all received the questionnaire in class, except the ones being absent from school. Well, the absent ones probably have some characteristics in common.', 'Yes! **More data does not offset the problems caused by non-random data collection** -- in fact, more data will exacerbate the problem by making you not only wrong but **confidently wrong**.\n\nCheck out the (incredible) paper ""Statistical paradises and paradoxes in big data"" by Xiao-Li Meng. He shows that the bias of a sample average can be decomposed into a product of three terms:\n\n1. A ""data quantity"" measure, sqrt((1-f)/f) where f is the fraction of the population covered by your sample\n\n2. A ""problem hardness"" measure, simply given by the population-level standard deviation of the quantity being measured\n\n3. A ""data quality"" term, which measures the correlation between your sampling procedure and the quantity being measured. If your experimental design is not *perfect* or your data is observational, this term will almost certainly be nonzero.\n\nWhen your sampling procedure is systematically related to the quantity of interest, the data quality term contributes to a bias in the direction of this systematic relationship. While a large quantity of data will drive the overall bias closer to zero, it will also rapidly decrease the variance of your sample mean, causing any confidence intervals you construct to typically exclude the true population mean.\n\nOf course, you can correct for this bias if you somehow know your data quality term a-priori, but this is not a very reasonable thing to know in practice. \n\nI hope this is helpful!', 'Situations where the treatment you want to test has a direct cost associated with it or it’s ethically questionable\n\nFor example, if you want to test whether giving coupons is beneficial to your business, you’ll want the lowest number of individuals receiving coupons that are necessary to validate the effect. This is because giving cupons has a cost associate with it, and giving coupons to everyone (despite giving a better estimate of the effect) would hurt revenues a lot\n\nAnother example would be evaluating the impact of higher body fat on heart rate. You wouldn’t want thousands of individuals having an unhealthy lifestyle just to get a higher precision that doesn’t really affect your overall conclusion', 'paper link:\n\nhttps://statistics.fas.harvard.edu/files/statistics-2/files/statistical_paradises_and_paradoxes.pdf', 'Well, with Huge sample sizes and frequentist stats p is almost guaranteed to be very small. So things get complicated. Small is relative I guess?\n\nEffect size measures become very important']",48,37,https://www.reddit.com/r/statistics/comments/135jx2u/q_are_there_any_situations_in_which_a_smaller/
246,2023-05-02 12:58:50,[S] I made an app to brute force demonstrate the answer to my favorite stats puzzle questions,"I thought it would be interesting to let people see the answer resolve for each of these 2 questions, as both answers are counterintuitive to most. The code is also included, so doubters can actually verify a fair simulation is being performed.  Very simple app, but maybe some here will enjoy!

https://codesandbox.io/s/echarts-playground-forked-1qzwkz?file=/src/App.js","[""I'm not clicking a random link if you're not explaining what's going on."", 'codesandbox is a service to have a web-based IDE for snippets of code or small projects without needing to host/deploy anything. It\'s just running javascript, so under normal circumstances it can\'t hurt you.  I could provide a screenshot of the IDE and resulting output, but that doesn\'t prove anything either.\n\nIf you just want a description - it\'s a React app with Apache E-Charts to create two donut charts. It randomly generates between 1-~100 samples every 250ms of two children (with sex + day of the week they were born), and graphs the number of times one of two situations occurs, ""A family has 2 children; they have a son. what is the probability they have a daughter?"" and ""A family has 2 children; they have a son who was born on Tuesday. what is the probability they have a daughter?"".  Generated samples are discarded if they match neither of the two conditions; if they do match, the sex of the \'other\' is recorded and graphed, which updates in real time as samples are generated.', ""No, the answer is not the same for both. \n\nYou can obviously Google to get the answer, but it's worth trying to figure out the answer using pen and paper too."", 'If solved analytically, the answer to both questions is 33.33%, right?', 'Nope! The first is 2/3, while the 2nd is 14/27, which matches the results of the demonstration']",9,7,https://www.reddit.com/r/statistics/comments/135bpcb/s_i_made_an_app_to_brute_force_demonstrate_the/
247,2023-05-02 08:44:29,[Question] Help to interpret Two-way ANOVA results.,"Dear all,

I am a PhD student working in the field of wildlife management. I am new to ANOVA; I did my analysis on SPSS. I wanted to know if any relationship exists between communities (indigenous and non-indigenous) across different administrative divisions (sectors).

Variables: 1) Ratings on strategies (dependent variable), 2) communities (indigenous and non-indigenous), 3) administrative divisions (sectors).

Tests of Between-Subjects Effects:

1. Shows that the community (F(1,498)=5.123,p=0.024).
2. Shows the sector (F(3,498)=21.051,p=0.001).
3. Interaction (Sector \* Community): (F(3,498)=5.020,p=0.002).

Link: [https://ibb.co/cxMZfKd](https://ibb.co/cxMZfKd)

Graph:

Link: [https://ibb.co/2kZb6Sd](https://ibb.co/2kZb6Sd)

Post hoc results:

Link: [https://ibb.co/b5sZn24](https://ibb.co/b5sZn24)

Which results are essential? I am confused with the Interaction term. What does the meaning of significance for all the results under 'Tests of Between-Subjects Effects'? Could someone help me explain my results? Thanks in advance.","[""The ANOVA shows a sig difference, but you need to run post hoc tests to get the more in-depth info. The ANOVA just says there's a difference, but not what they are, thus the need for post hoc tests. There are guides on how to perform them on SPSS. They're relatively easy to do, especially if you know how to do ANOVAs. Those results will give you the info you want"", 'I don’t think it is that helpful to do all pairwise comparisons. It would make sense to follow-up effects with more than 1 df such as Sector with the Tukey hsd comparing  the three sectors. For the interaction, I would start by with a graph of means that has sector on the X-axis and separate line for each community with the  DV on the Y axis. Then describe the interaction in a way such as: “the effect of community was larger for Sectors 1 and 2 than it was for Sector 3.” You might want to test simple effects such as testing the effect of sector separately for each community.', ""Hi, thanks for your suggestion. Here are the post hoc tests: [https://ibb.co/b5sZn24](https://ibb.co/b5sZn24)\n\nI am not sure how to read it, well it's because I'm the first one at my lab to run ANOVA and my Prof is reluctant to guide me as he doesn't know how to run it."", 'Hi, can I DM you? Thanks', 'Sure. Read [this article](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0271668) first and then ask me questions about it if you have any.']",1,5,https://www.reddit.com/r/statistics/comments/1356k6q/question_help_to_interpret_twoway_anova_results/
248,2023-05-02 06:06:01,[Q] Hypothesis testing and statistical testing. Is this procedure correct?,"Lets say I am given a dataset and told to figure out why customers are leaving our company. I take a quick look at the data and see that each row is a customer and each column is a feature. Within the features, among other things, I have a column named ""churn"" which indicates whether or not a customer has left the company.

From here I take a look at other features and notice a ""monthly charges"" feature. I think to myself, ""Maybe certain customers are leaving because they are being charged more than the customers who are staying"". I want to form a hypothesis test off of this statement but before I begin have I already committed an error? The first question I have is should I form my hypotheses before I have even looked at my data?

Lets say I didn't commit an error and move on to form a null and alternate hypothesis. My null hypothesis would be:

H\_o: Monthly charges of customers who have churned <= Monthly charges of customers who have not churned.

H\_a: Monthly charges of customers who have churned > Monthly charges of customers who have not churned.

I set my significance level of 0.05

From here I would like to visually confirm my test by checking the monthly charges distribution of each group and comparing them to each other. In this example the distributions don't overlap much at all and would leave me to believe that I can reject my null hypothesis. My next question is, should I stop here? I would like to follow this up with a T-test to confirm my findings with a significant p-value. Is this appropriate?

Let's say it is appropriate and I conduct a T-test and find that my p-value is less than my alpha. I then reject my null hypothesis and say ""Monthly charges of customers who have churned > Monthly charges of customers who have not churned.""

Does all this seem like the correct way to conduct a hypothesis test, visualization of the data, and a statistical test?

Also, Let's say I am later tasked with creating a logistic regression model to predict whether or not a customer is likely to leave based on their current features. I have already done the analysis and found there is a relationship between customer churn rate and monthly charges. Would my assumptions that this would make a good variable to include inside my model be a bad one to make because I don't know how other variables would effect it(confounding variables)? What about variables that I test and find no significance. Should I leave those out of my model? Or should I just include everything and see how they interact and do some PCA/Regularization to slim down my feature space?","['Here a a few questions to think about:\n\n1. Do you have the entire population of your customer data, or just a sample? If you have the whole population you would not test for statistical significance - if the two means differ then they differ. At that point, you need to decide whether the difference has practical significance in that even if the numbers are different are they meaningfully different in a way that helps you. This will have to be determined through business knowledge. \n2. If you have a sample, doing a t-test would be a way to statistically test for that particular relationship. Of course, even if there is a statistically significant difference it does not immediately imply causation, particularly with other features available. This is a good exploratory step\n3. When fitting a logistic regression, you may find variables that have statistically significant differences between levels of your binary variable are not useful in the model when accounting for other predictors. \n4. Discriminant analysis is another avenue for you to explore as well.', 'Is this panel data?', 'I’m sorry if I did not explicitly state that I would form my hypothesis and then later split my data into three sets, a train, validate, and test set. I would only be doing the statistical testing on my train set.', 'Thank you for the reply. It’s funny, a few weeks ago I asked a question to response number 1’s line of thought when it comes to sample vs population when we have what I would think to be a population. https://www.reddit.com/r/statistics/comments/12cr9z6/q_sample_vs_population_do_i_have_this_right/?utm_source=share&utm_medium=ios_app&utm_name=ioscss&utm_content=1&utm_term=1 . More or less the counter argument to that was I would still want to treat my company data as a sample even if I have all the data of every customer. This is due being able to make inferences on data that would be collected in the future and or policies that would effect my company through time. Still not sure how I feel about that response though.', ""This was more of a hypothetical situation to check if my logic was correct. In this instance let's say no, its current customers as is in the company at present time and the data is updated whenever there is a change in a customer's information.""]",2,8,https://www.reddit.com/r/statistics/comments/1352t8v/q_hypothesis_testing_and_statistical_testing_is/
249,2023-05-02 05:52:31,[Q] Figure ideas to compare genes present/absent for 57 archaea species,"Hello everyone!

I am comparing 57 archaea species (which can be divided into 4 orders/groups) in terms of their potential metabolisms based on their genes and pathways present. I have annotated my species all with a RAST + DRAM combination on Kbase.

I have collected quite a bit of data using combinations of eggnog-mapper, KAAS, and interproscan.

With this data in hand I want to start making figures to show my data. Therefore, I have decided on showing my data via heat-maps, venn diagrams, bar graphs, and PCA plots. Moreover, as my data is not normally distributed I am using Kruskal Wallis for my statistical tests.

However, does anyone else have ideas for graphs or figures to show my data, in particular figures showing the difference between species and groups in terms of having genes/pathways present or absent?

If so, I would be very much appreciated of the help.",[],2,0,https://www.reddit.com/r/statistics/comments/1352h2r/q_figure_ideas_to_compare_genes_presentabsent_for/
250,2023-05-02 04:40:15,[Q] How can you determine if two survey questions are dependent?,"I want to test the dependence between 2 likert scale questions. Both are answered by the same population. The first question asks how often the respondent watches YouTube related to a certain field and the second asks if their career choice is affected was a result of watching YouTube videos. The options for both are:  


1. Strongly Disagree
2. Disagree
3. Agree
4. Strongly agree

I was initially going to use chi square test, but from what I've found it is usually used when there are different sample groups. Thanks in advance!","['Many studies compare likert scales as continuous variables. But if you want to treat these like rank ordered variables, then you could use the Spearman rank correlation test.', 'It makes no sense at all to respond to a question of the form ""***How often*** do you..."" with answers like ""Strongly Disagree"" or ""Agree"".', ""You have the results from two Likert-type items.  You probably want to treat them as ordinal variables, and you are looking to find an association, or correlation, between the two.  Kendall *tau*\\-c is an appropriate measure of correlation. Because there are equal number of levels between the two, Kendall *tau*\\-b is also appropriate. Also, Spearman correlation will also work fine (and may be more available in some software packages).\n\nIt's possible to treat the data as nominal.  In this case you could use a chi-square test of association, with the corresponding measure of association Cramer's *V*."", 'Its probably ""im watching youtube vides often"".  OP wasnt literally saying the question, s/he just said what the question is about', ""I'll look into it. Thank you so much!""]",4,11,https://www.reddit.com/r/statistics/comments/1350m98/q_how_can_you_determine_if_two_survey_questions/
251,2023-05-02 03:51:26,[C] Stochastic calculus or AI,"Edit: This should also be flaired as education \[E\] as well.

I am a PhD student in my department in the stage of identifying my advisors. There are two areas in my department that I am particularly interested in, (1) Stochastic calculus and (2) AI specifically, RL type problems.

I know there are ways to bridge the two, but due to departmental politics, it is unlikely I can even get faculty from (1) and (2) to chair my dissertation. RL is too applied for the interests of faculty in (1) and stochastic calculus is too theoretical compared to the interests of the faculty in (2).

At this point, it seems that I should pick one or the other, and I want to pick a topic that will prime me for a career in some sort of quantitative research in tech or finance industries. My current training is more theoretical, and thus I have an easier time reading books on stochastic calculus, compared to reading say barto and sutton's RL book. Actually RL is still one huge black box to me, but it seems more and more job postings in quant research seek general ML skills. That said, I am willing to learn it, as their research questions they are asking these days is quite interesting to me.

TLDR: ""which technical skill is more useful for a career in quantitative research in industry, stochastic calculus or RL?""","[""I'm sorry you're getting hit by academic politics so early into your career. \n\n*Everyone's* doing ML right now. It's probably way less competitive to be the top of your research domain if you do the more theoretical route. Also, the more deeply theoretical stuff is harder to self teach. Once you get the main ideas and understand the notation, you'll be able to keep up with the ML/RL/AI research on your own. I bet there's a larger fraction of stochastics people who are comfortable reading RL research than RL people who are comfortable readings stochastics research.\n\nIn any event: focus on whichever topic interests you the most. This is *your* PhD, and the goal is for you to exit the program as the world expert in <incredibly focused niche>. Pick a topic that feels like you can commit your life to it. That you would look forward to spending all of your time thinking about and trying to push the envelope. Forget about the industry, you'll be fine."", 'I started out my PhD working SPDE theory, specifically large deviations, and I can tell you that I **never** use any of that knowledge in my career.  Might be useful in finance, but personally I think RL has much better promise right now.', 'If you have an easier time with stochastic calc books, do the RL topic and keep on top of stochastic in your personal time.', 'Job wise, you can do better in AI.', 'You should definitely ask r/quant but my best guess would be to do RL because it seems like you could self study stochastic calculus with greater ease.']",32,18,https://www.reddit.com/r/statistics/comments/134zcot/c_stochastic_calculus_or_ai/
252,2023-05-01 23:54:36,[Q] Sudden significance in OLS without mediation or correlation?,"Hi all,

I am conducting some data analysis and am having trouble interpreting significance in my multiple linear regression results. Specifically, when I go from \[Regression 1: DV \~ Treatment 1 + Treatment 2\] to \[Regression 2: DV \~ Treatment 1 + Treatment 2 + Covariate\], I am finding that\*,\* in R1, Treatment 2 is not a significant predictor, but that this is the case in Regression 2 *despite* Treatment 2 and the Covariate seemingly neither being a) correlated (r= 0.008) nor b) in a mediating relationship.

Why might Treatment 2 suddenly be significant without mediation or correlation? For reference, the Treatment variables are both dummies, and the Covariate a variable that takes on integers 1 through 7 (Likert scale).","[""Here's a screenshot of my Stargazer output, for reference: [https://imgur.com/a/JKUiQzs](https://imgur.com/a/JKUiQzs)\n\nI am referring to regressions (1) and (2)"", 'Statistical significance is a generally meaningless term. Adding variables decrease power. Think about a Bayesian model: “I added a variable and now the posterior distribution for one of my coefficients crosses zero a little bit more than it used to”. It makes no sense!  We can’t make any inferences from this.', 'Moreover: adding for covariate could explain more of the variation in your DV and change the which observations contribute to the parameter estimates.']",1,3,https://www.reddit.com/r/statistics/comments/134qalk/q_sudden_significance_in_ols_without_mediation_or/
253,2023-05-01 23:17:20,[Q] Power analysis post hoc?,"Hi all,

I just received a manuscript back from a journal, and I am trying to wrap my head around one of the reviewer's comments. Some background: I am not very knowledgable in stats.

Basically, we concluded that a difference we found was likely based on a large difference in sample size (one group had a n of 50, the other an n of 11). The differences were no longer significant at follow-up.

The reviewer mentioned that to justify this conclusion, we need to conduct a power analysis. I've watched a couple youtube videos on this, but I'm still confused on the basics of what it is/how to do it.

Any help/guidance is appreciated!","[""Post-hoc analyses are of no use and a reviewer should know better (unfortunately many don't). Here is a [good practical demonstration](https://data.library.virginia.edu/post-hoc-power-calculations-are-not-useful/) of why they're useless, with references to some of the primary literature. [Here](https://statmodeling.stat.columbia.edu/2019/01/13/post-hoc-power-calculation-like-shit-sandwich/) is a more acerbic account of why they're rubbish."", ""Power analysis is important, conducted before the study. Power analysis based on the sample you collected (*post hoc power*) is not helpful or informative; it's (almost purely) a monotonic transformation of the p-value.\n\n/u/dmlane gave a good list of references here:\n\nhttps://www.reddit.com/r/statistics/comments/12ucxre/q_help_replicating_power_analysis_from_a/jh6keif/"", 'Post hoc power analyses merely restate the p value in different mathematical form. If you search, there are many articles out there showing they should not be run and how to push back against reviewers who ask for them.', 'I don’t think it is justified to assume you have an effect and explain away the lack of significance based on a power analysis. Better would be to state upfront that you don’t have a conclusive outcome and give a confidence interval. The confidence interval would likely reveal that your data are consistent with large effect sizes but are also consistent with no effect. Better to be explicit about the uncertainty than to blame your experimental design. You could comment that a second study with a larger sample size would likely be informative.', 'GPower is a good option, as it allows you to put in your values and spits out your power (or sample size).\n\nI recently had to do this for a manuscript and it annoyed the hell out of me given that posthoc power calculations are inherently problematic (see Heinrich et al, 2010 if you want an explanation).']",19,12,https://www.reddit.com/r/statistics/comments/134pb7o/q_power_analysis_post_hoc/
254,2023-05-01 22:56:17,Help with a probability problem [Q],"My friend and his partner are considering joining the police force. Their apprenticeship rota consists of 6 consecutive days on followed by 4 consecutive days off and we're assuming the choice of rota is random and so all are of equal probability.

Can someone please work out the probability for each number of 'off' days that line up between them? The maximum being 4 and the minimum being 0 of course.

I could brute force all permutations but I'm convinced there's a statistical approach that's way easier and I'm just not clever enough to work it out!","['I\'m assuming a rota can start on any day. \n\nCall the first day on for person 1 ""day 1"" and look at the possibilities for Person 2 relative \nto Person 1:\n\n       Person 1:\n         + + + + + + - - - -  \n       Person 2:            \'off\' overlap\n         + + + + + + - - - -     4\n         - + + + + + + - - -     3\n         - - + + + + + + - -     2 \n         - - - + + + + + + -     1\n         - - - - + + + + + +     0  \n         + - - - - + + + + +     0 \n         + + - - - - + + + +     0 \n         + + + - - - - + + +     1  \n         + + + + - - - - + +     2 \n         + + + + + - - - - +     3 \n\nThe average overlap across all person 2\'s possible start days  is 16/10 = 1.6 days', 'This is exactly what I needed thank you dude!']",1,2,https://www.reddit.com/r/statistics/comments/134oqoy/help_with_a_probability_problem_q/
255,2023-05-01 21:14:08,How to find the relative dispersion of a list of numbers over a number in it? [Q],"In the game of chess a white advantage is defined as a positive number, a black advantage is a negative number, a draw is 0. 
Let me give an example of what I'm trying to calculate; let's say an engine from a certain position on the board calculates 3 best variations (from white perspective): 

| Variation | Evaluation |
| -------- | -------------- |
| Best One    | 0            |
| Second Best   | -50            |
| Third Best   | -120            |

How would I be able to find the relative dispersion of the various variations over the best one? 

I'm leaning towards using a coefficient of variation; my pain points are:

 - would zero here be considered ""meaningful""? From Wikipedia,

> The coefficient of variation should be computed only for data measured on scales that have a meaningful zero (ratio scale) and hence allow relative comparison of two measurements (i.e., division of one measurement by the other).

 - the dispersion would be more significant as much as values gets closer to 0 (e.g. a list like [0,1,2] would be much more disperded than a list [998,999,1000]). How would I be able to account for it?

Sorry if these questions sound naive but I'm a noobie in the field, and thanks for your time.","[""I don't have a perfect answer for you.. CoV will account for the second point as the mean will be higher for the 999 case vs for the 1 case. You could consider these on interval scale for sure. There are third or more degree moments if you want to look beyond standard deviations.""]",1,1,https://www.reddit.com/r/statistics/comments/134m6rb/how_to_find_the_relative_dispersion_of_a_list_of/
256,2023-05-01 20:35:22,"[Q] How to calculate Var(X+Y) given Var(X), Var(Y) and correlation coefficient between the 2 variables?"," 

I am adding the entire question below for context. **This isn't homework** and I already have the solution. I don't understand it though, so I would be grateful if anyone could explain.

While computing ""Value at Risk"" in finance, we are first required to calculate the standard deviation for the entire amount invested. In the given problem, our investment consists of 2 smaller investments, but the catch is that they are correlated. I'll add the question below:

This was the question given in 1 of my textbooks (Financial Management):

>Consider a portfolio consisting of a 20,000,000 investment in share XYZ and a 20,000,000 investment in share ABC. The daily standard deviation of both shares is 1% and that the coefficient of correlation between them is 0.3. You are required to determine the 10-day 99% value at risk for the portfolio?

The first step in the problem in the solution is to calculate the standard deviation of the entire portfolio of 40,000,000. If the 2 investments, which I take to be random variables here were independent, we would get Var(X+Y) = Var(X) + Var(Y). But here it's given that they're correlated. Plus the variance given in the solution is 0.65 (i.e. a standard deviation of 0.85%). I don't understand how this has been calculated.

Any help would be much appreciated. Thanks!","['var(x+y) = var(x) + var(y) + 2cov(x,y)\nyou can use the correlation coefficient to solve for the covariance', 'The explanation for this problem is written out here:\nhttps://www.ssei.co.in/wp-content/uploads/2019/05/SFM-DAWN-PAGE-219-TO-231.pdf\n\n1 lakh is an Indian term for 100,000\n\nIf you want to know where 0.65 came from, you may need to include a little more context surrounding that number.', ""[https://imgur.com/a/6FSyD3p](https://imgur.com/a/6FSyD3p)  \nI could not directly add the image in the post, so I posted it on Imgur and here's the link.   \nAs you can see, they calculate the standard deviation for 1 day first and the subsequent calculations come later.   \nOnce the standard deviation for 1 day is calculated, I know how to do the rest of the problem, it's just this part that I don't understand.  \nAgain, thanks for your help"", 'Sure.  Var(aX) = a^2 * Var(X).\n\nSo with the line starting ""Alternatively..."" they are calculating the variance in terms of percent.  The tricky part, though, is that 1% of the whole portfolio is composed of 0.5% of the first asset, and 0.5% of the second asset.  So, the equation is more like:\n\nVar(0.5X + 0.5Y) = Var(0.5X) + Var(0.5Y) + Cov(0.5X, 0.5Y)\n\nallowing the numbers inside the Var and Cov to be expressed as percentages.\n\nThus:\n\n= (1)^2 (0.5)^2 + (1)^2 (0.5)^2 + 2(1)(1)(0.3)(0.5)(0.5)', 'What you are finding here is actually VAR(0.5X, 0.5Y), 20M in each investment so 50% weights in both assets. \n\nAlso note you are not given the covariance but the correlation, so to get the covariance you should use, corr(X,Y)\\*SD\\_x\\*SD\\_y\n\nThe provided solution of 0.65 is correct.\n\n(0.5\\^2 \\* 1\\^2) + (0.5\\^2 + 1\\^2) + 2\\*0.5\\*0.5\\*0.3\\*1\\*1 = 0.65']",1,18,https://www.reddit.com/r/statistics/comments/134lag2/q_how_to_calculate_varxy_given_varx_vary_and/
257,2023-05-01 14:49:01,How should I set myself up for a good masters program? [E],"It’s basically exactly what the the title says. Currently I’m at the end of my freshman year, and I’m a finance and statistics major. I’ll probably end up dropping the finance and adding on a math major this summer, hopefully graduating in 3 years from my undergrad institution. 

Profile:
School- Indiana University Bloomington

Demographic- Latino, Domestic 

GPA- 3.46

Finished Calc 1 and 2, probably taking Calc 3, 4, Linear Algebra next year and will be on course to take 2 Real Analysis courses as well as Diff. Eq. Took 2 applied stat courses my freshman year, and some Python-based coding 

Is there anything else I should ideally aim for? Really just want to know what a good graduate program expects from their candidates.","[""Good math skills.  You should take Real Analysis, and the 400-level Linear Algebra class.\n\nIf you could take the core master's prob and stat sequence while in undergrad, you would be a shoe-in."", 'Agreed. Also, make sure you get to know some of your math/stats professors well. This will give you solid letters of recommendation for grad school and the potential for doing research during undergrad, both of which will be very helpful.']",0,2,https://www.reddit.com/r/statistics/comments/134ezhu/how_should_i_set_myself_up_for_a_good_masters/
258,2023-05-01 13:10:16,[Q] how to account for covariate in point biserial correlation in SAS?,"So I got a lot of good advice in my last post which I really appreciate; I did an ANCOVA and found that age is indeed a covariate. I wanted to do a t-test originally, but I don't know how to account for a covariate. I also wanted to try out a point biserial correlation but I also don't know how to account for a covariate in SAS. Ive spent a good hour searching the Internet for an answer and I haven't found anything. I'd appreciate the help!",[],0,0,https://www.reddit.com/r/statistics/comments/134daed/q_how_to_account_for_covariate_in_point_biserial/
259,2023-05-01 08:38:30,[R] Two-way non-parametric ANOVA tests?,"Hey there, I'm thinking that with my data set I need to find a solution for a two-way non-parametric ANOVA but wanted to confirm that and see what my best options are.

I have a study that measures the level of 48 different analytes in samples. The samples being tested received 2 different initial treatments (A or B), and then a later treatment (X or Y) before analysis. So with the testing I have groups of:

* A + X
* A + Y
* B + X
* B + Y

With the data I have I tried running a normality test in prism on the values for the analytes and only 5-10 came back with normal distributions. I'm under the assumption I need to run a 2-way non-parametric ANOVA but wanted to (1) make sure that is right, and (2) if so what test can I run.

Thanks!","[""2-way anova doesn't have an assumption about the distribution of values in each group, or the distribution of the dependent variable.  (I can't quite tell what you're looking at.)  \n\nYou can look at the residuals from the model.\n\nAlso, normality testing in this context is not useful at all.  It doesn't tell you what you want to know.  It doesn't give you any indication if it's reasonable to use the model.  Perhaps see, [https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90](https://towardsdatascience.com/stop-testing-for-normality-dba96bb73f90) .""]",1,1,https://www.reddit.com/r/statistics/comments/1347xil/r_twoway_nonparametric_anova_tests/
260,2023-05-01 07:42:57,[Q] Parameter vs Statistic,"  

Assuming I have a data sample of sugar concentration found in 25 regular coke and 25 coke zero where µcoke = 10.9 g, s = 0.5 and µzero = 2.5 g with s = 1. Since these are the population data, they are considered parameters. 

However, if I am only interested in regular Coke and hypothesize that 25 regular Coke drinks have a mean sugar concentration lower than 11 g, then my statistical hypotheses are H0: µ = 11 ; H1: µ < 11. 

Since this is a sample taken from a population of both regular Coke and Coke Zero drinks, the data for regular Coke drinks are considered statistics (x-bar =11) as they refer to the characteristics of the sample of 25 regular Coke drinks that I have collected.

Am I right on this?","[""I am confused with your setting. If you have population parameters, there's no sampling/estimation needed. No need to make hypothesis because you know the distributions already.\n\nIf you do not have the population parameters and instead you have a a sample and want to do hypothesis tests about the mean, yes, you could use the sample mean to create estimates, try to test stuff and determine confidence intervals.""]",0,1,https://www.reddit.com/r/statistics/comments/1346oxu/q_parameter_vs_statistic/
261,2023-05-01 03:33:43,"[Q] Should I use OLS robust standard errors, FGLS or PSCE for regression?","I am at the end of the road as I have posted this almost everywhere and I got no responses. 

My sample size is 8 banks over 10 years. There is heteroskedasticity but no serial autocorrelation. I use stata 13.

Correction: PCSE*","['What do you think the pros and cons are of those methods, given your panel data?\n\n(Instead of just posting the same thing for the 8th time in two weeks. Maybe try a different approach?)']",0,1,https://www.reddit.com/r/statistics/comments/1340t1a/q_should_i_use_ols_robust_standard_errors_fgls_or/
262,2023-05-01 03:15:27,"[Q] How do I calculate ""effect size f^2"" for post hoc power analysis in G*power for a hierarchical regression model?","While I understand that doing a post hoc power analysis is a bit rubbish thanks to the wonderful response I received on one of my previous posts here, I still have to provide the results of one to a reviewer anyways (with some added explanation as to why it is not good practice). With that said, I am doing a post hoc power analysis in G\*power using the F tests test family with the Linear Multiple Regression: Fixed Model, R\^2 increase option. To do so I need to input a value for ""Effect size f\^2"". Does anyone know how to calculate this? I tried reading the manual, but I'm totally out of my element. 

Thanks so much for any help on this!","[""I don't humor dumb requests from reviewers and neither should you"", 'Sorry the reviewer is giving you such a tough time. They probably learned in grad school you should do post-hoc power following a non-significant effect. You absolutely don’t have to give in to the reviewer. I don’t know but I have been around a long time (retired two years ago) and have never found an editor who would require a meaningless analysis if a good explanation and/or proper references are given. You have to please the editor and not the reviewer. More important, please yourself. It’s your article and it reflects very badly on you to report statistics that never should be done. You could say that to the editor. The critique of post-hoc power should be in your letter to the editor and not the article itself.', 'Calculate your observed f^2. The reviewer asked a stupid question and should get a stupid answer.', ""It's Cohen's f-squared.  You can find this online or in Jacob Cohen's book, c. 1988."", 'Okay, thank you. That makes sense. Most of my analyses were significant though, and I did not run a priori power analyses. Does that matter?']",2,5,https://www.reddit.com/r/statistics/comments/1340e40/q_how_do_i_calculate_effect_size_f2_for_post_hoc/
263,2023-05-01 02:01:43,[Q] Course about prescriptive statistics,"I recently started working as a data scientist in one company and they require me to do prescriptive statistics there. What they want me to do is find/prescribe rational values for some products that will reduce the cost of an action X. I use Python and I've never worked as a prescriptive engineer before so I need to find a course that will make me learn the things. I know some algorithms used for optimization, mostly for constraint based optimizations, like simplex method, simulated annealing, particle swarm optimization, karmarkar's algortihm etc. but I'd like to learn more. 

&#x200B;

If anyone knows a good course or a tutorial about prescriptive statistics with good examples, please do let me know.",[],4,0,https://www.reddit.com/r/statistics/comments/133ylie/q_course_about_prescriptive_statistics/
264,2023-04-30 23:49:10,What do top 15 departments have that “lower ranked” departments don’t? [Q],"I noticed when looking at phd placements at some top stats departments (by top I mean anything top 20-15) they place into really prestigious companies (research labs at FAANG, Quant Researchers at premier hedge funds). I compared this to  some other “Lower ranking “ departments (anything 30th or lower), and noticed that the placements are still solid, but there are not a high amount of placements into the same type of companies people from the top departments go to. 

My question is, with regards to industry prospects, what do the top departments have that can place students into these prestigious roles in industry, that a lower ranking department can’t? I know people here have said phd stats dept rankings don’t matter for industry, however, when looking at the difference in placements between these two baskets of schools schools, it seems that the top 20 programs place students *actively* in such companies, whereas in lower ranking departments it’s quite rare. 

Can anyone explain why? My goal if I was to pursue a PhD in stats is to get into such roles in the industry, (which I need to reevaluate if this is even a good motivation for doing a phd anyway), so I was thinking about reapplying to these programs after my MS in stats. Of course, they are quite competitive, but I feel as though I’m “missing out” on something that these top 20 departments have if I don’t go to one for my PhD.","[""Reputation often breeds reputation. It also breeds increased access to world-class faculty, funding, and students (and, consequently, training). It's a cycle."", 'Who cares if it\'s fair? If it\'s what you want, it\'s what you want. \n\nYou may want to consider what\'s good for your future and compare what options you have with no Ph.D. at all to what options you have with a Ph.D. from a ""lower ranked"" school, but you don\'t need a Ph.D. to have a happy life, so don\'t get the degree just to have the degree (unless that\'s something that is worth 5 years of your life to you).', 'Selection bias. Top schools and top companies have the same type of barriers to entry. People who are well connected can get into both.\n\nStatistics is the same regardless where you learn it.', 'You do understand that these rankings are 100% bullshit done by no reliable methodology and lacking any statistical measures of reliability or validity?', 'They have more famous professors. Also they can be more selective of their students.']",35,32,https://www.reddit.com/r/statistics/comments/133vdib/what_do_top_15_departments_have_that_lower_ranked/
265,2023-04-30 22:43:07,[M] What is the etiquette on this subreddit for asking for help without a statistics background?,"## Question to Answer

Is there a general etiquette, required set of information, and required background in statistics for the community to see a post as value-added?  I'm posting this not just for my own information; if I'm having these same questions, maybe others are as well?  And the subreddit description doesn't detail much regarding information required in a [Q] post.  

## Background
I'm new to statistics work (started learning in November) - engineer by trade.  And I believe many others in this community are in a similar boat.

I  was hesitant to post this since my previous post requesting help got downvoted immediately.  See link here: [Reddit Post Link](https://www.reddit.com/r/statistics/comments/1335jqh/gage_rr_help_next_steps_question/).  For my first post in this community, the response was discouraging.  And I'm 100% open to criticism, constructive or otherwise.

I see post after post of OPs asking for advice on which statistical test is best suited for a specific dataset, hypothesis, and questions to answer.  And it seems the majority of posts get supportive responses from the community.  I've very much enjoyed reading many of these posts and have been sent down many a rabbit hole trying to learn more.

However, my post was immediately downvoted.  Maybe I didn't use the right terminology or didn't provide all of the appropriate information, or maybe I provided too much information?  A reply in the post indicated that I should be paying for the help.  If so, no problem; however, the problem I'm trying to solve is more for my own learning.  In my uneducated opinion, it was a pretty simple request: what statistical test would be best suited for the problem statement, dataset, and hypotheses? 

Thanks for your input and feedback!","[""If you can't summarize your request concisely in 60 seconds you're unlikely to get good replies. For your previous question in particular, it's not immediately obvious what's going on (maybe it is if you work in this specific field, but then you've already narrowed down the pool of people that are willing to help you). So people have to spend a few minutes looking at your graphs and trying to decipher what it all means. It may be obvious to you but not to a random person looking in from the outside. So if you can summarize that all better you're likely to get better responses.\n\nBasically, rule of thumb for asking for help (at least in my experience) is make it as easy as possible for the other side to help you."", 'People are grumpy in every sun. I wouldn’t read into downvotes too much. It do agree with your assessment that this isn’t a super beginner friendly sub.', ""For basic questions you may get fewer downvotes in /r/AskStatistics than here. But it's mostly the same folks answering in both forums, so as long as you don't appear to be posting homework questions you'll probably get answers."", 'I agree with this. Many of the beginner questions posted are basically asking ""How do I analyze this?"". For someone to answer the question, they have to explain 2+ stats classes. That\'s assuming the poster had provided enough information about the data and problem... Which they usually haven\'t.\n\nOn the flip side, if you do post enough info, nobody is going to read it all.\n\nFor something like this, you just need to hire a consultant or spend the time to learn yourself.', ""That's a bummer that you got discouraged. IMO, giving people access to expertise for basic questions is one of the best things about reddit.""]",11,7,https://www.reddit.com/r/statistics/comments/133tqbc/m_what_is_the_etiquette_on_this_subreddit_for/
266,2023-04-30 17:36:48,[D] What is the chance of two people with the same phone number but with different area codes meeting?,,"[""This, and questions like this, is unanswerable without a model. Once you have a good model for how people with specific phone numbers and area codes interact, *then* you can start asking questions like the one you're asking."", 'The probability that both have identical numbers not counting the area code minus the probability that both have identical numbers, including the area code. Assuming persons and digits within number are independent draws, each of these two terms is a product of two identical factors, which is then summed over all possible assignments in each case. Since the summands are constant, the sum results in a third factor, the number of possible assignments in each case,  which cancels one of the other two factors in each case. That would be 1/\\(10^7 \\)  - 1/\\(10^10 \\), but we assume that the first digit of the area code and of the exchange must be nonzero. Hence\n\n     1/(9*10^6) - 1/(81*10^8)', 'First of all you need to turn the question into something that has a well defined answer. For example, compare the following:\n\n‘What’s the chance someone gets struck by lightening?’\n\n‘What’s the chance that I get struck by lightening if I stand outside my house for a year?’\n\nObviously the place and time period are necessary to give an answer. But the subject of the question and how its framed is just as significant. It’s almost always easier to consider the probability of an event happening to a specific individual as opposed to the probability of an event happening in general within a population. You can also use the individual probability to estimate the general one.\n\nNow looking at the original question, we can rephrase it as:\n\n‘What is the chance I meet someone with the same phone number as me in my lifetime?’\n\nNow in theory, this probability should exist. But it certainly isn’t one that is possible to calculate precisely especially using only basic theory. We have to make simplifying assumptions that turn the question into one that we can answer. Rather than make those assumptions right away, we’ll instead break the problem down first and make any assumptions we need as we go.\n\nOne way is to initially consider the probability that a new person I meet has the same phone number as me, then by estimating the total number of new people I’ll meet in my lifetime we can answer the problem with the binomial distribution (if you’re unfamiliar look this up because this is the core of many problems like this). \n\nSo to find the probability of the next person I meet having the same phone number as me, I’m going to assume that they have the same format as number and that they definitely have a different area code. I just need to calculate the probability that their 7 random digits are the same as mine. This is easy since there’s no constraints on the numbers (I’m not familiar with US numbering systems so I’m going with that) and it’s simply 1/10,000,000.\n\nFor the total number of new people, I’ll make the baseless assumption that it’s 50 a year for the next 60 years which totals to 3000. \n\nNow to finish the question of and get a solution, we need to calculate the probability that an event with p=1/10,000,000 occurs in n=3000 trials. Using any method you wish, you get an answer of ~0.03%.', 'Check my reply in the original thread :)', 'Thank you Sir, that does make sense to me. My gratitude for explaining me in detail.']",7,6,/r/AskStatistics/comments/133klp5/what_is_the_chance_of_two_people_with_the_same/
267,2023-04-30 15:35:01,[E] I tried to teach some basic statistics. Would love to get some feedback!,Here's the Link : [https://youtu.be/4AVBhBM8BgA](https://youtu.be/4AVBhBM8BgA),['Need to fix the audio. Lots of background noise. Use Audacity. It’s free and there are lots of YT tutorials on how to use it'],8,1,https://www.reddit.com/r/statistics/comments/133islu/e_i_tried_to_teach_some_basic_statistics_would/
268,2023-04-30 11:39:28,[Question] Calculating confidence intervals of AUC?,There’s a study that reports AUC in addition to Intercept (95% CI) and Slope (95% CI). I am trying to pool this data with some other studies that all report AUC (95% CI). So I’m trying to figure out how I can figure out the overall CI for the AUC. I don’t completely understand what the intercept and slope CIs indicate.,"['Maybe using some sort of bootstrapping on the validation data?', 'Sorry, not able to help with that one.', ""I was thinking along the same lines... As far as I know there's no rigorous way to connect CIs on the underlying model to the AUC itself - so a hacky approach is to just simulate many ROC curves based on the variance implied by the underlying CIs. It may take a while to converge though, and if anything the resulting CI is just a heuristic."", '[deleted]']",3,3,https://www.reddit.com/r/statistics/comments/133evrt/question_calculating_confidence_intervals_of_auc/
269,2023-04-30 09:33:44,[Research] Need help choosing my statistical test.,"
It’s been a long while since stats class, and I’ve decided to drive myself crazy and write a paper for work. Any help is appreciated. 

I am doing a chart data review of transgender patients with intentional ingestions. Factors I will be looking at will be age, location, gender identity, medications ingested, treatments needed, and medical outcome. 

Am I correct that a MANOVA is the correct test for this?","[""You need to clearly identify your DV or DVs, how they're measured, and what research question *exactly* that you're trying to resolve (what's the research hypothesis?)."", 'That doesn’t negate the importance of the questions in the comment you replied to. Whether you’re the one collecting the data or not, you have to be able to clearly articulate what kind of data it is and what you want to find out from it.', 'MANOVA is a test with multiple RESPONSE variables. For example, if I wanted to test if average height and average weight and average age were equal between United States and China, MANOVA would be an appropriate test. \n\nIf, however, I wanted to know if the average age differed by a set of variables, you would not use MANOVA since you are trying to test a single response. \n\nThink about what question you are trying to answer with your data - what is the variable of interest and what are your factors. Then use that to find the appropriate hypothesis and test it.', 'Please see my other comment. I am just doing a chart review, and not actually doing an experiment.', 'It’s difficult because I’m simply doing a chart review- I have a pile of data and just want to review potential trends, things like x cases in y location, x cases with y Med category, number of cases in each gender category, etc. I’m not actually manipulating any data. Apologies if I’m just thinking too hard about it.']",10,6,https://www.reddit.com/r/statistics/comments/133cimd/research_need_help_choosing_my_statistical_test/
270,2023-04-30 09:05:29,[Research] Multiplying odds ratios together in moderation analysis?,"I am a public health student and I ran a moderation analysis in STATA. I am looking at age of first marriage and the outcome of intimate partner violence in Uganda. I ran a moderation analysis, controlling for husband’s alcohol use, and the interaction between age at first marriage and husband‘s alcohol use. I ended up with three significant odds ratios for age at first marriage (age 15-17; reference group 18+), husband’s alcohol use (binary variable yes/no), and the interaction between age at first marriage and husband’s alcohol use.
Can I simply multiply these odds ratios together to get the odds of intimate partner violence compared to my “base case” (being married at age 18+ and husband doesn’t drink)? Thanks in advance!","['Yes, logistic regression. Ok great! Yes a base case would be a woman who was married as an adult (18+) whose partner does not drink, as those are considered low risk categories for intimate partner violence.', ""I'm assuming you're talking about doing a logistic regression. If that's the case then I believe the answer is yes, but depending on what all your base case factors are that could make sense (from an interpretation point of view) or it could not. It would be most powerful to use it this way when you're careful about picking which values represent the base case and encode them respectively"", 'Yes you can multiply to get OR. Be careful this does not apply to confidence interval though.', ""It that sounds great. I'm pretty sure that would work fine. I'm not an academic, but it sounds like being clear that the base case does not represent the average woman is important""]",3,4,https://www.reddit.com/r/statistics/comments/133byjm/research_multiplying_odds_ratios_together_in/
271,2023-04-30 06:41:31,"[D] I'm doing dual credit this summer online, and am taking Elementary Statistics and College Algebra. I have a few questions, any advice would be greatly appreciated.","So I'm a junior in high school doing dual credit and have chosen to do Elementary Statistics (MATH 1342) and College Algebra (MATH) for the first 5 weeks during summer. I am also doing 2 courses, Government (GOVT 2306) and Speech (SPCH 1315), for the second 5 weeks. I am trying to shave off 1 year of community college and am pursuing a career in Quantitative Finance. I plan on getting a Mathematics degree with a minor in statistics and spending my personal time learning programming.

I was wondering if I could get some advice on this since I feel like I might have signed up for too much for the summer. I do like math and enjoy helping others with their questions if needed in class. I'm doing pretty well in Algebra 2 right now, but I feel like I haven't exactly retained a lot of what I've been learning. I also have never taken any stats course and I'm afraid that taking an intro class to statistics could be a big rush and I might not retain most of it later on. Couple that with College Algebra and I'm going to have to deal with a lot.

I have no idea if I'm actually ready. I think I could do it since I actually enjoy math and I have a lot of free time that could be spent studying, but I need some advice. What are good studying habits that can help? And how can I remember all that I'm going to be learning? Any apps or school life hacks from anyone studying mathematics/statistics? What works and doesn't work? I'm doing all this online so maybe it won't be as bad as in person.

I would really appreciate all the advice given to me. Thank you for reading my question. Have a wonderful day!","[""Alright. I think I'll do it with paper/pencil as a starting point, and maybe use the equation editor later on if needed. There must be tutorials for it online, right?\n\nAlso, I don't get to see a lot of info on the classes online. just their class number, credit hours, prereq, vague description on course teaching, ect. Not really anything about textbooks needed or specific course material. I'm going to have to either find the emails of the teachers of the math classes classes, or ask my hs counselor on info for that."", ""Ok, I read my stats class syllabus, and it seems that in my stats class I can either do the stats work by hand, or using 'StatCrunch'\n\nhttps://www.statcrunch.com/\n\nHave you used this?\n\nTo answer you on the topic of programming, I have no programming skills, but I have a 12 hr video saved that I will use to learn python and its basics. \n\nBTW, I've been asking a lot of questions to my teacher, and he has been really helpful during my year in algebra 2. I asked him for help on quadratic equations and he really cleared up a lot of what I had been confused on by giving me 3 quadratic questions, and letting me work through them. I did have to ask or help, but he explained things thoroughly to me, and it really made me understand it all."", ""I do not any apps, but I love mathematics too.\nThough I am in Accounting now, but whenever I face mathematics I instantly remember what I have learnt.\n\nSo, I would suggest do not worry if you don't remember anything. If you love mathematics you will be doing great. When needed you will retain everything.\n\nGo ahead don't worry about retaining."", ""I wouldn't recommend doing two 5 week math courses at the same time, unless you're _really_ good  at all of: time management, general study skills, and learning math in particular.  Keep in mind that most college math courses are 14-15 weeks in a normal semester, so condensing that down to 5 weeks means you have to learn about three times as much material each week as you would if you were taking it on the regular timeframe. \n\nDoing two 5 week courses at once is probably roughly equivalent to doing six 15 week courses at once in regards to how much material you'll need to learn each week.  Can you handle that kind of workload without getting overly stressed, especially if it's all math? If you really think you can handle it after really thinking it through and considering your previous experience with similar workloads, OK great.  But if not, saving a bit of time in getting your degree isn't worth making yourself miserable for weeks at a time due to the stress of the workload.  And another consideration is whether you'll retain the information as well if it's all condensed into a few weeks.\n\nTLDR: If you're not reasonably certain you can handle a very intense workload, don't take two 5 week math classes at the same time.  Honestly, even doing _one_ 5 week math course will likely be a real challenge due to how fast things will go.  If only condensed courses are offered over the summer and you just want to get some credits over that time, you might be better off getting some introductory humanities, social science, or other gen ed courses out of the way over the summer (since, even if you plan to major in math, must colleges in the U.S. will require about half your credits be gen eds, though the particular requirements will of course depend on what college you plan to go to) and wait until the regular semester starts to take the math courses as regular 14 or 15 week courses."", ""I can't change it anymore, sadly. I honestly just wanted to get the 1 year shaven off, as that's what a lot of people aren't retelling me to do. Some people at even getting 2 years of college shaven off doing all the ap and dual credit stuff, and skip CC. It honestly just makes me feel horrible. \n\nAbout the difficulty, idk. I like mathematics, and I like to figure out all its patterns and little nuances that it has in it, but yes, I really don't know if I can handle 2 math classes. I did wish I had an additional math class my junior year, not just 1. But that spans around the whole year. \n\nI plan on doing the prereqs my 1 year of CC, and doing calc 2 1st semester, and 3 and diff eqs second. \n\nDo you have any advice for handling the workload, or any tips or tricks that can work?""]",2,18,https://www.reddit.com/r/statistics/comments/1338x2v/d_im_doing_dual_credit_this_summer_online_and_am/
272,2023-04-30 06:01:21,[Q] Does there is a certificate(s) or exam(s) or designation(s) I may set for to prove my knowledge of Statistics,"There is CPA, and CMA ...etc for accounting as an example.

There is CFA for finance as an example.

So, does there is something like that for Statistics?

\-Later Edit-

I am from 3rd world developed country in Africa.

I was thinking about post-graduate or a master's in Statistics, but after searching in local universities I've found it very time-consuming and may extend to 3 or 4 years plus the curriculum is not updated.","['The ASA may have something', 'The closest that comes to mind is the Exam-P test for actuaries.\n\nEDIT: I understand this is not a pure statistics exam, but it is difficult for many and could showcase knowledge of probability concepts.', 'A degree from a good institution is hard to beat.\n\nWhy and to what level do you need to prove your knowledge of statistics?', '>ASA\n\nWhat is it? as google brings many abbreviations not related to statistics.\n\n\\-later edit -\n\nDo you mean [American Statistical Association](https://www.amstat.org/), and I sent them an email.', '>Why you need to prove your knowledge of statistics?\n\n2 reasons, get promoted at my job and find an online chance to gain extra money.\n\n>what level ?\n\nI do not know the right levels, but I read and learn a lot on my own, and I studied many textbooks on my own.']",0,13,https://www.reddit.com/r/statistics/comments/13380l9/q_does_there_is_a_certificates_or_exams_or/
273,2023-04-30 05:57:48,[D] Documentaries or movies that use statistics to solve practical problems?,,"['Moneyball and The Big Short are the ones that come to my mind now.\n\nPS - I guess many works about gambling like movies about poker and blackjack too', 'I really enjoyed the BBC documentary [the Joy of Data](https://www.youtube.com/watch?v=l6oKriR-RjM).', 'Can’t wait for them to do a documentary/movie on 3 point shooting becoming the meta in the NBA', 'Moneyball?', 'The imitation game']",27,9,https://www.reddit.com/r/statistics/comments/1337xfc/d_documentaries_or_movies_that_use_statistics_to/
274,2023-04-30 05:57:24,"[Q] What statistical techniques are used for ""item-level analyses""?","I have run an experiment where we have participants see a sentence, with or without a photo, and then decide whether the sentence is true or false. I have been tasked with doing an ""item-level analysis"" on the sentences to see whether some sentences were more likely to be seen as true or false. 

I am somewhat knowledgeable in social science statistics, but I have never done an item-level analysis. I am wondering:

1. What techniques are typically used? I have read that a multilevel model is often used for this, but I can't seem to wrap my head around how.  
2. Are there any resources that you know of that walk through this stuff from beginning to end?

Thank you!","['You can probably get pretty far by adapting the latent variable setup in [item response theory](https://en.wikipedia.org/wiki/Item_response_theory) to this problem.\n\nIn IRT, each _question_ has a latent difficult parameter and each _individual_ has a latent strength parameter. You have lots of modeling options here, for example you probably don\'t actually want a strength parameter for each of the _individuals_, so maybe just use a single ""strength"" (truth-answering) intercept parameter for everybody and let only the sentences have their own truthiness *b_i* parameters.\n\nPseudocode (following the [STAN example](https://mc-stan.org/docs/2_20/stan-users-guide/item-response-models-section.html)) might be something like:\n\n    model {\n      ... // priors\n      for (n in 1:N)\n        answered_true[n] ~ bernoulli_logit(\n           true_intercept\n           + picture_was_shown[n]\n           + b[sentence_idx[n]]\n        )\n    }\n\nEdit: just to clarify, it\'s believable that individuals vary in their truth-leaning (credulousness? suggestibility?) so you definitely could add individual-specific truth-leaning parameters -- in fact, you\'d probably want to do this for a better model fit if you had plenty of data. The reason I assumed not above is just that (1) it didn\'t sound like you would have tons of data and (2) didn\'t sound like a quantity of interest for the research, but obviously it\'s easy to add to the linear predictor if desired.\n\nAlso, to respond to this part:\n\n> Are there any resources that you know of that walk through this stuff from beginning to end?\n\nThere are tons of good articles and libraries for IRT, just depends what environment you\'re using (e.g. R, Python) and modeling approach (e.g. max likelihood or probabilistic/MCMC with Stan/PyMC/etc).', 'Another, simpler way to approach it is to compute mean response times for each item in each condition for correct responses. Then do the anova. See Clark (1973).']",3,2,https://www.reddit.com/r/statistics/comments/1337x3i/q_what_statistical_techniques_are_used_for/
275,2023-04-30 05:11:01,[Q] What conditions are necessary to use the Partial F-test?,"I know that for the Full F-test we have to satisfy the regression conditions:

1. Linearity
2. Homoskedacity of residuals
3. Normality of residuals

However in my statistics class they never access the conditions before using the Partial F-test. **Is this because we are using the partial F test to compare to the partial linear regression model to the full regression model?** Meaning that we are going to fit the regression line/curve no matter whether or not the conditions are satisfied.",[],7,0,https://www.reddit.com/r/statistics/comments/1336u2q/q_what_conditions_are_necessary_to_use_the/
276,2023-04-30 04:41:29,[Question] Trying to pick a stats analysis programming language to learn as somebody without CS background. Any recommendations?,"After years of procrastinating on actually learning how to stats program to speed up data analysis / graph generation, I think it's time I actually learned a script-based stats language rather than just relying on pre-made stats packages.

In the past, I've just been relying on stats software packages that don't require any CS/programming skills, such as Minitab, SigmaPlot, GraphPad Prism, etc. However, I've been increasingly finding that I'm having to use more complex statistical models that those programs simply do not currently support, which is ofc delaying some manuscript/abstract/publication submissions, as well as ofc my thesis.

Currently, I'm contemplating between learning one of the following stats languages that are based on or compatible with custom scripts written with computing languages such as Java or Python:

1. IBM SPSS
2. SAS
3. MATLAB
4. R

Some statistical models that my current stats packages can't really support (and I kinda need for deeper layers of analysis) including PROC MIXED and Within-Subject adjusted Repeated Measures Two-way ANOVAs with Covariates, as well as some more specific Post-hoc tests. The ability to further customize graphs using the same software as for stats analysis would also be a plus.

Admittedly, my background in CS/biostats is kinda limited, so relative ease of learning is also a factor I'm trying to consider atm.

Thanks in advance for any input!","[""In order, for stats:  \n1. R.  \n10. SAS  \n18.  Matlab  \n1000. SPSS \n  \nJust go with R and don't look back. Matlab is an engineering language with some other functionality hacked on. SAS is a walled garden ecosystem. SPSS is a pile of shit."", 'R is free, which is a big deal if you don’t have an employer footing the bill. SAS has some biostat advantages, I suppose, depending on your goals. (And frankly, that may be dated info anyway.)', 'R is also top of class for visualisation (as someone that switched to python after a couple of years in R).', 'R can do everything SAS can do and everything it can’t.', ""Hey, before I comment: I only have knowledge on R,Matlab and Pyhton. \n\nFirst of all, don't bother with matlab. I think the most relevant choices are either R lr Python. I use R in my master and used it mainly for academia stuff, and then I use python for my company. \n\nPersonally, I prefer R for stats/data handling, as I really like some niche features and handling data with tidyverse package feels more natural to me. On the other hand, Python I seems as a more flexible language that allows me to go into more complex programs (genetic algorithms or  more object oriented programming for example come to mind). However, I think a lot of stuff can be done in either R ir Pyhton, specially basic statistical tests/simulations and data analysis.\n\nIn conclusion, I recommend either R or Python, as both are great. If you will mainly use your coding in industry then probably Python will be better as it is more widely used.""]",21,55,https://www.reddit.com/r/statistics/comments/133654z/question_trying_to_pick_a_stats_analysis/
277,2023-04-30 04:15:57,Gage R&R - Help & Next Steps [Question],"Edit: Anybody care to let me know why this is getting downvoted?  Not sure what I did wrong here.  

[Seaborn BoxPlots](https://imgur.com/a/VfVddQw)

The above plot was generated with Seaborn using data from a field experiment at one of our sites.  I'm don't have a programming or statistics background, so bear with me.  Here's a breakdown of the data: 

* 14 operators total, 6 in Lab 1, 8 in Lab 2.
* 3 Samples each prepared and analyzed in triplicate
* Lab 1 used a single instrument and method
* Lab 2 used two instruments each using a separate method.

| Trial | Operator | Lab | Sample | Method | Instrument | Iteration | Value |
| ----- | -------- | --- | ------ | ------ | ---------- | --------- | ----- |
| 1     | 1        | 1   | A      | Auto   | 3          | 1         | Value |
| 2     | 1        | 1   | A      | Auto   | 3          | 2         | Value |
| 3     | 1        | 1   | A      | Auto   | 3          | 3         | Value |
| 4     | 1        | 1   | A      | Manual | 2          | 1         | Value |
| 5     | 1        | 1   | A      | Manual | 2          | 2         | Value |
| 6     | 1        | 1   | A      | Manual | 2          | 3         | Value |
| 7     | 1        | 1   | B      | Auto   | 3          | 1         | Value |
| 8     | 1        | 1   | B      | Auto   | 3          | 2         | Value |
| 9     | 1        | 1   | B      | Auto   | 3          | 3         | Value |
| 10    | 1        | 1   | B      | Manual | 2          | 1         | Value |
| 11    | 1        | 1   | B      | Manual | 2          | 2         | Value |
| 12    | 1        | 1   | B      | Manual | 2          | 3         | Value |
| 13    | 1        | 1   | C      | Auto   | 3          | 1         | Value |
| 14    | 1        | 1   | C      | Auto   | 3          | 2         | Value |
| 15    | 1        | 1   | C      | Auto   | 3          | 3         | Value |
| 16    | 1        | 1   | C      | Manual | 2          | 1         | Value |
| 17    | 1        | 1   | C      | Manual | 2          | 2         | Value |
| 18    | 1        | 1   | C      | Manual | 2          | 3         | Value |
| 19    | …        | 1   | …      | …      | …          | …         | …     |
| 145   | 9        | 1   | A      | Auto   | 1          | 1         | Value |
| 146   | 9        | 1   | A      | Auto   | 1          | 2         | Value |
| 147   | 9        | 1   | A      | Auto   | 1          | 3         | Value |
| 148   | 9        | 1   | B      | Auto   | 1          | 1         | Value |
| 149   | 9        | 1   | B      | Auto   | 1          | 2         | Value |
| 150   | 9        | 1   | B      | Auto   | 1          | 3         | Value |
| 151   | 9        | 1   | C      | Auto   | 1          | 1         | Value |
| 152   | 9        | 1   | C      | Auto   | 1          | 2         | Value |
| 153   | 9        | 1   | C      | Auto   | 1          | 3         | Value |
| 154   | …        | 2   | …      | Auto   | 1          | …         | …     |
| 190   | 14       | 2   | A      | Auto   | 1          | 1         | Value |
| 191   | 14       | 2   | A      | Auto   | 1          | 2         | Value |
| 192   | 14       | 2   | A      | Auto   | 1          | 3         | Value |
| 193   | 14       | 2   | B      | Auto   | 1          | 1         | Value |
| 194   | 14       | 2   | B      | Auto   | 1          | 2         | Value |
| 195   | 14       | 2   | B      | Auto   | 1          | 3         | Value |
| 196   | 14       | 2   | C      | Auto   | 1          | 1         | Value |
| 197   | 14       | 2   | C      | Auto   | 1          | 2         | Value |
| 198   | 14       | 2   | C      | Auto   | 1          | 3         | Value |

I'm trying to figure out how to properly analyze this stuff.  I don't really want a true repeatability and reproducibility test since it's all at the same site.  I want to be able to answer the following questions statistically: 

1. Which lab performed best?
2. Which method performed best?
3. Is there an inherent bias or greater variance between Instrument 1 and 3 (same test method)?  

I have the answers, more or less, from the graphed boxplots.  However, I'd like to also have the answers numerically.  I've reviewed ANOVA, but the setup for ANOVA confuses me and I'm not 100% clear what type of ANOVA test I'd be using or how to properly reshape the datatables. 

The answer to the above won't necessarily dictate which lab to perform the work in, however it will provide greater guidance.  Any help would be greatly appreciated.  Thanks!","[""Typically for a gage r&r, you use the variance components of a random effects model and the mse to determine if the repeatability and reproducibility are sufficiently small relative to the tolerance (Precision to tolerance). \n\nYou're trying to compare multiple methods at multiple labs which isn't as typical. Its also not typical to do a comparison to determine which subgroup is best.\n\nWhat you're trying to do can certainly be done, but its going to require a lot more background knowledge than you're going to find on a reddit post. Consultants with this knowledge will probably charge ~300$ an hour."", ""I don't think a gage r&r is the appropriate tool for the job.. its probably resulting in the down votes."", 'How do you define ""best""? Are there acceptance criteria to evaluate against?', 'Thanks. What would be the right tool?  \n\nGage R&R is industry standard for this type of analysis in the chemical engineering industry.  It’s a chemical processing laboratory test method analysis to get repeatability (equipment) and reproducibility (operator) results. It helps an operator establish error bands on their data and determine if it is operator or instrument / environment related.  Which is exactly what I want to determine, statistical method of determining which instrument and which technicians are best suited to perform the analysis based on reducing the variance. \n\nNote there are many other factors to consider besides variance in the data but this is the first step. \n\nMaybe people aren’t familiar with Gage R&R study?  Because from its description and general application in the industry, it’s generally what I’m wanting. In the chemicals industry, test methods need to be repeatable and reproducible. And this requires specific training on the methods. There’s often many standardized test methods that get the job done, but each comes with its own sources of error. And each requires a different level of training and accessory equipment to perform properly.']",2,4,https://www.reddit.com/r/statistics/comments/1335jqh/gage_rr_help_next_steps_question/
278,2023-04-30 02:47:38,[Q]Mudlak model with categorical predictors,,[],3,0,/r/AskStatistics/comments/1333hus/qmudlak_model_with_categorical_predictors/
279,2023-04-29 23:58:12,[Q] Feature importance and contribution interview problem I can't solve.,"A few weeks ago, I had an interview for a data science job. Of all the questions they asked me, I was unable to solve the following one. I couldn't even attempt it because I didn't know anything about it. I have researched, but I can't find any book, paper, blog, etc. that shows the question.

So here's what I remember from the question:

The following table has the ""feature contribution"" of feature i with observation j. Additionally, each feature is divided into different categories. Category 1 has features 1, 2, and 3, Category 2 has features 4 and 5, and Category 3 has only feature 6.

||Feature 1/Category 1|Feature 2/ Category 1|Feature 3/Category 1|Feature 4/Category 2|Feature 5/ Category 2|Feature 6/Category 3|
|:-|:-|:-|:-|:-|:-|:-|
|Obvservation 1|\-4|4|1|2|\-2|0|
|Observation 2|2|1|\-1|0|4|2|
|Observation 3|1|4|5|3|3|1|
|Observation 4|0|2|1|1|4|\-3|
|Observation 5|\-3|\-2|0|\-2|2|1|

&#x200B;

If I recall correctly, they asked me to obtain the feature importance of each category. So the expected result was three numbers, one for each category.

During the interview, I attempted to calculate the means and sums and then means again, but the answer was never correct. Do any of you have any references on how to solve this problem?","['Did you try to calc for each obsrvn percentage of category x from all categories? Then mean from all obsrvns?\nOr maybe sum of category x from all obsrvns, then percentage from all categories and all obsrvns?\nSounds right.']",2,1,https://www.reddit.com/r/statistics/comments/132zfor/q_feature_importance_and_contribution_interview/
280,2023-04-29 23:40:51,[Q] Publication reviewer made this comment about my use of factor analysis. How should I proceed?,"Hi all, 

I created a psychological questionnaire that I am now publishing. I subjected the questionnaire to various reliability and validity tests with excellent results, including a test of discriminant validity that looked like the following: I took my questionnaire and a pre-existing questionnaire that was deemed to be theoretically related but ultimately measured a distinct construct and ran an exploratory factor analysis on the pooled items. The idea here was that the items on each questionnaire should form their own distinct factor (with strong factor loadings), while remaining highly correlated, suggesting that the scales do in fact measure distinct, but related, constructs. The analyses were successful and my questionnaire (while being significantly correlated with the other questionnaire) diverged from the other questionnaire in the EFA in the way I described above, (i.e., items from each questionnaire forming their own clean, distinct factor, with strong factor loadings).

The reviewer wrote this comment: ""For the discriminant validity of your questionnaire, there is a risk that the EFA results could be partly attributed to the different likert-scale anchors used (your measure in terms of agreement and the other measure in terms of truth). 

My impression is to address this comment by making note of this in the discussion section by saying that while the results do provide strong evidence for the discriminant validity of this questionnaire, the results should be interpreted with some caution as the results could be partially due to the different anchor labels. Something like that, written better of course. 

I wanted to run this by you guys and see if you had any suggestions for addressing this beyond what I already plan on doing or if you had some suggestions for strengthening the plan I already have. And also how valid is the reviewer's comment? 

Thank you so much. If you need any information above what I have put, please feel free to ask.","['EFA is not usually used for discriminant-related validation', 'The reviewer is right, you could end up with distinct factors simply due to method. Traditionally discriminant validity has been evaluated with an MTMM matrix. Another alternative that is well regarded is the method developed by Westen and Rosenthal.\n\n\nWesten, D., & Rosenthal, R. (2003). Quantifying construct validity: two simple measures. Journal of personality and social psychology, 84(3), 608.', ""I will say, as an academic, I'm always a little wary that I'm wrong so take my advice with a grain of salt. It could be that your advisor knows something that I don't.  But I would just do an MTMM matrix. It's quick and easy and it is widely accepted i.e., a reviewer shouldn't give you grief about it (the methodology at least)."", 'How did you determine how many factors to extract in the EFA? Parallel analysis?', ""Well, the comment seems to imply that they think there's a possibility that if you extract 3 factors, you might get a method loading and your items co-mingling?""]",13,13,https://www.reddit.com/r/statistics/comments/132z0fc/q_publication_reviewer_made_this_comment_about_my/
281,2023-04-29 21:11:31,[Question] [Q] Z-scores for each item of a questionnaire?,I have to run correlations and regressions with all items from multiple questionnaires. Each questionnaire has its own scale. Should I normalize each item by using z-scores and then run the analysis? Does it make sense?,"[""I don't understand what you are doing or what you are asking. Going to need to provide more context/information."", 'Are the questions ""agree vs disagree style"" eg\n""I like stats""\n1) I strongly disagree \n2) I slightly disagree \n3) I neither agree nor disagree \n4) I slightly agree \n5) I strongly agree\n\nIf so, you can\'t standardise your data since the SD (and to a lesser extent the mean) are meaningless\n\nMore info about the data you\'re analysing would be useful', ""You _can_ but that type of questionnaire is a likert scale under the hood which is _trchnically_ ordinal data rather than interval data (ie the true distance between answer 1 and 2 isn't necessarily the same as between 2 and 3)\n\nThat said, most people do treat it as interval data even if it isn't strictly speaking, hence why I said you can't take the mean _to a lesser extent_\n\nThere's also been studies that show that the granularity of your scale can affect how people answer, since OP says they're working with questions on different scales, I'd stick to using the mode or summarised categorises for my analysis"", ""I have applied 5 questionnaires to 170 people. Each questionnaire has its own scale. I want to run correlations and regressions on all items from all questionnaires in order to find the right ones for a model. I haven't done this before. I m pretty new to statistics and I don't know if it makes sense to apply z-scores on each individual item of each questionnaire."", 'Yes, this is what they look like and each option is represented as an integer in my database. Questionnaires are about laterality, self-efficacy, and learning styles.\n\nThank you for your answer']",12,12,https://www.reddit.com/r/statistics/comments/132sk9s/question_q_zscores_for_each_item_of_a/
282,2023-04-29 09:29:22,[Q] Conditioning on other variables question,"This is purely a hypothetical question:


Let's say I want to regress salary on years of education in a random sample of the population. Then I want to control for age, because I think it could be the case that older people were less likely to go to college, and age may have an effect on salary.

If I were to limit my sample to-only people of the same age and run a regression, and then do this for ecery age in my sample, is there a way to aggregate those up to what the regression coefficient would be if I just ran a regression of salary on years of education and age?

The reason I am wondering is because my understanding of conditioning on age in the multiple regression is i am now making comparisons between people with the same age, but different levels of education. The latter example then is quite literally restricting the data so that you are only making comparisons between people of the same age, so
on a surface level it seems like they should coincide/bring the same coefficient?","[""> If I were to limit my sample to-only people of the same age and run a regression, and then do this for ecery age in my sample, is there a way to aggregate those up to what the regression coefficient would be if I just ran a regression of salary on years of education and age?\n\nThis is precisely what happens if you treat age as a categorical variable, representing each distinct age value with dummy variables.\n\nThe negative is that you lose all context about nearby ages being similar.\n\nThe positive is that you gain a ton of flexibility in the modeling process, though the large flexibility mandates a reasonable sample size for each of the distinct age values, otherwise you're essentially overfitting."", 'I might be speaking bullshit buy maybe the question would be fully answered if you revisit how conditionals work in linear regression. Kutner book ftw.', '>This is precisely what happens if you treat age as a categorical variable, representing each distinct age value with dummy variables.\n\nNot quite. The models that are split up by age are almost equivalent to treating age as a categorical variable and including the interactions between age and every other variable. \n\nI say almost because doing things this way also will estimate different standard deviation terms for each model (assuming linear regression), which may result in slight differences in inferences.', 'Let’s assume that the relationship between years of education and salary is a simple linear equation, y=a+b*x+e where y is salary and x is yrs of education, a is the y-intercept and b is the slope. Let’s say you have data on 50 people that are 30-years old, 50 that are 40-years old and 50 that are 50-years old. If you fit (using least squares regression) the simple linear equation separately to the 30 year olds, 40 year olds, and 50 year olds, you will get three different equations with different estimates for a and b for each age category. The error variance will also differ for each age category and is estimated from the 50 data points in each subset. \nYou can get the same a and b model parameter estimates (but not the same error variance) by combining the three age group subsets into one data set and creating indicator variables z1, z2, and z3 such that z1 = 1 if age is 30, and z1=0 otherwise; z2=1 if age=40, and z2=0 otherwise; and z3=1 if age=50, and z3=0 otherwise. Then fit the model y = a1*z1 + a2*z2 + a3*z3 + b1*z1 + b2*z2 + b3*z3 + e\nTry writing this equation for each age group and you will see that the z’s serve to create three equations, one for each age category, just like you had when you subsetted the data manually.\nThe error variance in this combined model with the z’s, is now based on all 150 observations, so it will not be the same as the three estimates you got when fitting the age groups separately. \n\nThere are many ways to define a set of indicators variables (the z’s). Some software creates them for you automatically using the first age category (30 yrs) as the reference category and some use the last category as the reference (50 yrs in our example). You are technically getting the same fit, but it won’t look the same because the parameters for the other groups are in comparison to the chosen reference category. \nAnd one more way to build this model worth mentioning is instead of using indicator variables you could use q=age (that is when age is 30, q=30; when age is 40, q=40; and when age is 50, q=50.)\nNow when you fit y = a + b*x + c*q + e you will be fitting a surface; in this case the surface is a flat surface like a piece of plywood. If you fit y = a + b*x + c*q + d*x*q+e you will get a torqued surface where a slice at any point on the x-axis or on the q-axis is still a straight line, but the overall surface is twisted. You will not get the same fitted lines when age is parameterized as continuous (defined by q) as either of the previous models using z because using q implies that the change in salary from 30-yr olds to 40-yr olds is the same change as from 40-yr olds to 50-yr olds.\n(My apologies - I do not know why part of my answer is italicized. I am new at this.)', 'Not quite exactly the same, unless you separately fit the intercept term for the entire sample separately.']",6,8,https://www.reddit.com/r/statistics/comments/132fphc/q_conditioning_on_other_variables_question/
283,2023-04-29 03:20:46,Correlation matrix with numerical and categorical data [Q],"I’m trying to make a correlation matrix with a dataset containing both numerical and categorical values, but I’m not sure how to do it. Could I just change the categorical values to numbers (1, 0 etc) and run that?","[""correlation is not defined for categorical variables, in particular for those with more than 2 categories. So no you can't. Coding categories with numbers is ill advised most of the time. Something like factorial analysis of the mixed data (FAMD) may be helpful tho."", 'And yet basically all of the regression implementations in statistical programs turn the categories into 0s and 1s. I’ll agree that it’s a bad idea to turn an unordered category into a numerical scale, but turning categorical variables into dummy variables (implemented as 0s and 1s) is standard practice. \n\nFor interpretation, there’s no real difference between “a change from category a to category b” and “a change from 0 to 1”.', 'I am well aware of that. But that is for different purposes, other than correlation construction. In fact that is the way FAMD codes categorical variables to work with. Op was taking about coding 1, 2, 3 instead of a, b, c, which is not good most of the times', 'You can find the association between two nominal variables.  Cramer\'s *V* works for this.  But the interpretation of what may be a ""large"" association depends on the number of categories.\n\nIf your categorical variables are either nominal with two groups, or ordinal categories, there are different options.\n\nIf you have a nominal variable with more than two categories and a numeric variable, I don\'t know of a standard method for this.  But practically speaking, you could use a one-way anova, and take the square root of the r-squared value.', 'WOE?']",2,10,https://www.reddit.com/r/statistics/comments/13275lg/correlation_matrix_with_numerical_and_categorical/
284,2023-04-29 01:52:51,[Q] Estimation of covariance matrix from elevation data grid,"Hello all,

I have data in a grid format from elevations of a terrain. I want to estimate a covariance matrix from that data. For that, I have calculated a semivariogram, where I have fitted a Matern function to the empirical data. 

However, I have some doubts. From that empirical function, I have an estimated nugget value that is smaller than half the squared standard error in the measurements, which has also been given to me by the data source. From my understanding, this is not possible.

Does this mean that I did something wrong in the semivariogram estimation? Could I set the nugget value to match the standard error in some way? Would that be the same than rescaling the covariance matrix obtained?

Thanks in advance for providing any insight.",['It is certainly possible to have very small (or 0) values for the nugget. It’s not inherently incorrect.'],0,1,https://www.reddit.com/r/statistics/comments/1324sf7/q_estimation_of_covariance_matrix_from_elevation/
285,2023-04-29 00:57:38,[D] Discussion: Which Statistical Tests Would You Perform?,"# Problem

I want to know which ***type of investors*** and which ***combination of the type of investors*** (funding partner mix) are empirically predominant at each life-cycle stage of European startups that had a successful exit. The life cycle stage refers to which point in their life the startup is in. I have divided the life cycle stages into three: early, mid, and late stage. 

In other words, I want to find out which individual type of investor is predominant at each of these stages. So for example, lets say at the *early* stage I find that angel investors are the most predominant, followed by venture capital. 

Then, I also want to know if there is a combination of investors in that stage that is predominant (since most startups have more than one type of investor). For example, lets say I find that at that same early stage, the predominant ***combination*** of investors is accelerator + venture capital. The second most predominant combination is angel + venture capital + venture debt. And the third most predominant combination is solely angel investors (angel investor + nothing).

# Datapoints Available

I have the following columns in my sample dataset: startup name, startup founding date, startup exit date, investor name, date the investor invested in the startup, type of investor. 

I will determine the stage to which each investor belongs based on the difference between the startup founding date and the date the investor invested. Early (invested in 0-2 years since founding), Mid (3-5 years), late (6+ years).

**Note:** All of the startups in my sample dataset had a successful exit, I could not get my hands on data on startups that did not exit. That means I cannot perform any tests that may require startups that did not have an exit as a control.

# Question

With this information, **which statistical tests should I perform to find which** ***type of investors*** **and which** ***combination of investor types are empirically predominant at each of these three stages*****?** 

My statistical knowledge is very basic and I am using either BlueSky statistics software (GUI for R) or Excel to perform the necessary tests. Therefore, the preferable thing would be to find the simplest way to obtain what I want. However, I also appreciate suggestions for complementary tests I could perform to either strengthen my results or enrich my analysis.","['> All of the startups in my sample dataset had a successful exit, I could not get my hands on data on startups that did not exit. That means I cannot perform any tests that may require startups that did not have an exit as a control.\n\nWell, that’s kind of a big deal.  I would think that your problem statement is inherently unanswerable without negative examples. And not cherry-picked examples like a lot of business books present (eg Good To Great), but either more comprehensive or a random sample.', ""I thought about it more and still think the negative data is necessary.  Different funds have different specializations, e.g. early stage, growth stage, late stage, private equity, and because of this the distribution will be different across stages.  Another factor is industry.   Your dataset might reflect that some fund/investors have better performance, but that might just be an industry bias.  Like if your data was from 2020-2021, you would think crypto funds were the path to success when we all know (now at least, and some of us before) that was an inevitable blip.  Or you may see one player in 10% of the exits, when the case is they are in 10% of all funding at that stage.  \n\nThere's so many factors you have to control for that you just can't do without the negative data."", ""Thank you for the input. I really appreciate it. Well, seems like I'm in a bit of a pickle then. For context, I'm analyzing startups that fall within the ICT sector, so although that would not fix the industry issue, it should at least alleviate it.\n\nOn the other hand, I see what you mean with the different distributions for different funds. That is also a big problem. My intention when beginning this research was to use a control sample with startups that did not have an exit. However, I had some technical limitations regarding it. I can still obtain a control sample, but I am unsure of how representative it may be. Let me explain.\n\nI used Crunchbase to extract the sample of startups that had an exit. About 2,700 startups approximately were extracted, and those were ALL of the startups with an exit that Crunchbase had in its databse. Now, in that same database there are around 80,000 startups that did not have an exit. The problem is I cannot extract ALL of them due to maximum extraction limits Crunchbase has. I could potentially extract around 3,000/80,000 startups without an exit. Therefore, if I was to extract startups without an exit to use as control, **the question would be how can I extract a representative sample of those 80,000 startups?** And could I use such a sample to compare it with the successful exit sample I already have?\n\nI would love to hear other opinions on this issue. What would be the best theoretical way to go about this? I may not be able to execute that theory in practice given some of Crunchbase's limitations, but maybe if we come up with a theoretical solution I can work around them and get a representative sample.""]",0,3,https://www.reddit.com/r/statistics/comments/1321wmw/d_discussion_which_statistical_tests_would_you/
286,2023-04-29 00:14:17,[Q]How to work out confidence intervals for the following two scenarios? (please see description below)," I've  got a linear regression sas output. So, I know the standard error from  there. I need to work out the 95% confidence interval for the change in y  as x increase by 1, which is basically the confidence interval of the  slope I think. So, according to some research I have done, I need to  work out critial t value first. I have sample size of 31, so df=31-2=29,  and found the critical value for 95% to be 2.045. So I took the  parameter estimate of x +/- t-critical value(2.045)\*standard error of x.  parameter estimate and standard error taken from the aforementioned sas  output for regression. I am just not sure if I have done this correctly  or if I should take z-critical value, if so how?

2nd  scenario is chi-sq test. 3 treatments. recorded yes or no for disease. i  worked out the expected values this way. (total number of treatment 1 \*  total number of diseased) / total sample size, for expected number of  people who have disease with treatment 1 and so on. Please comment on if  I got this right. and then I worked out the chi-sq. And then, I need to  work out the 95% confidence interval for difference in rate of disease  incidence (yes-disease) between participents of treatment 2 and 3. I am  just completely at a loss here. i found thid potentially useful formula.  CI = sample mean +/- z\*(s/square-root of n). s is sample standard  deviation. but no idea what sample mean would be in chi-sq test or how  to find s or z.",[],1,0,https://www.reddit.com/r/statistics/comments/131z6vj/qhow_to_work_out_confidence_intervals_for_the/
287,2023-04-28 21:56:01,[Q] Is linear regression suitable for discrete numeric dependent variable when responses are bunched?,"I am conducting regression analysis and my dependent variable is derived from dividing the number of candidates standing in an election by the number of vacancies. This gives me a discrete dependent numeric variable, which is throwing up issues in the diagnosis of my linear regression model.

Are there alternative types of regression I could use - or is it generally fine to use linear regression?



| cand/vac         | count        |
|------------------|--------------|
|     1            |      1098    |
|     1.3333333    |       159    |
|     1.5          |       659    |
|     1.6666667    |       316    |
|     2            |      7552    |
|     2.3333333    |       952    |
|     2.5          |      1933    |
|     2.6666667    |       902    |
|     3            |     14244    |
|     3.25         |         3    |
|     3.3333333    |      1291    |
|     3.5          |      1416    |
|     3.6666667    |       847    |
|     4            |     15528    |
|     4.3333333    |       458    |
|     4.5          |       445    |
|     4.6666667    |       222    |
|     5            |      7777    |
|     5.3333333    |        31    |
|     5.5          |        51    |
|     5.6666667    |        11    |
|     6            |      2048    |
|     6.3333333    |         2    |
|     6.5          |         3    |
|     7            |       336    |
|     7.5          |         1    |
|     8            |        35    |
|     9            |         3    |","[""I'd try a model suitable for count data such as a negative binomial regression with an offset. The outcome is the number of candidates whereas the (log) offset is the number of vacancies."", '1. What is the goal of the analysis?\n\n2. What exactly are your explanatory and response variables? The data you have included here is not something you would use a regression model for.\n\nAs a general answer to your question about discrete data, it can sometimes be appropriate to use linear regression for discrete responses, provided the residuals are approximately normal (in addition to the other assumptions of course). If normality is a concern, then other models should be considered (such as Poisson regression, which is common for count data).', 'Thanks Chatgpt.', '\n\nIf your dependent variable is a ratio or a proportion, linear regression may not be the best option as it assumes the residuals to be normally distributed and to have constant variance, which may not hold for such variables. Instead, you may consider using regression models specifically designed for analyzing proportions, such as logistic regression or beta regression.\n\nLogistic regression is used when the response variable is binary, whereas beta regression is used when the response variable is continuous and bounded between 0 and 1. Both models allow for heteroscedasticity, and can handle data that have a non-normal distribution. \n\nBefore deciding on which model to use, it is important to understand the nature of your data, and the research question you are trying to answer. Consult with a statistician or an experienced data analyst to help you select the best model for your specific case.', 'You are welcome.']",4,5,https://www.reddit.com/r/statistics/comments/131shky/q_is_linear_regression_suitable_for_discrete/
288,2023-04-28 20:09:19,[Q] Best way to estimate a three-way interaction,"I am testing the effect of being female on chances of electoral success in German Federal Elections. I would like to find the interaction between female, a binary variable (female), electoral tier, a binary nested variable (estier), and political party, a categorical variable with 4 discrete categories (PartyID).

I would expect the estimated three-way interaction terms to closely approximate the actual differences I observe in the rate of female success between estier = 1 and estier = 0 across each of the 4 political parties.

&#x200B;

|Political Party|Proportion of Women Elected (estier = 1)|Proportion of Women Elected (estier = 0)|Difference (estier = 1 - estier = 0)|Expected Magnitude and Direction of three-way interaction|
|:-|:-|:-|:-|:-|
|CDU|0.370 (73 DF)|0.097 (217 DF)|0.273|Strongly Positive|
|SPD|0.331 (118 DF)|0.322 (152 DF)|\-0.008|Near-Zero|
|AfD|0.052 (38 DF)|0.375 (24 DF)|\-0.322|Strongly Negative|
|GREEN|0.0625 (144 DF)|0.285 (214 DF)|\-0.223|Strongly Negative|

Note: DF indicates number of women candidates in this case

The problem I am facing is that I can't find a model that replicates what I should expect from the table above.

After trying a few options, this particular model seems reasonable.

    lm(elected ~ female + PartyID + female:estier:PartyID))

From what I've read about nested variable, I exclude the 'estier' as an independent predictor variable, as it is only substantively meaningful when included in the interaction term. I also excluded the sub-interactions, such as female:estier and female:PartyID, as these would drastically increase the p-values of the three-way interaction terms, I think this is because of multicolinearity. Applying this model, I get the following results.

                               Estimate Std. Error t value Pr(>|t|)    
    (Intercept)                 0.19611    0.02073   9.458  < 2e-16 ***
    female                     -0.04054    0.02103  -1.928  0.05394 .  
    PartyIDCDU                  0.07192    0.02685   2.678  0.00744 ** 
    PartyIDGREEN                0.02366    0.02848   0.831  0.40606    
    PartyIDSPD                  0.16382    0.02860   5.729 1.13e-08 ***
    female:PartyIDAfD:estier   -0.10293    0.07439  -1.384  0.16657    
    female:PartyIDCDU:estier    0.14237    0.05405   2.634  0.00849 ** 
    female:PartyIDGREEN:estier -0.11673    0.04167  -2.801  0.00513 ** 
    female:PartyIDSPD:estier    0.01113    0.04574   0.243  0.80781    
    ---
    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
    
    Residual standard error: 0.4232 on 2499 degrees of freedom
    Multiple R-squared:  0.03484,	Adjusted R-squared:  0.03175 
    F-statistic: 11.28 on 8 and 2499 DF,  p-value: 8.445e-16

So clearly the model is a decent approximation of the expectations from the table above. However, the coefficients are not really proportional to what we should expect. For instance, the AfD party should have a more negative coefficient than the Green party, which makes me believe that I'm forming the model incorrectly

I've also tried running the linear model with solely the three-way interaction, and no other predictors:

    lm(elected ~ female:estier:PartyID))

However, as I understand, this is methodologically improper. This model also results in three-way interactions that don't match expectations from the table.

So how can I best estimate the three-way interaction to match the expectations?

Note that I'm not really interested in the independent effect of female, estier or PartyID on election outcome, only on the three-way interaction.

\*\*UPDATE\*\*

I tried the following model.

    lm(elected ~ female * estier * PartyID)

At first I thought this model couldn't be right because the three-way interaction coefficients did not at all match what I expected from my table above. However, I used predict () to double check this and it does in fact result in the correct predicted values, so this is the right model.

Now, my question is, how do I substantively interpret the model's three-way coefficients? I'm not understanding how the three-way coefficient for CDU candidates could possibly be negative. Advice has been very helpful and appreciated so far!

                               Estimate Std. Error t value Pr(>|t|)    
    (Intercept)                 0.41727    0.03414  12.223  < 2e-16 ***
    female                     -0.04227    0.08897  -0.475 0.634782    
    estier                     -0.36236    0.04244  -8.539  < 2e-16 ***
    PartyIDCDU                 -0.26180    0.04297  -6.093 1.28e-09 ***
    PartyIDGREEN               -0.17165    0.04597  -3.734 0.000192 ***
    PartyIDSPD                 -0.16418    0.04653  -3.528 0.000426 ***
    female:estier               0.04000    0.11320   0.353 0.723876    
    female:PartyIDCDU          -0.01642    0.09666  -0.170 0.865106    
    female:PartyIDGREEN         0.08170    0.09808   0.833 0.404941    
    female:PartyIDSPD           0.11155    0.09991   1.117 0.264306    
    estier:PartyIDCDU           0.71792    0.05653  12.701  < 2e-16 ***
    estier:PartyIDGREEN         0.16191    0.06159   2.629 0.008620 ** 
    estier:PartyIDSPD           0.56232    0.06079   9.250  < 2e-16 ***
    female:estier:PartyIDCDU   -0.12246    0.13105  -0.934 0.350169    
    female:estier:PartyIDGREEN -0.06209    0.12918  -0.481 0.630822    
    female:estier:PartyIDSPD   -0.23181    0.13095  -1.770 0.076815 .  

&#x200B;

FINAL UPDATE

Thank you all for your responses, I understand now. The three-way interaction is heavily dependent upon the higher-order estier:PartyID coefficient, which provides most of the predictive power, alongside the other interaction coefficients. In other words, there is not a statistically significant difference between male and female outcomes across electoral tiers within the same political party. 

I suppose the takeaway is that I was trying to substantively interpret the three-way interactions without realizing they are model-specific, dependent upon the values of their constituent sub-interactions. This was probably obvious to many of you but it took me a bit to get on the same page. 

Funnily enough, now the statistical significance of my results has all but evaporated. Seems my discussion section will be dramatically shortened. At least I am doing it correctly lol.","[""> I also excluded the sub-interactions, such as female:estier and female:PartyID\n\nWell that’s the problem, excluding the sub-interaction terms amount to assuming their coefficients are fixed at zero. If this assumption is not true, the rest of coefficients will have to change to account for the misfit. You can find more resources [here](https://stats.stackexchange.com/questions/27724/do-all-interactions-terms-need-their-individual-terms-in-regression-model#27726).\n\n\n> I exclude the 'estier' as an independent predictor variable, as it is only substantively meaningful when included in the interaction term\n\nI’m not sure what you mean by this, since 'estier' is included in your three way interaction."", 'Here is better formatted version of the summary() with only the relevant variables.\n\n|elected|female|Party ID|estier|\n|:-|:-|:-|:-|\n|Min: 0.00|Min: 0.00|Length:2508|Min: 0.00|\n|1st Qu. 0.00|1st Qu. 0.00|Class: Character|1st Qu. 0.00|\n|Med: 0.00|Med: 0.00|Mode: character|Med: 0.00|\n|Mean: 0.245|Mean: 0.391||Mean: 0.475|\n|3rd Qu. 0.00|3rd Qu. 1.00||3rd Qu: 1.00|\n|Max: 1.00|Max: 1.00||Max: 1.00|\n\nTIL that Reddit does not like summary() output tables...', 'The coefficient for three way interaction is by itself not enough to compute the marginal difference between election types, you need to account for the lower level terms as well.\n\nRun the full model (including estier) and then compute difference between:\n\n\n    intercept + female + partyIDSPD + female:PartyIDSPD\n\nAnd\n\n\n    intercept + female + estier + partyIDSPD + female:PartyIDSPD + estier:partyIDSPD + female:estier + female:estier:partyIDSPD \n\n\nYou should see the same result as in the original table (hopefully I didn’t forget about any term…)', "">  I am not interested in the effect of estier itself on electoral chances, only when it is interacted with the other variables.\n\nThat doesn't mean you can exclude it as a lower level variable. Later interactions may not provide a meaningful interpretation of variance if you exclude it."", 'elected is dichotomous?  if so, `lm` is a bad choice of model.\n\nIn regards to `estier`, I understand now the nested nature of it and it is related to electoral tier, which is not included in the model (AFAIK). to have a saturated model were all the possible combinations of female, estier and PartyID have unrestricted means you have to especify a full interaction model: \n\n`lm(elected ~ female*estier*PartyID)`\n\nThis will specify all marginal, second and third order interaction terms. This will allow for any effects that you observed in the data.']",10,15,https://www.reddit.com/r/statistics/comments/131pv08/q_best_way_to_estimate_a_threeway_interaction/
289,2023-04-28 19:43:28,[Q] Figuring out statistical test to use and feasibility of experiment,"I'm an undergrad games design student and doing a project trying to find how different variables affect the relationship between types of game rewards and player experience (an example being players with certain traits might prefer certain reward types).

&#x200B;

First off, I'm creating a test game where I will be able to put the specific reward types I want in (5 reward types in total), for the experiment I'm planning on having the 5 reward types in game and the participants will rate their experience with each on a score from 1-10. These scores on the reward types will be the dependent variables and I'll be looking at how player traits (5 scores for five player traits) effect the reward type scores. I'll also be gathering age and gender to look at how those could effect the scores on the reward types in a different test.

&#x200B;

I'm completely new to statistical analysis so I'm not exactly sure what test to use, I looked at some flowcharts and believe a multiple regression is the correct statistical test for this type of experiment but would like to know if that makes sense.

&#x200B;

If you guys know any good resources for the appropriate test or where to learn more about this sorta thing that would be greatly appreciated, and if you need some more info for the experiment just let me know.","['You need repeated measures MANOVA or MANCOVA to analyze  all DVs in one model. You might also analyze each DV separately and run ANOVAs or OLS regressions.', ""I'll try look into these and see how to use them, appreciate it!""]",0,2,https://www.reddit.com/r/statistics/comments/131pa82/q_figuring_out_statistical_test_to_use_and/
290,2023-04-28 13:06:04,[Q] Opinion on whether to trust correlation results when compared to graphical data,"Hi everyone, I asked a question a few days ago, but seems it didn't have the information needed to be clear enough so have put some graphs together. 

Basically I have an issue where a kendall's tau correlation is giving a highly significant (P=0.001) positive correlation for the dataset even though I cannot see any visual correlation on scatter plots for the raw data or ranked data. Basically this is ""in the wild"" (so not carefully measured lab conditions where I can get even results) data of a pollutant exposure on the x axis, compared to the measured health response on the y axis. (Kendall's tau is being used due to the type of data is is.)

Any suggestions? I'm at a loss as to whether to call this significant or overanalysed. Kinda frustrating as this data took a long time to gather but sort of seems like I can't report it either way? Thanks in advance!

Links for images of the raw data and ranked data plots (sorry I don't think I can attach them directly to this post for some reason.)

https://ibb.co/C8YR0Pb

https://ibb.co/jRn6kNd","[""There's a huge stack of zeros in your first image.  Try doing it again without the zeros, it's probably throwing your calculations off.\n\nSame with the second one.  There are some outliers and your graph is too zoomed out.  The stack of zeros on the x axis probably not helping either."", 'Not sure I’d say “throwing your calculations off.” I’d just say dig into what’s going on a bit more. \n\nAlso a significant result for anything doesn’t necessarily mean it’s a substantive large relationship. I have datasets with 75m observations where basically everything is significant. Doesn’t mean they’re meaningful.', 'You didn’t report the actual tau.\n\nI would never judge a p value by visual means.', 'I agree that the zeros are probably resulting this to happen and should be removed to get a clearer picture. \n\nBut what if in other experiment where the inflated zeros are valid observations and could not be removed? Are there any tools to report or tackle with this phenomenon?', ""There are two methods that I usually use.  I'm an econ guy so these might not apply to your field.\n\n1.  Only use the non zero parts and make inferences that are restricted to those only.  Ex: Only use full time workers and all the results are only valid for full time.\n\n2.  The data is truncated.  Ex: you can only see the wages of people that work.  Use Heckman correction, preferably with a good instrument.""]",12,9,https://www.reddit.com/r/statistics/comments/131i7ko/q_opinion_on_whether_to_trust_correlation_results/
291,2023-04-28 10:22:14,[Q] What do I use to find correlation between a nominal and ordinal data,"So I have 1 dummy variable: ""do you have this disorder?"" yes/no 

and I have socioeconomic status split into low, medium, high

I want to find out if the socioeconomic status correlates with whether or not people have this disorder.

Right now I went for the χ2 test but I'm not sure thats right nor am I sure if there are more test I could do

I can't do whitney u because my dependent variable isn't ordinal

I can't do logistic regressions because I have high multicollinearity among the independent variables

and I cant do odds ratio since its a 2x3 contingency table","['Oh a parsimonious model is essentially just a fancy way of saying “a model with fewer independent variables.” \n\nI think in this case logistic regression still makes the most sense. Those VIF numbers are a little elevated but I’ve seen 10 proposed as a rule of thumb regarding when to be concerned about multicollinearity. I’ve seen 2.5 and 5 proposed as well, but those are quite conservative thresholds. So I think you’re okay! If the smoking status doesn’t improve the model much you could also consider taking it out of the model.', 'What are the other independent variables and how high is the variance inflation factor (VIF) for each? Could you drop a couple of them and find a more parsimonious logistic regression model that fits reasonably well?', 'yes/no can be treated as ordinal\n\n> I can\'t do whitney u because my dependent variable isn\'t ordinal\n\nSure it is. ""Yes"" is further along the ""havingness of the disorder"" scale than ""No"" is.\n\n> I can\'t do logistic regressions because I have high multicollinearity among the independent variables\n\nPlease clarify; you mean the socioeconomic dummies have high multicollinearity? If that was strong enough to make a big difference, none of the analyses you\'re considering would be any good.\n\n> I cant do odds ratio since its a 2x3 contingency table\n\nWhy would that prevent calculating an odds ratio?', 'My recommendation would be to clarify what it is you are asking.\n\nIs the question just about the association between an ordinal variable and a dichotomous variable ?  Or is there some broader question about logistic regression ?\n\nIs one of your variables being treated as the dependent variable ?  If so, which one ? Or are you just looking for the association of two variables, neither one being treated as the dependent variable ?\n\nDo you want to treat the ordinal variable as ordinal ?  Or as a nominal variable ?  (As suggested by your suggestion of a chi-square test of association.)\n\nThere are relatively easy approaches to address any of these situations.  I think some of the confusion stems from not being clear --- to yourself --- about what you are trying to do.', 'It\'s only one more variable: ""smoked during pregnancy"" with yes/sometimes/no\n\nThe VIF is 5.5 for socioeconomic status and 3.6 for smoking\n\nI don\'t know what parsimonious logistic regression models are 😅']",2,17,https://www.reddit.com/r/statistics/comments/131exyh/q_what_do_i_use_to_find_correlation_between_a/
292,2023-04-28 09:05:20,[E] Design Effect,"Hello all,

I am a long time mathematician / engineer but my statistics are somewhat weak. I recently was introduced to ""Design Effect"". Can anyone assist in suggesting (1) Books (2) Papers and/or (3) Videos to educate myself on the subject? Theory or practical applications are both great. Any help would be greatly appreciated. Thanks in advance.","['I\'m not sure what you mean by just the phrase ""design effect"". Do you have some context?', ""Sampling: Design and Analysis by Lohr contains a short introduction. IIRC Lumley's Complex Surveys also discusses it."", 'Barely, but sure. I have serious stats guys who have never heard of it. Here are a couple links, Wikipedia is decent here I think.\n\n[https://en.wikipedia.org/wiki/Design\\_effect](https://en.wikipedia.org/wiki/Design_effect)\n\n[https://www.statisticshowto.com/design-effect/](https://www.statisticshowto.com/design-effect/)\n\n[https://www.sciencedirect.com/topics/mathematics/design-effect](https://www.sciencedirect.com/topics/mathematics/design-effect)']",1,3,https://www.reddit.com/r/statistics/comments/131d9na/e_design_effect/
293,2023-04-28 04:11:19,Modelling the revenue of a startup using a simple Patreo Distribution [Q],"Disclaimer

&#x200B;

\*\*It is generally not safe to assume that revenues for businesses in most industries can be modeled by the Pareto distribution. While the Pareto distribution is commonly used to model the distribution of income and wealth in a population, it is not a universal distribution that can be applied to all types of data.\*\*

&#x200B;

I am a mechanical engineer by trade, and it's been a while since taken a formal stats class, but I thought I'd give this a go!

I want to have a super rough ball park idea about my market penetration should I start a business that begins to perform in the bottom 25% or bottom 40% of this industry in terms of revenue.

&#x200B;

The industry:

&#x200B;

Total Revenue: $444.6 Million

&#x200B;

Number of businesses in my state: 343

&#x200B;

Average revenue: $ 1.296 Million

&#x200B;

Equations:

&#x200B;

f(x; α) = (α / x)\^(α + 1)

&#x200B;

F(x; α) = 1 - (x / x\_min)\^-α

&#x200B;

&#x200B;

&#x200B;

x\_min = $1,294,000

&#x200B;

x\_sum = $445,000,000

&#x200B;

n = 343

&#x200B;

α = 343 \* ($1,294,000 / $445,000,000) ≈ 1.00

&#x200B;

&#x200B;

&#x200B;

Distribution Function

&#x200B;

f(x; 1.00) = (1.00 / x)\^2

&#x200B;

&#x200B;

&#x200B;

Cumulative Probability Function

&#x200B;

F(x; 1.00) = 1 - ($1,294,000 / x)\^-1.00

&#x200B;

&#x200B;

&#x200B;

However,

&#x200B;

I feel like this is wrong because when I plot the x values for revenue, the probability function approaches zero and goes negative after x=$1.294M which intuitively does not seem correct.

There are surely businesses making more than $1M in this industry so my best guess would be that I need a more robust equation to model the maximum values of this distribution, but I am not sure.

What would be a better way to model this?

&#x200B;

Thank you!!","['Awesome! I revised the formula with the type 2 distribution and got $235,000 in revenue for the bottom 35 percentile. I used k as the shape parameter and made the maximum revenue no more than 10 times the average k= 1/(1-(1/M)) where M=10 thus k=1.11. \n\nThanks so much for the advice and guiding me in the right direction.\n\nThis is not a crazy accurate estimation I would assume but it gives me an idea of what to expect in general if I were to start this business or any other business.', ""What you want to use for such things is a generalized pareto distribution rather than a simple pareto distribution. You can also try using the 2 parameter versions of pareto type II distribution (Lomax distribution).\n\nI think Lomax would be the most suitable and robust way to do it. It gives you a scaling parameter on top of the shape parameter (maximum you're using right now). This allows for you to have greater maxima and even lower minima."", ""Your computed F is wrong, that's why it doesn't behave correctly. \n\nIt doesn't have the same form as the general form of the Pareto F you gave above it."", ""I'm glad it worked for you. You can try getting some real world data and adjust max and average accordingly based on line of business for even better approximations.\n\nGood luck on your new business ventures!""]",11,4,https://www.reddit.com/r/statistics/comments/1314xeo/modelling_the_revenue_of_a_startup_using_a_simple/
294,2023-04-28 00:40:36,[Q]Converting data from non-normal to normal distribution,"Hey guys
  
I have data i want to convert it from non normal to normal distribution so i can apply control charts on the data
  
The data is so fucked up , i try every possible way to convert it and it didn't convert.
  
So I tried one last try on a method but I'm not sure about it , the method is by ranking data ascending at first , then apply this formula ( (R-1)/(N-1) ) Where R is the rank number and N is the number of data i have, finally we apply the z score by using the value calculated from the last equation.
  
The big question mark about it is If there are several measurements that you want to convert from the same product, the conversion results for both measurements will be the same, is that even okay? (I hope I explained it in the best way so that you can understand me).
  
Any solution or suggestion will help me.","['OP should post a histogram of said ""fucked up"" data.', ""You mean you did a QQ plot and some normality test? Forget the test, it's a waste of time.\n\nWhat are you measuring?"", ""What is the current (non normal) distribution of your data? From what you are telling us it doesn't even look remotely close. \n\nJust to be clear, the fact that your response variable isn't centered and somewhat symmetric is kind of a good indication that you are NOT in presence of a stationnary (in control) process. \n\nMaybe the control charts (in the classical sense) aren't the best solution here. Can you find the source of these erratic variations?"", ""I don't see how rank transformation will help you actually monitor actual control the process. Indeed I'm not at all convinced that any transformation will be much use for that.\n\nWhat do you mean by:\n\n> data is so fucked up\n\nexactly"", ""One way if you can try to come up with sums of random data's you might trigger central limit theorem and data might converge to normal.\n\nTransformation of one distribution to another distribution might be meaningless and misleading.\n\nUse R or Matlab to fit the dist with Normal and notice the RMSE or IMSE to determine the goodness of fit.""]",10,11,https://www.reddit.com/r/statistics/comments/130v794/qconverting_data_from_nonnormal_to_normal/
295,2023-04-27 23:14:40,[Q] What would be the ideal way to represent how much someone has progressed with their workout and their strength using percentages?," I  don't know if I am in the correct subreddit for this but I have a  question about how to calculate how much someone has improved their  strength over a period of a year.

I  am making a personal project website where users can input their  strongest lifts for a particular exercise and at the end get stats on  their strength increase in %.

So I  was wondering how do I calculate the strength increase? My current  process is that I get the first lift of the user and I grab the last  lift the user has made. I ignore the strongest lift because for example,  it could go like this:

January: 10kg

August: 75kg

October: 50kg

December: 60kg

And  in the above example, while the strongest lift is 75kg, the user can no  longer lift that weight therefore their progress has dropped. But if I  use the strongest and weakest lift to calculate the progress in %, then  it would be like the user has not experienced any loss of strength.

Here is my formula for how I get the %

    improvement = ((max - min) / min) * 100 

Sorry if this is the wrong sub for this.","['Are they only lifting one weight per month? Or do the months represent the strongest weight lifted that month?', 'It is the strongest weight lifted per each month. \n\nI am planning on including weekly and daily statistics later on but for now I am working with the users PB for each month in the span of a year.', 'You may want to use a moving average to start, try taking an average of last two months and you won’t have to eliminate your maximum values. In your example the averages would be 42.5kg, 62.5kg, and 55kg. This may or may not help, but I hope I’ve assisted in getting you closer :)']",2,3,https://www.reddit.com/r/statistics/comments/130qpr0/q_what_would_be_the_ideal_way_to_represent_how/
296,2023-04-27 20:53:03,[Q] time intervals in regressions,"Hey gus,

must all variables have the time interval within a model? I have calculated the rolling 30 day volatility for Bitcoin, Gold and the US-Dollar. I want to include them to a model for the bitcoin volatiltiy but I also want to include other variables which may not be in dayli frequency like money supply or main refinancing interest rate. Model would look similar to this

    btc_vola = b0 + b1 * gold_vola + b2 * dollar_vola + b3 * money_supply + .... + bn * interest + ui 

So how do I handle different frequencies. Thanks guys. :)","[""Well you can't have NAs for your money supply variable, so you'll have to do something to fill in the blanks. You could use last observation carried forward or some kind of smoothing technique like an average between the two most recent observations.\n\nAlternatively, you could rollup the daily variables to whatever frequency you have for money supply. That's what I would do."", 'Specific recommendations are going to on specific details of each variable you are interested in. What time periods are they measured on, are they co-occurent with other frequencies of variables you are interested in, are they measure me in the same frequencies for each of the time series, etc.\n\nThe measurement properties of each as well as your goals predictive and inferential goals will matter as well. Are you trying to predict future volatility, or do you want to assess impact of policies on volatility. For the measurement properties, are the variables measured at specific times or aggregated over time? Are they controlled and change at discrete times, or are they reported at the end of measurement periods?\n\n\nAppropriate modeling strategies are going to depend on answers to each of these questions for each variable separately.', 'As I understand it if I do a simple multivariate-linear OLS model I am looking at the impact of independent variables on the dependent variable. So the question I want to answer is what drives the volatility.\n\nIf I want to predict the volatility which models are used here?']",4,3,https://www.reddit.com/r/statistics/comments/130l7vb/q_time_intervals_in_regressions/
297,2023-04-27 16:52:13,[Q] Interrater-reliability for binary variable?,"Hey,

&#x200B;

I wanted to calculate cohen's kappa for binary variables (0= no, behavior is not shown, 1= yes behavior is shown). The issue is that for one variable, one rater always stated ""0"", which makes the variable a constant. Thus, I cannot calculate cohen's kappa, SPSS at least tell me so. I mean it's understandable given that there is no variation. What could I do instead?

Thanks!","[""Thank you so much, I could easily implement it in R. I struggle a little bit with the explanation. I study psychology so nobody is expecting that I will understand it to the fullest. However, I should at least understand the argumentation of people who know more than me and cite them (e.g. [https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-13-61](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-13-61))\n\nSo, is it impossible to calculate Cohens kappa in my case or is it an SPSS thing? How can I justify taking Gwet instead of cohen in my case? As far as I understand it, in my case, due to the high prevalence of one outcome, kohen's cappa is problematic (Kappa paradox). One advantage of Gwet is that it's more robust when change in prevalence happens."", '[Here](https://stats.stackexchange.com/questions/523605/unable-to-calculate-kappa-or-weighted-kappa-when-1-raters-give-the-same-rating) and [here](https://stats.stackexchange.com/questions/29717/inter-rater-statistic-for-skewed-rankings) are some posts about your problem (frequently called the ""prevalence problem"").\n\nIn short, I recommend using Gwet\'s AC1 instead instead. This is not available in SPSS but very easily calculated in R using the `irrCAC` package.', 'I recommend checking out rogers tanimoto dissimilarity with the weights arranged according to the variance of all population per stimulus.', 'I think you need to back up a step. Statistics is for making sense of things in the presence of variability. These data don\'t have that.\n\nMore concerning, it means your judges disagree on 100% of the ""Present"" observations and presumably some fraction of the ""absent"" observations. Even after getting a coefficient, how do you interpret it?', 'At the end of the day, all agreement statistics depend on variability between and within raters. If you have any rater which does not have any variability in their own scores (constant scores), then they effectively provide no information and this can cause estimation problems.']",8,12,https://www.reddit.com/r/statistics/comments/130ena3/q_interraterreliability_for_binary_variable/
298,2023-04-27 16:06:14,[Q] Probability of Blocks,"There are 4 red, 4 blue & 4 white blocks that are placed randomly in 3 boxes of 4 blocks each. The probability that at least 2 of the boxes receive identical collection of blocks is x/y where x and y are relatively prime positive integers. Find x+y","['We can start by counting the total number of ways the blocks can be distributed among the boxes. Since there are 12 blocks and 4 in each box, there are a total of (12 choose 4) \\* (8 choose 4) = 495 \\* 70 = 34650 ways to distribute the blocks.  \nNow, we need to count the number of ways in which no box has an identical collection of blocks as another box. We can do this by considering the number of ways to distribute the blocks such that no two boxes have the same collection of blocks.  \nThe first box can have any of the 34650 combinations of blocks. The second box can have any of the remaining 8 combinations that do not match the first box. Finally, the third box can have any of the remaining 4 combinations that do not match either of the first two boxes. Thus, there are a total of 34650 \\* 8 \\* 4 = 1108800 ways to distribute the blocks such that no two boxes have the same collection of blocks.  \nTherefore, the probability that at least 2 of the boxes receive identical collection of blocks is 1 - (1108800/34650) = 31/165.  \nThe sum of x+y is 31+165=196. Hence, the answer is 196.', 'Thanks a lottt']",0,2,https://www.reddit.com/r/statistics/comments/130du61/q_probability_of_blocks/
299,2023-04-27 15:04:38,[R]Facing the Unknown Unknowns of Data Analysis,"[https://journals.sagepub.com/doi/full/10.1177/09637214231168565](https://journals.sagepub.com/doi/full/10.1177/09637214231168565)

Abstract

Empirical claims are inevitably associated with uncertainty, and a major goal of data analysis is therefore to quantify that uncertainty. Recent work has revealed that most uncertainty may lie not in what is usually reported (e.g., p value, confidence interval, or Bayes factor) but in what is left unreported (e.g., how the experiment was designed, whether the conclusion is robust under plausible alternative analysis protocols, and how credible the authors believe their hypothesis to be). This suggests that the rigorous evaluation of an empirical claim involves an assessment of the entire empirical cycle and that scientific progress benefits from radical transparency in planning, data management, inference, and reporting. We summarize recent methodological developments in this area and conclude that the focus on a single statistical analysis is myopic. Sound statistical analysis is important, but social scientists may gain more insight by taking a broad view on uncertainty and by working to reduce the “unknown unknowns” that still plague reporting practice.","[""This is what I love about modeling for financial markets (specifically options). The immediate feedback lets you know very quickly whether your model is good or not. The quality and speed of the fresh out-of-sample data is wonderful for avoiding the absolute tar pit of trying to figure out if your model (or someone else's model) is valid."", 'Physicists have known this forever.  They try to quantify both random error (for which statistics helps) and systematic error (for which statistics is no help, you have to use theory).  So these social scientists are trying to get a clue about something physicists have been doing for centuries.  Maybe they should look at how the physicists do it (although, of course, there will have to be changes to work in social science).', 'This isn’t new even in the context of social sciences, e.g. Gelman has been going on about garden of forking paths for like ten years by now. Most people just don’t care.', 'No. ""Garden of forking paths"" is data snooping, which statisticians were woofing about before Gelman was born.  Systematic error is completely different, something statisticians (including Gelman) have nothing to say about.  Systematic error is the error that statistics says nothing about.']",24,4,https://www.reddit.com/r/statistics/comments/130craf/rfacing_the_unknown_unknowns_of_data_analysis/
300,2023-04-27 14:08:46,[Q] Regression variable interpretation (Excel),"I know the rules say no homework, but this is more of a comprehension issue. Not looking for answers to homework but comprehension on regression.

I am doing a university paper. I am comparing the satisfaction of two types of persons in excel (required). A) people who use banks and b) those who use credit unions. I have been directed to use regression and therefore using the number (1) for banks and (2) for credit unions. However I am unable to discover which population is responsible for any of my results. Should I sort them, even if I am directed to put them in the same column?","['In fact, it does not matter. Coding 0-1 or 1-2 are both fine as long as there is a single unit of difference. The only thing it will change is the interpretation of the intercept, as it will not be interpetable with the 1-2 coding. The same stands for interactions and conditional simple effects.', 'I would start by computing the means of both groups since that will tell you the direction of the effect. Regression is fine but a t-test is simpler (and identical). Moreover, if you have heterogeneous variance in the t-test analysis you can use the Welch test but the solution with heteroscedasticity in regression is more complex.', 'Thank you for your response, I did that and got the same results, I believe 1 and 2 are dummy variables in the regression.  Thanks for your help anyway', 'You should be using a dummy variable, not coding them as 1 and 2. Look at your notes again.']",1,4,https://www.reddit.com/r/statistics/comments/130bq89/q_regression_variable_interpretation_excel/
301,2023-04-27 14:02:19,[D] Handling Drastic One-Time Increase/Decrease In Sales Data,,"['It depends on why it happened and what you are using the data for. There isn’t a single answer to that. You might treat 1,1,1,1,5,5,5,5 differently than 1,1,1,1,5,1,1,1.']",6,1,/r/MachineLearning/comments/1301gwq/d_handling_drastic_increasedecrease_in_sales/
302,2023-04-27 11:51:33,[Q] Universal Significance of the Coefficients in the Simple Linear Regression,"  
Hello everyone. I have somewhat of a silly question about when we can actually say that our independent variables are significant. 

&#x200B;

I want to study whether the effect of Western financial aid on GDP growth is significant in the Gambia. I ran a regression with GDP as the dependent variable, Net financial aid received as the independent, and a couple of other independent variables for control. Suppose I get an insignificant p-value for the Net financial aid.

&#x200B;

My question is — can we truly say that financial aid is irrelevant to growth? I feel that it is a very simple model, and we cannot make such strong statements by doing a simple linear regression. There are hundreds of papers written studying the effects of financial aid on developing countries. If linear regression was sufficient, there would not be so much controversy over the topic. 

&#x200B;

I know about natural experiments in economics. I understand that comparing similar regions but that differ in a particular effect can make our results more rigorous and say confidently if the effect is significant. But then, when would having a linear regression be enough to say universally whether the effect is significant or insignificant?","[""1. Linear regression can only measure linear relationships (or the absence of such relationships). That is very far from declaring overall independence.\n\n2. Failing to reject the null (i.e. insignificant p-value) does not mean that the null is true. It simply means we cannot definitely determine that the alternative is true. This could mean that the null is true, or that our test is under-powered, or that our test is poorly specified (i.e. assumptions aren't met), or simply that we got unlucky with a non-representative sample (which happens a nonzero amount, to put it simply)."", 'Maybe to understand it best, you just need to back up to the first assumption of any statistical test: ""My model is correct.""\n\nIf you apply an incorrect model, all bets are off for the parameters.', 'Yes, the regression is linear in its parameters. This means that it can only tell you about the linear relationship between its inputs & output, allowing that those inputs can be nonlinear transformations of other inputs. But once the regression is constructed, it\'s only describing the linear relationship of those specific inputs that you gave it. In the context of OPs question, you can\'t look at a regression and conclude ""no general relationship exists here between these variables""; you can only say at most that ""there is no linear pattern from these inputs to the output"". You are of course free to apply nonlinear transformations and reattempt.', 'Differencing will likely make it stationary.', ""Not OP, but yes linear regression models linear sums of the parameters provided. You won't detect multiplicative effects between parameters, for example. If you want to find things like splines or power associations of your original variables etc, you need to explicitly transform your original data to create those parameters (or use other approaches like model selection to spit out transforms and find the best fit) - the model won't magic them up for you.""]",10,8,https://www.reddit.com/r/statistics/comments/130904r/q_universal_significance_of_the_coefficients_in/
303,2023-04-27 10:49:24,[E] Need help on Adjusted R-squared decreasing,"Hi peepos, so I am working on my assignment which requires us to conduct regressions to identify independent variables affecting the dependent variables (exchange rate). I opt to use backward elimination with a confidence level of 95%, but there was a problem: After I eliminate the highest p-value variable (the largest p-value and also larger than the threshold of 0.05), the regression after that has a lower adjusted R-squared.   
I know that adjusted R-squared decreasing is kind of a signal that the accuracy of the regression is low, but I don't understand why. Can anyone explain it to me? Thanks!","[""> I know that adjusted R-squared decreasing is kind of a signal that the accuracy of the regression is low\n\nI don't think that's necessarily true at all."", 'When you get rid of a covariate, the R-squared will almost always decrease, even if that covariate is not significant. This is because the information provided by that covariate improved the fit to the training data (even if the improvement was only marginal).\n\nThe “accuracy” of the regression model, measured by R-squared, is often subjective. In some fields, an R-squared of 0.3 is considered great, while for other problems an R-squared of 0.8 is terrible. Hopefully this isn’t the only metric of model fit that you are using.']",5,2,https://www.reddit.com/r/statistics/comments/1307oxy/e_need_help_on_adjusted_rsquared_decreasing/
304,2023-04-27 05:28:28,[Q] what data do I need in order to set up a linear regression analysis on minitab?,"I'm not sure whether this question is very simple, or more complicated than I understand. For my statistics class I need to input data on minitab and do a linear regression for it. However, my issue is that I don't understand what data I need to use. I would like to do something in regards to level of education and salary earned, but I just don't know what data I need. I'm sorry if this question is either overly complicated or overly simple","['Thank you so much! I think I realized with the answers, but all I needed was a way to quantify education level. I didn’t even think of just using number of years, I was thinking of the type of degree earned. Thank you so much again!', 'One column of data with eduction level (n years?) and another with salary ($ per year)?\n\nThis looks like they have data\nhttps://www.kaggle.com/datasets/codebreaker619/salary-data-with-age-and-experience', '> what data do I need in order to set up a linear regression analysis on minitab?\n\nAt least two columns of data (pairs of values).\n\n> I would like to do something in regards to level of education and salary earned\n\nThen you would need to think about how you\'re measuring level of education and salary, and collect pairs of values (one of each number) from some set of subjects. That is, you need to collect both numbers from every subject.\n\nI\'d suggest using *years of education* instead, as it\'s more suited to a simple regression. ""Level of education"" is at best ordinal which will complicate things somewhat and complication is not something you should seek at this stage of understanding.\n\n(You might also consider whether the relationship might be nonlinear.)\n\nAlso consider that your subjects might consider this information rather personal, and may be disinclined to answer one or the other question (and may be less inclined for particular values than others; e.g. someone may not want to admit a low level of education). This non-random missingness is going to *bias your estimates*.\n\n(You would also need to worry about whether you\'re really getting some random sample from some distribution of interest, and if you\'re not, what the basis for inference would be, if you\'re doing any tests or confidence intervals.)', 'This makes perfect sense and explained everything! I didn’t realize the only thing I really needed was a way to quantify the level of education with a number that I could calculate with. This explanation is perfect, thank you so much!', 'No worries!']",1,5,https://www.reddit.com/r/statistics/comments/13008os/q_what_data_do_i_need_in_order_to_set_up_a_linear/
305,2023-04-27 04:22:56,[Q] Hello I have a problem where I have to determine the sample size with some given info.,"Hello, Iam pretty new to statistics and I have a problem. I have a mean of 297.5645, and I know that the width of the interval is maximum 5 and that the standard deviation squared is 200. Iam asked for a sample size needed to have the parametrs of the maximum width and the deviation. I am supposed to run a two-tail test with 99% confidence. I have found a formula that incorporates the width and deviation and got back 213 when I devided the width by 2 since it is a two-tailed test. Have I performed this correctly or does something change due to me knowing the mean? Tank you very much.",[],0,0,https://www.reddit.com/r/statistics/comments/12zyktv/q_hello_i_have_a_problem_where_i_have_to/
306,2023-04-27 03:12:36,[Q] A priori G*power analysis for Mixed Anova Design,"For my research class we have to conduct a study. 
Our basic design: 
Two groups (treatment and control group), both have to fill out a questionnaire=pretest (t01), then the treatment group does their treatment/control group does nothing for a week, after that period of time both have to fill out the same questionnaire=posttest(t02). We assume that the treatment has some effect that lead to higher test scores in t02. 

To determine the sizes of both groups I need a priori power analysis, but I’m a bit lost when it comes to ANOVAS and what those input parameters even mean. 

I assume it’s a mixed Anova design. 

I know our hypothesis is one tailed, alpha level at 0,05 and power at 0,8. Possible effect at may be 0,3 (middle sized). 

Can someone explain the other parameters to me or guide me to any help?","['Boring Bayesian answer, just do a large number of simulations of the experiment at different sizes, either assuming one true effect size or a prior distribution of potential effect sizes and record the operating characteristics. Plot the relationship between size and operating characteristics and choose the smallest size that gets you acceptable results and acceptable number of times.\n\nI’m assuming someone will come along with a better, closed form solution.', 'Chapter 13 of Doing Bayesian data Analysis is good if you do want to go this route.', 'Your analysis is equal to an independent t test on posttest - pre test scores. It might be easier to view it that way for your power analysis. You could also do ANCOVA with pretest as the covariate but that might be a little too complicated for your research class.', 'This guide is really helpful: https://scholar.google.de/scholar_url?url=https://files.osf.io/v1/resources/pcfvj/providers/osfstorage/5dcea84d5b97bd000e57aba0%3Faction%3Ddownload%26version%3D1%26direct&hl=de&sa=X&ei=R7FKZJznBLeNy9YP4P6AwAg&scisig=AJ9-iYsL05m8bSyEtZ-YIMS2fnpC&oi=scholarr  \n\nAnd here is the app they recommend for simulation: https://shiny.ieis.tue.nl/anova_power/ \nThey recommend that on the basis that G*Power underestimates sample sizes for interactions (i.e. within-between interaction, or the interaction between group and time). Does your course require you to use G*Power? If so, regarding the other parameters I would probably leave non-sphericity correction at 1 (meaning not corrected for sphericity) and put in 2 groups, 2 measurements.']",1,4,https://www.reddit.com/r/statistics/comments/12zvicm/q_a_priori_gpower_analysis_for_mixed_anova_design/
307,2023-04-27 02:57:28,[Q] Two-Way ANOVA results make no logical sense?,"I have a data set that is a time series with very different means. i.e. we have a drug concentration measured at different timepoints. The two-Way ANOVA claims only differences between drug concentrations at the last timepoint are significant, but the results look something like this:

[https://imgur.com/NGTrfFg](https://imgur.com/NGTrfFg)

Is there a more appropriate test for this than two-way ANOVA? I just cannot see how 100-fold difference with that small error bars can be not significant in the second group of columns.","['An ANOVA is not really just a set of pairwise comparisons. A two way ANOVA: drug(2) x time (3) with repeated measures on the latter factor is the most common way to analyze this design. Incidentally, repeated measures ANOVA is a special case of a mixed model because “subjects” is a random effect whereas the others are fixed. Repeated-measures designs are designed to allow the within-subject levels to be correlated.', 'If I understood correctly, u are interested in how the drug concentration changes over time in serum vs normal culture? \n\nIf you have repeated measures, you cannot use two-way anova cause your drug concentration measurements have to be independent.', 'There is also a distinction between repeated measures ANOVA and mixed ANOVA; conventional repeated measures ANOVA has no between-subject effects, so you can’t use it to analyze two independent groups like here with serum 1 vs 2.', ""Yes, but isn't it surprising that this effect size doesn't get significance? How come the middle two bars are not significantly higher than the leftwise two?"", ""That is correct. The concentrations  are not independent - the drug concentrations will go up over time. What's the alternative test when I have two variables - time which is related to drug concentrations (Data 1, 2, 3 - timepoints) and status - serum starved vs non-starved?""]",1,12,https://www.reddit.com/r/statistics/comments/12zumtl/q_twoway_anova_results_make_no_logical_sense/
308,2023-04-27 02:28:20,[Q] Finding a paper/study that used a parametric test when it should have used a non parametric," I'm looking for something to illustrate the title of this post, and I can find many papers with mistakes and errors however finding something specifically that incorrectly used a parametric test when it should have used a non parametric has been much more difficult.

Have you guys found anything similar to this before?","['How would you determine when a parametric test should not be used?', '[deleted]', ""In practice, all or almost all statistical models are wrong,  in the sense that they're not exactly true. So any model, parametric or nonparametric is an approximation. \n\nThere may be instances where the inaccuracy of parametric assumptions were leading to an  inaccurate significance level (not the alpha you wanted, which also impacts  p values) or to low power. In this case a different set of assumptions would have been more suitable but that doesn't automatically imply that they should have been nonparametric.\n\nI wonder if you're conflating parametric with 'assumes normality' there."", 'How would that distinguish between the need for a different parametric assumption and no parametric  assumption?', '[deleted]']",2,5,https://www.reddit.com/r/statistics/comments/12zstal/q_finding_a_paperstudy_that_used_a_parametric/
309,2023-04-27 02:09:59,"[D] Bonferroni corrections/adjustments. A must have statistical method or at best, unnecessary and, at worst, deleterious to sound statistical inference?","I wanted to start a discussion about what people here think about the use of Bonferroni corrections.

Looking to the literature. Perneger, (1998) provides part of the title with his statement that ""Bonferroni adjustments are, at best, unnecessary and, at worst, deleterious to sound statistical inference.""

A more balanced opinion comes from Rothman (1990) who states that ""A policy of not making adjustments for multiple comparisons is preferable because it will lead to fewer errors of interpretation when the data under evaluation are not random numbers but actual observations on nature."" aka sure mathematically Bonferroni corrections make sense but that does not apply to the real world. 

Armstrong (2014) looked at the use of Bonferroni corrections in Ophthalmic and Physiological Optics ( I know these are not true statisticians don't kill me. Give me better literature) but he found in this field most people don't use Bonferroni corrections critically and basically just use it because that's the thing that you do. Therefore they don't account for the increased risk of type 2 errors. Even when it was used critically, some authors looked at both the corrected and non corrected results which just complicated the interpretation of results. He states that when doing an exploratory study it is unwise to use Bonferroni corrections because of that increased risk of type 2 errors. 


So what do y'all think? Should you avoid using Bonferroni corrections because they are so conservative and increase type 2 errors or is it vital that you use them in every single analysis with more than two T-tests in it because of the risk of type 1 errors? 


-----------

Perneger, T. V. (1998). What's wrong with Bonferroni adjustments. Bmj, 316(7139), 1236-1238.

Rothman, K. J. (1990). No adjustments are needed for multiple comparisons. Epidemiology, 43-46.

Armstrong, R. A. (2014). When to use the B onferroni correction. Ophthalmic and Physiological Optics, 34(5), 502-508.","['A simple reason why the Bonferroni method should never be used is that the Holm-Bonferroni method is uniformly more powerful.\n\nThe question whether to use multiplicity adjustments at all or when to use them is a different question that has nothing to do with any specific method.', 'https://xkcd.com/882/', '>Not really adding much discussion to the discussion \n\nIt does, actually.', ""There are lots of alternatives to Bonferroni -- Bonferroni's main merit is being very simple, easy to calculate and easy to explain to non-technical people.\n\nBut doing *something* that lets you control the overall error rate of a batch of tests seems essential. There are plenty of people who don't like hypothesis tests at all; but I think there are very few people who believe in hypothesis testing, but don't believe in adjusting for multiple comparisons.\n\n(In my own work I have most often used Scheffe, rather than Bonferroni - with the idea of covering every possible combination of groups in one shot, rather than asking which particular combinations you want to test.)"", '""Sure mathematically, but does it make sense in the real world?""  and\n""Doesn\'t account for type 2 errors""...\n\nWhy bother with statistical inference at all?\n\nThe point is that since Neyman and Pearson, our attempt at discerning signal from noise is to choose statistical procedures to optimize power at a given threshold for science...e.g. alpha. All of this is designed for one test. If we do multiple tests, we have to do something about our method of protected inference (control type I errors in some way)  while doing something about power.\n\nBonferroni or Bonferrni-Holm protects the Family Wise error rate, e.g., prob that there is one of more false positives. Under independent test statistics, this probability is exactly alpha, otherwise its bounded by alpha. What about power? Average power, e.g., the one test power function at alpha/m, is the expected proportion of true signal tests  declared significant, e.g., true positive rate. So these two concepts, one type I-ish and the other, type II-ish are but one set of choices. There are many more. For multiple test power in a given circumstance we may wish to fix the probability that we declare a given portion or better  of the true signals significant, so called TPX power, and for controlled inference we may be happy to instead control the expected false positive rate (Benjamini Hochberg procedure), or we may prefer to control the probability that the false positive proportion exceeds a given value is kept small, so called FDX control (Lehman Romano procedure, or the procedure in my forthcoming publication). It\'s just foolish not to do anything. Anyone who says otherwise is praying that enough false positives equal tenure.  Have a look at my R package pwrFDR. Nice shiny preview at\n\nhttps://www.izmirlian.net/shiny-apps/pwrFDR/']",45,47,https://www.reddit.com/r/statistics/comments/12zs90h/d_bonferroni_correctionsadjustments_a_must_have/
310,2023-04-27 01:59:20,[Q] Annualized data vs 3m average for turnover,"Sorry if this is the wrong sub. Im trying to figure out the best calculation for employee turnover. Right now we’ve got an annualized turnover (avg # terms ytd)*12 / avg staff 

The problem with annualized is any operational changes aren’t really captured in the annualized calculation. 

The other metrics Im displaying next to turnover are all 3 month averages and I’d like to do a “3 month average” of turnover.

I don’t know how to calculate that in a way that makes sense.",['What do you mean by operational changes ?'],5,1,https://www.reddit.com/r/statistics/comments/12zrygf/q_annualized_data_vs_3m_average_for_turnover/
311,2023-04-27 01:44:09,[Q] Can a statistic that is not complete achieve the CRLB?,"Can a statistic that is not complete have a variance that achieves the Cramer-Rao Lower bound?

eg: if X\_i are iid samples from N(0, sigma\^2), the sufficient statistic T = sum(X\_i\^2)\^(1/2) is not complete by Basu's theorem. Will its variance achieve the CRLB?",[],3,0,https://www.reddit.com/r/statistics/comments/12zrjww/q_can_a_statistic_that_is_not_complete_achieve/
312,2023-04-27 00:53:25,[Q] Cronbach's alpha,wDoes anyone know if there is a way of converting Cronbach's alpha into a Pearson's correlation? I am interested in the reliability of a task but some people only report Cronbach's alpha for split-half reliability whilst I need the Pearson's correlation between odd and even trials.,"['[just going to leave this here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2792363/)', ""AFAIK no. Cronbach's alpha can be seen as the mean of all possible split test reliabilities (if the number of items is even) and there is no way to extract result for a specific split."", 'Thanks so much!', 'Thank you!']",1,4,https://www.reddit.com/r/statistics/comments/12zq559/q_cronbachs_alpha/
313,2023-04-26 23:01:56,Which statistical test would be most appropriate? [Q],I am trying to figure out if there is a positive correlation between breast milk intake (continuous variable) and whether the country is assumed to be developing or not (categorical). I also have age which effects milk intake but not the development status. I've been trying to figure out what to do for the last couple of days so I'd really appreciate the help!,"[""I'd initially explore ANCOVA test and check if your dataset follows its assumptions. Milk intake is your response, development is your factor and age is your covariate."", ""I'd be worried about autocorrelation between age and breast milk intake. A country that has high breast milk intake when the babies are 3 months will also likely have a high rate at 6 months. This can cause problems. I'd choose one age and use that. Then check to see if your results meaningfully differ for different ages. I'd be somewhat surprised if they did. If your results do change based on which age you examine, then your analysis gets more complex.\n\nVisually, you can take the average breast milk intake for all developing countries at each age. Then the same for developed countries. Then plot a line graph showing the average breast milk intake as age change using one line for each type of country (developing/developed)."", 'Piggy backing off this:\n\n`model <- lm(milk_intake ~ country_status + age, data = your_data_frame)`  \n`summary(model)`\n\nI also suggest you explore a possible interaction between `country_status` and `age`. You can do that by changing the `+` to a `*`. Plot your data. And as Nirvana5b says, check if assumptions hold by running diagnostics on your residuals.', 'I second this, but I’d not include age in the model since it’s not associated with development.', 'A point biserial correlation is what you want.']",2,5,https://www.reddit.com/r/statistics/comments/12zk5ec/which_statistical_test_would_be_most_appropriate_q/
314,2023-04-26 22:33:17,[Q] How to make non-stationary Time Series data stationary?,"I am currently working on building Time Series forecasting model to predict quarterly population volume of Ontario ([https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901](https://www150.statcan.gc.ca/t1/tbl1/en/cv.action?pid=1710000901)) based on the last 22+ years. The issue is that around in the last 10 quarters, the data becomes non-stationary. I took several differences, log transforms, and lags but non-stationarity in the most recent quarters still exists.

Thanks","['How do you arrive at the conclusion that the data is non-stationnary? If you used a unit root test, it’s pretty much worthless with such a low observation number.', 'Differencing should remove any trend. Look in to garch models, which allow for non-constant variance', 'You can try fitting a Fourier series to the data and specify a few important bases after doing an FFT on the data. Once you get a good fit, you can subtract the series from the original signal and train your model on the residual. \n\nThen when you are generating samples, you can add back in the Fourier series.', 'Yes, for my use cases I am often working with some kind of physical stochastic data like energy demand or wind speed.', ""Seems like you are dealing with a break. I don't know the specific question of your research but you can always add a dummy =1 for t>T-10 and 0 otherwise""]",30,16,https://www.reddit.com/r/statistics/comments/12zjdzv/q_how_to_make_nonstationary_time_series_data/
315,2023-04-26 22:08:07,[Q] AMOS 24 (Badly Need Help): Where do I find Cronbach's Alpha and AVE in AMOS 24? I also need the P Values of Direct and Indirect Effects,"I've been trying to look for these values but I cannot seem to find one, or maybe I just dont know what its name is in AMOS. Please help. Thank you so much.",[],0,0,https://www.reddit.com/r/statistics/comments/12zippp/q_amos_24_badly_need_help_where_do_i_find/
316,2023-04-26 05:49:58,[Q] Do highly correlated variables affect feature importance in gradient boosted trees,"I want to ask this because I have not found very convincing / conclusive answers on the web. 

The question is: do highly correlated variables affect feature importance in gradient boosted (small) trees. And why. 

I know prediction accuracy measures don't get affected as much. 

I suppose we can limit the discussion to the implementations from sklearn and xgb.","['Yes they do. The simple answer is that when two variables are highly correlated no method can reliably distinguish between the influence of one vs the other. The information to distinguish between them simply is not there. This is only made worse by having highly flexible models like xgboost that can model complicated nonlinearities and accommodate for one or the other.\n\nThink of the extreme example where the two are perfectly correlated. Adding one or the other makes absolutely no difference as they can substitute directly for each other.\n\n\nYou didn’t ask this, but this is also relevant to the common question of whether or not collinearity/correlation matters for prediction. The answer is yes, especially if you need to predict at all far from the support of your data. This is because of the same underlying problem, the model may misattribute the effects of one variable to another. If the effects are different in the regions of interpolation/extrapolation the predictions will be bad.', 'What measure of feature importance are you using? Information gain/entropy? Contribution to variance? \n\nIs your response binary?', ""You explained this without using any new vocabulary words I'd have to look up and it all made sense. Thank you!"", 'For most feature importance measures I expect that it will make the features each considered less important on average, but also be more variable. E.g. features may be rated as highly important, more so than they actually are, due to random variability. To what degree probably depends on which measure you use, but it can be said that the feature importances are simply less reliable in these case.\n\n\nFor that answer, I believe they are addressing that from a mode fitting perspective. For linear models for example severe collinearity can pose an issue in that software may not converge, provide errors, and you simply don’t get a reasonable model, or sometime any model, at all. Tree based models are immune to this in that the algorithms will happily chug along and fit the model in spite of the correlation. The correlation is still an issue when you consider performance of the models though.\n\n\nFor your last question, yes the exact way it is handled can be implementation specific and may depend on tolerances, order of variables in the data, or even be random.', ""Thanks for the answer. I agree.\n\nThe part about prediction is really good. I haven't thought about it at all.""]",8,12,https://www.reddit.com/r/statistics/comments/12yx9or/q_do_highly_correlated_variables_affect_feature/
317,2023-04-26 05:11:03,[R] Is it still fixed effects IV to lag the independent variable?,"Hi everyone,

Hoping to get some advice at an undergraduate level. Working on an observational study using panel data - it's a development econ project.

Had a sit-down chat with my supervisor today where he told me I was doing the fixed effects instrumental variable (FE IV) method wrong as I wasn't lagging my dependent variable but actually my independent variable.

I've tried to do some reading on it and it seems that in summary, you should only lag your dependent variable if you believe the current value is heavily determined by its past value. I think this may be true in my case BUT I also think I was doing the right thing by lagging my main independent variable.

I hypothesised that there's an information lag effect between my dependent and independent variables. Essentially, economic agents are not responding to a situation contemporaneously, they are using past information to inform their current decisions. Therefore, any predicted values for the dependent variable would be reliant on the observed values of the independent variables from the past period. This would essentially be dealing with a reverse causality concern discussed in some political economy papers.

My questions then are -

1. Is it doing FE IV wrong to not use the lagged dependent variable as the instrument?
2. How can I include both the lagged versions of the dependent and independent variables in my model specification? Would I have to treat them as separate changes to my methodological approach or can I group together?

I hope I've asked these questions clearly enough but I can definitely clarify if not. Thanks in advance.","['Wait, can you say more. What’s the newest research?\n\nEdit: \n[Holy shit.](https://web.mit.edu/insong/www/pdf/FEmatch-twoway.pdf)', 'Is your prof up to date with the literature? We aren’t doing instruments. And two way fixed effects have a host of problems.', 'Undergrad dissertation generally for internal grading only so I think they’re less concerned with it being up to date with the literature for the most part.']",5,3,https://www.reddit.com/r/statistics/comments/12yw7fa/r_is_it_still_fixed_effects_iv_to_lag_the/
318,2023-04-26 05:08:47,[Q] What statistical tests could I use to measure the similarity between rankings of two lists?," Say I have two lists, List A and List B, and both lists have 100 variables that are ranked 1-100. The variables are NOT quantitative. The two lists are independent of each other. What tests are available to measure the similarity between the rankings of the two lists?","['Spearman rank correlation is probably the thing to google my dude.', 'Wilcoxon too', 'What might be helpful for your decision is reading through these guidelines.   \n[https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf](https://www.jmlr.org/papers/volume7/demsar06a/demsar06a.pdf)  \nI know the application in this case is the ""Comparisons of Classifiers  \nover Multiple Data Sets"" but you should easily be able to transfer the tests and methods described there to your problem.', 'Check out the Hubert-Arabie adjusted Rand Index. I think it’ll work for this case.', 'Yes, then maybe a null Hypothesis test (could also to right/left tail depending on your aim:\n\nMake an H\\_0: r = 0\n\nt = \\[r\\*sqrt(n-2)\\]/\\[sqrt(1-r\\^2)\\]\n\ncompare with t with t-critical\\_vale @ level of significant, df=n-2']",39,7,https://www.reddit.com/r/statistics/comments/12yw55s/q_what_statistical_tests_could_i_use_to_measure/
319,2023-04-26 01:37:23,[Q] Question about holding variables constant logistic regression,"Hi :) 

I have a question about ""holding variables"" constant. For uni, I am supposed to visualise my binary logistic multiple regression with 3 predictor variables. They all use the same continouus scale. The Y-axis should visualise the likelihood of one outcome, and the X-axis should be the scale that all predictors use. We should visualise all predictors in one graph.

So far so good. I calculated the coefficients, I know how to calculate the likelihood. I know that I can visualise the predictors separetly by changing the X of one of them, and keeping the X of the others constant. However, how do I keep the other predictors constant?  The professor said that we should decide whether to  choose the median of each predictor, the middle of the scale or 0. 

Is one of them preferrable to the others? I have no clue- my first guess would have been to make X=0, because anyways the coefficiant of the predictor that I want to visualise is a result of the presence of the other variables. Does that make sense?  
Or am I on the wrong path here?  


I would appreciate any help :)","['Of note, these principles only work if there are no interaction terms relevant to the variables you are plotting.', ""I generally choose the mean or median. If there is a very strong mode (for example, if almost all value of x2 are 0), then I may use that modal value. It's useful to understand a couple key properties of marginal effects:\n\n1. The marginal effect of x1 is the change in y (kind of a slope) as x1 changes holding all other variable constant. This marginal effect/slope is the same regardless of whether x2 is 0, .5, 1, or any other value. \n2. The only thing about the marginal effect of x1 that changes when x2 is 0 vs 0.5 is that the \\*intercept\\* value for the marginal effect changes. \n\nIn short, the slope of the line you plot for the marginal effect of x1 will not change based on the value you choose for x2. The intercept will change, but we usually don't care about the intercept. Therefore, we don't usually care too much about what value x2 is set at when plotting the marginal effect of x1."", 'Thank you so much this is very helpful! I will read myself a bit more into marginal effects so to understand what you are saying 100% :) our professor did not really introduce the theory behind this, so I am sure that this will be useful :) \n\nHave a good day/night :)']",1,3,https://www.reddit.com/r/statistics/comments/12yq9x7/q_question_about_holding_variables_constant/
320,2023-04-26 00:32:54,[Q] Simulation of collider bias,"Hi, I could use some help diagnosing a problem with my simulation. I'm trying to convince myself of the effects of collider bias. I've simulated some data of two variable X1 (stroketx) and X2 (tumor)  that cause Y (death). X1 and X2 should be independent, but as you will see later there's reason to believe they aren't (?)

    set.seed(210423)
    
    sampsize <- 10^3
    simulns <- 10^3
    strokeeffect <- -8
    tumoreffect <- 4
    
    
    simul <- function(i) {
      stroketx = rbinom(n=sampsize, size = 1, 0.5)
      tumor = rbinom(n=sampsize, size = 1, 0.5)
    
      # the linear predictor
      lp = strokeeffect*stroketx + tumoreffect*tumor
    
      # Get probability based on linear predictor; assuming logit link
      vecp = 1/(1+exp(-lp))
    
      dth = rbinom(n=sampsize, size=1, prob = vecp)
    
      # X1 and X2 have no dependence (supposedly)
      marginal.x1x2 = summary(glm(stroketx ~ tumor, family=""binomial""))$coefficients[2,1]
      conditional.x1x2 = summary(glm(stroketx ~ tumor + dth, family=""binomial""))$coefficients[2,1]
      marginal.x1y = summary(glm(dth ~ stroketx, family=""binomial""))$coefficients[2,1]
      conditional.x1y = summary(glm(dth ~ stroketx + tumor, family=""binomial""))$coefficients[2,1]
    
      return(c(marginal.x1x2, conditional.x1x2, marginal.x1y, conditional.x1y))
    }
    
    
    v <- lapply(1:simulns, FUN=simul)
    
    
    # Get mean of unbiased association between X1 and X2
    mean(sapply(v,""[["",1))
    
    
    # Get mean of the biased association between X1 and X2 (included collider Y)
    mean(sapply(v,""[["",2))
     
    
    # Get marginal association between X1 and Y
    mean(sapply(v,""[["",3))
     
    
    # Get conditional association between X1 and Y (included X2 which is not a confounder)
    mean(sapply(v,""[["",4))

So while I do see that the association between X1 and X2 is biased because I've conditioned on a collider (Y). What troubles me is finding that:

1. My marginal association between X1 and X2 isn't close to 0 as I had hoped
2. My marginal association between X1 and Y is nowhere near the intended value of -8

&#x200B;

    > # Get mean of unbiased association between X1 and X2
    > mean(sapply(v,""[["",1))
    [1] -0.001861118
    > 
    > 
    > 
    > # Get mean of the biased association between X1 and X2 (included collider Y)
    > mean(sapply(v,""[["",2))
    [1] 3.563349
    > 
    > 
    > 
    > # Get marginal association between X1 and Y
    > mean(sapply(v,""[["",3))
    [1] -5.992347
    > 
    > 
    > 
    > # Get conditional association between X1 and Y (included X2 which is not a confounder)
    > mean(sapply(v,""[["",4))
    [1] -8.518688","['Two comments here:\n\nFirst, \n\n>(b) the marginal association of stroketx on death should be closer to 0 than -12\n\nNot quite. It should not be -12, and should be differ from -12 in the direction of 0, but it will likely be closer to -12 than to 0.\n\n&#x200B;\n\nSecond, what you are likely dealing with in this case is complete/quasi-complete separation in the regression models. A coefficient of -12 is an absolutely massive effect size for a logistic regression. As a result the model fitting process begins to diverge towards infinite values of the parameter rather than providing accurate estimation. That said -8 is quite a large effect size as well, but not so much you ran into issues here apparently. \n\nA more reasonable coefficient of -2 for example should confirm what I said in simulation.', 'For (1), the marginal association between X1 and X2 is -.0019, this is quite close to zero, and I’m sure if you considered the confidence intervals you would have good coverage of zero.\n\nFor (2) logistic models are non-collapsible which means the marginal odds ratio and conditional odds ratios (as well as associated parameters), are not equal to one another. It should be expected that the marginal parameter will take a different value.', 'Cheers for the answer to (1). \n\nRegarding (2), this is new to me so thanks for making me aware. So non collapsability explains why my marginal and conditional coefs are different when modelling Y. But how does non collapsability of the OR explain the marginal association being different from the intended value of -8?', 'Your initial definition of lp includes both stroke effect and tumor effect, which means that the -8 association between stroke effect and Y is a conditional association (conditional on the covariate value for tumor effect). The marginal model you fit estimates the marginal association which is some number different from -8. This is usually closer to zero like the estimated marginal coefficient of -6ish.\n\nSo it’s different here because the intended value of -8 is a conditional, not marginal association. They should not be the same.', 'Thanks for continuing to help with this. I am not sure I fully understand what you\'re saying. To test your claims, I\'ve tweaked the value of stroketx to -12 instead. If it is true that the value of stroketx in my definition of the lp is indeed the conditional association, and if it is also true that the marginal coefficient should be closer to 0 than what I had specified, then we should observe:\n\n(a) the conditional association conditional.x1y should be closer to -12 than marginal.x1y; and\n\n(b) the marginal association of stroketx on death should be closer to 0 than -12\n\nBut this is not the case empirically for both claims\n\n>\\# Get mean of unbiased association between X1 and X2  \n>  \n>mean(sapply(v,""\\[\\["",1))  \n>  \n>\\[1\\] 0.008412075  \n>  \n>\\# Get mean of the biased association between X1 and X2 (included collider Y)  \n>  \n>mean(sapply(v,""\\[\\["",2))  \n>  \n>\\[1\\] 3.63549  \n>  \n>\\# Get marginal association between X1 and Y  \n>  \n>mean(sapply(v,""\\[\\["",3))  \n>  \n>\\[1\\] -19.76235  \n>  \n>\\# Get conditional association between X1 and Y (included X2 which is not a confounder)  \n>  \n>mean(sapply(v,""\\[\\["",4))  \n>  \n>\\[1\\] -22.79851']",2,6,https://www.reddit.com/r/statistics/comments/12yogvh/q_simulation_of_collider_bias/
321,2023-04-25 23:14:22,[Q] why does the lognormal distribution tend to show up often where you wouldn't expect it?,"*My name is peperazzi74 and I am an addict - for cumulative density function calculation. /s*

Now for the serious part:

I've been doing some training on data science and statistics and have collected or scraped a variety of data sets. One of my current interests is Monte Carlo-type modelling, for which I need to have a PDF or CDF to generate the random samples.

Typically one would expect the lognormal distribution to appear for data that has a large spread over multiple orders of magnitude, but I found lognormal behavior in things like:

* the age at which US presidents come into office (spread \~ 40-80 years-old; 2x)
* HOA monthly spend ($10,000-$40,000; 4x)
* the tenure of HOA board members (spread \~3 months - 4 years; 10x)
* number of homes for sale simultaneously in my neighborhood (spread 2 - 25; 10x)
* outstanding delinquent sums in HOA ($3500 - $53,000; 15x)
* and some others that are similar in max/min ratio, but cannot share (work-related)

What makes this type of data so likely to have a lognormal distribution vs e.g. a normal distribution that we usually tend to use?  


  
*Note: a lot of data is related to my neighborhood/HOA, because they publish monthly and have done so for a long time, so lots of free and somewhat publicly available real data.*","['The log-normal distribution is a reasonably flexible distribution for strictly positive data, and can arise as the result of a central limit theorem where many independent factors have multiplicative effects on the outcome.\n\nHowever, you data very well may also not be log-normal and may be better described by other common distributions. What have you done to check log-normality and what have you compared against?', 'Lognormal distributions arise (among other ways) when many small random factors have multiplicative effect, just as normal distributions arise when many random factors are additive.\n\nA lot of financial processes have that kind of multiplicative effect, where your gain or loss this year is a percentage of how much money you had last year.\n\nIt is also very easy for a lognormal to look similar to anything with a power-law right tail, and possible for it to look similar to a Gamma or Weibull with shape parameter much less than 1.\n\nThat said, I would not expect the lognormal, or any other very thick-tailed distribution, to arise, except coincidentally, for variables like age or tenure. (The fact that some of these remain confined to narrow ranges feels like quite strong evidence you need something with thinner tails.)', 'Have you also tried comparing against a chi-squared and/or gamma distribution? Those also are flexible enough to look like a log-normal distribution.\n\nIMO, doing a Q-Q plot against a log normal distribution and seeing its somewhat close isn’t enough to conclude the data is actually log normal. But this Q-Q plot comparison could be enough to conclude the log normal is a good model for your data (but it may be the case that some other positive distribution like a gamma could also be a good model as well.)', 'If you’re actually going to try and fit a gamma distribution to your data, you might have to be careful about how you do it because if I recall correctly, closed form MLEs/moment estimators don’t exist for the parameters in this distribution so you have to use an iterative algorithm (so would be good to find some place online on how to do it). I guess what I was getting at was that there’s probably a few distributions which will give you good looking matches on the Q-Q plot, which just means that a Q-Q plot alone is not sufficient to conclude that your data follows some very specific distribution like a log normal.', ""In environmental statistics, it seems most things are lognormal-ish or at least rather skewed. Partly because things like concentrations can't be negative?""]",4,15,https://www.reddit.com/r/statistics/comments/12ymb7d/q_why_does_the_lognormal_distribution_tend_to/
322,2023-04-25 22:46:14,[E] Help with stats graduate programs,"Hi everyone,

Hoping to get some advice. I’ve started working as a data analyst and have started to go through textbooks since I’m incredibly passionate and excited about doing research work with data, and think that a statistics theory foundation would be very helpful. 

During undergrad, I didn’t do so well in my initial math and stats courses (first year). I am planning on taking more advanced math courses this summer at a community college to get pre-requirements and hopefully show I’m more serious about my studies. 

I also currently work, so have been trying to find programs that offer part-time / online programs. I live in Chicago and have been looking around but I’d also like a strong program overall. 

Essentially:
- Is taking classes this summer a worthwhile investment?(not the pre-requirements but other advanced ones just to feel more comfortable and hopefully have it boost my application)
- What is the best balance between choosing a name school vs a program that matches my needs? (I’m not sure how to weigh name-brand for programs so some overall tips on how to value program recognition and connections would help)
- Since it seems most Masters Stats programs are full-time and in-person, is it worth quitting my job to go back full time? (I know this one’s personal and based on circumstances. Currently, I make a good amount so don’t feel it’s smart to stop working right now, but I crave to learn and want to get promoted and do research work)

Thank you for your help!","['Is real analysis an option? I’d suggest kicking ass at that if you want to take more advanced math and demonstrate how serious you ate', 'I would think about a few things:\n\n1. Most programs will require Calculus 1, Calculus 2 and Multivariable Calculus along with Linear Algebra to consider you. Probability Theory makes use of a lot of Calculus and working with multiple variables requires Linear Algebra. Getting these classes together should be the bare minimum starting point for you. Real Analysis along with any other math that shows competency will be a bonus (or requirement depending on the program)\n2. If you want to boost your application or see if you actually like stats, you can do a graduate certification to let potential schools know you are serious. I would not necessarily do this unless you have a serious gap since it is expensive and time consuming. Penn State has a program:[https://www.worldcampus.psu.edu/penn-state-online-applied-statistics-programs?cid=CPC39813&gad=1&gclid=CjwKCAjw9J2iBhBPEiwAErwpeVuCQRCege9JIL67kizRV30fGUelQI0hqKxm2jy\\_uWbLeRZEBf2dWhoCR\\_8QAvD\\_BwE](https://www.worldcampus.psu.edu/penn-state-online-applied-statistics-programs?cid=CPC39813&gad=1&gclid=CjwKCAjw9J2iBhBPEiwAErwpeVuCQRCege9JIL67kizRV30fGUelQI0hqKxm2jy_uWbLeRZEBf2dWhoCR_8QAvD_BwE)\n3. What do you want to do with it? Do you want to do research or are you planning on working mainly in a professional environment? It sounds like you may want to do Applied. Instead of the certification, you can do a full online Masters with Penn State']",1,2,https://www.reddit.com/r/statistics/comments/12ylj5q/e_help_with_stats_graduate_programs/
323,2023-04-25 21:33:07,[Q] Is small sample size a problem with randomization besides power?,"Randomized leads to an unbiased estimator regardless of sample size, so besides power, is there a 'problem' to be aware of with small sample sizes? I was thinking maybe the sampling distribution - if we can't assume normality of the errors/rely on the central limit theorem, does this mean we can't do inference generally without making stronger distributional assumptions?","['> Is small sample size a problem with randomization besides power?\n\nWhat are you randomizing there? Assignment to treatment? Or are you talking about random selection from a population of interest?\n\n(I did wonder briefly whether you were asking about randomization tests, but it seems from the later part of the body of your question that this isn\'t what you mean.)\n\n> if we can\'t assume normality of the errors/rely on the central limit theorem, does this mean we can\'t do inference generally without making stronger distributional assumptions?\n\nI don\'t quite get what you mean by ""stronger"" there\n\n(a) You can use *weaker* assumptions perfectly easily (unless the sample size is extremely small, when other issues come into consideration, like limited available significance levels). For example, permutation tests or bootstrap tests.\n\n(b) other distributional assumptions still often end up with either an asymptotically normal or an asymptotically chi-squared distribution of standard test statistics. e.g. tests based off likelihood, including Wald tests, score tests and likelihood ratio tests. Consider inference in generalized linear models or survival models (both parametric and semiparametric)\n\n(b) outside of that, many tests -- including ones that *don\'t* have the CLT applying - nevertheless have asymptotic distributions (e.g. consider the asymptotic distribution for an ordinary Kolmogorov-Smirnov test; the distribution of Dₙ√n approaches the [Kolmogorov distribution](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution); the CLT doesn\'t apply because you\'re dealing with the largest difference there)', ""Yes. For CLT reasons but also because randomisation only balances groups *on average*, not for each and every individual trial. You're much more likely to end up with very imbalanced groups in a small trial and it is harder to compensate for this by using stratification or minimisation to force balance because the sample size is too small to do it well. And you'll also have too small a sample size to do a reasonable adjusted analysis to account for the imbalance after the fact.\n\nAnd low power is a very bad problem, of course. If I've got 40% power at 95% confidence in a reasonable real life scenario, I'll pick up about 40% of the 10% of hypotheses where there is a real difference (4%) to find and 5% of the 90% where there isn't (4.5%). That means that just over half of results with p<0.05 will be false positives. We're in coin flipping territory."", ""> i would have to impose an additional assumption like normality of errors \n\nYou'd only need that\\* if you're using a test that assumed normality. \n\n\n> So permutation tests are fully distirbution free and work in finite samples? \n\nYes (given the assumptions hold for the specific one you're doing), and yes, given your specific alpha (chosen from the available levels\\*\\*), it will have the usual properties it's designed to have.\n\n> maintains sound statistical properties/is not theoretically wrong\n\nThe specific properties for a hypothesis test are the significance level (do you get the one you chose) and the power curve (e.g. is it reasonably high for effect sizes you care about at your sample size)\n\n>  If the sanple wize is very small, is it still 'valid' \n\nThe available significance levels may become limited if sample sizes are extremely small. For example n1=5 vs n2=3 has a smallest two-sided significance level of 0.0357 for any permutation test\\*\\*\\*  because there's only ^(8)C₃ = 56 combinations of 8 distinct (i.e. all untied) values into two groups of 5 and 3, so if you wanted to conduct a test at the 0.5% level, you're out of luck; if you fix the sample sizes at 5 and 3 you really need some sort of continuous parametric distributional assumption there to get exact small significance levels, but they're only exact if the parametric assumption is true)\n\nIn general, don't use extremely small sample sizes if you don't want to make a parametric assumption that your significance level may have sensitivity to. You can either compound your low power problems by taking a much smaller alpha (either knowingly, with a permutation test, or unknowingly with a test under some parametric assumption) or you may risk inflating alpha (again, unknowingly). With something like a two sample t-test, the lower alpha is much more common, but more generally it depends on the test and the specific kind of violation\n\n---\n\n\\* (approximately, in some specific sense depending on the specific situation, where the degree and form of non-normality that you might choose to tolerate for its impact on your type I error rate and power would vary from test to test - and likely from situation to situation)\n\n\\*\\* By relying on permutations of the treatment labels (/group labels) there's only a finite number of permutations. Randomization to treatment should in normal circumstances satisfy the assumptions for this class of tests.\n\n\\*\\*\\* and nothing any closer to 5% either. Assuming you're doing the equivalent of doubling tail probability for the other tail it's 3.57% or it's 7.14% (in some cases the exact next available alpha could be higher depending on your specific permutation statistic)."", ""No.  The problem is low power.  Except if the sample size is so low that discreteness is a big issue.  If the P-value isn't quasicontinuous, then you don't know how to interpret it."", ""Think of the target image here: https://wiki.socr.umich.edu/index.php/File:SMHS_BIAS_Precision_Fig_1_cg_07282014.png particularly at the imprecise and unbiased target. \n\nUnbiased means that on average if you took that small sample size again and again, the mean of those small samples would be the same as the population. It says nothing about just seeing one small sample. You don't know where you are on the target.""]",17,7,https://www.reddit.com/r/statistics/comments/12yjksl/q_is_small_sample_size_a_problem_with/
324,2023-04-25 08:56:23,[Q] Is there a metric that measure inter-rater reliability for a set of rankings?,"I'm familiar with inter-annotator agreement (or inter-rater reliability) metrics for data that has categorical annotations, but what about a set of data samples that are ranked by several annotators? Would measures like Cohen's kappa still apply here?

The specific context that I'm talking about is within machine learning (text generation to be precise). For a set of items, several machine learning models generate explanations about what the items are in textual form. A set of human annotators then rank the explanations made by the different models.

There seems to be some subjectivity and disagreement among the annotators, but I'm wondering if there's a way to quantify that. Thanks.","[""I have a few work on this in the quality of experience for image/video domain. From what I understood from your explanations I think those might be applicable to your scenario. I can also share some state of the art from our domain that I think might work for your case. If you want to further discuss, I can give you my university email and we can chat further but below I will give a quick summary and some links to the works I am referring to.   \n\n\nLet me start by clarifying a few things to see If I understood your problem correctly.   \nYou have a tabular data, where each row represents a unique rater, and each row represents a unique stimulus. You know that you can use Cohen's kappa can be used to assess the reliability of raters (each row, cumulatively) but you are wondering if we can quantify the variance in stimulus ratings with similar metrics.  \n\n\nIf this is the case, we often refer these terms in Quality of Experience for image/video domain as follows:\n\n* subject inconsistency/ subject uncertainty (sometimes coupled together with a subject bias) / subject reliability : This terms refer to how reliable is each rater/participant in a subjective experiment.\n* content ambiguity: This term indicates how ambiguous or difficult to rate a stimulus is.   \n\n\nIn this context, I have a model (currently under peer review, so I prefer not to share the code publicly here) that can estimate both of the terms above. On another front, Netflix Sureal package has most of the state of art (ITU recommendations) implementation in its github page, including the one that they propose (often called MLE-CO and MLE). However, many of these models are not capable of estimating the content ambiguity term. I understand that this is what you are looking for. If true, the only option in Netflix Sureal package on the github (link below) is MLE. you can use this model by following the instructions. The model expects a json file, but should be straightforward to reformat your data into the required format.   \n\n\nNetflix Sureal repo: [https://github.com/Netflix/sureal](https://github.com/Netflix/sureal)  \n\n\nBest of luck,"", 'You can compare any two rankings using Kendall\'s Tau or Spearman\'s Rho. Let\'s compare the top eight rankings from the [MLB Power Rankings](https://www.mlb.com/news/mlb-power-rankings-for-2023-week-4) and [538\'s MLB rankings](https://projects.fivethirtyeight.com/2023-mlb-predictions/) as an example.\n\n`library(conflicted)`  \n`library(tidyverse)`  \n`dta <- tibble(`  \n  `teams = c(""Rays"", ""Braves"", ""Mets"", ""Brewers"", ""Yankees"", ""Astros"", ""Blue Jays"", ""Dodgers""),`  \n  `power_ranking = 1:8,`  \n  `fivethirtyeight = c(2, 1, 5, 7, 6, 4, 8, 3)`  \n`)`  \n`# Kendall\'s tau`  \n`kendalls_tau <- cor(dta$power_ranking, dta$fivethirtyeight, method = ""kendall"")`  \n`cat(""Kendall\'s tau:"", kendalls_tau, ""\\n"")`  \n`# Spearman\'s rho`  \n`spearmans_rho <- cor(dta$power_ranking, dta$fivethirtyeight, method = ""spearman"")`  \n`cat(""Spearman\'s rho:"", spearmans_rho, ""\\n"")`  \n\n\nYou could compute pairwise statistics to see how each one of your raters compares to the others. If you plotted the pairwise comparisons in a heatmap, you might find a rater who always seems to disagree with the others.\n\nIf you want everything organized in a single test, you could try Intraclass Correlation. I haven\'t used this one before, so YMMV.\n\n`dta <- tibble(`  \n  `teams = c(""Rays"", ""Braves"", ""Mets"", ""Brewers"", ""Yankees"", ""Astros"", ""Blue Jays"", ""Dodgers""),`  \n  `power_ranking = 1:8,`  \n  `fivethirtyeight = c(2, 1, 5, 7, 6, 4, 8, 3),`  \n  `power_ranking_reversed = 8:1`  \n`)`\n\n`library(irr)`  \n`icc_results <- icc(select(dta, -teams), model = ""twoway"", type = ""agreement"", unit = ""single"")`  \n`icc_results`']",11,2,https://www.reddit.com/r/statistics/comments/12y2x5t/q_is_there_a_metric_that_measure_interrater/
325,2023-04-25 05:39:36,[Q] How to identify skewness form unique boxplots?,"I'm working on improving my knowledge in statistics for future careers of interest and am self-teaching myself. I read online through various resources that with boxplots, you can identify skewness with the location of the median and the whiskers.

Right skewed: If the median is closer to Q1, and the lower whisker is shorter than the higher.  OR if the box itself appears to be positioned more to the left of the overall graph (i.e. left whisker is smaller, right whisker is larger).

Left skewed: If the median is closer to Q3, and the higher whisker is shorter than the lower. OR if the box itself appears to be positioned more to the right of the overall graph (i.e. left whisker is larger, right whisker is small).

Normal: If the whiskers are mostly even, and the median is in the middle.

**My question is:** What if the median is closer to Q1, *but* the lower whisker is longer than the higher? And same with if the median is closer to Q3, and the higher whisker is longer than the lower?

Can the whisker length cause a distribution to ""level out"", making it more normal even though the median is closer to a Q1 or Q3?

I know QQ plots and histograms offer additional perspective, I'm seeing if there's anything I'm missing with identifying skewness with boxplots before I add those.","[""1. Not all asymmetric distributions are clearly left or right skew. Different parts may give different indications, there may not be a single descriptor that's clearly capturing it overall. In other words, it's quitepossible to have a population distribution with features like what you're describing for the boxplot.\n\n2. Boxplots are a fairly poor way to judge shape in general. \n\n   You can often get some idea, but -for example- it's perfectly possible for a clearly skewed sample to yield a boxplot that is either symmetric or even skewed the wrong way.\n\n  Its shape  information is very coarsely measured. \n\n3. A boxplot with contradictory skewness  indications is not necessarily telling you that the population distribution is  nearer to normal than one where they all point the same direction. Sometimes, sure, but it's  easy for it not to be the case.\n\nIf you want to judge shape, plots that show all the data are better choices. Boxplots are more valuable for showing many groups, where quick cross-sample comparisons of location and scale are useful. There's a decent summary of some of the issues raised in this answer over [here](https://stats.stackexchange.com/a/96556/805)."", 'I updated my earlier post by editing in a link at the end, which I think makes some useful points and has an explicit example of how a single boxplot could correspond to some very different shapes.', 'Thank you for taking the time to throw in your thoughts.\n\nHonestly, what I feel that I\'m learning from boxplots is -- as you say -- they\'re not the best with identifying distribution.  It feels more subjective than a histogram and QQ plots, and that a boxplot with a ""unique"" shape could be anyone\'s guess in regards to what its distribution likely is.']",0,3,https://www.reddit.com/r/statistics/comments/12xxibu/q_how_to_identify_skewness_form_unique_boxplots/
326,2023-04-25 04:48:36,[E] What to Look for in MS Biostatistics Programs,"So I'm considering an MS in Biostatistics and I'm curious about what elements of programs I should be looking at. Right now I'm looking at some in-state universities which are decently ranked, and I'm not sure about what they offer and if I should look elsewhere. 

Some extra context: I'm in the US, I'm a traditional student - moving straight from undergrad to a masters, and I'm not currently interested in a PhD, though I'm also not ruling it out.

Here are my main questions:

1. **What's the deal with funding?** I've seen conflicting advice ranging from 'prioritize funding above all else' to 'expect to pay so go somewhere cheap' and I'm not sure what end of the spectrum is more realistic, and if funding is even something I should consider. \[The universities I'm currently considering are cheap, but offer no funding\]
2. **Are there key courses/topics I should be looking for in coursework?** If I'm getting a degree I'd want it to cover everything relevant, so are there certain topics or classes that are necessary/especially helpful (in your opinion) that not every university covers? (and does applied vs. non-applied matter?)
3. **Is there any other metric I should be judging programs by?** Faculty, networking/connection to opportunities, related degrees? 

I recently switched from looking at an MPH in Epidemiology to an MS in Biostatistics (if the extra math goes well), so I'm just trying to readjust my expectations and what I'm looking for. Thanks in advance!","[""(1) Depends on your available resources and how debt-averse you are. I probably wouldn't borrow 200k to do an MS in Biostats, personally. If you get some kind of funding/scholarship then by all means, that's lovely. If you have a family member or spouse helping with tuition that's great too. \n\nOn the other hand I wouldn't fret if you don't go to a top 10 program or whatever. Once you are in the industry, people don't really care that much where you went to school. When I'm considering to hire analysts, I am much more interested in their industry experience and that they have the right educational background, not where they went to school per se. \n\n(2) Assuming you want to do an MS and go into industry, I'd try to take as diverse of a course schedule as I could. You aren't going to emerge from an MS program already knowing everything you need to know. As an example, your clinical trials class will teach you nothing about CDISC standards or dealing with questions from the EMA. Because the person teaching the course has never gotten a drug approved more than likely (they are an academic, not a drug developer).\n\nThat's ok, just be realistic that you aren't going to learn everything you need to know in school. \n\n(3) I would consider location. The classical Biotech hubs in the US are Boston, NYC, DC, Raleigh, Seattle, SF, and San Diego. Chicago/LA also have opportunities but less so. There are also some very specific employers in other areas not related to the above (for example, Eli Lilly is headquartered in Indianapolis). \n\nYou will have more networking opportunities closer to where your school is physically located. It doesn't mean you can't go to school on the East Coast and then find a job on the West Coast. But it does mean things like the senior leadership at a company like Seagen probably has at least 1 or 2 UW alumni that were probably PhD students of professors at UW."", 'I would ask programs how easy it is to get a graduate research assistant job for a MS student. Experience working with real world data, like for a clinical trial, is a big plus on job apps after you finish grad school.\n\nThe location comment above is a good one.\n\nAnother thing I’d look for is what programming language the classes in the program focus on. Personally, I’d look for programs that focus more on Python than R or SAS because I think that opens up more job opportunities since that’s becoming more of the preferred programming language. Unless you’re hell bent on working in pharma after grad school which is mainly SAS.']",1,2,https://www.reddit.com/r/statistics/comments/12xvwqz/e_what_to_look_for_in_ms_biostatistics_programs/
327,2023-04-25 03:30:01,[Research] Prepping for ODSC (Data Science conference)….how?,"
I work as a data science but feel like I have some significant gaps in my knowledge of data science and what not. 

I am attending this conference in a few weeks and should have a few solid days to study a bit. **Anyone have tips on how to best prepare?** I really want to make the most out of this conference, learn things, and implement it/convey back to my team. 

They have a boot camp, but it is quite expensive (and I already had to get the conference tickets plus travel arrangements). Hence, I’d like to keep it free to relatively cheap (Udemy cheap). If people have suggestions,‘please let me know. 

Something well rounded (a little of everything, but not everything in detail) might be the best way to go.","['Brush up your basics and make short notes of all talks you attend(don’t wait to go back and do it, you either won’t or will not remember everything). The main goal here is one to network and two is to understand how this field is being used by professionals across industries.\n\nP.S - I am attending the event as well. Guess we can meet each other there.', 'Fair points. What did you think of day one? Maybe I’ll see you there tomorrow', 'Ah it was grand! Getting to know how people solve issues we usually face is interesting.', 'Exactly! What were your biggest takeaways or the highlights of what you learned? I’m curious too, did it unlock something in what you can bring back to your workplace?']",2,4,https://www.reddit.com/r/statistics/comments/12xtgvb/research_prepping_for_odsc_data_science/
328,2023-04-25 01:13:09,[Research] Advice on Probabilistic forecasting for gridded data,"We have a time series dataset (spatiotemporal, but not an image/video). The dataset is in 3D, where each (x,y,t) coordinate has a numeric value (such as the sea temperature at that location and at that specific point in time). So we can think of it as a matrix with a temporal component. The dataset is similar to this but with just one channel:

&#x200B;

[https://i.stack.imgur.com/tP1Lz.png](https://i.stack.imgur.com/tP1Lz.png)

&#x200B;

We need to predict/forecast the future (next few time steps) values for the whole region (i.e., all x,y coordinates in the dataset) along with the uncertainty.

&#x200B;

Can you all suggest any architecture/approach that would suit my purpose well? Thanks!","['You could use a space-time Gaussian process or spatiotemporal CAR model.', 'Not a straightforward problem. I know one example where this was successfully done \n\nSee https://icenet.ai', 'Do you know something about the spatial / temporal dynamics of your data? If so, maybe a state space model / partially observed Markov process might be something to consider.', 'Came here to say the same, spatio-temporal GP. If you’re talking a small area, Cartesian coordinates is probably ok, but large enough and you’ll want to model on the surface of a sphere.', ""That's the right class of model. My guess without reading it is that the paper you linked gives a fast approximate solution for that type of GP. If your data is not too large, you can just use off-the-shelf software like Stan or PyMC for fitting the GP.""]",41,13,https://www.reddit.com/r/statistics/comments/12xpj5o/research_advice_on_probabilistic_forecasting_for/
329,2023-04-25 00:51:12,[Q] Question about correlation between two time-series variables,"Hi guys! I am trying to measure the extent of the relationship between two time-series variables. These two variables are also categorized by country (8 countries in total). 

 I don't know what procedure or method I should use. So far I have made 8 line charts (representing each country) showing how these two variables moved over time. Is there any other statistical method or graphical representation to show that?","[""Dynamic Time Warping is a similarity measure that's specifically used to capture similarity between different time series.  Sort of like Euclidean distance, but better suited to capture sequential dependence.  \n\nIt's a common pre-processing step for other techniques in time series clustering and classification because it's a helpful way to transform time series data into something more meaningful.""]",1,1,https://www.reddit.com/r/statistics/comments/12xow3m/q_question_about_correlation_between_two/
330,2023-04-25 00:42:19,[D] Reverse Bait and Switch in Statistics,"This  post by an economist argues that people tend to change stastitical  questions to harder ones, when the easy questions give uncomfortable  answers. Thoughts?

[https://unreasonabledoubt.substack.com/p/the-gender-pay-gap-and-the-reverse](https://unreasonabledoubt.substack.com/p/the-gender-pay-gap-and-the-reverse)","['> the gender pay gap disappears when controlling for occupation, \n\nThe link is to a tweet of a picture that does not give the source, but from the research I’ve read, this is not even close to true. The gap is reduced (e.g. a ratio of .9 v .7), but interpreting the 10% gap as the “true” gap and the 30% as “biased” ignores the fact that women are less likely to be hired for higher-paying positions even when their qualifications are equal (see the randomized resume experimental results). \n\nIn other words, these two statistics tell separate parts of the story. The first part (30% difference) could be considered the overall effect, while the second part (10% difference) could be considered the adjusted effect within profession. \n\nI disagree with the author’s take on this. You should never stop at the first statistic that agrees with their opinion. Always continue to think about and test hypotheses on the meaning of the statistic.', 'Completely agree on everything up to ‘test hypothesises’. Always good things to be done in just presenting posteriors or considering other decision rules.\n\nBut yeah, I agree, it’s like the simpson paradox example. Even in that story, yes it’s not that the departments prefer male candidates, but the school is clearly not providing enough funding to the female preferred departments. 2 statistics for two stories, both true and both being useful for differing decisions.', ""Stats prof here.  If you throw in enough covariates you can... what? create a ridiculous model. The first assumption of any statistical test is the model is correct. \n\nIf I'm motivated to remove gender as a predictor, all I have to do is get a few covariates that correlate with gender. Same crappy approach that let people claim smoking doesn't cause cancer when you control for A, B, C, ... X, Y, Z. This isn't bait n switch, it isn't clever, it's just algebra and crappy misuse of the methods. \n\nI saw Jordan Peterson make this claim a while back and looked into it. Don't get me wrong, there's parts of his work that I think are great. Clean your room! But even if he's a good clinical psych (IDK), he's not a good data scientist if he fell for this amateur trick.\n\nI do believe we're on our way to gender equity because of the large advantage girls have over boys academically in K-12 and women have over men at university. We're not there yet. Whenever it happens, you have to know whoever is the president/prime minister will make a lot of political hay over it.""]",0,3,https://www.reddit.com/r/statistics/comments/12xomwf/d_reverse_bait_and_switch_in_statistics/
331,2023-04-24 23:50:22,[Q] What to do if my descriptive statistics don’t support a hypothesis but the p value > .05 ?,,"['Huh? Can you clarify or provide more detail?', 'Please clarify (by editing the question text). It sounds as if you may be confused', 'p > .05 indicates no meaningful difference', 'It implies that your sample size was too small to validate the claim. \n\nP-value logic requires something to be quite extreme to overcome the role chance will ordinarily play in data when the null is true. \n\nThe most you can say when something is significant is that if the null is true, then the observed data is surprising, but quite possible due to chance. When it is not significant, then you cannot rule out the null as true as the result is not unexpected. \n\nYour results were not extreme enough.', 'This feels like a question from a multiple choice test, without the options being provided in the post.']",0,16,https://www.reddit.com/r/statistics/comments/12xmr5i/q_what_to_do_if_my_descriptive_statistics_dont/
332,2023-04-24 23:43:55,[Q] Should I choose University of Toronto Statistics MSc over a cost-comparable American program?,"Hi, I'm a current American statistics senior debating between two master's programs, and I was interested in some input.

One is the University of Toronto Statistics MSc. It is 2 semesters, $21k USD in tuition, coursework only, 4 mandatory classes that cover material I have already taken, and 4 electives.

The second is an accelerated program at a mid-ranked large American public university. I have taken 2 graduate level theoretical courses and 2 graduate level applied courses already. It is 2 semesters, $19k in tuition, 3 electives per semester, in a low cost of living city.

I have already accepted the American program as a baseline. My specific question is: by your experience or knowledge, is the UofT Statistics MSc program high-quality or reputable enough to accept over the American program? I am largely interested in working in industry, though I also have a not insignificant interest in pursuing a PhD.","[""Canadian checking in. Some thoughts:\n\n* UoT is one of the best in Canada for graduate studies in general and also for Statistics. You could say it is THE best and few would argue. It's hard to truly define.\n* Toronto is one of the highest CoL in Canada. Though, you make freedom dollars so that might not be an issue.\n* Are you trying to get Canadian citizenship? It's WAY better for taxes to have Canadian (or literally any other country) citizenship than US citizenship if you plan to work abroad (not including the US).\n* If you plan to work in Canada, that's cool too, we have a great work/life balance. But you will be considerably underpaid in Toronto compared to many places in the US.\n* The pay gap is even larger if you want to enter tech or machine learning.\n* However, if you want to stay in academia, UoT is a solid place to do your MSc. Keeps plenty of academic doors open.\n\nOverall, the Stats will be great and you'll learn enough to be dangerous, not the perfect choice for optimizing earning potential, but not a bad one either."", 'You should follow your heart and listen to the wind.\n\n&#x200B;\n\nJust kidding. What are the placements from both universities? In terms of pure ROI, what is the tuition plus COL from one school to another? Will you be taking out loans?', ""U. Toronto has a pretty strong reputation, so, other things being equal, you might learn more / have better teachers / meet more interesting people while in school, and get somewhat better opportunities when you look for a job.\n\nI'm not too hung up on school status, but it sounds like the programs are comparable on other dimensions.""]",3,3,https://www.reddit.com/r/statistics/comments/12xmawn/q_should_i_choose_university_of_toronto/
333,2023-04-24 22:49:17,[Q] Appropriate to use Fleiss' Kappa on data from 4 raters - answer doesn't make sense to me,"Hello, thanks for reading. I don't have a good stats background, so I appreciate the help.

**Background**: I have data from 4 raters who evaluated 9 different articles using 2 different quality assessments: [PEDro](https://pedro.org.au/english/resources/pedro-scale/) and Methodological index for non-randomized studies (MINORS). These assessments have questions related to the study that ask if certain things are found in the study (e.g. blinding). PEDro is 11 questions that are scored as 0 or 1  (criteria is absent or present). MINORS has 12 questions (we only used 1-8 due to the studies not being comparative) that are scored 0, 1, or 2 (criteria is absent, partially met, fully met). I am trying to assess the inter rater reliability of the scoring to provide a numerical representation of the degree of agreement between the 4 raters.

**Questions**: Is Fleiss' kappa appropriate for this? The scales of 0 or 1 and 0, 1, or 2 are categorical if I'm not mistaken (specifically, this would be ordinal data?). From the [wiki](https://en.wikipedia.org/wiki/Fleiss%27_kappa), Fleiss' is suitable due to the number of raters being >2; however, the following quote makes me question this because the raters were all the same:

>Fleiss' kappa specifically allows that although there are a fixed number  of raters (e.g., three), different items may be rated by different  individuals (Fleiss, 1971, p. 378). That is, Item 1 is rated by Raters  A, B, and C; but Item 2 could be rated by Raters D, E, and F. The  condition of random sampling among raters *makes Fleiss' kappa not suited  for cases where all raters rate all patients*

Does this mean I need a different measure?

**Follow up:** (this may be more appropriate for an R subreddit - happy to ask there)

I have tried to calculate Fleiss' kappa in R but the numbers don't make sense to me on a few of the cases. I'm wondering if I can't apply Fleiss' kappa to an assessment with only two categories.

I have files with the data laid out as so: (print out from R studio)

        Rater1 Rater2 Rater3 Rater4
    1       1    1      1    1
    2       1    1      1    1
    3       1    1      0    1
    4       1    1      1    1
    5       1    1      1    1
    6       1    1      1    1
    7       1    1      1    1
    8       1    1      1    1
    9       1    0      1    1
    10      1    1      1    1
    11      1    1      1    1

My code:

    mydata <- read.csv(""C:\\mydata.csv"")
    kappam.fleiss(mydata, detail = TRUE)

There are only two discrepancies between 4 raters yet the result is:

     Fleiss' Kappa for m Raters
    
     Subjects = 11 
       Raters = 4 
        Kappa = -0.0476 
    
            z = -0.387 
      p-value = 0.699 
    
       Kappa      z p.value
    0 -0.048 -0.387   0.699
    1 -0.048 -0.387   0.699

If kappa = 0 is the result of pure chance, how is data with only two different values almost the same as chance?

EDIT: MINORS scale questions:

    1. A clearly stated aim: the question addressed should be precise and relevant in the light of available literature
    2. Inclusion of consecutive patients: all patients potentially fit for inclusion (satisfying the criteria for inclusion) have been
    included in the study during the study period (no exclusion or details about the reasons for exclusion)
    3. Prospective collection of data: data were collected according to a protocol established before the beginning of the study
    4. Endpoints appropriate to the aim of the study: unambiguous explanation of the criteria used to evaluate the main outcome
    which should be in accordance with the question addressed by the study. Also, the endpoints should be assessed on an
    intention-to-treat basis.
    5. Unbiased assessment of the study endpoint: blind evaluation of objective endpoints and double-blind evaluation of subjective endpoints. Otherwise the reasons for not blinding should be stated
    6. Follow-up period appropriate to the aim of the study: the follow-up should be sufficiently long to allow the assessment of the main endpoint and possible adverse events
    7.Loss to follow up less than 5%: all patients should be included in the follow up. Otherwise, the proportion lost to follow up should not exceed the proportion experiencing the major endpoint
    8. Prospective calculation of the study size: information of the size of detectable difference of interest with a calculation of 95% confidence interval, according to the expected incidence of the outcome event, and information about the level for statistical significance and estimates of power when comparing the outcomes.

&#x200B;","[""First, it's not immediately clear that this is an inter-rater reliability question. Whether or not certain things are found in these studies sounds like a matter of being correct or not.  In other words, there's no guesswork from what I understand. Additionally, are these raters independent? If not, the guesswork may be even less of a factor.\n\nThis is an important point because Fliess' Kappa adjusts for chance, which is meant to more specifically account for guessing among raters who are not sure but must input some score. Paradoxes can arise from the statistic, and those are well documented, particularly if assumptions aren't met from what I understand. Try looking through a few articles.\n\nAdditionally, I've seen a bit of pushback at this statistic as some suggest that simply assessing sensitivity and specificity is appropriate without adjusting for chance. This approach would make your analysis more akin to difference in proportion 'correct' per rater relative to accuracy or the overall accuracy of your review system."", ""OK so you're right about PEDro, there should be correct answers to the questions (e.g eligibility criteria were specified, subjects were randomly allocated to groups, allocation was concealed). The MINORS has a few more nuanced things but definitely most should be either correct or incorrect answers. I have posted the questions for MINORS in an edit above. I have no doubt that some of the answers are different because the raters did not understand the assessment that well (we're students with little research background). \n\nI was advised to do a weighted kappa on this data but IDK how closely that person understood the project. Is there something else I should do or should I just leave this out. We used the modes for reporting final assessment scores."", ""Maybe it's best to back up a little bit. What is the utility of this analysis? In other words, regardless of the person who told you to use kappa, what is the primary question and what actions will be taken based on its answer?"", ""We are performing a systematic review of 9 articles. We wanted to assess the methodological quality (or risk of bias) of each article so we picked these two assessments (PEDro and MINORS) and each person (4 total) filled out the assessment to the best of their ability (again, we don't have much experience). From the assessments, we compiled the data and made a table of the modes as the final score for each article, which we intend as a general indicator of the methodological soundness  of the research article. This was to evaluate the included articles to develop statements regarding our confidence in the research findings. My professor wanted us to provide some calculation of agreement in scoring between the raters for each article and mentioned a weighted kappa. Now it kind of seems like this isn't that useful...\n\nEDIT: in the original publication of MINORS they report a kappa in table 4: https://www.unisa.edu.au/contentassets/72bf75606a2b4abcaf7f17404af374ad/6f--minors1.pdf"", ""Thank you for that summary. It may be that you simply report the statistic that you've computed because your professor suggested it. You **might** include the caveat that the statistic has presented a bit of a paradox, which is relatively common for this statistic. Depending on where your professor wants to go with this conversation (how statsy), it could be a very valuable discussion. One point to address is that there are only two options per item (0/1) which increases the possibility that raters will agree by chance. That point is inherent in the mathematics of the statistic.\n\nInterrater reliability metrics like the one you've used are designed to account for chance agreement in contexts where a lot of subjectivity is at play. But here, there are only two potential answers per item, and whether or not they are correct doesn't seem to be subjective. \n\nIf there were 11 questions each about some abstract takeaway from the article that each rater was to score from 0 to 10, then you would find a more appropriate context for this statistic because chance agreement would be much lower.\n\nAnother option is to use a joint probability approach that doesn't account for chance agreement. That will be more aligned with the proportional difference between raters.""]",3,7,https://www.reddit.com/r/statistics/comments/12xiiwo/q_appropriate_to_use_fleiss_kappa_on_data_from_4/
334,2023-04-24 21:06:56,[Q] Difference between time-varying variables and time-varying coefficients?,"Hi, I’m working on a Cox PH model for a survival analysis and I’m having a hard time understanding the difference between time-varying variables and time-varying coefficients. Can you provide me a brief explanation?","['I am also learning about it now. My understanding is that time varying variables are variables that change over time (for exmaple, you can have a variable that is ""suffered a heart attack"" that starts at 0 and then if the individual suffers one at time t becomes 1 for any time greater than t). If you are dealing with this only, the proportional hazards assumption stil holds and you can make the cox regression with the time dependent variables taken into account (don\'t put them as constant variables known from the beginning).\n\nThen, having time varying coefficients is different, because then the PH assumption does not hold any longer. This is not as clear to me yet, but this can be due to a number of reasons, and I think adding a frailty can help deal with that (since this can be caused by ommited variables). This last part maybe someone else is more knowledgeable with it, as I recently asked a question related to frailty related somewhat to this.', 'A lot of models at their center have a linear model, a sum of things that look like beta\\_i \\* x\\_i. (The cox model has some extra subscripts, but that’s not important for the general idea here.) When we move to time-varying land, we have some choices. The most general being to make both beta\\_i and x\\_i functions of time, so the sum would have beta\\_i(t) \\* x\\_i(t) in it. But we could also assume one, or both, of those is constant in time. Consider the difference between beta\\_i \\* x\\_i(t) and beta\\_i(t) \\* x\\_i. One of these models a covariate ((independent) variable) which is constant but which has an effect that changes through time. The other models a covariate which changes through time, but where the per-unit effect of that covariate doesn’t change.\n\nSay you know that the actual facility of treatment matters for patient outcomes, so you throw that into your Cox model. But, you also know that during the span of treatment, one of those facilities underwent a big policy change which could make a difference in that. The covariate is still a hospital ID, that hasn’t changed, but the coefficient on it may now need to vary in time.\n\nOn the other hand, consider some sort of risk factor or comorbidity which can affect your patient’s outcome. The effect of this may not change over the course of the study (the coefficient could be constant), but if you’re tracking whether a patient displays this you absolutely need to let the covariate vary in time (even if it is just an indicator variable for whether some condition is present).', 'Time varying variables: The numeric value of the variable changes over time. The effect does not change over time. Ex: A patient’s CD4 levels can change over time. The health implications of CD4 levels do not change over time; low CD4 levels in year 1 are just as bad as low CD4 levels in year 20.\n\nTime-varying coefficients: The values of a variable may or may not change over time. The effect of the variable changes over time. Ex: The the effect of CD4 levels diminish as a patient lives longer. Low CD4 levels are really bad in year 1, but don’t make much of a different by year 20.\n\nI completely made up the medical example. That should give you an idea though.\n\nYou need to use the [counting process form](https://grodri.github.io/glms/r/recidivism) of survival models for both types of models. It’s easier to think about like that anyway. Time varying modes have a single coefficient for a variable. Time varying coefficients are an interaction between the variable and time.', 'Thanks a lot', 'Thanks a lot']",11,6,https://www.reddit.com/r/statistics/comments/12xf66u/q_difference_between_timevarying_variables_and/
335,2023-04-24 17:45:27,[Research] Literature review articles: Where to submit them?,"Hello, 

Sorry if this sounds like publishing for the sake of publishing but as a phd student there are graduation requirements for me to fulfil. 

I am a phd student in statistics working on survival analysis and missing data. Over the last two years, I have written a lot of notes from papers I have read, some derivations and all that, literature review for quals. 

I am wondering if I were to compile it into a comprehensive literature review article, is it publishable / or can i submit it to a place like International Statistical Review? 

 Is there any other venues that accept review articles (I know I can possibly post it on arXiv) that you could recommend me? 

Thanks!","[""Without knowing your topic, it's tough to tell. However, there are so many opportunities - the classic is the BMJ stats notes. \n\nMost journals will take a lit review as long as it's reasonably rigorous."", 'Not sure if this would apply depending on what subfield you’re in but that’s what the APA’s Psych Bulletin journal is, it publishes review papers in the field of psychology. Survival analysis is definitely used in the field and could be an appropriate topic for the journal. Not sure if they accept methods papers.   \n\nhttps://www.apa.org/pubs/journals/bul', 'Thank you so much! I will look into it, by any chance do you know if those review papers need to have a touch of psychology in them? While I might be okay for the stats, I definitely dont have the domain knowledge for the psychology aspect.', 'Thank you for your suggestion! I was abit afraid since reviews are typically given by invitation to established people in the field!', 'I’m really not sure, but I’m assuming if you’re reviewing a type of statistical method it might have to be framed in the context of how it’s used in psych. But that’s just my guess, I’d look into the stats papers they’ve published before and their submission requirements.']",11,5,https://www.reddit.com/r/statistics/comments/12xa4v8/research_literature_review_articles_where_to/
336,2023-04-24 14:09:31,[Q] Is there an app for tracking random statistics over a long period of time?,"What I’m exactly trying to do is, my grandma has a notebook and she writes down everything that happens every year. First day of snow, first day the snow is gone, first day the humming birds return, the first time the geese return from the south, etc. it’s pretty cool to see her have over 50 years of dates on this stuff. 
I would like to do the same using an app. 

Is there something out there that I could enter these types of things and then search it later? Say if 10 years I want to search first day I golfed each spring and then it would pull up all those dates. Rather than me having to flip through 10 years of calendars or notebooks.","['Excel', 'MS Excel bru', 'Type it into an excel spreadsheet?', 'Every useful calendar software will let you search for events of a given type. A spreadsheet does that, too.', 'Google sheets']",2,5,https://www.reddit.com/r/statistics/comments/12x5h9d/q_is_there_an_app_for_tracking_random_statistics/
337,2023-04-24 11:29:22,[Q] How can I conduct Cost-Effectiveness Analysis from Indirect Treatment Comparison?,"Hey everyone,

Thanks so much for checking out this post. There are a lot of elements at play, so I will try to ask my question as directly as possible.

Singh et al. previously published a network meta-analysis of randomized controlled trials (RCTs) that compared various intra-articular treatments for knee osteoarthritis (paper included in Google Drive link below). They analyzed 6-month follow-up pain and function improvements separately (via VAS and WOMAC scores, respectively, abbreviations below. WOMAC is an organ-specific system that’s been developed and validated to assess knee function). However, they normalized their data and report it as a mean difference (MD) when compared to placebo. These results are summarized in Figures 2 and 3 from their publication, respectively. There are 4 treatments that they analyzed: PRP, PRGF, HA, and corticosteroid (abbreviations below).

I’d like to know if and how I can use the MD values to perform an indirect treatment comparison (ITC) for these 4 treatments with a different type of treatment (treatment ‘x’). The common comparator would be placebo. There are a few RCTs for treatment ‘x’ available currently—from my understanding, I would need to perform a meta-analysis for treatment ‘x’ in order to conduct ITC and subsequent CEA.

The ultimate goal would be to use the ITC results to then conduct a cost-effective analysis (CEA). From my understanding CEA requires EQ-5D data to calculate quality-adjusted life year (QALY). There is a mapping equation that has been developed by Bilbao et al. (paper included in Google Drive) that aims to convert WOMAC scores to EQ-5D specifically for the purpose of conducting CEA.

Here are my questions:

Is it possible for me to use the MD values from the Singh study in order to perform an ITC? 

If so, can I convert a derived value from MD to EQ-5D via the mapping equation in order to then perform a CEA? 

Is it correct that I would need to perform a meta-analysis for treatment ‘x’ in order to conduct ITC and subsequent CEA?

Again, I really, really appreciate any help you can provide. I know these are very specific questions, and I can try to clarify any of these points as much as I can, given what I know. Thank you so much for taking the time to at least read this, you have been so helpful so far!!

Google Drive with publications referenced above:

[https://drive.google.com/drive/folders/1Lo\_IFVIvcD3W53k2YejfQPX2CtgrhwxY?usp=sharing](https://drive.google.com/drive/folders/1Lo_IFVIvcD3W53k2YejfQPX2CtgrhwxY?usp=sharing)

Abbreviations: 

VAS, visual analog scale

WOMAC, McMaster Universities Osteoarthritis Index

PRP, platelet-rich plasma

PRGF, plasma rich in growth factors

HA, hyaluronic acid",[],0,0,https://www.reddit.com/r/statistics/comments/12x1hs0/q_how_can_i_conduct_costeffectiveness_analysis/
338,2023-04-24 05:10:08,[Q] Master of Applied Statistics vs Master of Data Science?,"Hello! I’m really confused by the responses I’ve read about this question, so I’m hoping someone can help me? 

I’ve read some people say that data science is a useless degree since apparently it can be a cash cow for a uni, but then others say that it’s more practical than a statistics degree so therefore more useful. On the flip side, I’ve heard that a statistics degree will prepare you better for the theoretical and give you a leg up, but then others say it’s not really worth getting? IM CONFUSED! 

I really enjoy math and statistics — I find it very enriching. But I also love computers. I can program at an intermediate level in Java and python already, but I wouldn’t mind exploring more of the technical side with data science. 

At the end of the day, I want to do the degree with the best job prospects and with the best  salary outlook. 

Which is more worthwhile? 

Based in Australia. I’m transitioning from a different field, so it’ll be a transitionary post grad qualification into the masters.","['From my experience names can be misleading in these kind of programs. 2 ""masters in statistics"" from 2 different unis can give you very different preparation. Look into the program specifics, and also ask students if you can about how they feel about the program/what they want to do after the master to get a feeling for it. From what you say you\'d probably like a stats master that offers applied/programming courses as electives.\n\nI find that a strong theory and understanding of how things work will always take you longer. From data science students I\'ve met in some of my courses I get the feeling that they usually know many more methods and names, but then don\'t delve into why they work, so I\'d recommend stats master. Again, I\'m in stats myself so I am biased.', ""it maps to the professional world\n\ndata scientist is a newer and now larger career track ,  mostly applicable to medium/large Technology companies.  DS help product managers run experiments (A/B)  or they work to develop ML models for some specific domain  (eg fraud detection).  DS cirricullum is more coding,  more ML.\n\nApplied Statistician is an older professional specialization. They are more the people who biotech, pharma, and healthcare companies will call to help structure their trials and interpret results (and hack their p-values).   Also related to Applied Statistics is the work of Actuaries in insurance companies.  Applied statistics requires a stronger foundation in theory, experiments & bayesian statistics. It's likely less coding driven, although these days who knows."", 'The statistics degree will be far more worthwhile. If you can handle the math, this is the degree you should do.\n\nStatistics degrees are long established with curricula that are a known quantity. Data science degrees are... what exactly? 15 years ago data science meant ""statistics applied to e-commerce"", now it means anything from machine learning to business intelligence, and it\'s pretty cursory given the breadth of potential topics.\n\nYou won\'t learn to program in either degree, even if you do manipulate programming languages more in the data science one. Frankly, if you are even half as good at Java as you claim to be, I doubt you will learn anything about programming in either degree.\n\nI once made the mistake of showing interest in a data science master\'s from a top 10 US university. Even knowing nothing about me, they would not stop calling me. The emails were borderline predatory. Obviously a total cash cow and they were jumping on the hype train.\n\nData science as a descriptor feels like a race to the bottom. Only ten years ago it was the reserve of quantitative PhDs. Two years ago one of my undergraduate interns graduated into an analyst position as a ""data scientist"". Solid intern, but the job title has become meaningless because it can apply to a huge range of background skills and job descriptions, very few of which you (and me) will be actually qualified for.\n\nSource: too many years in Silicon Valley.', '""the job title has become meaningless because it can apply to a huge range of background skills and job descriptions...""\n\nThis is exactly how I feel about it. I hope the title goes away at some point because it can mean literally anything.', 'Our team is a data science team at my company. We are all called data scientists and we are under the chief science officer. My manager had me go through some resumes to fill out headcount and I was specifically told not to bother with those with MS analytics or MS data science on the grounds that their programs are usually not rigorous, designed to get people in and out, and they cannot pass our interviews. \n\nI don’t agree with this viewpoint, but I think there is some merit to the statement as it is in fact true that data science students generally do not do well in our technical screening - which involves coding and statistical knowledge. \n\nSince you asked about job prospects, it would be reasonable to hear how a hiring manager thinks, even if you disagree with the sentiment.']",26,13,https://www.reddit.com/r/statistics/comments/12wqafg/q_master_of_applied_statistics_vs_master_of_data/
339,2023-04-24 05:02:53,[Q] R^2 from a multiple mediation analysis,"
Hi all, 

I’m currently completing a study where I have carried out a parallel mediation analysis on the jamovi software.
I have the following:
1 independent variable 
2 mediators
1 dependent variable.

I am struggling on how to calculate an R square value for the two mediators.

If anyone knows anything about this I would greatly appreciate your help.

Thanks in advance!","['R2 of mediators is from regressing the mediators (separately) on the IV', 'Thank you for the speedy response! Would I then have to report on each mediators R2 individually rather than a collective R2 figure for the mediation', 'for the DV, you report one, final R2']",1,3,https://www.reddit.com/r/statistics/comments/12wq2m0/q_r2_from_a_multiple_mediation_analysis/
340,2023-04-24 00:38:39,[Q] Adding regressions together for business revenue forecast,"I’m attempting to introduce some basic regression analysis to improve how many company thinks about forecasting revenue. After some testing in excel I have created a set of variables that I think work somewhat well. It’s about 8 products and each one has a renewal business regression and a new business regression, using about 48 months of data to predict next month’s revenue. Adding them up gets my total forecast and I can create confidence intervals on the total error.  I’m wondering if my current approach could be sufficient for a starting point, where the R-squareds for the regressions are generally .65 -.93. In many cases the p-values for some of variables are high, some are <0.05, so I’m not sure how to think about that? Once I become more confident in my own understanding I could take my datasets python to speed it up but wanted to make sure my approach was decent enough as a v1.","[""It's probably best to bring the analysis into python or R now for the diagnostic side of the analysis, ie, check assumptions and fit, and outliers, etc. There are some libraries or base functions that can help you determine how your model is fitting and perhaps why it's fitting that way. \n\nMaybe more importantly, you have multiple models with varying degrees of accuracy. Interesting, but why? How do these results relate to outcomes you intend to predict?\n\nLastly, if you intend to operationalize the model(s), you should cross-validate and test their predictive accuracy. The cross validation may also give you somewhat different r-squared metrics, eg."", ""This sounds like times series forecasting, you may want to look up the assumptions required to perform regressions on time series data if you haven't already.""]",6,2,https://www.reddit.com/r/statistics/comments/12whfwb/q_adding_regressions_together_for_business/
341,2023-04-23 18:33:03,[Question] The martingale system on roulette,"I know it’s gamblers fallacy but I’m trying to wrap my head around it.

It’s almost 50/50 for red or black no matter what the previous roll was, but the odds of continuously getting the same colour is low. 

Doesn’t that mean that the martingale system technically increases your odds of getting the colour you’re betting on increases with each play. Even though it is 50/50 regardless?","[""Correct. The Martingale system is flawless. If we were to play an imaginary game of roulette, with imaginary money, with me as the casino and you as the gambler, you would win every time.\n\nYou wouldn't even need to bet on red or black. You could bet on anything. As long as you kept sufficiently increasing your bets, you'd always win in the end.\n\nLose 100 bucks?, No problem just bet 1000 next time. Lose 1000? No problem, just bet 10,000 next time.\n\nEventually you'll win and cancel out all your previous losses and be a millionaire.\n\nOf course reality has two major differences to our hypothetical game.\n\nFirstly, I don't know your finances but I'll assume you don't have unlimited money. If your money runs out before you hit the big win, then your Martingale plan comes to an untimely end \n\nSecondly, even if you do have enough money, casinos have house limits. You might need to bet 20,000 on your next spin to break even, but they might not let you bet that much.\n\nWhat that means is that casinos are more than happy to let people try out the Martingale system. They know that most people will run out of money before they do.\n\nAnd if someone with unfathomably deep pockets does come in, they can just politely escort them from the building whenever they like."", ""Yeah, but here's the rub: Let's say you want to beat the casino by $20k.  That is, you want to leave with $20,000 more in your pocket than you had when you came in.\n\nYou need to play the martingale system 10,000 times, successfully.  In a row.\n\nNow, the odds of a failure of the martingale system are actually 0.00610351% if you've got a $20k limit.  And that failure will cost you about $33k (that's the sum of 14 martingale bets).\n\nOver the course of 10,000 tries, you have about a 54% shot of getting one failure.  Every individual try is extremely likely to succeed, but 10,000 is a lot of tries.\n\nSo, that's the downside of the martingale system.  Success is common and cheap; failure is rare and expensive.  You need to go through a lot of tries to make any significant money, and doing so magnifies your failure exposure."", ""Yes, that's right. Eventually you'll win.\n\nBut you'll have potentially risked a lot of money, simply to win a small amount.\n\nThe odds of you winning a small amount of money with this strategy are very good. Start with 2 dollar bets, you'll probably win 3 or 4 rounds without going higher than 32 or 64 dollar bets.\n\nBut then you've only won 6 or 8 dollars.\n\nYou'll have to play it a lot if you want to win something decent.\n\nAnd the longer you play, the higher the likelihood of a devastating loss.\n\nIf you lose 10 times in a row, which eventually will happen, you'll lose over 2 grand, which means you'll have to win another 1000+ two dollar Martingale rounds to break even, by which time you'll have another devastating loss.\n\nThe only way you can really win with it in the real world is to play a few low stakes rounds, and walk away with 20 bucks or so in winnings. The system means you'll likely win small in the short term, while losing in the long term."", 'It’s a mathematical certainty that you will eventually hit the bet limit.\n\nIf your bet size is small, it will take more tries, but you also will need to play more in order to make the same amount of money.', ""There's a low chance of losing many times in a row which is why it is a viable system for any reasonable number of bets.\n\nBut once you've lost 4 times, it doesn't matter that losing 5 times is pretty rare. You're now in a place where your fifth loss has a 50% chance of happening.""]",16,21,https://www.reddit.com/r/statistics/comments/12w46ra/question_the_martingale_system_on_roulette/
342,2023-04-23 16:24:00,[D] Is there a concise textbook for statistics?,"Every book that I've came across is a 500+ page beast with 100x more words than math. I mean, we are just talking about one or several large vector of numbers right? How hard can it be to just describe some of the operations that can be performed on it?

Is there a concise (and credible/useful) textbook on statistics?

I'm specifically interested in learning about hypothesis testing (I have no idea what a hypothesis is in statistics).","['No there’s no such a thing and that’s because it’s a vast field that is more philosophical than most branches of mathematics.', "">How hard can it be to just describe some of the operations that can be performed on it?\n\nYou will find that descriptive statistics usually take up only one or two chapters of these books.\n\nSo, yes, an introduction to that can be had relatively concisely. But that's also not what statistics is about."", 'They\'re complaining about too high a words-to-mathematics ratio; while they\'re not 500 pages, I don\'t think those titles really get at that ""more words than mathematics"" issue.', ""I don't think he meant you should avoid learning about hypothesis testing. pvalues are still everywhere and it's important to understand what they represent, and for that one needs to understand the framework of hypothesis testing. It's an important topic even if just for the sake of being able to understand what people mean and why X or Y study is crap :D"", 'It depends on your familiarity with statistics.\n\nBack in the days in grad school, we used ""All of Statistics"" for reference and we dived deep into Asymptotic Statistics. \n\nBut if you were new to statistics, you would be drowning in terminologies. \n\nFor self-taught purposes, a good thorough massage with a beast textbook is a good starting point.\n\nIf you come from an engineering background, the book ""A Modern Introduction to  \nProbability and Statistics - Understanding Why and How"" might serve you well.']",0,33,https://www.reddit.com/r/statistics/comments/12w15yg/d_is_there_a_concise_textbook_for_statistics/
343,2023-04-23 15:16:41,[D]Chi-Square Goodness and other measures of association,Hello everyone! Do you know if there's an ebook or epub for the two topics? I need it for my report next week. Or any other related source that would help understand the topic is also okay.,"[""You might try this. Be warned: this is a big topic, and it's hard to get the basics right without much more than a week's study. Even trickier is the fact that different fields of study use different measures for very similar calculations. \n\n[https://us.sagepub.com/en-us/nam/book/correlation](https://us.sagepub.com/en-us/nam/book/correlation)"", ""It's difficult to understand what you are looking for, but I'll try to give you some hints.\n\n• There are many different measures of association.  The appropriate measure depends on the nature of the two variables compared.  That is, continuous x continuous, nominal x nominal, nominal x ordinal, and so on.\n\n• These are distinct from hypothesis tests.\n\n• Measures of association should be unaffected by sample size, whereas hypothesis tests are affected by sample size.  That is, the correlation between A = (1,2,3,4), B = (2,4,3,6) will be the same as A = (1,2,3,4, 1,2,3,4), B = (2,4,3,6, 2,3,4,6).  But the hypothesis tests for these correlations not being 0 will be different.\n\n• In the continuous x continuous case, the common measure of association is Pearson correlation.  Spearman correlation is also common.\n\n• In the nominal x nominal case --- where a chi-square test of association might be used --- the common measures of association are *phi* (in the 2 x 2) case, and Cramer's *V*.  I have other options included here: [https://rcompanion.org/handbook/H\\_10.html](https://rcompanion.org/handbook/H_10.html) .\n\n• I have some measures of association for ordinal variables described here: [https://rcompanion.org/handbook/H\\_11.html](https://rcompanion.org/handbook/H_11.html) ."", 'What types of variables are you looking for measures of association for ?  (Nominal, ordinal, continuous...).', ""I think all of those 3 that you mentioned. I need to have an oral report for that topic. I'm looking for sources online and so far I got the Chi-Square Goodness of Fit test."", ""The chi-square goodness-of-fit test isn't a measure of association.  It's a hypothesis test.  ...  I'll try to give you some hints in another response.""]",2,7,https://www.reddit.com/r/statistics/comments/12vzl3s/dchisquare_goodness_and_other_measures_of/
344,2023-04-23 15:04:47,"[Q] R is posisitve in linear regression, what now?","Hello, Im writing a thesis and have a hypothesis, where higher score in one factor (negative emocionality) predicts lower score in other factor (attitudes toward autonomous vehicles). To test it I run linear correlation, and R came out positive (so positive relation?) at p was in significant range (.007). Everything seems as they are positively related, except for residual plot, where when score in one gets higher, the score in another one gets lower. Can anyone explain to me, why that plot is like that if the relation between them is positive?","['In the table you have shared, the slope of regression line is negative (-0.402), so I’m not sure why you think that the variables are positively related. R is positive simply because it’s a square root of R squared, it represents only magnitude if the effect, not direction. I.e. it’s not Pearson correlation coefficient per se.', 'Please provide the full linear regression calculation and plot if possible.  \n\nWhat is the intercept, coefficient, R squared values?', 'While squaring Pearson\'s correlation coefficient gives the amount of ""explained"" variance (R^2 ), taking a square root of R^2 doesn’t necessarily gives Pearson\'s correlation coefficient. This simply because  R^2 is always positive, so there is no way to get the direction of the relationship from it. The software you are using is doing the later.\n\n\nSince you are using linear regression, you should be reporting regression coefficients to communicate the exact nature of the relationship.', 'For clarity, the 95% CI in this case is for the unstandardized b-weight, not for R. I think that’s where the confusion is.', ""You are confusing several differences between correlation and linear regression.\n\nCorrelation: produces an r value.  There is no such thing as a residual plot or R.\n\nRegression: produces coefficients, standard errors, p-values, R2.  R2 is a measure of quality-of-fit and cannot be interpreted as directionality of effect.  Residual plot is a useful diagnosis, but also cannot be interpreted as directionality of an effect.  The coefficients and standard errors are for interpreting directionality.\n\nIt may seem pedantic to get so specific about all the terms, but I promise it's not.  Without precision in the terminology, it's impossible to communicate your conclusions or even to reach the correct conclusions.""]",0,16,https://www.reddit.com/r/statistics/comments/12vzb3h/q_r_is_posisitve_in_linear_regression_what_now/
345,2023-04-23 14:36:16,[Q] Advice appreciated- What to do when the statistics don't appear to match the visual data trends,"Hi there. Wondering if anyone has some advice. I've run some statistics on a few different datasets (Kendal's Tau) and it's giving a strong positive correlation with the two variables (like P = 0.001 type ranges). However if you look at a scatter plot, if anything it appears to be a roughly negative association. I realise Kendal's Tau ranks data in a way that is less sensitive to outliers (of which my data does contain) but what do you do if you just can't see the trend the stats are saying are there? Do you assume it has been over analysed and bin it? Or something else? (Sorry I know probably hard without the data here to see. But it's been doing my head in trying to work out what is going on here. Data isn't suitable for parametric testing which is why Kendal's has been used. (And Kendals rather than Spearmans to deal with the outlier situation.) Thanks!","['My first advice would be to plot the data in the same way the test is interpreting them ( i.e. as ranks). I think in R this would be done with this code ""plot(rank(x), rank(y))"". Does the new visualisation better align with the result of Kendall\'s test?', 'Binning data is generally discouraged - it’s a waste of information. What sort of question are you trying to answer? Kendell’s Tau isn’t really comparable to a trend line. How do you know it isn’t suited for parametric tests?', 'Thanks for the suggestion! I did that and still cannot see any visual correlation. TBH I have no idea how the correlation has come up with the results it has.', 'I don\'t need a trend line per se, just a correlation (or lack of, one way or the other). It seems to fail various assumptions on every parametric test I\'ve run it through. Largely because there\'s clusters and outliers I think. I\'ve also got some (not all) datasets which have ordinal data against continuous which doesn\'t suit a lot of the stat tests out there too. They\'re ""messy"" datasets with a lot of uncontrollable variables which is pretty par for the course with the type of research this is.\n\nI\'ve got a few datasets that are in the ""I don\'t know what to do with this"" category where a scatterplot indicates one thing, and the a correlation test (usually Kendal\'s) says the complete opposite. \n\nI guess short answer is I want to know if one factor is influencing a second factor in a particular way. In this particular case you\'re looking at levels of a pollutant in a species of wildlife against various health perameters. Is there a direct correlation (ie more pollutant = sicker animal), threshold effects (ie over a certain level of pollutant, start to see sick animals) or no apparent correlation at the observed levels. \n\nMy issue is with some of the datasets, there appears to be a limited correlation that is probably negative in nature, but kendal\'s throws up positive correlations with p= 0.001 values which makes it confusing to decide how to report it. (Which is why I\'m wondering whether to just discard a whole heap of work due to this.)']",3,4,https://www.reddit.com/r/statistics/comments/12vyn0n/q_advice_appreciated_what_to_do_when_the/
346,2023-04-23 14:28:37,[Q] Is there a way to find the threshold point of a dataset statistically?,"Hi there, I was wondering if anyone could advise on a test to figure out
where the threshold effect is on a dataset? For example say a test
group had been given different doses of a medication in mg and the
response recorded. Could you find at what point there appeared to be
getting a response to that medication in at least some members of the
test group?","['How do you define ""response""?', ""If your response is binary (yes/no) then you're looking for a chi-square or logistic regression."", ""What are your research question and hypotheses? Without those it's virtually impossible to give useful advice."", '1. How granular is the dosage? You could discretize the dosage into some meaningful set of intervals (e.g, 1mg, 2mg, etc.). What to do next depends on how the response is operationalized.\n\n2. How is ""response"" operationalized? If the response is a continuous measure, you could estimate a linear or generalized linear model. Any global test for the model will address the question of whether there are any differences in the response across the dosage levels. The individual beta parameters will tell you whether the average response at that dose is distinguishable from the average response at the lowest dose.\n\nThis is assuming that the dosage is assigned at random.', 'Yes, this is known as dose-response modeling. Look into the EPA software called Benchmark Dose Software, it provides the tools to fit a variety of model forms for either continuous or dichotomous responses. You will specify the benchmark response which is the amount of response that is meaningful to you, and you are returned the benchmark dose which is a point on the curve and represents the dose associated with your benchmark response.']",3,13,https://www.reddit.com/r/statistics/comments/12vygjd/q_is_there_a_way_to_find_the_threshold_point_of_a/
347,2023-04-23 12:34:47,[R] Linear Regression with ordinal DV and continuous IV,"Hi guys, I'm writing my thesis currently. In my thesis I want to see whether mental toughness can predict sport performace. Sadly in my questionnaire the only determinant to sport performance I used was the level of league in which athletes play (not the smartest option). After some research I've come to the conclusion that I have to use Ordinal Logistic Regression. I'm am using Jamovi. I'm not sure whether I can interpret McFadden's R\^2 as a I would interpret a typical R\^2. Can I interpert it as the typical R\^2 for variance? If you could see an option where I could also use another test, or have knowledge of how Ordinal Logistic Regression works any advice would be greatly appriciated. Thanks guys!","[""Here are some resources that explain how to interpret McFadden's pseudo R^(2):\n\n* https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/\n* https://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation\n* https://thestatsgeek.com/2014/02/08/r-squared-in-logistic-regression/\n* https://stats.stackexchange.com/questions/3559/which-pseudo-r2-measure-is-the-one-to-report-for-logistic-regression-cox-s\n\nIn short: No, you can't interpret it as you would an R^(2) in an ordinary linear regression (proportion explained variance). It is typical for R^(2) measures for logistic regressions to be quite low compared to the R^(2) for linear regressions. Personally, I would only use pseudo R^(2) for comparing two models but I wouldn't interpret the value for a single model."", 'Hi thanks for the response. I ended up using a regular Anova. I am no longer predicting however but just seeing whether the level of league a player plays in has an impact on their mental toughness.\n\nThanks for all the info!', 'Why can’t you use linear regression?']",8,3,https://www.reddit.com/r/statistics/comments/12vvpfl/r_linear_regression_with_ordinal_dv_and/
348,2023-04-23 05:04:41,[Q] What test can I use for my (large and confusing) dataset?,"I have collected data on cell survival for various radiation experiments.

The output/dependent variable is ""Percentage Survival Change"".

There are probably about 500 rows of data points from many different experiments. Each data point has multiple categorical variables attached to it. Independent variables include: Cell line, Sex of the cell line, Organ of the Cell Line, protein status (mutant, wild-type, or null), radiation source used, etc.

I'm looking to see if any of these independent variables have a statistically significant impact on the ""percentage survival change"" variable.

When looking at the distribution frequency of the ""Percentage Survival Change"" (values range from -99 to 140), it appears to be normally distributed.

When comparing ""protein status"" for example, the number of mutant, wild-type, and null cell lines are not equal (or even close to equal). All of these independent variables will be unbalanced.

I do not have a strong foundation in statistics and I'm struggling to find a test that will work for my particular case. I'm not sure how to approach this.

I appreciate any help or advice that you can give! Please let me know if you need any more information!","['One potential option could be a glm model. The link here is for running it using Stata, but other software packages can run similarly\n\nhttps://stats.oarc.ucla.edu/stata/faq/how-does-one-do-regression-when-the-dependent-variable-is-a-proportion/', ""The unequal sample sizes shouldn't cause problems with most common analyses.\n\nIt sound like you need to consult with someone.  Maybe there's someone at your company / university department you can talk to ?"", 'I’ll look into this. Thank you very much!', 'Yes I’ll have to check in with my supervisor this week. Thank you!', 'How does one have the training to conduct radiation experiments but have no idea how to analyze the data?']",1,5,https://www.reddit.com/r/statistics/comments/12vj95b/q_what_test_can_i_use_for_my_large_and_confusing/
349,2023-04-23 04:55:27,[Q] [E] Top Applied Stats Ph.D Programs for Academia positions?,"I’ve read and heard, both on this subreddit and in my own math department, that if I expect to get a position in academia with an (applied) stats PhD, I need to be at a top 25 stats PhD R1 institution. This makes sense. However, what schools are considered to be in this upper echelon? I feel like its different depending on who you ask and want to know your thoughts!

EDIT: e.g. Northwestern has a smaller Stats department, some consider it to be highly ranked, but US News has it at 37 (outside the mystical “top 25” moniker).","['I can\'t believe none of the posts so far have mentioned this. \n\nWho your advisor/post doc advisor is is a fuckton more important than ""what"" school you are from. \n\nA well known leader in their field from a ""lower"" school >>>>>>>>>>> some brand spanking new assistant professor from Harvard. \n\nGranted, Top 10 and Top 25 programs have a larger pool of well known leaders in their fields. But they aren\'t all in these institutions.', 'If you want to do applied stats work as a researcher and teach course work as a professor in a more applied flavor you will almost certainly benefit from being in a Bayesian department in my opinion. A very frequentist department are often times essentially applied math departments, they will want people who have very solid coursework at the PhD level in measure theory and probability which you may or may not get from an Applied Statistics PhD.\n\nNow why does this matter? Because you can dramatically improve your odds from going to a Bayesian department for your PhD that will allow you to stand out from other applicants and be able to bring something to the table that others don’t. Additionally (as a PhD student at a very Bayesian program) the network you build as a Bayesian may be a bit closer knit than going to a normal or frequentist department because there are just a lot less of us in academia, everyone pretty much knows everyone. \n\nAdditionally, just practically speaking, the simulation and computational techniques you learn in the Bayesian framework is going to be very useful to pretty any applied problem you encounter.\n\nIn my opinion, if I were you I would strongly consider the well known Bayesian departments and connecting with the professors and getting a feel for whether you like it or not. Some do very theoretical work, and some do very applied work. \n\nHope this helps! Good luck on your journey! :)', 'Duke, UT Austin, UCONN, UC Santa Cruz, UC Irvine, Columbia. Obviously Stanford, Berkeley and UW have amazing Bayesian work too, but I was trying to give you schools outside of the Top 5.', ""I'm not an academic (I went to industry after my PhD) so take anything I say with a slight grain of salt but...\n\n1. Does the person have a track record of graduating students (check their CV)\n2. Do they have a long history of funding and high impact publications (total citations, h index, etc.)"", 'I would look at the US News stuff, but not take it as the tell-all of things. (e.g., you may not want to travel cross-country)\n\nThere are three in NC that I’d recommend… the big three — UNC, NCSU, and that one in Durham who, for reasons pertaining to my undergrad affiliation, will remain nameless (jk it’s Duke) are great. No, they aren’t Harvard, Stanford, or UC Berkeley or UChicago or whatnot, but:\n- you may not like cold (Harvard)\n- Cali is expensive \n- Chicago is a BIG city\n\nI say all this to say that, while I only applied to 2 PhD programs, I chose mine because of the *culture* before the name.']",4,22,https://www.reddit.com/r/statistics/comments/12vizbj/q_e_top_applied_stats_phd_programs_for_academia/
350,2023-04-23 04:20:10,[Q] [E] Question about finding the appropriate lower boundary for what is considered a famous tiktok book,"Hi!

  
For my master's thesis, I am writing an algorithm that can predict which books will become popular on Tiktok. Right now I am working on getting the right labels for the dataset, based on the viewcounts of the books on Tiktok. The dataset I have by far doesn't include all the currently popular tiktok books, so for now, I just have a list of 19 books that have done well on Tiktok, with their viewcounts (there are probably more, but I had to make a list myself of what I know for sure are famous books). I want to use their viewcounts to set a baseline for what is considered popular on tiktok and what is not considered popular. However, the viewcounts all lie pretty far apart. They are the following numbers:  
66089172, 909551, 14159253, 5771561,  68456152, 20982050, 6767132, 61012995, 39505320, 1299157, 27307,  38193455, 34048345, 9830311, 87600000, 37921810, 88484025, 55764970,  108154.  


I have considered using the mean or median, but since the numbers lie pretty far apart and they aren't normally distributed, I don't think I can use those. I then considered using the mean - 3 times the standard deviation, but this gave me a lower bound of zero, meaning that all books would be considered tiktok famous. I also tried using the 25 percentile - 1.5 times the interquartile range, but he same thing happened. 

Right now I am thinking I could just use the lowest number of the list, since I know that one is Tiktok famous so ones with even slightly more views will be considered famous as well, but this feels like it's very wrong, statistically speaking, so I was wondering on your opinion on this, and if you had any advice or recommendations?  


Thank you so much in advance!","['I would try log-transforming your data, calculate statistics on the log domain, and then back transform when you need to see the linear scale.    That will avoid your zero errors, and also help stabilize the variance in your data.\n\nJustification for a log-transform:  the [change in] popularity of a book will depend on how many social media users have posted about it.    This is the key trait that means growth is exponential.', ""It seems like you have already decided what counts as 'popular' by picking these 19 books. Unless you want to come up with a more agnostic criteria and gather more data to see what meets these criteria, just use the smallest on the list that you have already generated. You still need to justify why these 19 are what should count as popular, but doing more math to the 19 data points you have won't help you there."", 'Thank you so much, that actually makes so much sense, I will try doing that!']",1,3,https://www.reddit.com/r/statistics/comments/12vhy7o/q_e_question_about_finding_the_appropriate_lower/
351,2023-04-23 02:41:05,[Q] Conducting a lot of multiple regressions on the same data set,"Working with a social science student who has a data set that includes a 25 potential predictors. These can conceptually be grouped into three basic types - demographics, social factors and digital device usage, with approximately 7 to 8 predictors per group. It is not though possible to sum any of these to create a single measure. There is a single outcome variable, which for the sake of this question I will say is amount of money spent (continuous). Sample size is 350 participants.

His plan is to conduct three regressions - one with the demographic predictors, one with the social predictors, and one with the digital device predictors. Same outcome variable in each case. My concern is that by doing so he is effectively just doing one single regression model, except that by splitting it into three models he will not pick up on potential issues such as multicollinearity between predictors (e.g. if they are in different models). I know there are probably more complex analysis he could do that would be more appropriate than a regression, but there is a limit on how complex an analysis he can be expected to do at this stage in his studies. It is also a good learning experience for him to understand the pros and cons of using regression for this data. However my Yoda like attempts to guide him to his own realisations are being hampered by my own lack of understanding, as I have never encountered a situation where someone has so many predictors for a single outcome. I inherited this student from someone else who has even less stats knowledge. Any advice?","[""Why exactly do they not want to run a single regression with 25 predictors? Are they worried that 25 is too many for your sample size or for interpretability? If that's the case, then they could use some kind of penalized regression like LASSO, ridge, elastic net, etc. that will likely shrink some regression coefficients to 0 or near 0 which may help with interpretability."", '> It is not though possible to sum any of these to create a single measure\n\nYou can always run dimensionality reduction with PCA or FA. If I remember correctly running PCA on the predictors and then doing the regression as `outcome ~ component1 + component2 + ... + componentN` (where the PCA selected N components) is a discrete approximation to L2 regularised (Ridge) regression.\n\nIncidentally the recommendation elsewhere in the comments for Ridge|LASSO|elastic-net regularised regression is a good one, and technically better. The PCA approach is conceptually simpler and on the way to what you want.', 'In addition to other answers, you could also just fit all the scenarios you’re concerned about. Y ~ all_predictors, Y ~ demographics, Y ~ social, Y ~ device\n\nAssess the models as normal. Look at the Adjusted R squared to see which had the best fit. You can take it a step further and compare the models with Anova testing. The best fitting model with have the lowest AIC score in that case', 'Unless you want to be able to interpret your predictor variable with any degree of reliability. PCA needs, well, PCs.', 'A single dataset with a single outcome variable. I think his rationale for doing three separate regressions is that it will help him better understand how important each of the three groups of the predictors are compared to the other groups. My initial reaction was that he could just do a single hierarchical regression that would allow him to i) identify which predictors are significant and ii) determine how these significant ones compare to each other in predictive strength. Not sure if that is feasible though.']",23,15,https://www.reddit.com/r/statistics/comments/12vf3fw/q_conducting_a_lot_of_multiple_regressions_on_the/
352,2023-04-23 01:16:18,[Q] Show sum of exponentials is chi-square,"\*\*Sorry, I can't change the title. I am looking to show that 2 times the sum(X_i - X_{(1)} is Chi-Square(df = 2n - 2) \*\*

\---

if *X1, . . . , Xn* are iid with pdf

*f(x|s, t) = s*^(-1)*exp(- s****^(-1)****(x - t))* ***1***\*(x > t)\* , (exponential with scale *s* and location *t*)

How do I show that *2s****^(-1)****\[ ( X1 - X(1) ) + . . . + ( Xn - X(1) ) \] \~ Chi****^(2)****(df = 2n-2)* ?

where X(1) = min(X1, . . . , Xn) and ***1***(x > t) is the indicator function which is 1 when *x > t* and zero otherwise.

\-----

My first thought was to decompose the terms in the sum into *( Xi - t )/s - ( X(1) - t )/s,*

which would then give *2( n\*(Y1 + . . . + Yn) + Exp(V) )*, where Yi  are iid \~ *Exp(1)* and *V \~ Exp(1)*. But *Yi* is not independent of V.

Someone suggested showing that *sum( Xi - X(1) )/s* are independent of *( X(1) - t)/s* using the joint pdf of the order statistics. But after this I am not sure how to proceed beyond this.

I feel like I'm missing something obvious","[""No they're dealing with an exact finite sample result\n\nAn (unshifted/ one-parameter) standard exponential, when doubled, is chi^(2)(2)"", '**Hint:** show that exponential spacings are independent and remain exponential random variables (possibly with different rate parameters). This can be done by looking at the order statistics of iid exponentials and applying a change of variables. You can then write the sum you are interested in as a linear combination of the spacings. You can take t = 0 and s = 1 WLOG.', ""Reddit uses a version of Markdown. So if you use things that correspond to Markdown formatting, that's how they'll be interpreted."", 'Yes. Each X_(i) - X_(1) is a sum of spacings.', 'Sorry for the mediocre formatting of the math. Reddit seems to change things when I edit.']",2,12,https://www.reddit.com/r/statistics/comments/12vcnk9/q_show_sum_of_exponentials_is_chisquare/
353,2023-04-23 00:27:47,[Q] Master's Degree: Statistics or Computational Statistics and Machine Learning?,"Hi all, I am choosing between 2 postgrad programmes, Statistics or Computational Statistics and Machine Learning in one of the universities in the UK. My future goal is to be a quant/trader or a data scientist/machine learning engineer (but not interested in being a 'sql monkey' or doing dashboard and visualization stuff, but more on modelling stuff).

I am torn between these two programs but one is seen as more academical and traditional and the other may be seen as more modern and practical (but some may think it's just buzzwords and just a cash cow programme).

I am keen in both traditional statistics, but also interested in coding. I have a bachelor's degree in Data Science (which again, may not be seen as too academical), which involves already involves both statistics and coding . I know this is a STATISTICS subreddit, but would love to hear opinions from everyone!

What would my best choice be?","["">I am keen in both traditional statistics, but also interested in coding.\n\nIn my experience it's far easier to learn traditional stats formally and coding on the side than the other way around."", 'I think the selection of modules you can take from CSML are more varied and more applicable to different job fields', 'Depends what you want. Example, if you plan on just trading or working with ml models then CSML is a great option. If you want to work with ml models and have a deep understanding of them then statistics. Now if you want to create your own ml models get a PhD stats.', 'In my opinion, I would get a stats MS. If you want to go the quant or industry researcher route, you will need to be very strong in your maths. In my opinion, as a Stats PhD student in a very Bayesian department, you should look for a program that is going to make you learn some good statistical foundations AND modern simulation techniques/methods. In my opinion, you can’t really do modern statistics without being very competent in your coding and simulation techniques. Obviously I am biased, but I would look for a place that has a Bayesian slant so you will get a good mix of classical statistical asymptotic inference/methods and the more computational simulation techniques leveraged in most Bayesian research or course work.\n\nYou would be surprised how many departments simply don’t teach you any coding or simulation techniques in favor of things like mixed effect models, ANCOVA and measure theory. There is 100% a time and place and need for those things, but not for the types of things it sounds like you are interested in.', 'Hi, thank you for your suggestions. Unfortunately looking at the course structure, it seems the two programmes do not focus on both statistical foundations AND modern simulation techniques/methods. It is either or. Would you be able help me take a look at the curriculum and suggest which would be better?']",8,5,https://www.reddit.com/r/statistics/comments/12vb5vh/q_masters_degree_statistics_or_computational/
354,2023-04-22 12:23:50,[Q] How would you determine if a new measurement is different from a series of previous measurements?,"
There seem to be many approaches to this type of question for different fields. What do you view to be the most appropriate analysis method to determine if a new measurement within an individual is significantly different from the previous series of measurements within that same individual?

 I am purposely trying to not use any leading jargon here to see the widest variety of answers.","[""One approach would be to treat the value of previous measurements as a distribution. For a new measurement, the number of standard deviations away from the distribution's mean would inform how different the new value is from previous values. \n\nNaturally there are caveats, exceptions, ways it can fail, and thresholds to determine significance. This may or may not be appropriate for a given data set but works in a wide variety of contexts."", 'It very much depends on what you want to assume and what alternatives you seek power against.', ""This is a good approach if we don't expect a trend. If there can be a trend (e.g. following weight of the individual) then we should probably remove that trend first."", 'You say, ""within an individual"". How many individuals are there? How are they related? What are the expected patterns within individual as suggested by the theory of your specific context?\n\nThe issue is that the approaches vary so widely that it\'s nearly impossible to answer this question without context and more detail. \n\nFor example, if the suspected change is at the end of a series of values then you might use statistical process control. If not you might use standardised residuals. But these are not so much approaches as they are entire fields of statistics.', 'Depends on the measurement.']",15,11,https://www.reddit.com/r/statistics/comments/12uugb9/q_how_would_you_determine_if_a_new_measurement_is/
355,2023-04-22 11:17:24,"[Question] How to calculate the probabilities of certain outcomes in Magic: the Gathering tournaments? (This is an interesting question, I swear!)","*Edit: This has been* ***solved****! Thank you to* u/WhosaWhatsa *for pointing me towards Markov chains. Now I'm off to go figure out a good, free way of modelling those...*

Earlier I was trying to calculate the expectation value of some events in Magic: Arena. They generally have the structure of ""Play games until you win X or lose Y"". For instance, one event might continue until a player has *either* 7 wins *or* 3 losses. I wanted to calculate the EV of the rewards for these events, factoring in my winrate in the game.

I've taken some stats classes, and gotten As in them. This seemed like a pretty approachable problem, so I  googled it, found [an existing thread](https://www.reddit.com/r/statistics/comments/11wzl9f/q_odds_for_magic_the_gathering/) that offered some pointers, opened up Excel and R, and started messing around with the negative binomial distribution. I can't bend my head around how to do this.

So my end goal is to find the expectation value of an event in-game, where the rewards are based on the number of wins achieved (up to 7) before reaching 3 losses. If I'm understanding it correctly, this means the probability space is {0-3, 1-3, 2-3, 3-3, 4-3, 5-3, 6-3, 7-3, 7-2, 7-1, 7-0}. The problem is that these are obviously not equally weighted--a 7-0 record is less likely than a 3-3 record, and the chances of each depend on an individuals winrate.

As far as I can tell, the Negative Binomial Distribution is *should* to being able to address this, but doesn't actually. To my understanding, it gives us the number of *attempts* we must make before seeing a certain number of failures, which we can extrapolate a probability from. This seems like it should work--I set up a spreadsheet that you punch a winrate into, and then it tells you the probability that you'll end with any given win-loss record in that probability space. But I keep ending up with total probabilities that aren't even close to 1, particularly for very low winrates. ([Here](https://docs.google.com/spreadsheets/d/1hSKoi3-Mbs3UbpVwZCSqO0nw_tiIRUci/edit?usp=sharing&ouid=109837159073213104462&rtpof=true&sd=true) is a link to the spreadsheet I was working on--it doesn't look like much, but it's as close as I could get to an actual solution. I also spent a decent amount of time in R, and didn't get far with that either.)

This has driven me completely insane. I think part of the issue is that not every string of possibilities is of equal length. (ie, 1-3 vs. 7-0.) Also, not every permutation of wins and losses is permissable. (WWWLLL is allowed, LLLWWW is not.) Finally, not every string ends in an L. (0-3 through 6-3 must all necessarily end in losses, but any 7-win sequence ends in a win.)

How the h\*ck do I calculate this? Is my approach fundamentally flawed? Am I totally misunderstanding how the negative binomial distribution works? Or am I missing something else totally obvious? At this point, I'm pretty well beaten by this problem, and I don't have any clue how else to proceed. Can someone give me some pointers?

Thanks y'all.","['It may be helpful to consider these probabilities as part of a markov chain. In other words, the probability of winning or losing a match in-game may be more dependent on the present status of the win/loss situation at that time.', 'Yes, because I thought it would be funny.', 'This article may be a helpful start as it also addresses markov processes in the context of another discrete time period based competition in tennis. Not entirely sure if it lines up, but do Magic games have points-based damage counts eventually leading to a win or loss? If so, it seems similar to tennis scoring, as well as martial arts competitions. \n\nhttps://towardsdatascience.com/markov-chain-models-in-sports-7cb907a6c52f', ""Fortunately I think that it's going to pan out in a relatively simple way. Each state's transition matrix only has two nonzero transition probabilities, and I think I can get away with modelling those as each being a constant derived from a players winrate. (Which I think I can get away with modelling as unchanging, for reasons that have more to do with gameplay than math.)\n\nActual complex Markov chains look intimidating as hell, but this is a little baby one haha."", ""OK, as an update on this, I found a very crappy tool intended to teach college students markov chains, and it is in fact the correct way to model this process. Thank you very much! (A markov chain wouldn't be suitable for predicting the outcome of an individual game like in Tennis, but the *tournament structure*, which is what I was looking to model, is very, very similar. Seriously, thank you so much for helping me make that connection!)\n\nOf course, now I need to go and find some actually good tools for modelling markov chains...""]",26,29,https://www.reddit.com/r/statistics/comments/12usw0n/question_how_to_calculate_the_probabilities_of/
356,2023-04-22 06:49:38,Can you help me articulate the error with the approach?[Q],"I’d like to determine the average ratio of CEO to worker pay in the USA. Someone would like to take the average CEO salary and divide it by the average worker salary. I think this wouldn’t be a statistically valuable approach, but I’m having trouble explaining why. Can you help? Or explain why I’m wrong and it is a good approach?","['The first thing to be clear on is that the ratio of average and the average of ratios  (average of workplace ratios) will be different, so you need to be sure you have the one you want. \n\nAnother thing to beware of is that CEOs often have substantial benefits that are not cash salaries. These often get missed in such comparisons', 'This sounds like you have a narrative you\'d like to support and are looking for ""statistics"" to back it up.\n\nI would expect the CEO pay is heavily right skewed as CEOs of large corporations make significantly more than CEOs of small corporations. This means the [average](https://en.wikipedia.org/wiki/Arithmetic_mean) is less than meaningful for CEO pay. There also appears to be a false dichotomy, in that there are either ""CEOs"" or ""workers"", apparently ignoring the rest of the c-suite and professional workers who can easily pull down large pay checks as well.', "">I understand it’s a political subject.\n\nIt's not political at all. It just sounds like you're starting with a conclusion and want to find an analysis that will provide results that support that conclusion. That's not how science is done.\n\nFor that, and other reasons pointed out by u/jentron128, you're practicing bad statistics."", 'As I explained to him, we can’t event get to whether there’s a stance if we don’t agree on the quality of the metric in the discussion.\n\nIs the methodology described an approach that will provide insight or is there a reason the data would be skewed in some way and another approach should be used? \n\n\nIt seems to me that both of you decided bc of the political nature of the any discussion around CEO pay that I have a foregone conclusion. I’m no statistician but I do work with data and actuaries daily and do enough analysis to have a hunch something isn’t right with the approach but I’m not advanced enough to follow my hunch to either validate or invalidate. I had a hypothesis I don’t know how to test. So I came to a subreddit and stated where the question arose, and the methodology in question. \n\nCan you please give me your thoughts on the method to approach the question ?', ""One approach asks about the ratio of the compensation of average CEO compared to the average non-CEO, across all companies, which isn't very useful at all.\n\nThe other approach asks about the ratio of compensation of CEO to average non-CEO, by company, which is only barely more useful.\n\nPartitioning workers as CEOs or non-CEOs is an extreme oversimplification, and will likely lead to results that offer little insight.""]",2,12,https://www.reddit.com/r/statistics/comments/12ulhq8/can_you_help_me_articulate_the_error_with_the/
357,2023-04-22 04:31:06,[Q] Paid Ads Probability of Never Converting,"Hello All - appreciate any help or insights on this question:

I am the paid ads manager for my company. I am running ad campaigns where we bid a specific amount on individual keywords related to a product. There are sometimes that the keywords are distantly related to the product and never drive a conversion. I am wondering how many clicks (or views) on a specific keyword we need to be sure that the keyword is x% likely to never deliver a sale. I will give data as a specific example.

The Product is a ""Knee Brace"" in general the knee brace converts 10% of all visitors. Across all traffic to the listing.

We are bidding on the term ""arthritis products"" we have had 100 visitors to our knee brace product and had 0 conversions.

Is there a way to give a % to the certainty that the term ""arthritis products"" will never deliver a sale for the knee brace?","[""I might be totally misunderstanding the problem, but here's a suggestion.\n\nLet's say there are n clicks for a particular keyword, each with probability p of converting to a sale. Assuming each click is independent, the number of conversions x has a binomial distribution x \\~ Bin(n,p). You know x and n, but not p.\n\nYou're interested in p, the probability of conversion. In particular, you want to know the chance that p is 0, or very small, if x is 0.\n\nOne approach is Bayesian - the [beta-binomial](https://en.wikipedia.org/wiki/Beta-binomial_distribution) distribution. This is where x has the binomial distribution described above, and uncertainty in p has a beta distribution. The distribution for p is updated after you observe x (there's a formula on wiki). You can then calculate the probability of p being less than 1% given that x=0, for example, by integrating the updated distribution.\n\nThe other approach is frequentist. Basically, you consider the hypothesis test with the null hypothesis p > 0. Neyman-Pearson theory can tell you the n (how many clicks) you need for the hypothesis test to reject the null hypothesis. I'm not too familiar with this approach, but might give you an idea to start looking. Note that this doesn't give the chance that p is 0, but rather says the data are unlikely if p is not 0."", 'Do you have data on other keywords / conversion rates?  Are others about 10% also?', 'I think you are understanding it clearly.  And I appreciate your suggestions.  In short I trying to understand if I want to continue bidding on a particular keyword.  \n\nSo, I believe the conversion rate of each keyword needs to be treated independently.  My goal would be to determine the likelihood that the conversion rate is low enough that I am not willing to bid on it.  To give further detail on the knee brace example:\n\nTerms:\n\n""Knee Brace"" - hyper relevant - 10% conversion rate\n\n""Help my knee arthritis"" - moderately relevant - 2% conversion rate\n\n""Honda Accord"" - not relevant - 0% conversion rate\n\nI am trying to understand, how many samples do we need to be sure that the conversion rate of the ""Honda Accord"" term is 0% or statistically significant that it is below .1% lets say.  Its obvious with the human eye that ""Honda Accord"" is not relevant, but I am trying to build an algorthm that does not require that human eye', 'I do and would have data on all keywords and their conversion rates.  For the sake of this exercise I made this data up.  But basically my goal is to understand when I should stop bidding on this keyword and I can be sure it is not likely to drive a purchase.', 'Ok so if you have some kind of baseline data, you could use that to estimate the probability of x # of failures before the first success using the negative binomial distribution.  \n\nIf we used the 10% baseline the formula in excel would be:\n\n= 1 - (NEGBINOM.DIST(100,1,0.1,TRUE)) = .0000239\n\nThis is the probability of 100 or more failures before the first success if the probability of success on each trial is .10 (10%).']",2,5,https://www.reddit.com/r/statistics/comments/12uhaa4/q_paid_ads_probability_of_never_converting/
358,2023-04-22 03:30:42,[Q] why do we opt to test the null hypothesis instead of testing our alternative hypothesis instead? Is it because we don’t have enough data to make the alternative hypothesis specific enough yet? Or because multiple alternative hypotheses could yield similar data?,,"['Because the sampling distribution of the test statistic is known under the null hypothesis, that is assuming that the null is true.', 'Not exactly. It\'s more about the structure of hypothesis testing and how it works. \n\nIn null hypothesis testing, we usually assume a point null, that is, a null hypothesis that is a single point. This null is specific enough that we can describe the distribution the test statistic would have if the null were true. Then we look at where the test statistic calculated from our data lies within that distribution. If it\'s very extreme (indicated by the critical value or a small p value) then we reject the null hypothesis because the test statistic we got from our data would have been unlikely if the null were true. The alternative hypothesis in this setting is really just ""not the null"".\n\nThere are other kinds of hypothesis tests that compare two point nulls, (e.g. mu=0 vs mu=1) but are performed using different methods.', 'Usually the alternative hypothesis encompasses a range of parameter values, not just a single one. This makes it impossible to calculate how the test statistic will be distributed under the alternative without knowing *which specific* value under the alternative it will take, which in turn makes it impossible to develop a test for.\n\nIn case where the alternative is also a point hypothesis I guess it\'s possible to test it. E.g. H0: μ = 50 vs H1: μ = 60. But then it\'s mostly semantics which one is the null since you can flip them around at will.\n\nAlso for hypotheses that are not about parameter values of a model, this same idea of null = one case, alternative = many cases still holds. For example in normality tests you have one hypothesis that says normality and another that says non-normality. There\'s an infinite amount of non-normal distributions, hence you cannot calculate how the test statistic will behave in general under ""non-normality"". So in normality tests, the null is always normality.', 'It’s convention because to test a hypothesis you need to start with a hypothesis that can be rejected (null).\n\nThe alternative hypothesis isn’t designed to be rejected - it’s there to show the null hypothesis is incorrect.', ""I think it's also important to appreciate the historical reasons for this approach.  Once they were developed and disseminated, simple tests in the *null hypothesis significance test* paradigm were relatively easy to compute, with just a table of critical values and, later, a hand calculator. \n\nIt might be argued that now, with everyone having computers, there are are different paradigms maybe we should teach as the default approach, perhaps Bayesian inference.""]",13,14,https://www.reddit.com/r/statistics/comments/12ufg3c/q_why_do_we_opt_to_test_the_null_hypothesis/
359,2023-04-22 02:07:27,[Q] Help replicating power analysis from a published article,"Hi all, I hope you're all doing well!

I'm a student assisting a MD with a research project, and I have been tasked with trying to replicate a power analysis from a published paper. Specifically, I am trying to replicate the steps the authors completed to generate the results found in Table 1 of this article: DOI: [10.1177/23259671221110851](https://doi.org/10.1177/23259671221110851)  but for my own data. 

In their methods, the authors wrote the following: ""The statistical power to detect a difference equivalent to the lowest reported MCID between tenotomy and tenodesis groups was calculated using G\*Power with alpha = 0.05.""

I have a very little experience using G\*Power, and I have not used the MCID in power analysis previously. If possible, I would appreciate if anyone can help me to understand how to use the MCID in the power analysis and what exactly the authors' entry in G\*Power looked like to get these results. I would also appreciate any tips, general advice, or recommended readings/videos regarding power analysis, as I feel that my grasp on this is very weak. 

I am happy to answer any questions or provide any more info as needed. Thanks","[""Not to get you in trouble with the person you are assisting, but computing power after collecting your data is a poor statistical practice. You might ask the MD to consider these articles:\n\nHoenig, John M. and Heisey, Dennis M. (2001), The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis The American Statistician 55:19-24. DOI:10.1198/000313001300339897\n\nLevine M, Ensom MH (2001) Post hoc power analysis: an idea whose time has passed? Pharmacotherapy 21:405-409 DOI: 10.1592/phco.21.5.405.34503\n\nGoodman SN, Berlin JA. (1994) \nThe use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results. Ann Intern Med 121:200-6. doi:10.7326/0003-4819-121-3-199408010-00008 (Erratum in: Ann Intern Med 122:478. doi:10.7326/0003-4819-122-6-199503150-00029)\n\nThomas L (1997) Retrospective power analysis. Conservation Biology 11:276-280. DOI: 10.1046/j.1523-1739.1997.96102.x\n\nYuan K-H, Maxwell S (2005) On the post hoc power in testing mean differences. Journal of Educational and Behavioral Statistics 30:141-167. DOI:10.3102/10769986030002141\n\nWalters SJ (2008) Consultants' forum: should post hoc sample size calculations be done? Pharm Stat 8:163-169 DOI: 10.1002/pst.334\n\nMiller, J. (n.d). What is the probability of replicating a statistically significant effect?. Psychonomic Bulletin & Review, 16(4), 617-640.\n\nGreenland, S. (2012). Nonsignificance plus high power does not imply support for the null over the alternative. Annals of Epidemiology, 22(5), 364–368.\n\nWilkinson, L., & Task Force on Statistical Inference, American Psychological Association, Science Directorate. (1999). Statistical methods in psychology journals: Guidelines and explanations. American Psychologist, 54(8), 594–604. https://doi.org/10.1037/0003-066X.54.8.594\n\nHow post-hoc power calculation is like a shit sandwich | Statistical Modeling, Causal Inference, and Social Science\n\nZhang Y, Hedo R, Rivera A, et al Post hoc power analysis: is it an informative and meaningful analysis? General Psychiatry 2019;32:e100069. doi: 10.1136/gpsych-2019-100069"", '>  ""The statistical power to detect a difference equivalent to the lowest reported MCID ...\n\nWait... they\'re computing *post hoc* power?!?\n\nYikes. And you want to help someone commit the same error?\n\nPlease, if it\'s at all within your power, dissuade them from this misguided endeavour\n\nEdit: was about to come back and edit to offer a reference, but /u/dmlane has you covered', '> How post-hoc power calculation is like a shit sandwich\n\nYou seem to have lost the link on the second-last item there\n\nI presume you were looking for :\n\nhttps://statmodeling.stat.columbia.edu/2019/01/13/post-hoc-power-calculation-like-shit-sandwich/', 'That’s the one, thanks.']",5,4,https://www.reddit.com/r/statistics/comments/12ucxre/q_help_replicating_power_analysis_from_a/
360,2023-04-22 00:47:56,[E] Hello! I'm learning EDA statistics from scratch and wanted to know whether this is a good source for it,"Source I'm learning from: [https://www.itl.nist.gov/div898/handbook/](https://www.itl.nist.gov/div898/handbook/)

&#x200B;

I started going through it and it hit me with a lot of words that I do  not understand (I looked most of them up, and didn't find definitions  for things like ""flat graph""). Apparently some of them will be handled  afterwards, still, I'm wondering if its a good starting point for  someone who wants to apply EDA Statistical methods on quality standard  related projects",['Seems reasonable.'],14,1,https://www.reddit.com/r/statistics/comments/12uai0o/e_hello_im_learning_eda_statistics_from_scratch/
361,2023-04-21 22:50:08,[Q] Why run a T-test instead of an one-way ANOVA?,"I am sure there must be a very obvious thing that I am missing but I can't seem to see why people would choose to run a T-Test over an ANOVA other than the T-test just being more simple and easier. 

Ross et al., 2017 states that you can run an ANOVA to compare the means of two or more groups. So you can run an one-way ANOVA instead of a T-test. 

So that would just make them interchangeable but since ANOVA's can also control for increased Type 1 error probability caused by multiple comparisons. (Kim, 2017) Doesn't that just make one-way ANOVA's entirely preferable? 

If I have two groups and four dependent variables why would I ever run a T-test and then go through the hassle of running an bonferroni's correction in order to just get the exact same result from just running a One-way ANOVA?","['If you’re using software to run the analysis then a one way ANOVA on two independent samples *is* a T-test. There’s no difference in the results.', 'Both one-way ANOVA and two-sample T test can be seen as having the formula:\n\n     Y ~ X\n\nIn two-sample T test, the `X` is a binary variable (e.g., male/female, young/old).\n\nIn the one-way ANOVA case, the `X` can be of *any number* of categories (e.g., male/female, low/mid/high, red/green/blue/purple/yellow/...).\n\nSo two-sample T test is essentially a special case of one-way ANOVA. In ANOVA you need to consider multiple comparisons *because multiple comparisons are possible*:\n\n\n1. low vs. mid\n1. mid vs. high\n\nBut if you take the two-category case (e.g., male vs. female), there is only 1 possible comparison you can do.', '1940s statisticians are shook.', 'Keep in mind that ANOVA uses an F-test which allows for testing of multiple factors/factor levels at once. Basically, it checks it there’s a change in means of the dependent variable due to at least one factor level but it doesn’t tell you which factor or factor level is significant.\n\nA t-test can then be used to look at which specific factors and factor levels are significant. There are ways to correct for multiple comparisons such as a Bonferroni Correction, Scheffe’s method, and Tukey’s HSD. \n\nWhen there is only one factor with two factor levels (two groups you’re comparing) then ANOVA is equivalent to a t-test. In fact, if you square the t value from a t-test, you’ll get the F value from a one-way ANOVA with two factor levels.\n\nAlso, MANOVA is only going to be used if you expect covariance/correlation between multiple dependent variables. Otherwise it operates (mostly) the same as ANOVA.', 'Mic drop']",47,27,https://www.reddit.com/r/statistics/comments/12u4zgj/q_why_run_a_ttest_instead_of_an_oneway_anova/
362,2023-04-21 18:03:25,[Q] Collapsability vs Non collapsability,"I've been learning recently about the Cox regression models, and frailty theory. Basically, I've been taught why a Cox model that does not include all relevant covariates will have biased estimators for the regression coefficients. This already breaks my mind a little (having a model that could do perfect survival predictions but have very off coefficients), but then theoretical explanation was that this model, and logistic regression as well, are no collapsible.

I get the intuition as of why bias would be introduced, but then I wonder: why do other models like linear regression not suffer from this? What is the mathematical proof behind and the theoretical definition of collapsability?

Also, on an end note, learning about frailty and non collapsability made it seem like Cox regression can be much less ideal than I first thought (it seems hard to draw conclusions for experiments using Cox regression due to this potential bias of the coefficients). So, why could this models still be preferred over collapsible ones?

It was maybe a lot for one post, but the questions all came from the collapsability concept.","['Fraility is (to my knowledge) the same thing as random effects variables in linear models. Maybe making that parallel can help you get a better picture of what it means? \n\nI surprisingly never heard of collapsability, but I assume that it means that if you ""collapse"" the different random effects levels (by that I mean ignoring them entirely) , the model becomes less significant and/or viable. Therefore you HAVE to consider the random effects variables in order to get good results.', 'I can’t comment on Cox models, but as far as logistic regression goes, the root of noncollapsibility is unobserved heterogeneity. We can think of the binary dependent variable as a proxy for latent/unobserved continuous variable. For example, binary ""survived/died"" variable is a proxy for ""propensity to survival"". Because the latent variable is unobserved, we don’t know what it’s mean or variance is. Instead, we fix to some constants, say 0 for mean and 3.29 for variance. So far so good. \n\nThe problem appears when we start adding predictors into our models. Each predictor increases the amount of explained heterogeneity, as it successfully predicts some amount of the dependent variable\'s variance. But remember that the amount of unobserved heterogeneity stays the same, as the variance of the latent variable is fixed to a constant! This essentially means that as you add more predictors, the total variance of the latent variable changes, even if you are using the same data. This means that regression coefficients from different models are on different scales and cannot be easily compared to each other. Also note that this is not a problem just for comparing coefficients across models, but also across sub populations (e.g. men and women).\n\n\nThe reason why linear regression doesn’t have this problem is that there is no latent variable and hence no unobserved heterogeneity. All the variance of the dependent variable is observed and the total variance stays the same across models (we just keep shuffling it from ""unexplained” to ""explained"").\n\nThe easiest solution (for logistic regression) is compute average marginal effects on probability scale, which don’t suffer from this problem.\n\nFor more details, you can read this paper: (https://www.su.se/polopoly_fs/1.341161.1501927873!/menu/standard/file/Eur%20Sociol%20Rev-2010-Mood-67-82%20(1).pdf', 'I may have misunderstood your question, but linear regression with omitted variables does have biased estimates: https://en.wikipedia.org/wiki/Omitted-variable_bias', 'We shouldn’t be relying on coefficients anyways. I don’t know about how to do it for the Cox model but for Logistic regression you can use G computation (marginal effects) and get a risk difference or risk ratio from the predictions and these are collapsible. \n\nImo its a better way anyways, and it can be used on black box models. This whole non collapsibility and G computation thing kind of shows how the whole “logistic reg is interpretable because of coefficients, ML models aren’t” is such a vast oversimplification and is not right. In the case of GLMs though you get the uncertainty via delta method. R has a marginaleffects package for this. \n\nThe non collapsibility is just a result of the link function (logit) and working out the math. Link functions like identity and log are collapsible', ""Wow, Cox regression and frailty theory can definitely be mind-bending concepts to learn about. To answer your first question, linear regression models typically do not suffer from non-collapsibility because they do not involve survival analysis or time-to-event data. Non-collapsibility occurs in survival analysis when conditioning on certain covariates changes the effect estimate of another covariate, which is not observed in linear regression. \n\nIn terms of mathematical proof and theoretical definition of collapsibility, it refers to the property of a statistical model where the effect estimate of a covariate is the same regardless of whether or not other covariates are adjusted for. A model that is collapsible is one where conditioning on a covariate does not change the effect estimate of another covariate. \n\nIt's true that non-collapsibility can introduce bias in Cox regression models, but they are still preferred over collapsible models in certain situations because they are more flexible and can account for time-varying covariates and non-proportional hazards. It's important to carefully consider the potential for bias in any statistical model and interpret results with caution. Great questions!""]",11,13,https://www.reddit.com/r/statistics/comments/12twrbt/q_collapsability_vs_non_collapsability/
363,2023-04-21 17:50:52,[Q] Regression advice!,"Hey all! 

I’m a psych student and am running a hierarchical regression to see if test anxiety predicts performance on a numeracy task. 

I have to control for two variables. Age is one which I’ve dummy coded. 

I’m back and forth between controlling for age and education attainment but my question is in several parts: 

1. I find it more interesting to dummy code the scaled age variable as opposed to keeping it as a scale. 
A) When I use 65+ as my reference group all 3 other groups have significant interactions.
B) when I use 16-24 as my reference group, only 65+ group is significant 
So, I’m just trying to decide which comparison is more interesting. Either being 65+ compared to 16-24 predicted poorer performance, or being 65+ compared to all other groups predicted poorer performance. 

2. Is this even of interest since it is my control variable… would it be better to keep my control variable as a simple scale and not get bogged down? 

3. If I did education level I will probably dummy code it into 3 groups, so same story as above really. 

4. Bonus question! For some reason more of my variance is explained (diff of around 2%) when I dummy code age than when I keep it as a scale, even though descriptives are the same. Just interested as to why this is? 

TIA!","[""The choice of reference group doesn't matter for the model itself (it's the same model), the only difference is in which comparisons are easy to read directly from the coefficients.\n\nFor the last question, this is probably because the relationship with age is nonlinear, but using it as a continuous measure imposes linearity. Breaking it into categories functions as a crude nonparametric way of allowing for a nonlinear relationship."", 'I would definitely keep your age as a continuous variable rather than artificially make it a factor. What is the benefit of making age a factor? Do you think of age as a category or is it continuous? Probably the latter.', 'Yes, I definitely will keep it continuous! I’m just learning and was trying to get my head around it all last night. Thank you! :)', "">What is the benefit of making age a factor?\n\nAs I mentioned in another comment, it's a quick-and-dirty way of allowing for non-linear effects in a way that's easy to interpret. I don't think it's necessarily a bad approach.""]",3,4,https://www.reddit.com/r/statistics/comments/12twhuu/q_regression_advice/
364,2023-04-21 12:38:49,[Q]Type I and Type II errors,"I have been reading about these two types of errors and a part of the definition called my atttention.

It is said that the type 1 error is rejecting the null when this is true, and this is the power of the a test when the null is true, and later it says that the error type II is accepting the null when this is false and that is also 1-the power of the test when the null is false.

So following that it is incorrect to say that 1-probability of type 2 error = probability of type 1 error, but when I look at the graph of different power functions, it is defined like that, the lower part like probability of error type 1 and the upper as 1-probability of type 2 error,

So, what part am I mistaking? Is it possible to have one equation that connects the probability of error type 1 and the probability of error type 2?","['Your alpha isn’t related to power. As you point out it’s the probability that you are willing to accept rejecting the null when it is true (type 1 error). Your confidence of this inference is 1 - your predefined ‘willingness threshold’.\n\nBeta relates to power since the goal of your statistical test is to reject the null when it is false. Accepting this null is beta and your power is the ability to reject i.e 1-beta. \n\nThere is a direct relationship between alpha,beta and power (increasing your willingness to reject a true null i.e alpha, decreases your probability of retaining a false null i.e beta, and hence increases power) but this comes at the price of lower confidence in your test. There is no equation that links both alpha and beta because they’re answering slightly different questions', '> So, what part am I mistaking? Is it possible to have one equation that connects the probability of error type 1 and the probability of error type 2?\n\nYes: I think the one ingredient you are missing is that you have one kind of Type I error -- the null is true and you don\'t think so --- but Type II errors come in all shapes and sizes, and as a practical matter, the null is essentially never true, except in constructed experiments. P(Reject null hypothesis | parameter value) is usually a smooth curve, with a minimum (for a 2-sided test) at the point where the null is true.\n\nSay you want to find people\'s average income, and test whether it is $100,000 or not. You build yourself a test that has a type I error rate of 5%. Great.\n\nNow suppose you apply this test to a population where the average income is $100,000.01. You have about a 5.0000001% chance of ""correctly"" concluding their income is not $100,000, and a 94.999999% chance of committing a Type II error. You may have a 6% chance of detecting that $99,000 is different from $100,000, and a 50% chance of detecting that $90,000 is different from $100,000.', ""I would suggest you ask yourself some questions. Can you have a false positive (type one error) when the null is false? Can you have a false negative (type two error) when the null is true? Broadly, there are four possible combinations of two separate pairs that you can consider. There's whether or not we reject the null (do we *decide* there's a positive result or not) and then there's the truth (is it positive/negative). And remember, it's very very important to keep track of what is being declared or assumed to be true when dealing with significance testing."", 'Would [this page from PSU](https://online.stat.psu.edu/stat415/lesson/25/25.2) help?', '> and this is the power of the a test when the null is true,\n\nI think considerably more commonly (though perhaps not all sources, some are weird) it\'s the *rejection rate* or  *type I error rate* when the null is true; but normally the rate of rejection is only called power when it\'s correct to reject. (Nonetheless a *power function* will typically fill in the null case; when that\'s done it\'s strictly a rejection-rate function, but power function is the term people tend to use.)\n\nConsider: It would be truly bizarre to call a test with a high chance of incorrectly rejecting the null ""powerful"" in that situation.']",13,5,https://www.reddit.com/r/statistics/comments/12tqhv9/qtype_i_and_type_ii_errors/
365,2023-04-21 06:09:51,[Q] Is it possible to calculate approximate wage range for a given position with two to three years of wage numbers,"I know I may have to make some assumptions but for example if I made $50,000 and I know that that wage is 107% of the median.  The following year with a raise I have $55,000 and maybe that penetration is 105% and the next year I'm making $60,000 and the wage penetration is 104%.
I guess I can calculate the median pay fairly easily since I know I was making 7% above 5% above than 4% above. And I also realized that median pay is also probably increasing throughout the years. Is there a way to back calculate maybe assuming a bell curve for pay, maybe what percentage wise the +/-20% of employees are making around me.  Or to get some other kind of relevant range of what I can expect The range of my possible pay to be at my current position?

Or is getting any kind of meaningful possible pay range data not really possible from this limited data set?","[""You have 2 pieces of information across 3 years: what the median is and how far above it you are. With this you know that you make more than 50% of the people with this position, and unless there are a total of 3 or 4 people with this position, there isn't much else that can be inferred.\n\nYou do not have an average (mean), a standard deviation, a count of the number of people with the position, or the knowledge that the pay distribution for the position can be appropriately approximated by a bell curve. If you look at certain industries with winner-take-all effects, there's a good chance it won't look normal (distribution wise).\n\nSo what you have mentioned is not sufficient to calculate what the employees around you are making, and you would need more data."", ""It's a fairly large company and I think it would be a fairly ok assumption (bell curve shaped pay distribution) for my end goal of just wondering realistic pay ranges.  I know the total number of employees in my job category can probably be approximately to ~300.  Is there specific other information I would need to get a rough approximation of the pay range?"", 'If in the USA, check job postings for similar positions, some states require that they post the ranges. A lot of companies comply with that just to make things easier, while some just refuse to hire in those particular states.']",1,3,https://www.reddit.com/r/statistics/comments/12tgvqv/q_is_it_possible_to_calculate_approximate_wage/
366,2023-04-21 05:43:49,[Q] How would you go forward calculating the greatest score separator of a golf hole,Say you have data from a tournament of strokes per hole for every player on every hole. How would you go forward ranking every hole based on how much separation you get in score from the hole? Is it simply by calculating the variance of every hole? What would the value of the variance then mean?,"[""It depends on how you want to define 'separation'. \n\nVariance would be one criterion you could use to compare\\* holes -- though consider that the variance for a par 5 would naturally have a bigger spread than a par 3, so you'd pretty much always just be picking out the par-5 holes. \n\nThis seems to suggest that maybe you want to consider some form of par-adjusted measure. There's no obvious way, since it's not necessarily a given that the spread will simply scale in direct proportion with the par-value. It's probably a decent first approximation, but I imagine a simple scaling for that (dividing spread - say sd - by par) probably overcorrects and just picks out par 3's instead. It might require some model -- perhaps a regression-like model to find something suitable. I imagine that (say) generalized linear models have already been used for per-hole golf scores, and that would be a starting point to try to pick out the holes that seem to have more variation that you might otherwise expect.\n\nIf you're trying to find a hole that specifically tends to sort the sheep from the goats (in effect pick out the very good players from the casuals), and you have external information on who they are (like *handicap*), then you'd take a different approach.\n\n---\n\n\\* If you want to *interpret* the value rather than just compare values, however, convert back to a standard deviation, which will be easier to think about."", 'I\'m not sure what you mean by, ""based on how much separation you get in score from the hole""... Might need to explain what you mean a little bit more.']",0,2,https://www.reddit.com/r/statistics/comments/12tg6i7/q_how_would_you_go_forward_calculating_the/
367,2023-04-21 03:05:25,[Q] Is there a null model in conditional logistic regression?,"I am modelling site selection of an animal using an m:n case-control design. My understanding of regression is that null models are typically models the assumption that all betas except the intercept term equal zero. Since the conditioning cancels out the intercept term we can't use the intercept term as a null. Is there another way to make a null model?

I am using package Survival in R

I have searched everywhere I can think an no one explicitly answers this. There are some people that claim we can just do something like  clogit(response ~ 1 + strata(stratification_variable)). I think they are mistaken and this does not work using the clogit package. 

Does anyone have any insight on this or a reference paper that I can look through? This is driving me insane. Thanks!","['It depends on what your goals are here. What are you trying to use the null model for? You can directly use clogit, removing the strata term and all covariates from the formula. This should give the totally null model that conditions out a single intercept with no individual effects and no covariate effects. This will likely take a very long time to fit, but will give you a model object that you can use for testing against the null model.\n\nIf you are looking at testing whether or not their are significant covariate effects, it likely makes sense to consider the null model the model that includes subject specific intercepts, i.e. the model with the included strata term. This may make more sense than the single intercept null model because the conditional logit model is often motivated by study design.', ""I'm using AICc to determine the top model (and model average). In other regressions I would typically include a null model in this set. I didn't in conditional since I didn't think there was one.\n\nAre you suggesting clogit(Response ~ 1)?\n\nClogit doesn't run with that the same way other regressions do.""]",2,2,https://www.reddit.com/r/statistics/comments/12tbpxu/q_is_there_a_null_model_in_conditional_logistic/
368,2023-04-21 02:45:39,Possible to use COVID vaccine data to determine exactly how many undocumented immigrants currently reside in the United States? [Q],"I was thinking about the use of total vaccinations, and if it was possible to determine if there are more people here that are undocumented then the estimates suggest.

Do you think it's possible to use any data from the lockdown to determine if there are millions more warm bodies here than are on record?","['No, that cannot tell you exactly, per your title. Indeed such an attempt would require implausible assumptions. \n\nFrankly this sounds like an attempt to indulge in hefty doses of confirmation bias.', 'Lol. There’s no good data on illegal border crossings', 'How would you even go about doing this? Say a county has 1000 vaccinated. Does that mean there are more or less undocumented immigrants than you previously suspected?', 'Please leave this subreddit and go back to playing with your sex robots.', 'No. Because of the volume, most state IIS systems have more unique people in them than there are people in the state. And people used fake names and addresses out of fear other people would know that got a Covid shot. The is no way to merge Robert Smith with Bob Smith unless they have tons of matching pieces if information. \n\nVaccinators did not check to see if patients were already in the system so every vaccination created a new person.']",0,23,https://www.reddit.com/r/statistics/comments/12tb4af/possible_to_use_covid_vaccine_data_to_determine/
369,2023-04-21 02:04:45,[S] Significance differences between groups on SPSS,"
Im working with 3 different samples. Each sample is treated with 10 methods. Then I calculate concentration.

I want to create a bars graphic with concentration for each treatment, comparing signicance differences between all 30 treatment.

I have standard desviation for all of them. I just want to know if A is different enough from B or if C is different enough of A and B or just from B.

I have tried with t-student, Tukey and Anova but It doesnt seem to work :c
My variables are Run (1-10, nominal) which is determined by Time and Amplitud (Both continuous, isnt it?). 

Im working with SPSS and excel. TIA","["">Im working with 3 different samples. \n\nSo you have n=3. As in, three independent samples.\n\n>Each sample is treated with 10 methods\n\nWhat does that mean? All at once? One at a time? Are there interaction effects?\n\n>I want to create a bars graphic with concentration for each treatment, comparing signicance differences between all 30 treatment.\n\nWhat treatment are you referring to?\n\nYou need to explain your design better, what you're writing doesn't make sense to me."", ""This is still very unclear to me. You don't need to necessarily describe the specific variables or methods, but you need to be able to define what your dependent and independent variables and minimum experimental units are. Be clear and explicit. \n\n>Well, they are solutions with 3 different weights of solid fruit on oil (the objective is to extract chemicals from them (calculate concentration)). \n\nWhat are you measuring here exactly? Is it the concentration of an extracted chemical (solute) in an oil solvent from these mixtures? \n\n>Each of them has 10 different treatments (different times and amplitudes of Ultrasounds) (so 30 different tubes). Lets call them A1, A2...A10, B1, B2...B10, C1, C2...C10. And finally each one has been made 2 times. \n\nWhat does A, B, and C represent here? Again, you're not being explicit and clear. If you're going to ask for help don't make people guess."", ""Thanks for the clarification. \n\nIt's still a bit fuzzy, but from what I can tell your design looks like this:\n\nDependent Variable:\n\n* Concentration\n \nIndependent Variables:\n\n* Fruit Weight: 3 levels\n\n* Time_Amplitude: 10 levels\n\nYou have two independent subjects per combo of Fruit_Weight and Time_Amplitude, for a total n=60 (i.e. experimental units).\n\nIf you were to test for differences in concentration between groups, you would use a mixed ANOVA design that looks like this:\n\nconcentration ~ Fruit_weight*Time_amplitude \n\nThis tests for differences in concentration for Fruit_weight, Time_Amplitude, and the interaction between them Fruit_weight*Time_amplitude.\n\nHowever, there's a problem - you lack replication. You have only two actual samples per level of the independent variable. It means that there isn't enough replication of the experiment to provide any statistical inference. You need a minimum of three per group to make any inference, and even then, that's often not enough if the data have any noise in them.\n\nYou can still calculate the mean for each group combo (i.e. averaging the two measurements) and create a bar graph figure showing the concentration for each group (e.g. A1, C8, etc). But any error bars won't be valid since there are only 2 measurements per group."", 'Well, they are solutions with 3 different weights of solid fruit on oil (the objective is to extract chemicals from them (calculate concentration)). \n\nEach of them has 10 different treatments (different times and amplitudes of Ultrasounds) (so 30 different tubes). Lets call them A1, A2...A10, B1, B2...B10, C1, C2...C10. And finally each one has been made 2 times. \n\nI wanna know if treatment 1 is signif. different from 5 for example. (With all of them). to give a letter to significally different groups:\n\n- A1: a\n- A2: b\n- A5: ab \n\nWould mean that 1 and 2 are different but 5 isnt with any of them', 'Yes, concentration of solute in oil solvent. A, B and C refers to different quantities of solute on the solvent. 1-10 refers to time+amplitude combinations. So I have 3 quantities of solute treated with 10 different combinations of time (10-50minutes)+amplitude(10-70%) (30 tubes in total)\n\nFor example: A1 and A2 have same quantity of solute but got 2 different times and amplitude. While A1 and B1 have different quantity of solute but share time and amplitude.']",2,7,https://www.reddit.com/r/statistics/comments/12t9vkk/s_significance_differences_between_groups_on_spss/
370,2023-04-21 00:28:21,[Q] Very small scale pilot experiment for a project for University.,"So I’ve been tasked to run a pilot experiment, it needs to basically consist of a response variable and 1 or 2 treatment factor/s and/or blocking factors. My group has suggested plant growth in different water, or the average rating of the same drink but dyed with different food coloring. Does anyone have any suggestions for a different experiment to run?",[],3,0,https://www.reddit.com/r/statistics/comments/12t6y9t/q_very_small_scale_pilot_experiment_for_a_project/
371,2023-04-20 23:21:47,"[Q] If the introduction of external information is not desirable for Bayesian analysis, why not treat the likelihood function as posterior distribution?","Let me clarify some things:

1. I am aware that there is not a pdf/cdf since it does not obey Kolmogorov's axioms.
2. The likelihood function does not obey the measure theoretic triple, hence it is not truly probabilistic to begin with.
3. Sometimes the likelihood function is not even measurable, hence it can lead to some fringe conclusions.

However, I am also aware that:

1. Once we have sufficient amounts of data, the prior does not play a major role (Savage's results show that).
2. Being s times more likely sounds awfully similar to being s times more probable (even though it is used in an everyday sense, the two not being mathematically equivalent).
3. The likelihoodist view on statistics treats the likelihood function very similarly to how a Bayesian would treat the posterior, and the conclusions one would draw using a Jeffreys prior would be very similar to the ones one can draw from treating the likelihood as a posterior.

Are there any arguments for or against what I am saying? I am not in any way making claims that I am correct here, just curious about the topic.","['Yes, it can make a huge difference. Depends on the problem. If you have lots of data then a Bayesian method might not even be needed. In my experience, Bayesian methods are most useful in data-poor problems where you need to use intuition (aka a prior) to guide your answer.', '> If the introduction of external information is not desirable for Bayesian analysis\n\nwhy do you claim this?', 'p(x|y) = p(y|x) p(x) / p(y) \n\nYou want x, but good luck sampling that from p(y|x) without p(x). The likelihood still has lots of other uses but once you actually want samples from p(x|y) you need the prior.', 'My point is that there is nothing wrong with external information.', 'With lots of data relative to parameters']",0,9,https://www.reddit.com/r/statistics/comments/12t4327/q_if_the_introduction_of_external_information_is/
372,2023-04-20 20:28:33,Interpretation of an simplified GLMM [Q],"I am currently working on my master thesis in psychology and I was asked by my supervisor to utilize and GLMM, which i am not too comfortable with due to me not receiving too much formal education on this. However, I believe I understood the basics, successfully run my power analysis, fitted my model and simplified it. Now, i just want to ensure I drew the correct conclusions from it. 

What I did was conducting a 2 (factor 1, binary) x 2 (factor 2, binary) within-subject experiment with repeated measures for every factor combination. My outcome variable was also binary. My expectation were that the behvaiour i was looking at would be reduced from condition A.1 to condition A.2 and that this effect would interact with factor b. Furthermore, I had a questionnaire (C) and expected it's individual score to moderate this behaviour reduction. Now I constructed am GLMM like this: 

    fitted_model <- glmer(response ~  A * B * C + (1|Subject) + (1|Stimuli))

Furthermore, I also came up with three reduced models by stepwise simplification or theory insights:

    reduced_model2 <- glmer(response ~  A * B + (1|Subject) ) #based on theoretical insights dropping C, which is non-significant in my saturated_model and appears to have no influence on the outcome based on a visual inspection of the plotted data. 
    
    reduced_model4 <- glmer(response ~  A + (1|Subject) + (1|Stimuli)) #Most parsimonious model preferred by AIC; also preferred by LogLink and deviance compared to red_mod3
    
    reduced_model5 <- glmer(response ~   (1|Subject) + (1|Stimuli))) #Random effects only, preferred by BIC

Based on my visual inspection I would expect a main effect for A and potentially an interaction between A and B.  In my [fitted\_model](https://imgur.com/zs3XdqR) there are no effects. In my [reduced\_model2](https://imgur.com/3fxDjZU)I have significant effects for A and A\*B. In my [reduced\_model4](https://imgur.com/Pk17s6g) I have a significant effect for A. And these are the results of [reduced\_model5](https://imgur.com/ByHkaRW). 

So, based on my [anova() Comparison Result](https://imgur.com/8XqBidW) and the summaries of my models I would drwa these conclusions:

* Factor A appears to have a significant main effect, as this remains in the most parsimonious model.
* Factor C appears to be of no relevance for the outcome variable
* Factor B is a litte more complicated. When removing the random effect for stimuli from the model, it does lead to a significant effect for the interaction between those two factors indicating that B is relevant, too. This would be in line with previous studies and the literature. However, this is not clear and needs to be taken with a massive grain of salt.

Are those conclusions fair play, did I miss something or did I draw things from the data that are not in there?","[""You have a three way interaction in your fitted which is damn near impossible to interpret. My first question is why did you model stimuli a random effect? I would suggest that you think what exactly your hypothesis is, and fit a model to address that specific question. It's better than fitting multiple models. If you need to compare models for significance, I recommend comparing two models: one with and one without your main effect of interest, then do an f-test. Also, try to drop as many interactions as possible to improve interpretability. You may also run into power issues if your sample size isn't large enough."", ""> You have a three way interaction in your fitted which is damn near impossible to interpret.\n\nWhile sometimes complex, there's nothing inherently uninterpretable about a 3-way interaction. Or do you just mean it's not suitable here because there's no significant response?\n\n>Also, try to drop as many interactions as possible to improve interpretability. \n\nI agree they should frame the model based on testing explicit hypotheses but I'd be very cautious about this approach. You don't want to just exclude non-significant results and this can produce misleading interaction effects. Especially important to note is that you cannot drop lower-order interactions and keep higher order ones. I'm mentioning this for OPs sake.""]",5,2,https://www.reddit.com/r/statistics/comments/12sx1hy/interpretation_of_an_simplified_glmm_q/
373,2023-04-20 12:03:54,[D] I’ve just ordered Drunkard’s Walk. Can you recommend any other entertaining statistics/probability books?,Finishing up a minor in Statistics and really enjoyed all my statistics and probability classes. I’ve enjoyed Martin Gardner’s math puzzle books in the past too. Would love more recommendations for entertaining books covering probability or statistics concepts.,"['I found the Seven Pillars of Statistical Wisdom really enlightening and full of fun history. \n\nWeapons of Math Destruction is a classic warning tale. \n\nHow to Lie with Statistics is a book of short anecdotes of real life misuses. Prob my favorite of the three. \n\nFor a paper,.I get happy reading Cohen\'s ""The Earth is Round (p<.05)""', 'The lady drinking tea is an entertaining collection of nuggets from the history of statistics.\n\nTh theory that would not die is in the same style for the checkered history of Bayesian statistics.', 'Ian Stewart has a series almost as long as Martin Gardner\'s now. I think ""Game, Set, and Math"" (about how to calculate the probability of a tennis match, given the probability of winning a single point) was the first in the series, or at least the first I saw for sale, in the late 90s. They aren\'t all stat/probability-related. Some pure math, some physics... one about what kind of alien life forms might exist on other planets.', 'I have read Weapons of Math Destruction, my major is cybersecurity so it was perfect for my interests. The others look great too, love learning the history behind math concepts. Thanks!', 'easy on the drink there bud']",109,30,https://www.reddit.com/r/statistics/comments/12slcf6/d_ive_just_ordered_drunkards_walk_can_you/
374,2023-04-20 10:50:14,[D] How to Compare Regression Models?,"Hello everyone!

I am having confusion on how to evaluate and compare the quality of different regression models. 

- For example, I understand that classification models are more straightforward to compare and evaluate as metrics such as F-Score, AUC/ROC, Confusion Matrix are all bounded between 0 and 1 .

- However, in regression models, comparison metrics such as RMSE, Cross Validation Error, AIC and BIC are all unbounded - if several regression models are compared, the model with the lowest RMSE, AIC, BIC still might be an overall bad model even though its better than all the other models! (e.g. a turtle is faster than a snail but both animals are still slow!) 

This being said, is there any general advice on how to compare different regression models fit on the same dataset?

Thanks!","['R^2 is bounded between 0 and 1. Maybe that’s what you’re looking for. Of course if you’re comparing models that are predicting the same thing you’d want to use adjusted R^2 instead.', 'You always have to know and understand the context of the data and the purpose of the model. There is no general indicator in the sense of a number that pops up and if it\'s above or below whatever the model is ""good"".', 'It\'s good practice to use the same cross validation partitions for all of your models, then you can compare their performance on identical test sets.\n\nRemember that any missing value imputation or feature engineering/transformation that requires a sample statistic in the formula (e.g., calculating the Z-scores for a feature), it should be done inside the cross validation loop.\n\nAlso remember the wisdom of George Box: **All models are wrong; some are useful.** He elaborated:\n\n*""Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = nRT relating pressure P, volume V and temperature T of an \'ideal\' gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules. For such a model there is no need to ask the question \'Is the model true?\'. If \'truth\' is to be the \'whole truth\' the answer must be \'No\'. The only question of interest is \'Is the model illuminating and useful?\'.""*', 'Things like RMSE and MAPE can obviously tell you something about the absolute and relative quality of your models if you know the context of the data generating process you’re modeling. An RMSE of 100kg would probably not be very good for a model that models (normal) human weights (well, actually mass). Conversely, a model fitted on the same data with an RMSE of a couple of kilograms would likely be useful depending on the context, circumstances and the model’s use cases. It would clearly be more useful than the former.']",2,4,https://www.reddit.com/r/statistics/comments/12sjg84/d_how_to_compare_regression_models/
375,2023-04-20 07:31:15,[Q] Question about Autocorrelation and lags,"So I have been trying to decorrelate some data which are autocorrelated, but one thing I am quite unsure of is how lags works. So say that for example I have taken a autocorrelation test of a variable that goes from week to week and get some autocorrelation values back. So in lag 1, that means the autocorrelation from 1 week to the next week as I have understood it, but does it only take my first two values and looks at that, or does it look at every value in my variable and see them in a week for week, then averages a autocorrelation value. Or is it that every autocorrelation value is the same for lag 1? I am wondering the same things about 2 lags and 3 lags etc.","['for weekly data you look at all the values, shifted one week. \n\nIf you have 120 values and lag the data by 1, you get 119 values -- the original set of values shifted along one time unit.', 'Does that mean with a lag 1 that it has calculated the autocorrelation between each one week difference all the way down for all data and then given a average, and same for 2 lags etc?']",2,2,https://www.reddit.com/r/statistics/comments/12seda5/q_question_about_autocorrelation_and_lags/
376,2023-04-20 06:41:09,[Q] Sub-Gaussian Random Variables,"Besides fast tail decay, what are other desirable properties that can be extended to sub-gaussian random variables? Why is fast tail decay so important and how does this apply to statistical research?

The more statistics I'm exposed to, the less I feel like I understand it all. If I were to speculate, I would think that fast enough tail decay is important for hypothesis testing, in order to always be able to define a critical region. Is this right?","[""Sub-Guassian random variables are desirable since it's gives us a way to guarantee large deviations from the center of the distribution are rare. Ideas like these are used to show concentration results. In turn those concentrations can be used to show insights on how we can approach problems."", ""I don't see why fast tail decay would be important to hypothesis testing. If my test statistic were to have say a Cauchy distribution or a Pareto distribution under H0, there's no difficulty in using that in a hypothesis test, defining critical regions using it, etc.\n\nYou're going to have to explain what difficulty you see there.\n\nOnce you clarify what difficulty would arise, consider t tests and F tests. Are t's or F's  sub-Gaussian? How do they work?"", 'can you expand a bit on what you mean by ""insights on how we can approach problems?"" I am often asked to show concentration & large deviation results, without knowing why they are relevant.', ""I see what you're saying. In that case, why do we care about fast tail decay or large deviation results?"", 'Maybe unrelated to OP, but I have seen a sub-gaussian requirement for certain moderate-deviation bounds, which can be important for validity of testing in certain scenarios.\n\nMore specifically, in general we do not know the exact distribution of our test statistic and only its asymptotic distribution (using some type of CLT). In some instances we actually care about looking quite far into the tails (eg. in genomics/stat-gen when you have multiplicity to worry about). In those cases results like Berry-Esseen does not really justify use of p-values obtained from a gaussian:\n\nBerry Esseen gives us a result like (for Z\\_n a scaled, centered, sample mean of iid obs)\n\nP(Z\\_n <= t) = Phi(t) + O(n\\^{-1/2})\n\nWe generally want a \\[moderate deviation\\] result like\n\nP(Z\\_n <= t) = Phi(t) (1 + o(1))\n\nand it needs to hold uniformly(ish) in t\n\nIn the case that you use the true standard deviation, you generally require subgaussian tails for the original variables to get a moderate deviation bound, something like (I believe):\n\nP(Z\\_n <= t) = Phi(t) (1 + O(t\\^3n\\^{-1/2}))\n\nIt turns out if you use a t-statistic (so you standardize by the empirical standard error), then you only need an absolute third moment condition.']",1,6,https://www.reddit.com/r/statistics/comments/12sczci/q_subgaussian_random_variables/
377,2023-04-20 04:29:39,[Q] How to determine whether chi2 from one dataset is the same or different from another,"Hello,

The question is the title. Lets say your doing an OLS to solve for some values given some data, you have multiple different sets of data that give similar chi2 with different chi2 values

    dataset1-->OLS-->chi2
    dataset2-->OLS-->chi2
    dataset3-->OLS-->chi2
    #maybe you do some global fits and combine datasets
    dataset1,2-->OLS-->chi2
    dataset1,2,3-->OLS-->chi2
    #where chi2=(sum((data-model)/error)**2)/N) where N is the size of the dataset

How can you determine the correlation of these chi2 values? For example one gives you a chi2=2, another chi2=4? Is 2 really that much better than 4, or is it the same thing?

I feel like the chi2 distribution test is the answer here, but I'm completely confused how to use it. Do I look at the distribution of the chi2 before summing for each dataset?

    chi2 distribution = ((data-model)/error)**2) 
    dataset1-->OLS-->chi2 distribution-->sum-->chi2
    dataset2-->OLS-->chi2 distribution-->sum-->chi2

And there are so many different tests for looking at different things such as Pearsons test or CDF. So just looking for some guidance to filter out all the noise and find what applies to the question I'm asking.",['[Ratio of 2 independent chi square random variables divided by their degrees of freedom is F distribution](https://www.danli.org/2020/08/29/ratio-of-2-independent-chi-square-random-variables-divided-by-their-degrees-of-freedom-is-f-distribution/) This page has the derivation but you can safely take their word for it.'],6,1,https://www.reddit.com/r/statistics/comments/12s923o/q_how_to_determine_whether_chi2_from_one_dataset/
378,2023-04-20 01:11:49,[Q] How to speed up model fitting with R & Stan?,"I frequently need to fit Bayesian regression models, with just R or in some cases using packages that rely on Stan in the backend, such as brms. I usually have a number of models to fit (e.g. a variety of model specifications and outcome variables to predict). Right now, I'm just running everything on my MacBook and each model has to be fitted before moving on to the next. This means the whole process takes a long time. I am sure there is a much better way to set up my workflow, I'm just not sure exactly what it is. 

How can I change my workflow to speed up the process and run my code more efficiently? And what are some resources for learning to do this (e.g. online classes, books,, documentation). 

Some of the specific questions I have are:
- How could I transfer my work from my computer to AWS? What AWS Services should I use to set up a good workflow? 
- How can I fit multiple models at the same time?
- Should I learn a tool like Spark?

I don't know what I don't know, so if there are other tools or strategies I am missing, please share your ideas!","['There are a few things you can try to speed this up. Firstly, if you’re not already you can likely use multiple cores to run separate chains from your model. Depending on how many cores your laptop has this can speed things up dramatically.\n\nSecond, you may be able to optimize your Stan program. Use vectorization instead of loops when possible, and check if you are using the most efficient functions for what you are doing. For example Stan provides normal_id_glm which is usually more efficient than manually writing out your regression with the direct normal pdf.\n\nThird, Stan also supports variational inference which should be much faster at the cost of providing only approximate posteriors. I’m not sure if brms directly supports this, but you should be able to write out the Stan code from brms and then run variational inference with that code.', ""Stan is a PPL that's independent from R. Your model is compiled to C++, and supports parallelization of chains over multiple CPU threads/cores, and GPU acceleration. It's very fast and well optimized.\n\nIt has interfaces in R, Python and Julia. Whichever interface you use won't change your model's sampling speed."", 'Do your models take a long time to fit individually, or is it that you have a lot of models to fit ?', 'Parallelize what you can; multithread when you can; have more cores. Use vectorization instead of loops, especially for likelihoods and priors. Use the ""O1"" stanc compiler optimization option when using brms or cmdstanr.\n\nOther things you could do: if you have lots of data, one strategy is to create small bins for covariate combinations... e.g., if you have categorical predictors only, then aggregate to sufficient statistics and use weights; this can massively speed up your model. If you have continuous vars, then, depending on your model, you could round them to some bin value, and again aggregate data and use weighted regression. It\'s identical in the former case, nearly identical in the latter case, and you drastically decrease the number of likelihood evaluations. In general, you can use sufficient stats to decrease the amount of likelihood evaluations required. For example, you don\'t need to have 1000 rows that look like: x1=0, x2 = 0, x3 = 1, outcome = 1; you need \\*one\\* row of x1 = 0, x2 = 0, x3 = 1, outcome = 1, with n = 1000. Then just use brms\'s weights(n) functionality to add 1000 contributions from that row to the likelihood; in stan, this is the same as multiplying the log-likelihood for that row by 1000. \n\nUse non-uniform priors; use some semi-informative priors.\n\nBrms stamps out reasonably optimized or optimizable stan code, but if you\'re writing stan code yourself, definitely use vectorized lupdfs and lupmfs; definitely precompute constants; precompute anything you\'ll need repeatedly; use specialty functions when possible (e.g., diag\\_pre\\_multiply instead of doing matrix\\*diag(matrix); use inv\\_logit instead of 1/(1+exp(-x)), etc).\n\nStan is very sample-efficient, so you may only need a total of 500-1000 samples per chain.\n\n&#x200B;\n\nI would use ADVI as an absolute \\*last\\* resort, because Stan\'s ADVI is, honestly, terrible right now. There are improvements planned, but it\'s currently not good.', 'I’m going to echo folks and say that some more information on both what is taking time and how many things you need to run is going to be very important. The answer could easily be “don’t recompile models unnecessarily,” “shave off some iterations or burnin,” “optimize your Stan code,” “optimize the model’s parameterization,” or “exploit parallel capacity” before you hit something like AWS.']",5,7,https://www.reddit.com/r/statistics/comments/12s374j/q_how_to_speed_up_model_fitting_with_r_stan/
379,2023-04-20 00:35:37,[Q] Why doesn't flipping the axes of a scatterplot give an inverse trendline?,"Sorry if this is a basic question, but I'm trying to find a best fit equation to map two datasets, let's say they're student scores on Test A and Test B.  If I have their scores on Test A as the x-axis, and their scores on Test B as the y-axis, the best fit equation I get is .46x+39.  My understanding is this best fit equation is essentially saying ""If a student scores x on Test A, their score on Test B will likely be .46x+39.""  So, for example, a 90 on Test A would predict an 80.4 on Test B.

My assumption would be that you could invert this -- for y=.46x+39, you get x=(y-39)/.46, or y=2.2x-84.8.  However if I flip the axes on my chart, with the x-axis being their score on Test B and the y-axis being their score on Test A, I get a best fit equation of .389x+76.9.  This would mean getting an 80.4 on Test B would predict a score of 108.2 on Test A.  I understand best fit equations aren't exact, but I'm wondering why flipping the axes doesn't just produce an inverse of the best fit equation?","['This is illustrated on the cover of David Freedman\'s [""Statistical models: theory and practice""](https://www.amazon.com/Statistical-Models-Practice-David-Freedman/dp/0521743850)\n\nUsing that cover as an illustration, and imagining the ellipse as representing a level set of a bivariate gaussian distribution (ie, a rough outline of a cloud of points), the solid line represents the regression of y onto x.  If you did a ""pencil test"", ie, lay a pencil parallel with the y axis onto the graph and slide it across the cloud from left to right, half of the dots (or half of the shaded portion of the ellipse) should be above the solid line, and half below.  This line minimizes the sum of the squared y-distance of each point from the line.\n\nThat is a different line than the dotted line, which seems to go through the long axis of the ellipse.  This line minimizes the sum of squared distances in any direction of points from the line.  The optimal direction to minimize distance is perpendicular to this line.\n\nI think the property you are thinking of, that is, if x and y were swapped, would the new best-fit line have 1/slope of the old line, would be satisfied by the dotted line.', ""Consider the extreme case of two *unrelated* variables (neither has any relationship to the other, they're independent). When predicting Y from X, where's the population average of Y at each x-value? (i.e. given X=x)  \n\n .... It's just the population mean of Y.  Your population regression line of Y on X is horizontal.\n\nNow interchange the roles of Y and X (predict X). The answer doesn't change, only the roles -- the prediction of X at each value of Y=y is the population mean of X. \n\nThat is, you get two lines that are at right angles.\n\nYou're asking why isn't the second line vertical, which seems to forget what the exercise is (predicting X, not Y).\n \nAs the variables become more linearly related, the mean of Y at each given x will become less horizontal, and the two lines move from being at right angles to having some smaller angle between them.\n\nAs the  (population) correlation approaches 1 -- no noise, the lines coincide (the angle becomes 0). That's the case you were expecting for all possible relationships. \n\n(Similarly as the correlation approaching -1..., the two different lines approach each other)"", 'Wow that’s a very clear illustration, thanks! So in my example, is there one that would make a more “accurate” prediction? If my end goal was to take a Test B score and predict their Test A score, would I use Test B as the independent variable, or should I use Test A then invert it?', 'Thanks that’s an interesting way of thinking about it. Just to clarify, why isn’t the second line vertical? If you were analyzing the points’ x values in terms of y, that would be a vertical line, no?', 'It\'s customary to put the independent variable on the x-axis. If you have ""Test B"" scores and want to predict ""Test A"", then you would do a linear regression of A onto B with matched pairs of Test A and Test B for your training data.\n\nThe resultant equation will be the best linear unbiased predictor of ""Test A"" using ""Test B"" as input.\n\nIt might be easier to predict ""Test A"" from ""Test B"" than the other way around, it just depends on the data.  If everyone got exactly the same score on Test A but Test B had a range of scores, it would be easy to predict Test A from B (it\'s all the same score!) and hard to predict Test B from Test A-- because Test A basically provides no information.']",2,5,https://www.reddit.com/r/statistics/comments/12s21os/q_why_doesnt_flipping_the_axes_of_a_scatterplot/
380,2023-04-20 00:22:14,[Q] Is R squared truly ineffective for nonlinear regressions?,"Basically every article I've found says that R squared does not work for nonlinear regressions, but there are so many online calculators and programs that display R squared values for nonlinear regressions

Some examples:

[https://stats.blue/Stats\_Suite/exponential\_regression\_calculator.html](https://stats.blue/Stats_Suite/exponential_regression_calculator.html)

[https://www.easycalculation.com/statistics/exponential-regression-least-square-fittings.php](https://www.easycalculation.com/statistics/exponential-regression-least-square-fittings.php)

Even Excel shows R squared for nonlinear regressions in charts

Are they all wrong or can R squared be used for nonlinear regressions?","['> Are they all wrong\n\nYes: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892436/', 'There are pseudo r-squared measures that can be used.  The question is to determine what statistic the software is actually reporting and calling ""r-squared"".', 'The difficulties are multiple. Here are some issues off the top of my head. \n\nIn linear regression, equivalent ways to calculate R^2 and consequently, the ways to interpret it (proportion of variance, closeness of fit to the data, deviation from a horizontal relationship, etc etc) rely on properties of linear regression.\n\nAs soon as you move away from that, you no longer have these connections -- various ways to write R^2 are no longer the same, and the various interpretations relate to different things. \n\nFor example, typically R^2 will no longer be constrained to be between 0 and 1; if you manage to find or construct a definition that obeys that restriction, you\'ll then certainly lose another property you\'ll want to rely on.\n\nSo your first problem when you have something that calculates some ""R^(2)"" is figuring out *exactly what they calculated*.\n\nYour second problem is figuring out what *that* ""R^(2)"" actually tells you about the fit.\n\nYour third problem is avoiding you or anyone else thinking any other property of R^2 applies.', 'In what way are they ""wrong"" ?  \n\nA pseudo r-squared is a calculated statistic.  It may be useful or not useful in a given situation.  Can you say that calculating a mean is ""wrong"" ?\n\nBTW, maybe it\'s just me, but I didn\'t find that article at all convincing.  First, they are addressing only one potential pseudo r-square measure, which they just call R^(2).  But more importantly, all the models they are comparing have an r-squared of 0.99 or greater.  What good is that ?  What the paper really concludes is that Akaike weights, AICc and, BIC were more useful in selecting models in this case, which I think is just good advice: don\'t use r-squared to select among model, and consider measures like AIC or BIC.', 'Seriously, if you have the option just use a friggin information criteria.']",31,14,https://www.reddit.com/r/statistics/comments/12s1dng/q_is_r_squared_truly_ineffective_for_nonlinear/
381,2023-04-19 23:48:40,[Discussion] Paper on discrimination of NY police officers,"Hi, I am writing a paper looking at discrimination of who NY police officers have chosen to frisk, and whether or not they were carrying something. I have used a heckman sample selection model to account for the selection bias, and I was wondering if you have any suggestions to other things I could try? 

Furthermore The rho term in the sample selection is not significant, which is a bit worrying. They have stopped quite a few more black people than white, and I’m not sure if I should control for that somehow

Thank you","['Look at the work of mummolo and Knox. You need to know how many encounters police have with whites and blacks in order to make inferences', 'I have just read up on that actually, but he/they also say that how often racial groups encounter police is largely unknown. I only have data on people who where stopped and then wether or not they were frisked/searched', 'Write out your dag and be very careful with what inferences and assumptions you make']",2,3,https://www.reddit.com/r/statistics/comments/12rz5en/discussion_paper_on_discrimination_of_ny_police/
382,2023-04-19 22:55:55,[D] How often do the Linear Regression model-based assumptions truly hold in social science research?,"In Econometrics (and other social science research fields), researchers often have to make some model-based assumptions because their main tool of analysis is linear regression. An important assumption is that the functional relationship between the dependent variable and independent variables is *additive*.

To me, this assumption seems somewhat implausible unless there is a reason that the linear model is commonly found in the natural world. But I think this must be true, otherwise, why would so many papers rely on this assumption? So why does a linear functional form hold for many economics and social science research questions? Does this have to do with the CLT?","[""Linear regression always estimates the *best linear and additive approximation* to the true conditional mean function. This can be reassuring because this means our models will estimate something well defined and hopefully useful even when our models aren't quite correct. If the linear model is not correct, but only slightly so, it also means we will be correctly estimating a model that is only slightly incorrect. This can be very useful in practice if the truth is only slightly non-linear and slightly non-additive. \n\nWe can also often justify linearity and additivity if we are looking at a sufficiently localized version of some phenomenon. This is because Taylor series let us approximate most continuous functions as linear and additive functions in small neighborhoods.\n\nAdditionally, linear models can easily be extended to be more flexible. The linear component of the model can be replaced with a polynomial or spline type term, allowing non-linearities. Interactions can also be added that allow the model to accommodate for non-additivity."", 'It’s quite common to use generalized linear models or apply transformations to explanatory variables that alter the additive property. Interaction effects also make it so that while the explanatory variables are no longer individually  linearly combined, there is still a linear combination of combinations of explanatory variables. \n\nIn short, there are all sorts of somewhat nonlinear models.\n\nThat said, the less linear the model, the less well behaved the model is. Poorly behaved models are extremely difficult/impossible to work with in a mathematical sense. Classical statistics relies on some sort of linearizable combination of explanatory variables to estimate coefficients in the model.\n\nFinally, allowing too much flexibility in the model leads quite quickly to overfitting. It’s one of the reasons you should be extremely skeptical of excessive “bendiness” in your model, such as using cubic or quadratic models. So we like some kind of constraints on the model to avoid overfitting, and the linearizable constraint is generally seen as useful. \n\nThe central limit theorem certainly smooths out major fluctuations in our parameters. I suppose this could lead to the linearizable constraint being more applicable, but I haven’t thought about it that way before.', ""Lots of great answers here already, but I want to add some further commentary on justifying the linear model. There are two major reasons why we work with linear regression; the first is a bit tautological and the second perhaps is a bit more interesting.\n\nFirstly, given two random variables y and X, I can always write the model y = X beta + e, where E\\[Xe\\] = 0. In this case, beta is always identified via the OLS formula and thus I can estimate it via linear least squares and recover what some (in particular the econometricians) the average predictive effect of X on y. Whether this beta has the interpretation that you want depends on the setup of your data and the assumptions you are willing to make about it. Next I'll discuss perhaps a more interesting motivation for the linear model.\n\nConsider the standard setting a running a randomized controlled trial (RCT) where we flip a coin and use it to determine treatment. In this setting, the difference in means estimator to identify causal effects (i.e. taking an average from control and subtracting it from the average across treatment) is exactly the same as running linear regression where we regress the outcome on the treatment variables with a constant covariate. We aren't making any assumptions about the linearity of the outcome w.r.t. to the treatment, but when we write the linear model it happens that the coefficient **identifies** the causal effect. \n\nNow this setting may seem a bit contrived, as you could easily just do a difference in averages to estimate the causal effect. However the picture gets quite a bit more complicated when we either (a) block or (b) have additional covariates. I'll focus on the latter since that's what I'm most familiar with but the two settings have quite a bit of similarities. What happens when we have additional covariates is that we may not be particularly interested in them, but we want to use them to improve the precision/reduce the variance in our estimate of the causal effect. One way to do this is just to add them as additional covariates in linear model and then run OLS on our data, and then this will give us better estimates of the causal effect, all without requiring the linear model to be valid (its a lot more complicated but this is roughly the idea, see Freedman 2008 and Lin 2013 for further details on this topic).\n\nTL;DR: There are settings in which we can run OLS without necessarily believing that the outcome is a linear additive combination of our covariates, and the results will still be an estimate of what we are interested in."", 'In psychology, interactions are typical and often the most important result. Therefore, additivity is rarely assumed.', 'To add, we should be particularly concerned abt the necessary assumptions for causal inference being satisfied. I think more often than not, things like SUTVA or parallel trends are not actually satisfied']",6,14,https://www.reddit.com/r/statistics/comments/12rvy7b/d_how_often_do_the_linear_regression_modelbased/
383,2023-04-19 20:48:17,[Q] Multiple comparisons,"Hello,

My study investigates the effect of a particular variable (3 comparisons) across 2 outcomes (6 total comparisons). In calculating the number of pairwise comparisons for Bonferroni, do we use the total number (across all the outcomes) of comparisons, or simply the number of comparisons within an outcome? Are both methods acceptable?","[""That statement are you trying to make (i.e. what do you want to try to reject or don't reject)?"", 'By *outcomes*, you mean different dependent variables ?', 'Survival and disease recurrence outcomes', 'Okay, but are they different dependent variables in different models ?', 'By dependent variables do you mean covariables?']",1,8,https://www.reddit.com/r/statistics/comments/12rs5e6/q_multiple_comparisons/
384,2023-04-19 20:41:49,[Q] SPSS Q Merging Variables,"SPSS question. There is definitely a better way to do this but I have two variables (var1 and var2) that both look at size (the variables just differ in the year they were created in a large dataset (var 1 are cases from before 2016 and var 2 after) so theoretically each subject should have only one datapoint in either var1 or var2 but not both. I wanted to merge these two variables into one variable that has the size for all patients in the dataset. I couldn't figure this out so I decided to compute a new variable=sum.1(var1,var2). knowing that it would be okay to add them b/c each case should only have a datapoint for one of the variables. however, some cases put the size in both var 1 and var 2 so adding them doubles the actual size. It is a huge dataset so I started by going through and trying to delete the duplicates by hand but its taking too long. I then decided to select for cases if var1>0 AND var2>0 and copied those cases to a new dataset (over 1100 cases). I then recoded var2 (ELSE=0) to set all var2=0 so I could use my compute merged variable appropriately but now I do not know how to get these cases back into the original dataset and replace the duplicated cases only. There has got to be a better and more efficient way to do this lol but just learned stats and spss recently. can anyone help?","[""I don't know SPSS but in other languages, this is called *coalesce*"", 'You just need to use a little syntax:\n\nDo if not(missing(var1)).  \nCompute NewVar = var1.  \nElse.  \nCompute NewVar = var2.  \nEnd if.  \nExecute.', ""I'm so glad I could help.  I love SPSS syntax.  And I mostly learned it from reading the Command Syntax Reference guide which is under the help menu.  You might like the sections on DO IF, DO REPEAT, and DEFINE-!ENDDEFINE.  DM me anytime if you get stuck."", 'I am not sure I completely understand what you did...To clarify: You have two datasets that include the same variable, and you want to merge them into a single vector?', 'Thank u for replying it’ll help me with searching how to do it if I have a term. I’ll check it out!']",0,13,https://www.reddit.com/r/statistics/comments/12rryuq/q_spss_q_merging_variables/
385,2023-04-19 18:36:58,[Q] Ranked data and hypothesis testing,"I have a question regarding ranked data and hypothesis testing.

Hypothesis: Car drivers are more likely to consider the car as the most beneficial transportation method for mental health than other transport method users'.

Respondents were asked to rank 4 transport methods from least (1) to most (4) beneficial to mental health. Responses are in the table below.

|Outcome|Drivers|Non-drivers|
|:-|:-|:-|
|Car 1st|10|3|
|Car not 1st|35|82|

A Fisher’s Exact Test test gives me a p=0.001.  The Fisher's test shows that there is a significant relationship between the variables, however, that would prove a hypothesis such as: 'Mode of transport influences the perception on which transport provides the most mental health benefits.' However, I want to know if car drivers are more likely to think this way.  What test should I do here? Thank you for your help.

Kind regards,","['Did respondents rank order the transportation types from 1 to 4, or were they asked to provide a score out of 4 for each transportation type separately?', ""Given the caveats in other comments (which are important),  you are correct that the Fisher's Exact Test tells you the relationship between your variables (for your sample) is significantly different from 0. \n\nIf you want to understand the direction and magnitude of that effect, you might look into using an Odds Ratio.\n\nHere's a post that is specifically on how the Odds Ratio relates to Fisher's Exact Test\nhttps://stats.stackexchange.com/questions/211487/interpretation-of-the-fisher-exact-test"", ""They were asked to rank the order from 1 to 4. I didn't know it would make a difference. Could you explain, please?"", 'Take your 4 by 2 data and put it into POLR (proportional odds logistic regression, R function polr in R package MASS, but also available in other statistics software).  Or you can learn about and use some other method for ordered categorical data.  But if you have that low a P-value for your data when dichotomized, the strong statistical significance will probably get even stronger when not dichotomized.  But you never know until you actually do the analysis.', 'Thank you for your response. You are right in both points and also my phrasing was indeed wrong. I will check the model you mentioned. Thanks']",15,9,https://www.reddit.com/r/statistics/comments/12rossl/q_ranked_data_and_hypothesis_testing/
386,2023-04-19 14:38:05,[Q] Goodman and Kruskal's Gamma Resources,Does anyone know good academic sources (or textbooks preferably) that explain and show how to compute a Goodman and Kruskal's Gamma correlation?,"[""It's discussed in Agresti, A. 2007. *An Introduction to Categorical Data Analysis*, 2nd. Wiley. , but *very briefly*.\n\nYou might use that for a reference, but for examples of the calculations, maybe see something like:\n\n[https://www.statisticshowto.com/gamma-coefficient-goodman-kruskal/](https://www.statisticshowto.com/gamma-coefficient-goodman-kruskal/)""]",8,1,https://www.reddit.com/r/statistics/comments/12rjnt4/q_goodman_and_kruskals_gamma_resources/
387,2023-04-19 10:16:44,[Q] How to define/find this accuracy in data,"Pardon my misuse of terms; I am an engineer but I dont deal with stats very often.

I have 2 sets of data which are both estimations of a 3rd, unknown, set of data. I need to determine 1. How close the 2 known sets are and 2. How accurate the second one is... the problem is in how to define that accuracy

As an example:

There is a picture with 9 elephants. A human counts 8, and a software counts 10. The first dataset is built from the human counts, 8 in example, of a bunch of pictures. The second set is built from the softwares counts of the same data, 10 in example. The third, unknown set, would be the actual count, 9 in the example. 

Ideally, we would like to know how accurate the software is in comparison to the true counts, IE the count was 9 but we got 10 from the software. However, we dont know the true counts. The idea then is to assume that the human counts are off by some percent on average, and to see if the software counts are accurate enough to that.

The problem is I do not know what measures to be using for this so that I can say this:

The software is ____% accurate compared to human counts, which we estimate are within ____% of the true counts on average. This means that the software is ____% accurate to the true counts.

Any and all help appreciated, I am more than happy to discuss more in the comments!","['If we never know the true counts then we would need to make some pretty drastic assumptions. Is all the data we have is the faulty human and software counts then we would need to assume some further structure in the data to go off of. Do you have any more info about how the faulty counts could be generated?', 'So we can find the true counts for a portion of the data, but certainly not all and only with images that have a low number of objects just due to the difficulty in verifying.\n\nThe faulty counts on the human side are generated more regularly on images with high counts. For example, an image which has 20 objects is likely to be counted as 20, but once we reach the hundreds there is likely to be missed counts.\n\nAlso, I do have some formulas for estimating counts that I know the humans use. For example, in a 10x10 image with a lot of objects, the human count is generated by counting in a 1x1 grid square and multiplying by 100, which inherently has error.\n\nIs there any specific info you would like to know? I may be able to provide more details if it helps']",3,2,https://www.reddit.com/r/statistics/comments/12rd9er/q_how_to_definefind_this_accuracy_in_data/
388,2023-04-19 09:41:50,[Q] Interpretation of interaction terms,"Hey yall, hope this is not a dumb question. Not a native english speaker, so I might've missed already answered questions on this.

I try to estimate if a variable, lets call it ""impact"", has an effect on another variable, ""outcome"", after event X, which I denote by a dummy, ""post"", which is 1 if observations in my panel are after event X. For this, I did the following two regressions:

(1) outcome \~ post + error

(2) outcome \~ post + post\*impact + error

Now, in (1), post is significant, which I interpret as event X having an effect on outcome. In (2), post\*impact is significant, but post isn't.

Can I just disregard post not being significant in (2) and solely focus on post\*impact as my explanatory variable, or is there anything I need to address regarding post? I vaguely remember something from my stat classes about variables being hard to interpret when they're also in an interaction term in the regression, but that feels like ages ago and I can't find anything in my notes (and sadly can't read up in Stock and Watson for a few more days).

Any help is greatly appreciated!","['model 2 is telling you that there is no difference based on time except in the group that experienced the impact (that is, the impact has a measurable effect). model 1 is not really relevant to your question, and you need not interpret our report it. Incidentally, your design is called a before-after-control-impact (BACI) design', 'Thanks for the answer! That helps a lot, and especially thanks for the name of that design, now I can read up a lot more.', 's forgot to mention, you should also add the main effect of impact to ensure that there weren’t intrinsic differences between the groups:\noutcome ~ post +impact + post *impact + error', 'OP if you’re doing this in R just including the interaction terms will also include the uninterested variable.', 'Opps, forgot to include that in the post. Have it in the model tho. Is it an issue if that one is significant, or does that simply mean theres an additional time effect of impact on top of post\\*impact?']",1,9,https://www.reddit.com/r/statistics/comments/12rcb9l/q_interpretation_of_interaction_terms/
389,2023-04-19 08:11:50,[Q] Real estate: estimator for average residence time based on limited data,"I've been collecting real estate transactions in my neighborhood for a while and have data about resales since 2017. The neighborhood was established in 2005, so much longer than what I have data for.

I'm looking to estimate average residence time (time between sales of a single home) in two ways:

1. (# of total homes) / (# sales in the last 12 months): This is more or less a proxy estimate since it doesn't include an actual sale of a single home twice, as if it were a longitudinal sample. The average residence time that I get out of this is very comparable to the expected (10-12 years).

2. actual longitudinal tracking of sales: This method (for now) is severely flawed because the length of the dataset is much shorter than the age of the neighborhood. I cannot seriously state that homes are being sold every 22 months because a small fraction of homes that I have longitudinal data for (only have ~50 months of data, so this do not count people who would increase the average because they haven’t sold within the last 4ish years).

Besides collecting data for the next 20 years or so, is there a way to get an estimate out of the data I already have?

crosspost from stats.stackexchange where there were no answers so far.
https://stats.stackexchange.com/questions/613218/real-estate-estimating-average-residence-time-based-on-limited-time-date","['Maybe look into the methods used by quality control & medicine, “survival analysis”. I don’t know that world well myself but have a feeling I’ve seen work there on reliable estimation of time-to-event data like this where the observation period is much shorter than the typical time-to-event period.', 'The residence time is a so-called ""censored"" variable -- for people still in their homes, what you know is that the residence time is greater than what has already elapsed. Survival analysis deals with problems like these. You can squeeze some juice out of the censored data, but the flip side is that censored data contribute less to pinning down parameter values than uncensored data -- you can get something out of each censored datum, but you could get more if it were uncensored. I guess that\'s fair.']",10,2,https://www.reddit.com/r/statistics/comments/12r9uqr/q_real_estate_estimator_for_average_residence/
390,2023-04-19 06:35:41,[Q] Applying Different Statistical Methods to Certain Areas of The Feature Space," Hi [r/statistics](https://www.reddit.com/r/statistics/)

I'm trying to design a method to evaluate the price of an asset given certain features. I have lots of data to work with, so the # of observations is not a real constraint.

Based on my conceptual knowledge of the features, I expect most of them to have a linear/semi-linear relationship with the predicted value except for 2. For these 2 features, I expect the predicted value to have more of a clustering/radial relationship.

I can understand how to model each of the two feature-types and their relationship to the predicted variable separately, but how could I ensure that the interaction between them is captured as well?",['> how could I ensure that the interaction between them is captured as well?\n\nOne way - perhaps the most usual way - introducing interaction terms as features'],2,1,https://www.reddit.com/r/statistics/comments/12r74rk/q_applying_different_statistical_methods_to/
391,2023-04-19 04:14:41,[Q] .25 to .34 after mediation?,"Mediation Help? .25 to .34 with mediator…

I’m not sure how to really interpret this mediation, as I’ve only ever seen a a decrease….does this mean (in layman’s terms) that the addition of my mediation variable makes the relationship between variable 1 and variable 2 weaker?

For context, with variables..

The relationship between conscientiousness and GPA was .25. 

The relationship between a specific learning style and conscientiousness is .34.

The relationship between a specific learning style in GPA is  -.15.

After running the mediation with the specific learning style as the mediator, the relationship between conscientiousness and GPA increased to .34.","['The relationship between the part of conscientious that is independent of learning style appears to be more strongly related to GPA than is the relationship between all of  conscientious and gpa. Learning style suppresses an irrelevant portion of conscientiousness.', 'I wouldn’t say that. More like partialling learning style out of conscientious increases it’s relationship with GPA. I would explore it graphically. First predict conscientiousness from learning style and save the residuals. Then compare a scatterplot of those residuals on the X-axis with GPA (on the Y-axis) to a scatterplot of the full conscientiousness variable and GPA.', 'Hm okay…so then is it appropriate to say that being conscientious diminishes academic performance when utilizing the specific learning style?']",7,3,https://www.reddit.com/r/statistics/comments/12r2s1d/q_25_to_34_after_mediation/
392,2023-04-19 04:13:00,"[Q] Ph.D students and graduates, what was your first few months of your Ph.D program like?",,"[""At least at my school the first couple months are really focused on classes-honestly it's really expected in your 1st semester that you mostly focus on that, while also just getting a feel for the type of research areas you want to explore."", 'PhD in the Netherlands. My first 6 or so months was all about formalising my research proposal and coming up with a time plan and doing the preparatory reading and such for the first chapter of the thesis.', ""U.S. PhD student here, just about to finish my first year. Since this is a pretty generic question I'd like to talk about content and the other parts as well.\n\nI came directly from undergrad with no previous graduate school experience (that is, no graduate level coursework in my undergrad). The first semester (first few months) was completely focused on the coursework. There was no expectation to think about research or potential advisors until the first semester was almost done. The coursework in my program is very theoretical during the first year, so it is mostly theorem/proof style lectures with similar homework and exams. This was a bit of an adjustment since my stats undergrad did not have the same focus. Therefore, I needed to adapt to this style of learning and grasping the material, which took several months and one terrible midterm.\n\nRegarding the other aspects, I found that working with other people in your cohort is essential and they can become very good friends. Additionally, expect long hours studying (especially if you didn't do this in your previous academic experience) with people in your cohort and courses. Also, don't forget to take breaks with your study group to talk about things that aren't math/stats. The other experience is that of an emotional roller coaster, which was pretty universal in my cohort. For example, you might feel confident about your knowledge on a topic one day then after the next lecture you can have imposter syndrome setting in. I don't think this feeling goes away but it just becomes the background cycle of the PhD experience.\n\nI hope I answered your question and please reach out if you have any other questions."", 'Fun. Then the changes happened', 'I’m doing my PhD in Sweden. The first 3 months or so are reserved for you to catch up on literature and write a detailed plan about what your PhD project is going to be. Obviously it will be more detailed for the beginning than the end. After that we jump around between courses, lab, method development or field work. Depending on what is planned for you. In the beginning, everyone is very relaxed about the amount of time you work so you can get settled in the new city or country. This time is also used to socialise in your group or department. The last 8 months are then meant to be spend on writing your thesis.']",2,7,https://www.reddit.com/r/statistics/comments/12r2q4r/q_phd_students_and_graduates_what_was_your_first/
393,2023-04-19 03:07:59,"[Q] To those who are actively involved in statistical research and have received research grants, what is the money being used for?",,"['Wages + a largish server + travel + workshops.', ""Mainly wages (and the Uni's cut), but also publishing fees, travel and presentation costs, computers, maybe access to specific data, that kind of stuff.  Mainly wages... Why are you asking, are you going to write a grant?"", 'Almost entirely wages, either my own over the summer or for GRA positions. The overhead taken by universities is also substantial. Funding just one student requires university indirect (charged at 50% or so) plus tuition and fringe benefits before even getting to salary, so even a fairly large NSF grant doesn’t go all that far on its own.', 'Food and babies']",3,4,https://www.reddit.com/r/statistics/comments/12r0l3i/q_to_those_who_are_actively_involved_in/
394,2023-04-19 02:39:52,[Q] Something like a paired t test when you have unequal sample sizes?,"I've got survey data from the same population across two points in time, one before an intervention and one after. 

I want to test to see if the mean score has changed, so thought I should use a t test. But then, I don't think an unpaired t test is appropriate because the two samples aren't independent because they're the same population and some people are in both.

 But I also can't do a paired t test because the two samples aren't the same size: some people are in the first but not the second and vice versa.

What should I do here? I don't want to reduce the data set to only people who answered both times to force a paired test, as then I'm throwing out about half of each sample. Am I misunderstanding something about the independence requirements of a normal unpaired test and that's the appropriate one?","['This is about as straightforward as paired data gets. I don’t think you have a choice except to exclude unpaired data points.', 'It might be that imputation is another approach that can be useful in your situation as well (e.g., imputation of the mean). Depending on the underlying mechanism behind the missing data, i.e., missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR) it might work for your case. Imputation methods work best when the missing data are either MCAR or MAR. If the missing data are MNAR, imputation may introduce bias in the analysis.If you decide to use imputation in your case, multiple imputations could be a way forward as it accounts for the uncertainty associated with imputed values. After imputing the missing data, I think you can perform a paired t-test on the complete dataset.', ""If you can identify the scores of the people who are in both groups you might look at an overlapping samples test\n\nOn the other hand, if you've don't have the actual pairings, you only have one of the independent  samples t tests ... \n\nOne thing to worry about is whether the missing values were missing at random or not and if so whether it was completely at random. See the missing data article on wikipedia\n\nhttps://en.wikipedia.org/wiki/Missing_data\n\nIf most of the data are paired and the missingness is completely at random (or even just at random I think), you could drop cases that aren't paired without bias, but it would cause  some loss of power"", 'Thank you']",1,4,https://www.reddit.com/r/statistics/comments/12qzon0/q_something_like_a_paired_t_test_when_you_have/
395,2023-04-19 02:10:23,[q] would working in a bioengineering lab on machine learning make me more competitive for statistics grad programs,"Hi everyone. I'm a bit stumped on what I want to do with my life. I'm currently an undergrad majoring in applied mathematics and I know that I want to pursue a graduate degree but I can't decide if I want to pursue statistics or bioengineering. I'm interested in working for a pharmaceutical company in the future, but also I really enjoy statistics and can see myself doing statistics work in the future. I have the opportunity to start research in a bioengineering lab this fall working on machine learning and modeling. Would doing this type of research be good for statistics programs?",['Yes. Any relevant work will be better than some random job.'],4,1,https://www.reddit.com/r/statistics/comments/12qyqsz/q_would_working_in_a_bioengineering_lab_on/
396,2023-04-19 00:46:43,[q] [r] How to control age in my social science survey data?,"I'm working with survey data for college students' food choices. We want to calculate the rate of campus food security, but we find that it varies a lot by age (freshmen, who have access to the dining hall, are way more food secure than sophomores/juniors/seniors; people who have been in college 4+ years have the highest rates of food insecurity). Respondents to our survey opted in to a survey, so our 2000+ responses are not a perfect reflection of our campus. In particular, freshmen are overrepresented in our survey. As a result, we believe that we are over-reporting food security on campus (freshmen are the most secure). If we controlled for age, and had a sample that reflected the age break down of campus, we think we would have a much more realistic picture of my campus's food security.

Is there a way for me to do this ethically, without fudging the data? What sort of language and tests are done to do so? And how do I justify the fact that I chose to control by age -- as opposed to by income, or race, or any other variable?

Thank you!","[""In addition to just asking for age, you can also add a weight variable to each respondents.  If you know what the population demographic is supposed to be, you can back it out from a biased sample.  Ex: if there are not enough seniors in your sample compared to your population, then each sampled senior would represent more people compared to freshman.  \n\nYou should control for other variables as well, so long as you can get the data.  Income for example is especially important one to consider.  Using binned income/other variables to preserve some anonymity.  \n\nAlso, I'd suggest you look up how other surveys are designed.  \n\nHere's a link of a book by Statistics Canada on how do surveys.\n\nhttps://www150.statcan.gc.ca/n1/pub/12-587-x/12-587-x2003001-eng.pdf\n\nCh 6,7,8 and 12,13 seems to be the most relavent"", ""You need to do post-stratification weighting using student year in school. The  freshman weight would be the proportion of the school population that are freshmen divided by the proportion of respondents who are freshmen. Also calculate the sophomore, junior, senior weights the same way. Then apply this weight in whatever stats program you're using; they all have procedures for using weights."", ""Can't you just add one more point in the survey asking what year are they in? That would be the best solution."", 'Assuming you can\'t rerun your survey, then this seems like a reasonable thing to do. You seem to have justifiable extra-statistical reasoning for why you believe that class/age has a significant effect on whether or not a student has food security. \n\nThis is a commonly done problem in public opinion surveys and a common approach is ""multilevel regression and poststratification"", where you fit a model to estimate effects according to certain demographic traits, e.g. age, and the with a known distribution of those traits within the overall population, you can project those effects down onto the population and estimate a population level effect.\n\nThe logic is that you know your survey sample is not representative of the overall population, but you have data on your responses to estimate how biased the survey is compared to your population. Combining those two pieces of information allows you to generate population estimates.', 'i agree']",2,8,https://www.reddit.com/r/statistics/comments/12qvnvf/q_r_how_to_control_age_in_my_social_science/
397,2023-04-18 23:13:00,[E] Best graduate statistics and probability books?,"
I’m looking for grad level books on statistics. I’m already learned in measure theory and econometrics and want to round myself out.","['Brush-up: All of Statistics\n\nIn-depth: Hogg Tanis and Rao (intermediate), Casella and Berger (advanced)', '‘Bayesian Data Analysis’ by Gelman is a sorta the Bayesian Bible. Doesn’t sound like you’d need it but in case others see, I believe ‘Mathematical Statistics with Applications’ by Wackerly is the go to for math stats is many non-measurement theory classes', ""It's not clear from your post what level of graduate statistics you want -- since you have a measure theory background, you may want to consider an advanced treatment: _Theory of Statistics_ by Schervish or maybe something like _Asymptotic Statistics_ by van der Vaart. The suggestions in the other comments are more appropriate for more introductory graduate material, i.e. no measure theory."", 'Statistical Inference, Duxbury Press Davidson, J. (1994).\n Stochastic Limit Theory, Oxford Univ. Press \nDoob, J.L. (1994). Measure Theory, Spring-Verlag Dudley, R.M. (2002).\n Real Analysis and Probability, Cambridge Univ. Press\n Fristedt, B. and G. Gray (1997). A Modern Approach to Probability Theory,\nBikhäuser Kallenberg, O. (1997). Foundations of Modern Probability, Springer\n Shao, J. (2003). Mathematical Statistics, Springer', 'Well, it\'s not a technical book, but as for a set of concepts to frame the technical stuff, I recommend E.T. Jaynes, ""Probability Theory: the Logic of Science"", which is an exposition of Bayesian inference. It\'s marred a little by screeds against Ronald Fisher; oh well.']",37,22,https://www.reddit.com/r/statistics/comments/12qqudq/e_best_graduate_statistics_and_probability_books/
398,2023-04-18 23:09:22,[E] Is real analysis overkill?,"Would real analysis be overkill for getting into a applied stats / data science masters or career from undergrad? Would I get any ""use"" out of it in those fields? I could spend the time taking another course or picking up different skills.","[""Can't speak for data science, but real analysis is quite possibly the best preparation you can have for math stat, which will form the foundation of your stats education and will almost surely be needed in any stats program.\n\nThat being said, it is overkill in the sense that it is often not required for admission. But if you take it, understand it, and do well in it, you will have a huge leg up when it comes to understanding the mathematics behind probability, statistics, modelling, etc. Which in turn will make your stat program easier and less stressful.\n\nA worthwhile investment in my opinion."", ""What you can get out of real analysis is practice in proving things for yourself. This is a skill which is useful across all fields.\n\nIt turns out that in order to look something up, you have to have about half an answer in mind, so that you can recognize the rest of it. It's useful to have some experience with getting the half an answer -- which requires being able to state the question (itself a huge part of any problem) and scope out a solution -- and also doing the first half, and then just doing the rest yourself."", '[deleted]', ""No, it's not. It's a standard module for anyone who's involved in advanced statistical research & development. \n\nIt can boost your journal reading experience to the next level."", 'Plus, it would prepare you for a course in measure theory, which would really give you a leg up in any stat or data science track you may pursue']",6,11,https://www.reddit.com/r/statistics/comments/12qqmoq/e_is_real_analysis_overkill/
399,2023-04-18 23:07:01,[R] [Q] Question about multiple comparisons (neuroscience data),"Hi everybody,

I am performing EEG analyses. Our outcome is a time-amplitude progression. In the end we receive a characteristic curve, consisting of certain components (e.g. N100 -> negative deflection at the timepoint 100; P300 -> positive deflection at the timepoint 300, etc.).

We have 4 such components of interest. Furthermore, we have 4 conditions, across which these components can differ. We also have 64 channels distributed across the scalp where everything has been measured. 

In our main analysis we performed a Kruskal-Wallis test to compare each condition with each other for one channel. We did this for every component separately (i.e. component i of conditions w,x,y,z of electrode A).  In the post-hoc analysis we corrected for multiple testing, as we had 4 conditions in every test. The components are related to each other directly and are therefore looked at separately.

Now we want to compare each component separately within the same condition between 2 different channels, likely using a Wilcoxon test (component i of condition z between electrodes A and B) . Do we still need to correct for multiple testing, even though we basically only compare 2 distributions with each other?

Does all of this seem sound to you? Thanks for any replies!

Alf",['Yes. Source: dead salmon study'],2,1,https://www.reddit.com/r/statistics/comments/12qqhxz/r_q_question_about_multiple_comparisons/
400,2023-04-18 23:02:03,[Q] What are the general expectations for a stats PhD applicant? Do I need a stats degree prior to applying? Please help.,"Hi all, I have an undergrad degree in CS, but I wanted to get into stats, so I applied to many stats program, but got rejected from them all, owing to my weak GPA and less research experience.

But I eventually ended up getting into a predictive analytics course at UIUC, which has ~60% stats coursework and some finance/risk management coursework.

If I want to get into a top PhD program eventually, could this be a good stepping stone or I should wait 1 more year and try to get into a core Stats program?

I am really looking for some help here as I feel stuck, as I really want to get into a top PhD program eventually and I want to give myself a best chance to achieve that.

Even some information only slightly related would be very helpful.","['There are three axis\' that PhD apps hinge on:\n\n1. Coursework/ Grades: Has the student taken the courses we require (calculus, linear algebra, mathematical statistics, probability theory and then maybe a regression class and real analysis) and demonstrated that they can do well in them such that classes won\'t be a problem in their PhD.\n\n2. Research Experience- Has the student demonstrated that they can start and complete research? Do they have experience tackling open ended and hard problems and making some progress with them? Have they been around people who have done this and know the vibe?\n\n3. Letters of Rec- does the student have people that have intimately worked with them and can speak to 1 and 2. Do these people know what a PhD is? Do they advise graduate students/ have deep research experience and can they speak to if the student in question would be a good grad student? \n\nIn order to have a compelling, effective application you generally need two out of those three solidly met with the third being sufficient. If you are going for ""a top"" PhD program all three need to be excellent. If you really want to do a PhD in stats, you need to make decisions that maximize the likelihood of demonstrating some of those axis\'.', 'Have you tried asking your professor for your predictive analytics course for advice? Or go talk to them about this during office hours', ""That's right, I'm going to do that."", ""Hey I got the gist of it, can I send you a dm? \nI've also emailed my professors.""]",3,4,https://www.reddit.com/r/statistics/comments/12qq7h4/q_what_are_the_general_expectations_for_a_stats/
401,2023-04-18 21:31:47,"[Q] Does the assumption of sphericity in repeated ANOVA matter, where the only test of relevance is the between-subjects test","I know that an assumption of Repeated ANOVA is the sphericity assumption however to my knowledge this primarily concerns analysing the within subjects tests. 

If you were to only be interested in the outcome of the between subjects test would the  sphericity assumption still be important? 

If you could point me to academic papers which discuss this I would be most grateful.","['Sphericity applies only to within-subject effects. A between-subjects effect is based on only one score per subject, the mean across all levels of within-subject variables. Therefore, assumptions about a covariance matrix don’t apply. Also, sphericity is automatically satisfied for within-subject variables with 2 levels. Incidentally, I wrote [this tutorial](https://www.tqmp.org/RegularArticles/vol12-2/p114/p114.pdf) on the assumption of sphericity.', '""Discovering Statistics Using R"" by Field et al has a chapter on this method and it\'s assumptions including examples and literature references. I\'d give that a try. (there\'s also an SPSS version if you prefer)', 'Thank you very much. This was just what I was looking for.\n\nSo would you say that when looking at between-subjects differences in repeated or mixed ANOVAs it is more important that to have Equality of Error Variances ?', 'With unequal n homogeneity of variance is important. The Welch test can be used if assuming homogeneity of variance can’t be justified.']",1,4,https://www.reddit.com/r/statistics/comments/12qn7aj/q_does_the_assumption_of_sphericity_in_repeated/
402,2023-04-18 19:54:44,[R] [Q] Modified Rolling Regression with variable time block size,"Hello, I am trying to fit a model that follows generally: ln(Y) = n ln(time) + b which is straight forward enough. I am most interested in the value of n and it varies anywhere from \~0.3 - 2 (Always positive) and my dat has two distinct regions of n where you sort of have a fast rise (large n), then a flatter region where you've reached steady state. Generally people fit one half of the data then the other. The cross over region between the two is somewhat hand-wavy and I am trying to more quantitatively determine that point and compare it between samples and the difference is on the order of 10% of the x-axis so fairly wide given my number of repeats.

It seems like a decent method would be something like a modified rolling regression where I say - look at the first 10 minutes, calculate the a linear regression R1. Look at the next 10 minutes and calculate R2. Look at the total 20 minutes, calculate regression R3. If R1 and R2 fall within R3, keep R3, add 10minutes, repeat. If not, slope has changed, start over from t2. Might be even better if it calculates forward and backward to avoid overfitting either side if the change in slope is close. From the core science side I can even give the change in n some reasonable bounds like n=3.001 and 3.002 might end up as statistically different, but realistically changes <0.1 are within reason to be ""the same""

Does something like this already exist? I can't seem to find something more similar than rolling regressions. It's almost like fitting a regression model and looking at the residuals and backing off the data range until the residuals come into line but automated and not as problematic as ""it looks right to me""","['Do you want only the parameter n to vary over the window, or do you want the entire model to vary?\n\nEither way, you’re looking for regression methods with adaptive windows. Adaptive kernel regression is one option that is widely implemented in software.\n\nIf you only want one the n parameter to vary while keeping b fixed, additive models are a good option. For example in the mgcv R package, you can fit a time varying coefficient model for the coefficient n. With most methods this is roughly equivalent to a fixed window, but mgcv also allows for an adaptive penalty which is equivalent to an adaptive window.\n\nFor both types of methods, I caution that adaptive selection can be quite data hungry and may be unstable without large amounts of data.', ""Thank you for the input! I will take a look into adaptive kernel regression.\n\nI believe b should vary with the window also. For more context my reaction rate is expressed as: m\\^n = Kt -> n\\*ln(m) = ln(t) + ln(K) -> ln(m) = (1/n) ln(t) + ln(K)/n\n\n&#x200B;\n\nSo I'm plotting ln(t) vs ln(m) and yes I realize I could plot the other way and get n directly instead of 1/n but convention is the other way for some reason. Either way the intercept is a function of K / n so it should vary with n. \n\n&#x200B;\n\nFrom the reaction side, K traditionally varies with n but that may only be because we force it to. Ie this reaction is parabolic fit m\\^2 vs t and the slope is K. Or this reaction is linear fit m vs t. K constantly varies but maybe more fundamentally it shouldn't, were just forcing it to vary to account for the imprecise n value w fix at 0.5, 1, 2, 3 but that's a discussion for my PI lol""]",1,2,https://www.reddit.com/r/statistics/comments/12qkmk8/r_q_modified_rolling_regression_with_variable/
403,2023-04-18 19:23:47,[Research] Is it better to use Cummulative abnormal return or daily stock return for an event study?,"I am doing some regression for my bachelor thesis related to regression, regression is not my strong suit. I am attempting to measure the impact of Quantative easing announcements on stock price for reference. As a bonus question, I am also wondering if it is nessesary to use control variables when one is doing 1 variable regression. Tyvm for any guidance :).",[],1,0,https://www.reddit.com/r/statistics/comments/12qjymx/research_is_it_better_to_use_cummulative_abnormal/
404,2023-04-18 12:36:08,[Software] Bayesian Networks in >PyMC4,"I am trying to write a simple BN in PyMC for a research project. I found this discussion on the pymc discourse here about how to write a BN in PyMC3 [https://discourse.pymc.io/t/bayes-nets-belief-networks-and-pymc/5150/2](https://discourse.pymc.io/t/bayes-nets-belief-networks-and-pymc/5150/2?u=i_love_bn) . But I am confused about how to do this in PyMC4, because the theano.shared function does not exist in PyMC4. Can someone help me out with this?

I would also like to know if there is an easy way to create a BN where there are 10 input nodes and one output node because I do not want to create a function with 10 arguments like the reply above above.",['I have a similar question. Any help will be appreciated. Thanks!'],5,1,https://www.reddit.com/r/statistics/comments/12qak6r/software_bayesian_networks_in_pymc4/
405,2023-04-18 10:11:10,[E] ISL Python edition coming up!,"Good news for ISL fans who are Python users. Apparently it is the same book with labs worked out in Python.

https://www.statlearning.com/","['Thank God I have been procrastinating for months from reading that book because of R.', 'Rest assured I will keep all my tutorial content in R :D', 'haha still a good book though', 'Can’t wait for this.', 'me too, i’m keeping both books']",66,6,https://www.reddit.com/r/statistics/comments/12q6gir/e_isl_python_edition_coming_up/
406,2023-04-18 09:59:10,[Q] Would you trust repeated measures or a greater sample size more?,"Hi everyone,

Hoping to understand something my PI asked me to do. They previously did a repeated-measures structure analysis with 30 people for a total of 120 sessions (a session every 3 months). They now have 50 people for the baseline and want to re-analyze the data of just the baseline. They are curious to see if the results vary. If they were to vary, which analysis would you trust and why?

Thanks so much, looking forward to learning more.","[""It's hard to help with so little info. What's the outcome of interest? Do you mean 4 sessions per person?\n\nMy gut feeling based on almost no context is that your PI wants you to conduct a transversal sensitivity analysis to see if including more people will impact the baseline level of whatever it is you're measuring.\n\nIt's impossible to tell you anything about modelling and hypothesis testing with what you have provided."", 'What\'s getting measured?  With some things measuring the same person 4 times you end up with changes because of how the measure is done. For example with a lot of biological assays there\'s a lot of variability due to instrumentation - small changes in the reagents, temperature, etc. That ""bench variability"" may make it where the measures are effective for seeing differences between groups, but not stable values over time.\n\nOther types of measures, say if it\'s a math test, you would expect taking the test 4 times would result in improvements. Practice makes perfect, right? Seasonal variations can exist.\n\nGenerally speaking, which analysis I would trust really depends on the thing getting measured. In the presence of stable differences between the sampled items, repeated measures will be much more sensitive. If the items are fairly homogeneous and the measures are not stable over time then not so much.', 'What is being tested are cognitive exams for patients while they are undergoing weekly therapy that is suspected of having adverse side effects. And yes, 4 sessions per person, each session is 3 months from the last.\n\nThe hypothesis is this therapy may be associated with worsening cognition.', '>What is being tested are cognitive exams for patients while they are undergoing weekly therapy that is suspected of having adverse side effects. And yes, 4 sessions per person, each session is 3 months from the last.  \n>  \n>The hypothesis is this therapy may be associated with worsening cognition', 'What are the variable types?']",2,6,https://www.reddit.com/r/statistics/comments/12q63n6/q_would_you_trust_repeated_measures_or_a_greater/
407,2023-04-18 05:57:57,[E] Theory of Statistics online videos,Anyone know of some good videos to self learn theory of statistics. Some people might also call it Mathematical Statistics 2 or just the sequel to probability theory. I know of a few good books but I find that when I just read the book I have no clue what’s going on and just end up memorizing how to use the formulas.,"[""From your description I'm not entirely clear what actual topics you want covered, at what level of mathematical sophistication. \n\nDo you have a list of topics you're hoping for coverage of? (If you're using a book presumably there's a table of contents with named subsections; the ones giving you the most trouble shouldn't be hard to identify)\n\nAre you looking for something that assumes you have measure theory? Something that assumes calculus but not measure theory? something else?\n\n> I have no clue what’s going on and just end up memorizing how to use the formulas.\n\nNot sure videos would make a lot of difference to this unless you're also doing some other things\n\nHow many exercises are you doing on a given topic? Do you look for ways to apply what you're learning?"", 'Pretty standard, at least in the US. Basically the second half of C&B.', 'The book I’ve used is Probability & Statistics, 4th edition (Chapters 7-11). No measure theory but it requires knowledge of multi-variable calculus.\n\nThe reason I think I understand things so much better from videos is because they explain the notation. In books I’ve noticed it’s all just mathematical notation which I tend to have difficulty actually understanding what it means (I sort of just see it as a formula).\n\nI try to do a as many practice problems for each topic until I feel I understand the theory and could apply it to an arbitrary problem. OR I’ll try to find the mathematical way to do something (meaning I’ll know how to compute something if given numbers but generally don’t rely on the theory).', ""> Probability & Statistics\n\nThere's multiple books with that title; do you mean the book by DeGroot and Schervish?\n\n>  In books I’ve noticed it’s all just mathematical notation\n\nThe notation is usually defined in the books, unless it's completely standard stuff you'd know from earlier work like a calculus class or something (and often even then).\n\nHowever I think I am getting some sense what you mean.\n\nOffhand (assuming you do mean something at the level of DeGroot and Schervish) I don't have a direct suggestion right now but it might help someone else come up with something. \n\nOn the probability side (sorry, I don't remember what chapters 7-11 cover in that book but I assume that's past the probability stuff so this may be useless) there are youtube lectures for the harvard first course on probability, stat110, but failing that, I'd try searching by specific topics""]",15,4,https://www.reddit.com/r/statistics/comments/12pyjtb/e_theory_of_statistics_online_videos/
408,2023-04-18 05:52:04,[Q] Sample size calculation for study evaluating AUROC of new diagnostic method,"Is there any method that has been put out? I can’t find anything in literature. I have expected AUROC and prevalence of disease, but can’t find an equation to plug them into.",['There should be resources online for this.  I did a quick Google and found this:  https://search.r-project.org/CRAN/refmans/pROC/html/power.roc.test.html'],3,1,https://www.reddit.com/r/statistics/comments/12pycpy/q_sample_size_calculation_for_study_evaluating/
409,2023-04-18 05:12:02,[Q] Multiple clustering variables for linear mixed models?,"Is it common practice to have multiple clustering variables? For example, if individuals (ID) are being recorded multiple times (one clustering variable), but individuals are grouped by a different type of test. Would it make sense for the type of test to be another clustering variable for random effects?","['Sure', 'Yuppers', 'Sweet']",1,3,https://www.reddit.com/r/statistics/comments/12px0g9/q_multiple_clustering_variables_for_linear_mixed/
410,2023-04-18 04:39:24,[Q] What does this notation in linear regression mean?,"||y-X(theta)||^(2) 

with a subscript 2 as well, but i can't type the subscript in this post.","[""It's the notation of vector norms; the subscript means the L2 norm."", 'Double bars refers to the norm, kind of the length, of a vector. And I suspect that the formula within is calculating the difference between y and the prediction based on X. In other words, this is the way you represent the residual in linear algebra.', 'https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm']",0,3,https://www.reddit.com/r/statistics/comments/12pvxji/q_what_does_this_notation_in_linear_regression/
411,2023-04-18 03:00:42,[Q] Decorrelation of a singular autocorrelated variable," Hey!  
I have a dataset with several variables with average prices from week to week over many years. The problem is that each variable autocorrelates with itself heavily and I want to decorrelate variable for variable each by itself so that I can actually use different statistical tests and actually get meaningful answers on each variable. But I am having trouble finding what type of technique or methods I can use to achivie this type of decorrelation since the variable autocorrelates differently based on time. So for example it heavily autocorrelates week for week, but for each week going by it autocorrelates less and less. So I checked first out PCA, but that doesn't do that, I got mentioned mahanalanobis decorrelation, but I am having a bad time actually finding how to perform that decorrelation in excel, stata or python. So if anyone have any teqnique or methods to share that would be greatly appreciated.","['Would it work to use some sort of a (partial) difference operator to transform to stationarity, then use some type of [pre-whitening](https://rdrr.io/cran/TSA/) on the differenced series?', "">So for example it heavily autocorrelates week for week, but for each week going by it autocorrelates less and less.\n\nA very common pattern for time series, especially for prices.\n\nA very common transformation for prices that improves things a lot is to use percentage changes, or log differences. Depending on what you want to estimate or test, further simple differences might be in order. But autocorrelation itself (unlike non stationary) doesn't mean means, variances, covariances etc can't be estimated (and tested)."", 'Hmm will check it out tomorrow', 'So I tried calculating the autocorrelation in a price region by using this in excel: (context, my price data goes from C4 until C632) So I used first the Correl function by doing this =CORREL(C4:C632;C5:C633) and got 0,940688359 where I also calculated the confidence interval to know for sure that it does not fall in which is does not. Then I went to my price data and used =C5-0,940688359\\*C4 and this should be a decorrelated data I guess? But my answers seems kinda weird even though I am not sure how it is supposed to look, but my first 5 prices look like this:   \n58,02875  \n53,364125  \n51,77625  \n61,47875  \n65,63125  \n\n\nWhich gave me these decorrelated data:  \n\r  \n\\-1,222855057  \n1,577229219  \n12,77342503  \n7,798894483  \n3,711435325  \n\n\nIs it supposed to look like this or do you see some obvious mistake?', 'So I could use percentage changes or log differences to decorrelate? I am looking at spot electrical prices against fixed eletricity prices and to compare them I would need to decorrelate them to have satisfying answers']",1,5,https://www.reddit.com/r/statistics/comments/12psv7g/q_decorrelation_of_a_singular_autocorrelated/
412,2023-04-18 02:11:01,[R] How to compare the impact of different marketing campaigns on customer acquisition.,"I have been working on this project on and off for about a year and am stuck. I am hoping someone might be able to point me to existing research on the topic.

I am trying to measure the impact of different marketing campaigns on customer acquisition. I have used regression up to this point. The dependent variable is the volume of new customers by month, each dependent variable represents a different type of campaign (TV/Radio, Storewide sale, etc) and the observation is the number of occurrences of each campaign type in the given time period.  
My first attempt was to use monthly data, but I did not get any significant results. I have refined my measurements to weekly but repeatedly fail the linearity assumption. I tried box cox transformations to overcome this, to no avail.",[],1,0,https://www.reddit.com/r/statistics/comments/12pr9w1/r_how_to_compare_the_impact_of_different/
413,2023-04-17 23:16:03,[Q] Bayesian inference using MCMC: why?,"I  needed to simulate a posterior distribution for a simple discrete model, and I've gone through the process of learning the metropolis  algorithm.  Everything looked fine, but then I tried to do the same  using Bayes'  rule directly, and naturally,  the computation was not  only more precise but much faster.

My  question is:  what are the real-world cases where MCMC is used instead  of directly using Bayes' formula? I thought the issue was that  integrating to compute the Bayes' denominator takes time, but since I  have to compute the numerator for every value of the prior, why not add  up all of these numerators and use the sum as the denominator? If I can  do that, why would I  use MCMC? Even if the distribution is continuous,  couldn't I just sample many values, compute Bayes' rule for each, and  add them up to integrate?","['> My question is: what are the real-world cases where MCMC is used instead of directly using Bayes\' formula\n\nAlmost every ""real-world"" case which uses anything more complex than a simple toy model uses some kind of numerical approximation scheme, such as MCMC. It is generally impossible to derive the posterior analytically in anything by the simplest models.\n\n> since I have to compute the numerator for every value of the prior, why not add up all of these numerators and use the sum as the denominator?\n\nThere are infinitely many ""values"" of the prior in general, such as when the prior is continuous. In that case, the procedure you are describing is a simple approach to numerical integration, which is computationally infeasible in anything more than a few dimensions (see [the curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)).', 'There are a handful of problems for which you can do pencil and paper (symbolic) computation of Bayes rule.  For most non-toy problems you cannot.  Hence MCMC.\n\nI never understand the ""compute the Bayes\' denominator takes time"" stuff.  MCMC samples the posterior and allows Monte Carlo calculation of posterior probabilities and expectations.  Those are the integrals you cannot do symbolically.  You could use ordinary Monte Carlo instead, but there are no OMC methods for most multivariate models (only normal, multinomial, Dirichlet, and uniform on boxes, balls, and spheres).  So what your last sentence is suggesting, is exactly what no one knows how to do for most applications.  OMC works better than MCMC when it can be done.  But it usually can\'t be done.', ""Outside of conjugate priors, which involve the selection of a very specific assumption for the parametric distribution of the data-generating process AND very specific parametric assumption about your prior, there are very few posterior distributions which can be written down with an analytic solution. That's not to discount their use-- there are an endless array of situations where this is a very reasonable set of assumptions. But they are mostly constrained to univariate analysis. \n\nHence, there are lots of problems requiring a numeric solution. Hence, MCMC. \n\nOne key piece of intuition that is often lost is that MCMC is just a numerical integration technique. It has useful applications for evaluating integrals numerically that have absolutely nothing to do with probability."", "">what are the real-world cases where MCMC is used instead  of directly using Bayes' formula?\n\nUhhhhhhhhh this question?\n\nBayes' Formula= computing the solution to a complicated integral in the denominator, either analytically or numerically. Regardless if you arrive at that solution either analytically or numerically, it's the same problem.\n\nI explained to you why there are some cases where you may attempt to do it analytically, and why you may need to do it numerically when that fails."", ""In the equation p(θ|x) = p(x|θ)p(θ)/p(x), p(θ|x) and p(x) don't depend on each other. So for the sake of convenience we often just estimate p(θ|x) ∝ p(x|θ)p(θ), i.e., we ignore the normalizing factor p(x) and work with the relative magnitudes of the posterior probabilities. In other words, the denominator p(x) often isn't that important in practice, and it certainly isn't the main reason that MCMC methods are used.\n\nIn principle, if we could do something like a grid search over the support of θ and calculate p(x|θ) and p(θ) at each point to get the (unnormalized) posterior probabilities. But if θ ∈ R^(k), the number of points you need grows exponentially with k - or equivalently, if you have a given amount of compute available, the distance between the points you can test grows exponentially as k increases. At the extreme, an exhaustive search of the values that can be represented in 64-bit floating point would require something like 2^64^k computations of p(x|θ) and p(θ), which obviously isn't tractable in practice even for relatively small k. And at almost all of those points, the product p(x|θ)p(θ) would be very close to 0, since at least one of the factors would be very close to 0.\n\nThe value of MCMC methods (and especially of of modern MCMC methods like Hamiltonian Monte Carlo) is that they can efficiently focus their computation on the subset of R^k where p(x|θ)p(θ) >> 0, which puts them at a huge advantage over the naive strategy.""]",21,29,https://www.reddit.com/r/statistics/comments/12pkthp/q_bayesian_inference_using_mcmc_why/
414,2023-04-17 23:12:00,[Q] Book or textbook for statistical modeling (biology),"Hi, I am an European PhD student in population genetics. I have attendend some courses (42h in tiotal) on mixed models, geneal and genralized mixed model but i get very little out of them: may it be for the very concentrated lessons, issues with the language which is neither my native one or English, or simply I am not a peak in, well, wathever.

I want to dig better and more on these topics, as they can be very useful in the near future - *storms*, I am dealing with a glm with a logit as a link and I walk into the dark.

I don't like the idea of getting piece by piece from online dedicated page to single topics, so I am looking for siomething more comprehensive. In example, I found of great help in dire times *Statistical thinking from scratch* by M. D. Edge.

Is there something which could help me? I'd be very grateful if someone can point me a book - even better, *exegi ei monumentum aere perennius.*","[""I've been plugging away at [Modern Statistics for Modern Biology](https://www.huber.embl.de/msmb/). Weird that it's English only, I'm pretty sure one of the authors is German."", 'I like Rosner\'s ""Fundamentals of Biostatistics"" and Daniel\'s ""Biostatistics"" (https://archive.org/details/BIOSTATISTICS) as introductory textbooks', 'Thanks, that\'s a goldenful piece of information!\n\n(although, ""goldenful"" doesn\'t exist, but let\'s play with words)', ""Came to say this! \n\nI've been working through this book and I like how it jumps right into statistical concepts with relatable application problems.""]",9,5,https://www.reddit.com/r/statistics/comments/12pkmkb/q_book_or_textbook_for_statistical_modeling/
415,2023-04-17 20:22:35,[question] Hypothesis Testing exercise,"Let’s assume that someone has asked you to test the follow hypothesis:

“Does the name of males affect their blood pressure?”

Also let’s assume that these males live in England.

Finally, let’s assume that each sample point/ sampling has a cost.

I.e. you could measure everyone’s blood pressure and name but this would be very expensive.

What’s the most cost effective way you would do that hypothesis testing in order to be able to generalise the outcome for are males living in England?

Cheers


Edit: it’s not a uni exercise it’s a real question bothering me. I would appreciate an answer :)


Edit 2: this is a mental exercise I’m just interested in the approach you would take. If you can please tell how you would do this hypothesis testing if you had all the resources in the world. And if you can’t why you can’t. 

One approach for example would be to measure the whole population. What’s the next best using stats?","[""so you're taking a course on sampling eh?"", 'Your main worry here is omitted variable bias.', 'You need at least 20 observations with the same name or the same type of name to begin to make reasonable inferences.\n\nYou need a lot of additional theory to guide you in selecting names or in categorizing names.\n\nThere are a lot of restrictions on collecting medical information about people, especially with identifying information about that person.\n\nI doubt a true random sample is possible given privacy concerns, not just financial concerns. Imperfectly random samples are plausible, but as you note, probably expensive.', 'Hi thanks for your response. We are getting somewhere. \n\nThis is hypothetical problem so don’t worry about GDPR. I’m just interested in the approach. The cost element is to discourage people to use the whole population int their solution and take a statistical approach.\n\nWhat makes you think I need additional theory to select names. If so can you indicate a direction to that theory.\n\nAlso why at least 20 observations, where is that coming from?\n\nAnd finally still you haven’t talked about the approach to the actual testing.', 'hmm can we not use t tests? the max number of observations required for performing a t test is 30, so as long as we have a maximum of 30 males (data/info), we should be able to perform a hypothesis test? (altho with a very poor confidence score but hey..a hypothesis test nonetheless haha )']",0,7,https://www.reddit.com/r/statistics/comments/12pe0qf/question_hypothesis_testing_exercise/
416,2023-04-17 17:26:55,[Q] Ordering items from a list,"In a survey, respondents are asked to drag some items in a list in order of decreasing importance. Say the items are A, B, C, D, E and a respondent's answer might be D>A>B>E>C. What statistical analyses can I perform to analyse these data? Which test can I use to determine if the mean ranks of items differ? Are there any other commonly used techniques or tests to analyse this type of survey question? Thanks in advance!","['You could treat their rank as a straight ordinal measure and regress against other respondent characteristics?\n\nThere may be a fancier technique more geared towards this type of rank analysis; just mentioning what I would try first.', 'Analysis depends on what questions you want answered. Not something to do for the sake of it.\n\nFor instance if you want to segment respondents based on their preferences then clustering can be done. If you want to match how close a respondent is to a reference response rank correlation can be calculated. If you want to identify collective preference then various voting approaches can be used (rank choice, for example).', 'You could look at some Markov approaches too and quantify the journeys/ transition probabilities thru the different states. Maybe there’s a hidden Markov application too where some hidden state determines the respondents’ journeys', 'Have a look at exploded logit (plackett-luce) models. These models infer a latent ""worth"" for each option based on the ranks. You can then compare these worths (and their uncertainty) \n\nThey\'re used a lot in stuff like sports analytics (race results) and economics/psych (market research type thing). That should give you loads of flexibility also to include predictors and do all sorts of fancy stuff with your ranks!', 'Excellent approach!  Not the type of analysis I originally had in mind, but definitely an interesting idea that I can use. Thanks!']",9,13,https://www.reddit.com/r/statistics/comments/12p9hua/q_ordering_items_from_a_list/
417,2023-04-17 14:15:11,[S] JASP is deleting rows and columns.,"Hello, I have a problem with Jasp 0.17.1 in which I was doing descriptives and testing my hypothesis for my thesis. Does anyone encounter deleting rows and columns after saving data? For example in saved data I dont have column ""Gender"", it is completely gone even when I had it in descriptive statistics. Deleting rows can be seem in ""Age"" where now I have only 28 valid and 0 missing, instead of 158 valid and 0 missing.

Does anyone encountered problem like this?","[""It's unlikely that a release version would have a bug like that, and you would need to provide enough detail for a reproducible result - we can't see what you're dealing with (what do you mean by 'saving data'?). Screenshots arranged in a workflow, with annotation/description would be needed, I think. \n\nIt might be that you've got missing data in some columns and not others, and so when you do an analysis, JASP automatically drops incomplete cases. So that would seem like you've got more useable data in your descriptives than you do in a main analysis. Alternatively, your data might be imported in a weird format, so it's trying to drop rows or entries that are formatted incorrectly? I don't really use JASP, so I'm not sure about its default functionality.\n\nJASP has a bug reporting portal on its website, where you can get support too, so that should likely be your first port of call.""]",5,1,https://www.reddit.com/r/statistics/comments/12p4rft/s_jasp_is_deleting_rows_and_columns/
418,2023-04-17 09:48:57,"[Q] How to I write a function of how much longer a popular twitch streamer will stay online, based on how many hours they have already been streaming?","> ***Foreword:*** *Popular twitch streamers have bots in their chat that you can type commands such as `%online` and it will tell you how many hours that the twitch streamer has currently been streaming on any given session*

Here are my variables:

**µ** -- The mean amount of hours any popular twitch streamer will stream on any particular session

**σ** -- The standard deviation

What I need help with is the formula and fact-checking my guesswork. My guess is that the average popular twitch streamer streams 5 hours with a standard deviation of 2 hours. Does my guess for standard deviation make sense? Or should I use a much smaller or larger value such as 1 hour or 3 hours?

From what I remember from undergrad, the SD can be guessed by taking the range of which 95% of all streaming sessions occur for a popular streamer (5h plus or minus 2 SD's) giving a range of 1-9 hours using 5 for µ and 2 for σ. 

~ 

#Second question:

One of my favorite concepts from statistics/probability is ""survivor bias"". Since twitch won't show you streamers who are already done streaming for the day, this will make my data skew toward the longer tail of the bell curve, right? (a random streamer who streams for 9 hours is 9x more likely to be visible on my watch-list than a streamer who streams for only 1 hour on that particular day, right?)

This is purely for my own curiosity and I mainly want fact-checking on my mathematical reasoning, plus I want the formula if it is super easy to compile! 👍

**TL;DR:** How to I write a function of how much longer a popular twitch streamer will stay online, based on how many hours they have already been streaming?","['> Does my guess for standard deviation make sense? Or should I use a much smaller or larger value such as 1 hour or 3 hours?\n\n1. You should use data, not a guess\n\n2. You should not expect streaming durations to be anything like normally distributed, and doubly so if you\'re looking across streamers rather than within. (Within will be skew, across will be more skew)\n\n3. ""Since twitch won\'t show you streamers who are already done streaming for the day, this will make my data skew toward the longer tail of the bell curve, right"" -- it\'s NOT a bell curve but yes, if you\'re getting on at some random moment the chance you hit a given streamer will be proportional to both stream frequency and stream length (the second of which is the very thing you\'re estimating ... so yes, a naive estimate of the duration distribution will be biased). Its the same kind of bias you get if you try to estimate number of children per household by randomly sampling people and asking them how many children were in their childhood family (them + their siblings). You\'re much more likely to sample people from large families than small and you miss the households that had 0 children completely.\n\n   If it were possible to do, it\'s better to sample everyone over an interval of time, and to take account of the censoring if you\'re trying to estimate the distribution of complete duration (as with survival  analysis)\n\nDoing this stuff well is a nontrivial task', 'The distribution would be closer to poisson, no?', ""I see no particular reason to think so. \n\nUnder some simple assumptions it might be a reasonable approximation but I strongly doubt those assumptions would actually be the case.\n\nMy first thought was that a negative binomial might be a more-or-less reasonable approximation but I wouldn't consider asserting that even that more general model (it encompasses the Poisson as an edge case) was actually the case."", ""> https://nobody.live/ is a website that shows streams with no viewers. These should be more representative of truly random streams than twitch suggestions (which are heavily biased towards popular streamers, who presumably make longer streams).\n> \n> \n\nRight, that's why I specified popular streamers in the title and in the body. (preferably those who do it for a living)\n\nIf you've ever watched an unpopular streamer (with under 5 viewers), they are extremely dull & boring. 😣\n\nHowever, if I see someone playing a good old NES classic like Shadowgate or Metroid, then I'll watch them for hours, lol."", ""The Twitch algorithm might be more likely to show streams that are not expected to end soon.\n\nLet's assume we get a random streamer, we learn they are currently online, and we cannot use any information about that particular streamer except for how long they have been online. Find the stream length distribution on Twitch. This will not be a Gaussian distribution. Cut the part that's shorter than %online, then normalize the remaining distribution, and you should get a distribution for the remaining time of the stream you joined.\n\nIf we get a random *online* streamer then people with longer streams will be overrepresented and we should adjust the length distribution. That's a non-trivial task.""]",0,13,https://www.reddit.com/r/statistics/comments/12oxcs8/q_how_to_i_write_a_function_of_how_much_longer_a/
419,2023-04-17 09:37:37,[R] Trying to pick the best method for interpreting data of three groups with one value per group," For a project with 3 groups and each group has one single value over a period of time, just finding it difficult to see what the best method would be.","['How about finding the means?', ""I don't have a sample size big enough to use the means of the groups. It's the number of injections of a vaccine given over a period of time and there's 3 groups, in  three different weight classes""]",1,2,https://www.reddit.com/r/statistics/comments/12ox0yo/r_trying_to_pick_the_best_method_for_interpreting/
420,2023-04-17 06:31:46,"[Q] How to use effect size (e.g., Cohen's d) to conduct indirect treatment comparison and subsequent cost-effectiveness analysis?","Hey all,

I'm trying to conduct an indirect treatment comparison (ITC) so that I can then perform a cost-effectiveness analysis (CEA) between two treatments. I'd need to conduct an ITC because there are no head-to-head randomized controlled trials (RCTs) between my treatments of interest. There are meta-analyses that I believe are suitable for the ITC, but they are reported as effect sizes, particularly standardized mean differences (SMD), Cohen's d. My questions are:

How can I use effect size, specifically SMD or Cohen's d from 2 meta-analyses to conduct an ITC between 2 treatments?

How do I get from the ITC to CEA using effect size (SMD, Cohen's d)?

The meta-analyses use functional measures that can be converted to EQ-5D (via a mapping equation that's published), which I understand is necessary for QALY calculations and then for CEA.

Really appreciate any help with this!","[""You can use smd's weighted by one over the standard error...e.g.\n\n    sd_k, k=1,2...\n     Standardized mean difference in study k\n\n     se_k, k=1, 2, ...\n     Standard error of sd_k\n\n      theta = [ sum_{k=1}{K}  sd_k/se_k ]/ [ sum_{k=1}{K}  1/se_k ]"", ""I don't know what you mean by EQ-5D value. Please explain. And while you're at it, why not flesh out the whole thing a bit more."", ""Hey, I really appreciate your reply. \n\nThere is a meta-analysis already available for one of the treatments vs. placebo, and this study has standardized mean difference already available. It's only one value though. So Do I just divide the SMD by SE and that will give me the EQ-5D value? Sorry for asking such a basic question.. I'm still quite new to this. \n\nWould you mind providing an example or maybe a webpage that can explain this reasoning a little more in-depth? Thank you for your help!!""]",18,3,https://www.reddit.com/r/statistics/comments/12orm16/q_how_to_use_effect_size_eg_cohens_d_to_conduct/
421,2023-04-17 04:35:16,"[Q] Using Propensity Score Matching to Reduce ""Class Imbalance"" Biases?"," Suppose I have a dataset where 100 patients have the disease (e.g. information such as height, smoking, weight, age, disease status) and 10000 patients do not have the disease (i.e. class imbalance).

I am interested in using Logistic Regression to try and understand what patient characteristics appear to influence the odds of having the disease or not. As such, there are significantly more patients without the disease compared to those who do not.

I fear that fitting a Logistic Regression on the entire dataset might partly invalidate the results as patients without the disease will have more influence in the model estimates. To potentially mitigate this problem, I am thinking of using Propensity Score Matching to select 100 patients who do not have the disease - in a way such that we only select patients without the disease so that they have an ""approximate analog"" in the disease set. As a result, I will have a dataset with only 200 patients and the the ratio of disease to non-disease will be balanced.

I had the following question: By using this Propensity Score Matching approach, I will end up discarding lots of information corresponding to the non-diseased patients and a result might be forfeiting large amounts of valuable information that might be beneficial to the model. However, by including this information, I fear that I risk ""flooding"" the model with too much information corresponding to the ""non-diseased patients"" and suppressing information belonging to the diseased patients.

In general - can Propensity Score Matching be used to mitigate problems/biases associated with class imbalance when fitting regression models to such types of problems?","['I wouldn\'t bother. I think the problems of class imbalance are often exaggerated for LR. Trying to ""correct"" class imbalance can do more harm than good (e.g. [van den Goorbergh 2022](https://academic.oup.com/jamia/article/29/9/1525/6605096)). They don\'t cover a Propensity Score Matching approach, but I imagine it is fraught with possible sources of bias for your coefficients and throws out perfectly good information about the distribution of predictors in the control group.\n\nThe key things to do are:\n\n1) Let the sample size of the smaller class guide the complexity of the model. I.e. with 100 patients don\'t have 30+ predictors, even if you have 10,000 controls.\n\n2) If you care about sensitivity / specificity (i.e. you\'re not just going to use AUROC) then choose the classification threshold carefully. Either use some objective method of selecting a classification threshold that balances sens + spec (e.g. geometric mean), or just threshold at the observed prevalence (which is often not far off the former).', ""Short answer, no.\n\nA bit more detail:\n\n1). Propensity Score Matching is not typically used to balance your outcome class. It is used when you don't have a randomized experiment (or you have a poorly randomized experiment) to remove biases between your treated/non-treated groups.\n\n2). If the disease is truly rare and you do not have a selection bias then logistic regression will be robust due to it being a probabilistic prediction. \n\n3). If you are adamant in balancing your data then you could do a 2-step regression where in step one you fit the model to a data set that has the minority class oversampled resulting in robust parameter estimates for the covariates but a biased intercept. Then in step 2, you would fit the model to the original data to re-estimate the intercept coefficient."", 'Ok then you definitely shouldn’t use propensity score matching.', 'https://gking.harvard.edu/sites/scholar.harvard.edu/files/gking/files/0s.pdf', 'What are you trying to estimate?']",2,11,https://www.reddit.com/r/statistics/comments/12onzs8/q_using_propensity_score_matching_to_reduce_class/
422,2023-04-17 04:16:30,[Q] Undergraduate courses necessary to be competitive for a phd in statistics,"I posted [this](https://www.reddit.com/r/statistics/comments/11vok8u/q_what_courses_are_a_must_in_order_to_be_a/) a while ago asking about courses necessary to be competitive for a masters program in statistics. Out of curiosity, what would be the necessary courses that one would need to be competitve for a statistics phd program?","['The big one would be real analysis and I imagine most will require that as a prerequisite. Topology is also relevant for the same reasons. I tend to believe anything after topology, probability theory, math stats and real analysis is kinda buyers choice, so long as it’s upper level math. Typically people take a semester of abstract algebra, and sometimes things like numerical analysis and partial differential equations. \n\nA lot of people also have masters in stats or math and that gives them a little edge of just having completed more serious upper level math/stats that’s hard to complete as an undergrad.', 'The big three would be analysis, linear algebra, and probability theory.\n\nBy far and away, taking as much analysis as you can should be your first priority. If you can, also try and take a measure theory class (but this is not strictly necessary so don’t stress about it too much). Linear algebra should be your next biggest priority, try to take a difficult proof based linear algebra type of class (so not one of those linear algebra classes where you’re just learning how to multiply matrices…)\n\nIf you’re going really deep into stats then topology can be helpful but is certainly not necessary. \n\nIt helps to have taken other math classes like abstract algebra just so you have a better picture on what tools exist in math and for your development in constructing proofs, but in terms of subject matter itself I highly highly doubt you’re going to see very much abstract algebra/number theory type of math in a stat PhD program.']",3,2,https://www.reddit.com/r/statistics/comments/12onedq/q_undergraduate_courses_necessary_to_be/
423,2023-04-17 03:27:57,[Q] Statistical test selection,"I have data for 18 months prior to an intervention, and for 6 months after an intervention. 

I have one data point for each month and am looking for a test which can distinguish whether the intervention had a statistically significant effect on the the resulting data.

The data points are all rates (e.g., 0.45 specific infections per 10,000 patients).

Sorry for the noob question. Just looking for some thoughts/direction here, thanks!","['You could do a Pearson independent samples t-test. Dataset 1 would be pre-intervention data points and dataset 2 post-intervention data points. \n\nYour null hypothesis would be that mu1-mu2 = 0 or rather that post intervention and pre intervention data are same and the intervention doesn’t make a difference. Your alternative hypothesis would be that mu1-mu2 ≠ 0 or rather that your pre-intervention data is statistically different than your post-intervention data. \n\nIf you run the test and your p value is under 0.05, then you can claim that there was a statistically significant change in data pre infection vs post infection. BTW someone should correct me if I’m wrong']",4,1,https://www.reddit.com/r/statistics/comments/12olujk/q_statistical_test_selection/
424,2023-04-17 00:29:00,[Q] Appropriate Statistical Test to Compare Means Between Two Small and Imbalanced Samples,"I am currently working on a project where I need to test the statistical impact a binary variable has on a continuous variable, with the end goal of creating a linear regression model. However, each group has a small sample size and are different in size (n = 9 and n = 23). Due to this, is a two-sample t-test still the most appropriate test? I have confirmed that there are no outliers, the data in each group is approximately normal, and the variances are approximately equal. 

The t-test suggests that there is no significant difference between the two groups. Assuming this is the appropriate test, is there anything I can/should do to confirm that this is not a type-II error?","[""It sounds like *t*\\-test is fine for this application.  There's no way to get a better of idea of if the means in the sampled population differ except to get a larger sample.  You may want to report Cohen's *d*, hopefully with a confidence interval, to better describe the differences in the two means.\n\nBTW, there are lots of other tests to compare two samples, if the mean isn't what is actually of interest."", 'Agree that a t-test is appropriate here.\n\nAs for a type II error concern, due to the low sample sizes, you probably have low power and a high probability of a type II error. You could use a power analysis tool to find either the minimum effect size detectable with a certain desired power given your sample sizes, or the power for detecting a certain relevant effect size, given your sample sizes.', ""> Due to this, is a two-sample t-test still the most appropriate test?\n\nCan you talk about the response variable a bit please. What sort of thing does it measure / what values can it take?\n\n> Assuming this is the appropriate test, is there anything I can/should do to confirm that this is not a type-II error?\n\nIf such a thing were possible it would already be incorporated into all hypothesis testing so that we would avoid type II errors. If it was possible, *you'd know about it* because literally everyone would do it. The way to reduce type II error is to increase power, and that comes down to a good sampling design (or experimental design for an experiment), a suitable model for the conditional response and decent sample size (well, there's also the possibility of using a high type I error rate, but that's usually an unpopular choice for increasing power; there are a few situations where that makes sense though).\n\nYou will not have great power against small effects unless the variation around group means is very small. \n\nType II error is clearly a distinct possibility\n\nA t-test will likely be fine, and probably close to as good as you could do. \n\nIf you were worried about the normality assumption you could have done a permutation test but it wouldn't have been likely to change your conclusion. Similarly a generalized linear model would be possible, but it sounds like the model you have is already reasonable, so again, it would have been unlikely to change  anything much. Either way, you have already done your test so these possibilities are moot (any attempt to try something else to get significance on this issue would be p-hacking).\n\n[For next time ... I'd suggest doing a sample size calculation before you think about collecting data so you know what sample size is necessary to attain a reasonable level of power; that is the usual way people try to reduce type II error, but you don't control population effect size, so it's always going to remain a possibility. Do not attempt to compute post hoc power - i.e. using the information from your sample in a power calculation, like estimated effect size.]"", 'It’s not a good idea to conduct additional tests just because the one you did first was not significant. If there is at least a hint of an effect you could try a replication study.', ""[Welch's t test](https://en.m.wikipedia.org/wiki/Welch%27s_t-test) perhaps""]",10,7,https://www.reddit.com/r/statistics/comments/12og37n/q_appropriate_statistical_test_to_compare_means/
425,2023-04-16 19:09:50,"[Q] MPE and MAPE Inf, why ?","Hi everyone,

I guess I shouldn't be getting Inf for MPE and MAPE, am I doing something wrong ? 

[https://imgur.com/PhxMqza](https://imgur.com/PhxMqza)

This is the entire code, nothing special going on there

[https://paste.ofcode.org/33JaM3had6KnkCc89GJ3ZCn](https://paste.ofcode.org/33JaM3had6KnkCc89GJ3ZCn)

Thank you","['Because there are 0 values in return\\_PLTR\\_test, MPE and MAPE are returning infinite?\n\nMPE = (1/n) \\* SUM\\[(actual - forecast) / actual\\]  \nand  \nMAPE = (1/n) \\* SUM\\[|actual - forecast| / actual\\]\n\nso when actual = 0 it just errors out and says infinity', 'Consider using WMAPE. MAPE does not work when you have an observed values equal to 0', 'As side note --- since I was looking at the data to answer the question --- \n\nYou might plot the ""predicted"" values and the ""actual"" values.\n\n`plot(as.numeric(preds) ~ as.numeric(return_PLTR_test$PLTR.Adjusted))`', 'Thank you for your answer, so, it’s ok that it says infinity ? There are 0 values in return_PLTR_test, I can’t change that.', ""Hi, thank you for your help. I tried plotting predicted/actual values.\n\nI get this plot but I'm also not sure how to interpret it. \n\n[https://imgur.com/D0XfAOU](https://imgur.com/D0XfAOU)\n\nThank you again""]",1,8,https://www.reddit.com/r/statistics/comments/12o51cn/q_mpe_and_mape_inf_why/
426,2023-04-16 17:42:29,[Q] Calculate residency using Bayesian inference,"Hey all!

Hoping this is the correct place to post my question. I'm researching survival between two different sub-groups of Black-tailed Godwits. These groups breed on meadows in Frisia (the Netherlands) and migrate south after the breeding season. These birds are colour-coded and actively monitored (more or less) in all important locations during their life.

There are two destinations they can go, they either fly to sub-Saharan west-Africa or they stay in the Iberian Peninsula. These long lived birds choose one of these locations for life: they either go to Iberia or to Africa, but don't change that destination between years.

Since these birds are ringed with colour-codes, once a bird is seen in Africa we know they belong to a certain group. For Iberia, it is more difficult, because a lot of African birds use Iberia as a stopover/resting place. So if you see a bird in Iberia, you don't know if it will stay there or migrate further to Africa.

We know from GPS and solar-geolocator data, that for a certain week in Oktober, if a bird is seen in that week in Iberia, it will *probably* stay there for the rest of the winter. But, of course, this is still a mixture of both type of birds. Meaning: a bird seen in that specific week is probably an Iberian bird, but not a 100%.

What I would like to do, is **to ascertain what the chance is that a bird seen in that week is an ""Iberian"" bird**, with credible intervals. The data used for this will be from solar-geolocators and GPS trackers.

The difference in survival between African and Iberian wintering birds will be modeled with CJS ([Cormack-Jolly-Seber](https://www.montana.edu/rotella/documents/502/CJS.pdf)) models using Bayesian inference (using the program JAGS).

But what statistical test would I need to use to answer ""what the change is that a bird seen in that week is an ""Iberian"" bird, with credible intervals"". I'm a bit at a loss here.

This research is for my final Masters project and the idea is to publish it, that is why we need to be fairly certain we can correctly identify the wintering type and with what confidence, otherwise, my CJS model loses power.

Thanks!","[""Rather than split this into 2 models I strongly recommend building one model that estimates everything you're interested in. The probability of an Iberian bird being full time Iberian rather than a stopover will probably affect the estimates of differences in survival which is why you should take account of the uncertainty in that quantity, meaning you should have one model that estimates both things simultaneously"", ""I code my models in Stan, which is a fantastic probabilistic language. Your question would be pretty straightforward to implement and I'd be happy to assist.\n\nI'm teaching an introductory Bayesian worship soon and can let you know the date, if you're interested"", ""It's fine as long as JAGS can fit the model. Stan will typically run 10-100x as fast (in terms of effective sample size per second)"", ""> We know from GPS and solar-geolocator data, that for a certain week in Oktober, if a bird is seen in that week in Iberia, it will stay there for the rest of the winter. But, of course, this is still a mixture of both type of birds.\n\nI've read this section multiple times, and it seems like something is worded incorrectly bc to me it says that 100% of birds observed that week would be Iberian, but your entire project is trying to find out the split between birds types that week in Iberia, which suggests it can't be 100%"", 'Your research question is perfect for Bayesian inference!\n\nWith regularizing priors, the GPS data, and colour coding, you can estimate the probability of a bird seen in Iberia being an ""Iberian"" bird.\n\nA Bernoulli or binomial model (depending on data aggregation) erotic be able to directly model the probability and associated uncertainty.']",22,19,https://www.reddit.com/r/statistics/comments/12o2yo4/q_calculate_residency_using_bayesian_inference/
427,2023-04-16 02:57:14,[Q] Can someone ELI5 “contemporaneous correlation of errors” and how do I test for this in panel data?,I want to be sure that my model has contemporaneous correlation of errors before I perform PCSE regression. The Breusch Pagan Cook Weisberg test already confirmed heteroskedasticity. T > N.,[],17,0,https://www.reddit.com/r/statistics/comments/12nenoc/q_can_someone_eli5_contemporaneous_correlation_of/
428,2023-04-16 02:49:32,[R] Seeking Current/Former Grad Students for Survey on Motivations (For final dissertation chapter!),"Hey,

I am a doctoral student in public policy at Georgia Tech looking for participants for a survey on the motivations of current and former graduate students. This is for my last dissertation chapter, so participation would be greatly appreciated!

The **survey takes approximately 10 to 15 minutes** (average of 12 minutes). Before the survey, there are a few questions ensuring eligibility followed by the consent form (IRB Protocol H23126).

The survey begins with some scales measuring things related to motivation and needs satisfaction in graduate school and some associated outcome scales. After that, the survey asks for some information on your degree program and personal background. The survey finishes by asking a few questions related to social media use and the option to opt-in for a follow-up interview.

All information collected is password protected and information used for interview opting-in is additionally protected by an encrypted folder.

It goes without saying that you must be over 18 to participate, must be a current or former graduate student, and must not be located in China. You can find the survey here:

[https://gatech.co1.qualtrics.com/jfe/form/SV_et7R7JA6pN4R1v8](https://gatech.co1.qualtrics.com/jfe/form/SV_et7R7JA6pN4R1v8)

Thanks!",['/r/SampleSize/'],16,1,https://www.reddit.com/r/statistics/comments/12nef2s/r_seeking_currentformer_grad_students_for_survey/
429,2023-04-16 01:23:12,[Q] Why is my algebreic error propagation not match my covariance matrix calculation?,"Hello,

I've been attempting to understand error propagation by the old plug/chug method, and my math is wrong somewhere, but I don't quite know where.

Let's say you have a function

    x=2000
    x_error=4000
    y=2e-3
    y_error=3e-3
    f(x,y)=(x*y)/(y+1)
    #therefore
    f(x,y)=3.99

Now to calculate the error of f(x,y)

There are a number of ways to do this, the most straight forward (in terms of easy to understand) is the algebreic method:

    errorx*errory=x*y*sqrt((errorx/x)**2+(errory/y)**2)
    errorx/errory=x/y*sqrt((errorx/x)**2+(errory/y)**2)

So doing the math for that (assuming I've done it correctly), gives you

    f(x,y)=3.99
    error f(x,y)=10.74

Which surface level makes sense to me. The values themselves have high errors, so the function should have a high error. This, while tedious, I understand and can easily follow.

However, a more extended/linear algebra form of this is the covariance matrix

    [df/dx df/dy][covariance matrix][df\dx df\dy]^T

where

    df/dx=y/(y+1)
    df/dy=x/(y+1)^2
    cov(x,y)=sum(x-xavg)(y-yavg)
    #but since its a single value x=xavg so I assume cov(x,y)=0
    error f(x,y)=sqrt((df/dx)^2*error_x^2+(df/dy)^2*error_y^2)
    #plug and chug
    error f(x,y)=9.97

While close, this is not exactly the same as the other method.

    algebreic error f(x,y)=10.74
    covariance matrix error f(x,y)=9.97

Why don't these 2 values match? Did I make some assumptions wrong or is the math wrong somewhere?","[""Despite your statement that the algebraic method is simpler, I've never been exposed to it before, but the second method I use many times daily in my job....  This means I can't comment on why there are differences, but I do know that the second method is derived from a Taylor series expansion, but only retains the first order terms, so it's an approximation.  If you look into variations that retain higher order terms the answer may get closer to the algebraic method.  You can also look into using an unscented transform to approach the problem."", ""So are you saying if feasible, the algebreic method is always more suitable/accurate? From my understanding the Taylor series method should be quite accurate, even if it's just an approximation (especially for something like this)""]",5,2,https://www.reddit.com/r/statistics/comments/12nbrwi/q_why_is_my_algebreic_error_propagation_not_match/
430,2023-04-15 23:38:30,[Q] Randomized Test v. Nonrandomized Test,"I'm a bit confused about what a randomized test is. It seems that most of the tests that are taught in an elementary mathematical statistics course are non-randomized. Are these classified as non-randomized because once the data are observed, it is no longer random whether the sample is in the rejection region or not?

What is an example of a randomized test?

&#x200B;

Edit: Sorry, this is a bit ambiguous. To clarify, the definition I was given was:

A non-randomized test is a test where the probability of rejecting the null is either 1 or 0. Otherwise it is a randomized test.

Additional Edit: To add some context, I am in an ""elementary"" Mathematical Statistics course in a PhD program. I put ""elementary"" in quotes because I'm finding it very difficult.

&#x200B;

Final Edit:

The way we talk about statistical tests is strange.

So a non-randomized test is ""random"" in the respect that before we have observed a sample, we do not know with certainty what the statistical decision will be with a given critical region. However, once the sample is observed, the decision is dictated by the critical region with probability 1.

The ""non-randomed"" refers to after we have observed the sample. If we do not know with certainty whether the observed sample point is in the rejection region, then this must be because some other randomness was introduced after having observed the sample.

The statistical tests we use in most(all?) applications do not introduce additional randomness after the sample has been observed.","['Randomized tests like you are referring to pop up in discrete data settings. When data are discrete the test statistic will generally also be discrete and you often cannot find a test with an exact significance level corresponding to any particular alpha.\n\nIn this case you can supplement your data with a single draw from a continuous uniform distribution, and this provides the flexibility to satisfy an exact alpha level. It makes the test random though as you will get different p-values with the same dataset (as the supplemental uniform data point changes every time).', 'The definition is correct but not especially enlightening if you\'re not familiar with the idea.\n\nA randomized test doesn\'t just compare a test statistic with a fixed rejection region; that would be non-randomized; every time you had the same statistic the decision would be the same (you either reject for sure or you don\'t reject for sure).\n\nWith a randomized test there\'s (at least for some values of the test statistic) a chance of rejecting that\'s not 0 or 1.\n\nWhy would you do this? When you have a discrete test statistic; it allows you to attain an exact chosen alpha rather than being stuck with a few fixed significance levels. \n\nEven if you\'re not inclined to randomize a test in practice (and typically you wouldn\'t, though there can be situations where you might just consider it), they\'re extremely useful in the theory of hypothesis testing, and they do also have some practical uses (such as when comparing power of two different discrete tests). \n\nFor example in the latter situation,  the book *Discovering Statistics using SPSS*  by Andy Field (I forget the edition right now but I can try to find it again), he says that the Kolmogorov-Smirnov test should be used instead of the Mann-Whitney for small sample sizes:\n\n>  Kolmogorov-Smirnov Z: In Chapter 5 we met a Kolmogorov–Smirnov test that tested whether a sample was from a normally distributed population. This is a different test! In fact, it tests whether two groups have been drawn from the same population (regardless of what that population may be). In effect, this means it does much the same as the Mann–Whitney test! However, this test tends to have better power than the Mann–Whitney test when sample sizes are less than about 25 per group, and so is worth selecting if that’s the case.\n\nNo evidence is offered in the book for this claim, so how should we check it? \n\nIf we just directly compare power functions at some set of sample sizes, we\'ll give an advantage to one test or the other (whichever has the higher actual significance level); if we\'re very lucky we might be able to find a few values of n1 and n2 near some reasonable significance level where the attainable significance levels are very close to each other, and where the advantage is small enough to ignore (because the difference in power is substantial by comparison). \n\nClearly we don\'t want to rely on that working out the way way want. It\'s better if we can make both tests have the same exact significance level to compare power at any sample sizes we like, and the way to do that is to randomize between the significance levels either side of alpha (say for alpha = 5% given that\'s almost certainly the significance level the test would be done at by the people reading the book, and by the author).\n\n[... The claim in that book is quite false, by the way, but the claim provides a useful context for wanting to use randomized tests.]\n\nSo for example if at n1=4 and n2=5, our Mann-Whitney has available significance levels of 3.1746% and 6.3492% near 5%. In that case we\'d want to reject all the test statistics that a non-randomized test would reject at the 3.1746% level and *some fraction* of the cases at the 6.3492% level in order to make the overall rejection rate under H0 equal to 5%.\n\nIf we reject 0% of the additional cases we\'d get alpha = 0.031746\n\nIf we reject 100% of them we\'d get alpha = 0.063492\n\nIf we reject a fraction r of them, we get alpha = 0.031746 + r(0.063492-0.031746), so if we want that to be 0.05 we can solve 0.05 = 0.031746 + r(0.063492-0.031746) to find r:\n\nr = (0.05 - 0.031746)/(0.063492-0.031746) = 0.5750\n\nor about 57.5%. That is, when the test statistic is  in the rejection region when alpha = 0.063492 but *not* in the rejection region when alpha = 0.031746, you generate a random Bernoulli with p=0.575 and if it\'s a ""1"", you reject. Outside that ""on the boundary"" case, you act as you would for the non-randomized test. This new random rejection rule gives alpha = 0.05. We could then do the same kind of calculation for the two-sample Kolmogorov-Smirnov test and actually compare power functions for that n1, n2 combination by giving them the same significance level. The same trick could be used for a whole collection of other sample sizes in order to investigate the  general claim that the Kolmogorov-Smirnov has more power when n is less than 25 (and see that it\'s not true at all; given that (a) it\'s easy to check and (b) pretty clear it would be false a priori, how that came to be in the book is a mystery).\n\n[As it happens, when I addressed a question on this, rather than explain randomized tests, I instead chose a particular case where the two tests had a very similar exact alpha and it turned out that the effect was quite strong, enough to act as a clear counterexample to the claim (the MW was considerably more powerful than the KS on situation where you\'d reasonably use MW for n>25). Nevertheless, a full accounting of the advantage of Mann-Whitney over two sample Kolmogorov-Smirnov would really need use of many combinations of small sample sizes and hence randomized tests]', ""Well.  Randomized tests are an important part of Neyman-Pearson theory.  Without them UMP and UMPU tests for discrete distributions do not exist.  But AFAIK no one uses them in applications.  It's just theory.  The cited paper does try to make them suitable for applications (by not actually doing the randomization, just describing it, so decisions, confidence intervals, and P-values become *abstractly* random (rather than *realized* random))."", 'Basic statistical tests are just tests. They aren’t inherently random. They usually work best when applied to data that has been selected at random. That is, randomization typically refers to the data selection process, while the statistical test is a mathematical formula/algorithm designed to be applied to some kind of data. They’re separate concepts. \n\nThere are more advanced methods that rely on taking random samples of the observed data to compute plausible intervals for things like mean values. But that is about using randomization in the algorithm to compute certain statistics. There’s still a separate concept about whether the observed data is a random sample of some sort.', 'Do you mean [randomized controlled trial](https://en.wikipedia.org/wiki/Randomized_controlled_trial) (RCT) or randomization test (for which there is no good web reference), which is one way (but far from the only way) to analyze an RCT.  The latter is a permutation test for which the assumptions are justified by the randomization in the RCT (the permutations mimic the randomization).\n\nOr if you mean a randomized test like they teach in PhD level math stats and no one uses, see [Geyer and Meeden (2005)](https://doi.org/10.1214/088342305000000340).']",8,11,https://www.reddit.com/r/statistics/comments/12n7x8z/q_randomized_test_v_nonrandomized_test/
431,2023-04-15 21:04:31,[Q] newbie: what is the proper way of calculating odds ratio?,"2 people who are blue are tall. 3 people who are who are blue are short.


8 people who are red are tall. 2 people who are red are short.


How much more likelier are you to be tall if you are red? 


Is it: 8/(8+2) / 2/(2+3) = .8 / .4 = 2X more likely?

Or is it: 8/2 / 2/3 = 4 / .66 = 6X more likely?","['If the question is specifically odds ratio given red vs. blue, then the correct calculation is the second one;  6x higher odds ratio.\n\nThe first calculation is called the Relative Risk or Risk Ratio.\n\nIf you need more direct formulas/math respond.', 'If there is a 50% chance of someone being tall, well the chance would be 50% or 0.5. For odds it would be 1/1. Equal chance of both of them. You want the ratio between two odds.', 'Edit: **TL;DR: the second calculation is the correct way to calculate the odds ratio. But be careful about the wording/interpretation of what the odds ratio means.**\n\n>\tHow much more likelier are you to be tall if you are red?\n\nBased on the wording of this question and the examples, it sounds to me like you’re more interested in relative risk than odds ratio, but please correct me if I’m misunderstanding.\n\nFirst the basics, which you already have:\n\n-\tProbability of being tall given you are red: 8/(8+2) = 0.8\n-\tProbability of being tall given you are blue: 2/(2+3) = 0.4\n-\tOdds of being tall given you are red: 0.8 / (1-0.8) = 4\n-\tOdds of being tall given you are blue: 0.4 / (1-0.4) = 0.67\n\nThere are two distinct quantities being touched on that we can use to compare these probabilities:\n\n1.\tRelative risk: how many times likelier are you to be tall if you are red vs. if you are blue? In this case, 0.8/0.4 = 2 times likelier, so relative risk is 2. This one is more intuitive imo, it’s simply the ratio of the conditional probabilities.\n2.\tOdds ratio: this one is more complicated. Similar calculation to relative risk, but as the name suggests, we use the odds instead. In this case, 4 / 0.67 = 6.\n\nThese are different quantities with different meanings and interpretations. Anecdotally, I find that people are often more interested in relative risk, but odds ratios are used a lot in practice in part because the coefficients of logistic regression models encode changes in odds ratio.']",4,3,https://www.reddit.com/r/statistics/comments/12n1r21/q_newbie_what_is_the_proper_way_of_calculating/
432,2023-04-15 10:17:14,[Q] Help with setting up variables for word-based analysis,"Hello! It's been a long time since I've looked at a statistics program but I'm trying to get back into it to help me optimize an algorithm.  


Here's my use case:   


I need to analyze words and phrases based on my ability to complete an action using them. I'd like to then be able to determine things such as the likelihood of being able to complete an action based on the letters within a word, the number of vowels, and the length of the word/phrase.  


For a concrete example, let's say I have:  


addfueltothefire - not possible to complete action  
aerosolcontainer - possible to complete action  


Of course, I have a bunch of these to input. My thinking tells me to make each letter of the alphabet a variable and use it to add the number of that letter that appears, then a variable that is the word/phrase length, a variable that is the number of vowels, and then a variable that is whether I was able to complete the action with that word or phrase. And then run ANOVA tests? I think?

My end goal is to be able to determine the likelihood of being able to complete my action based on any given word/phrase.

Am I going about this the right way or am I way off course? Any guidance would be much appreciated!","['Weka will let you run a multinomial naive bayes on text where your actions are the class it’s trying to predict based on the input words.  It will then give you the probability that given some input text that it belongs to that class.  I’ve used it to scan text from a drug prescription transaction to predict if the transaction will be rejected.', 'It would help know more about what you’re trying to do.  Are you trying to do something like a boggle solver where given some letters you see what words it makes?  Or perhaps trying to find the edit distance between two words?', 'Fair enough! I make anagramatical puzzles where two parts of a puzzle are anagrams of each other. One part results in a word or phrase. When i build these puzzles, I start with the word or phrase to see if I can make the second part (a crossword-style grid). I have an algorithm that brute forces solutions by trying every possible combination, but I\'d love the ability to be able to tell the likelihood of being able to make a puzzle from any given word or phrase, instead of sitting and waiting for the algorithm to go through every possible combination.  \n\nMy end goal is feeding it a word or phrase, and it returns an estimated chance of a solution being possible based on the letters the word or phrase has. For example, say the phase has four ""j""s, then I already know a solution is highly unlikely, whereas a phrase with around half vowels with some ""s""s, ""t""s, and ""n""s has a good chance of finding a solution.', 'Say you have the phrase bad credit and want to quickly check to see if you can turn that into debit card. Normally, this would be done in factorial time complexity or O(n!) which would would be 9! different permutations or 362,880\n\nHowever, there is a simple way to check the if the phrases are anagrams that would only  be O(n) in complexity, where n is the length of the string\n\nTake the ascii value of each character in each string and add it up and see if it equals:\n\nThe sum of ASCII values of the letters in the phrase ""debit card"" is:\n\n100 (for \'d\') + 101 (for \'e\') + 98 (for \'b\') + 105 (for \'i\') + 116 (for \'t\') + 32 (for \' \') + 99 (for \'c\') + 97 (for \'a\') + 114 (for \'r\') + 100 (for \'d\') = 963\n\nThe sum of ASCII values of the letters in the phrase ""bad credit"" is:\n\n98 (for \'b\') + 97 (for \'a\') + 100 (for \'d\') + 32 (for \' \') + 99 (for \'c\') + 114 (for \'r\') + 101 (for \'e\') + 100 (for \'d\') + 105 (for \'i\') + 116 (for \'t\') = 963\n\nIf the phrases are equal in total ascii value it\'s highly probable they are anagrams.  \n\n\nThere could be exceptions to this or phrases where the ascii values sum up but are not anagrams but I think overall this approach could pan out.', 'Another way to think about this approach is if two phrases are anagrams, they will have the same sum of ASCII values. So if you had a 9-character phrase and wanted to check 100 different 9-character phrases looking for anagrams, you are only doing 100 ASCII sum operations or 900 Character lookups plus 100 sums / comparisons = 1000 total operations versus 362,880 \\* 100 = 3,628,800 brute force combinations.  That ASCII approach is going to run in seconds.  Once an ASCII match is made then creating a vector with entries marking the frequency of each letter and seeing if the sum of frequencies matches up per letter would then be a more definitive check.']",6,6,https://www.reddit.com/r/statistics/comments/12mmg9m/q_help_with_setting_up_variables_for_wordbased/
433,2023-04-15 09:49:36,[Q] trying to figure out the statistical test to use for my experiment,"hi! i am an undergraduate researcher who studies how early life stress and the estrous cycle impact the incubation of cocaine craving in female rats. i have three independent variables: day (1+30), condition (ELS+control), and estrous phase (estrus+nonestrous). i was originally going to run a three way anova but the day variable is repeated measures and estrous phase depends on the day. im not entirely sure if im articulating this correctly but basically estrous phase is an independent variable that depends on a different independent variable? i run all my statistics on prism if that matters","['If day is a predictor of both estrous phase and cocaine craving (or whatever your main outcome is), then including both day and estrous cycle as predictors in a model would probably yield spurious results because of the presence of a collider variable.\n\nI would suggest fitting 2 mixed models. One with day and condition as the predictors, and one with estrous phase and condition as the predictors. You can then compare the results.\n\nIn biology terms, run 2 two-way repeated measure anovas.', 'Agreed with this. You need to eliminate confounding variables in the form of your day and estrous cycle being measurements of the same thing. The interaction between the two variables will be very high in your ANOVA.', 'Would it be feasible to replace *day* with *cycle\\_count*, tracking the aggregate number of estrous cycles that have occurred since the experiment began?\n\nThat will result in *estrous phase* and *~~day~~* *cycle\\_count* being independent of each other, but still allow you to track time, albeit with lower precision. It might be problematic if the experiment itself affects the cycle length.', 'thank you so much!', 'i will definitely consider this, thank you!']",1,6,https://www.reddit.com/r/statistics/comments/12mlq9o/q_trying_to_figure_out_the_statistical_test_to/
434,2023-04-15 07:27:14,What Statistical Analysis Test for Salmon Emigration Over Time? [Q],"Dear All,

I am new to statistics, and have to complete a project over Pink Salmon Fry populations over time. 

I am unsure as to how I should proceed. 

The data is very numerous, spanning around 1100 pieces, over 10 years. This is not an even time distribution, and the data was taken from between march and may (roughly, since dates changed each year slightly).

Pink salmon emigrate from their home stream immediately upon hatching, and then return in 2 years, thus causing two distinct populations to form, this is split into two different population groups, 1 and 2. 

however, I am trying to assess the trend in populations over time, to see if the average peak dates, tail ends of spawning (season time), and population totals have changed over the past 10 years. 

This is the point where I am stuck. I have all the data, laid it into tables for each year, and made basic scatter plots of each year. 

I have done the same for the total amounts of fry per year, for each population.

I was wondering what Statistics tests I would use for this type of data? 

in my class, the instructor stated that it would be good to use the Shapiro-Wilk test to find normalicy, then use the breusch-pagan test for homoscedasticity. 

however, my data cannot be applied to the shapiro-wilk test, nor its larger sample size counterpart (the royson), as Fry count is discreet.

what is your recommended analysis, or breakup of my data? 

is my breakdown of data into different groups good (into the 2 population groups)? should I use a different approach, such as doing overall group changes? i've broken it down into both overall trend in population as well as the graphs of each year's counts over time. the latter is a bell-shaped graph, but I am unsure on how I would proceed with it.

I may do a graph on date changes for the peaks, but since the peaks can be outliers and off of the ""true peak"" that one would see in a normal distribution, I am unsure on how I would do it.

If needed, I can post images of the graphs I have laid out. This is my first ever statistical analysis project, so I'm struggling quite a bit.","['You have a time series, which could probably be analyzed a number of ways depending on what your specific question is. \n\nYou need to ask a question of the data first, then the statistical approach to answer that question is decided afterwards.\n\nSo, what is the question?', 'the question would be\n\n""how has this population changed over the time period?""\n\nthis incorporates several smaller questions, such as \n\n""has the average amount changed,"" \n\n""has the range of time changed,"" and\n\n""has the ranges in which the maximum occurs changed?""\n\n&#x200B;\n\nfor the maximum, I was wondering how to do that specifically. since it\'s got some peaks outside of the normal-ish range (a scatterplot of count vs time has it being a very close shape to a bell). in counting those peaks, it\'d be very hard to truly estimate when the maximum is likely to occur, since it\'s only for a single day out of the season. I was wondering if this could be solved by instead trying to find the range in which the maximum occurs? such as using the top 10% and their dates instead?', 'Model each year as a logistic function (with time as an X and % of total annual observations in a given year as the Y), where you accumulate observations over time in each year. Then the inflection point of that curve would most likely be the best estimator of ‘peak’ activity. You can also then play with elements like synchronicity, etc.']",6,3,https://www.reddit.com/r/statistics/comments/12mhru3/what_statistical_analysis_test_for_salmon/
435,2023-04-15 06:37:42,[D] How to concisely state Central Limit theorem?,"Every time I think about it, it's always a mouthful. Here's my current best take at it:

> If we have a process that produces independent and identically distributed values, and if we repeatedly sample n values, say 50, and take the average of those samples, then those averages will form a normal distribution.

> In practice what that means is that even if we don't know the underlying distribution, we can not only find the mean, but also develop a 95% confidence interval around that mean.

Adding the ""in practice"" part has helped me to remember it, but I wonder if there are more concise or otherwise better ways of stating it?","['Many samples, mean approaches normal.\n\nI think that is the most concise way to say it that is mostly correct. Because why use many word when few word do trick.', 'This is very concise', 'the underlying distribution needs finite first and second moments', 'What you stated there is not any of the CLTs, nor is it true.\n\nIf you care to actually state a CLT you must choose a specific one, give the conditions under which it applies (i.e. what was assumed in the proof) and then state what was proved.\n\nIt looks like the classical CLT comes closest to what you\'re after.\n\nBut be warned.\n\nIt says nothing whatever about any finite sample size. If you want to make a claim about n=50 or any other finite sample size, you\'ll either need another theorem or some other evidence for your claim, either sufficient to count as a proof ... or you\'d need to add enough disclaimers to differentiate what you did show from what you didn\'t. If you want to make a fairly vague  point about what usually tends to happen to the sampling distribution of  sample means for finite sample sizes, ... that\'s not the Central Limit Theorem.\n\nWhat you state there in the first paragraph is demonstrably false. (The second paragraph is explanation of an implication of it - were it true - not a statement of a theorem.)\n\n\n\n----\n\nEdit (was on phone before, had to wait until I could get to a decent keyboard for this bit): \n\nLet\'s attempt to translate an actual CLT into words before we worry about trying to make it any more concise. We\'ll take the classical CLT in mean form. \n\nAn actual statement of that CLT is more or less like this:\n\n> Let Y₁, Y₂, ..., be an infinite sequence of  independent, identically distributed random variables, *with mean μ and  (finite) variance σ^(2)*. Let Zₙ = (Ȳ-μ)/(σ/√n), with cdf Fₙ. Then in the limit as n→∞, Fₙ→Φ (the cdf of a standard normal).\n\n(by comparison, see https://en.wikipedia.org/wiki/Central_limit_theorem#Classical_CLT and its reference, Billingsley... I stated it somewhat differently, because I wanted something that (a) translated into words easier and (b) was easier to relate to what people try to talk about when they mention the CLT, but otherwise it should be equivalent)\n\nAttempt to translate the above into brief words without doing too much violence to it: \n\n> As the sample size increases to infinity, the distribution of a standardized sample mean of independent, identically distributed random variables *with finite variance* converges to a standard normal distribution.\n\nThat\'s pretty concise and not so far from precise. \n\n[There are other CLTs that relax the identically distributed assumption or the independence assumption.]\n\nAttempt to say something with practical implications that\'s not actually false (but isn\'t the CLT):\n\n> As sample sizes become sufficiently\\* large, the distribution of sample means (sampled under conditions that produce independent, identically distributed values) should be close to normal, provided the population variance is finite.\n\nThis is essentially true, but not much use on its own because we can\'t pin down ""sufficiently"" nor ""close"" from the CLT. \n\nWhat does the actual theorem from earlier tell us about sample means at n=50? Or n=30? or n=127? Of itself, nothing. Literally nothing. \n\nHere\'s an example *where the above stated CLT definitely holds*\n\nhttps://i.stack.imgur.com/741J0.png\n\nThis is a histogram of simulations from the **log** of the sum of a sample of  50,000 observations (the histogram of the log of the means would look exactly the same, only the numbers on the x-axis would change). If this histogram had looked normal, it would indicate that the collection means had approximately a *lognormal* distribution, which would itself be distinctly right skew given the standard deviation on the log-scale. But even its log is too right skew for that. So means of 50,000 values can still show strong - even extreme - right skew (much more right skew than a corresponding lognormal), even when the CLT holds perfectly. Even sample sizes in the low millions are not sufficient for this example. This is not some isolated case; a plethora of other examples are perfectly possible.\n\nWhat would it take to say something about what ""sufficiently"", ""close to normal"" mean and relate it to sample size? Another theorem. \n\nThe Berry-Esseen inequality does say something along these lines, but it relies on knowing something about the third absolute moment of the parent distribution to say something concrete about *how close* we get. In a practical sense, unless you restrict the distribution *shape* in some particular sense, you aren\'t going to be able to guarantee a useful practical outcome (you can\'t make a useful general statement without doing that). \n\nWhen will you know with a high degree of confidence conditions on the shape are met when you don\'t know the population distribution? Sometimes, perhaps, but not all that often.\n\nMuch later edit: even were we in a situation were we do have an explicit bound on |Fₙ − Φ|, at finite n, that doesn\'t grant us every property of a normal distribution. (a) What we do get: If we\'re looking up z-tables to get a tail probability, it tells us how large the absolute error might be, which is potentially useful in those situations (though in many cases a relative error would be more like what we want practically, so we can say something like \'with at least two figure accuracy\'). (b) An example of what we don\'t get: consider that for any finite bound on the absolute difference in cdf, |F-G|<ε there\'s a distribution F (fairly easily constructed) which has infinite variance but is within ε of G. So if you\'re relying on F *behaving like* G in a broad sense beyond the cdf-distance, such finite-sample bounds may not generally be much use.', ""this feels unnecessarily pedantic. There are plenty of quantitative versions of the CLT which give finite sample results, and don't require scaling; ie. slightly more formal versions of\n\nP(\\\\bar{X} > x) = P(Z > x) + o(n\\^{-1/2})\n\nwhere \\\\bar{X} is the sample mean of n iid obs with finite variance and Z is a gaussian RV with mean and variance that matches \\\\bar{X}, and x is any fixed real number; and for distributions with finite absolute 3rd moment we can bound that error term explicitly (eg. an unscaled version of berry-esseen).\n\nThat said, I do agree that most of what people wrote is not correct in important ways. In addition, it is amazing to me the number of people who don't understand how the LLN and CLT engage/compare (and why the most common statements have that \\\\sqrt{n} pre-factor... or alternatively, why my statement above needs to have an error of o(n\\^{-1/2}) rather than O(n\\^{-1/2}) in order to be meaningful outside of just restating a quantitative LLN in a very clunky way).\n\nps. I was not one of the downvotes. That said I do think your comment was poorly received because it isn't particularly helpful for someone trying to build intuition. Obviously the original poster could have looked up a formal definition of the classical CLT, and that is clearly not what they were looking for. (edit --- it looks like you made your response more cordial and useful since I posted)""]",66,61,https://www.reddit.com/r/statistics/comments/12mg9yy/d_how_to_concisely_state_central_limit_theorem/
436,2023-04-15 06:28:27,[Q] Do I use FGLS or PCSE? I don’t want to go the RE/FE robust standard error route.,I am working on a project where I am evaluating 8 companies over 10 years. There appears to be no serial autocorrelation as prob > F = 0.92 on the Wooldridge test for autocorrelation. Breusch-Pagan / Cook-Weinberg test for heteroskedasticity shows prob > chi2 = 0.0000. Which of the above is most appropriate?,"['Why don’t you want to use FEs', 'Do you have within unit variation in your DV?', 'The output did not display Wald > chi2 results. The space was blank.', 'If I understand correctly, are you referring to the unit root test?']",4,4,https://www.reddit.com/r/statistics/comments/12mfzyy/q_do_i_use_fgls_or_pcse_i_dont_want_to_go_the/
437,2023-04-15 05:31:40,[Q] What is the Power Function?,"Is the power function just the probability of rejecting the null hypothesis for a value of the true parameter?

I'm not sure why I struggle so much with this one. I think I get confused marrying this idea with the type I and type II errors. I want to decompose the power function into

*P( Reject Ho | Ho )* ***1****(Ho) + P( Reject Ho | Ha )* ***1****(Ha)* 

where ***1****(x) = 1* if condition *x* is true and 0 otherwise.","['There\'s nothing to ""decompose"". For a given value of the parameter, either Ho is false or it\'s true.\n\nStrictly, power is only when its false, though typically the type I error rate is included in the function to close the hole.\n\nSo power is  P( Reject Ho | Ha ). Focus on that. If needed, worry about the Ho case once you\'ve sorted that section of the curve (for an equality null that will generally be all but one point).', '> The fact that the power is equal to the type I error rate when theta = theta_0 (the null value for a simple Ho).\n\nIt\'s not strictly speaking ""the power""; power is still what it always was. You can however, define your \'power function\' to include it, but what you really have then is the rejection-rate function across the whole parameter space, but it gets called the power function (and in most cases, that\'s almost entirely accurate, since you\'re just filling in the missing point).\n\nIt\'s basically just a common (if slightly misleading) terminology. Don\'t sweat over that detail so much, there\'s lots of slightly odd terminology in stats.', "">Is the power function just the probability of rejecting the null hypothesis for a value of the true parameter?\n\nYes.  Your decomposition corresponds to the whole lower row of this table\nhttps://en.m.wikipedia.org/wiki/Type_I_and_type_II_errors#Table_of_error_types\nwhen power actually corresponds only to the lower right cell.\n\nTo get the your decomposition to resolve to a single unconditional probability of getting a significant result from your test (instead of a branching path, as it does), you'd have to apply a prior probability distribution to the whole range of possible parameter values."", ""Prior to taking a mathematical statistics class I always considered the power to be the probability of correctly rejecting Ho. However, the more general power function is usually what I am required to find these days, which includes the type I error rate. I think that's the part that gives me a headache. The fact that the power is equal to the type I error rate when theta = theta\\_0 (the null value for a simple Ho).""]",3,4,https://www.reddit.com/r/statistics/comments/12me86z/q_what_is_the_power_function/
438,2023-04-15 03:43:52,[S] Beyond 20/20 Data Browser Alternatives,"Hello, this is a rudimentary question about data browsing software, and based on a Google/Reddit search, this sub seemed the best place to ask this question.

In Canada, we use a data browsing software called Beyond 20/20 quite regularly, as this was the default program that Statistics Canada provided data for when looking for compiled data beyond CSV Excel files.

Its functionality is mirrored the most by Excel pivot tables. It looks similar, and provides similar functions, except that Beyond 20/20 is far more intuitive to use, and the data usually pre-built by Stats Can.

I was wondering if anybody might be familiar with software that can most closely mimic this functionality, something that does the same things that an Excel pivot table would do, being able to swap different dimensions out or sort data. I've been tasked to find such software, as Beyond 20/20 may not be an option for the future for our team possibly.

I've considered SAS EG, Stata, EViews, Power BI/Tableau, and IBM Cognos Powerplay so far, with Powerplay being the closest, but we need a software that's easier to build for than Powerplay. If anybody has any suggestions, will greatly appreciate it, thanks so much.

Some links for further info on Beyond 20/20,

[Professional Browser | Crime Insight by Beyond 20/20 (beyond2020.com)](https://www.beyond2020.com/professional-browser/)

[Beyond 20/20 Professional Browser (statcan.gc.ca)](https://www.statcan.gc.ca/en/public/beyond20-20)",[],1,0,https://www.reddit.com/r/statistics/comments/12maqqq/s_beyond_2020_data_browser_alternatives/
439,2023-04-15 03:10:10,[Question] Wordle Tie Breaker,I play Wordle with my two sisters daily with the winner getting the correct word with the fewest guesses.  Often there is a tie.  I've suggested that the tie breaker goes to the person who got less correct (less clues) in their first guess.  As their subsequent guessing is more difficult than those who got a correct letter.  My one sister suggests that it should go to the person who guessed the final correct answer with the fewest clues at that stage as that is the harder and more difficult route. We all play the hard mode.  Who is correct?  Which version is statistically more difficult?,"['I\'m inclined more towards the former since it seems like it\'d make sense to reward clever guessing past the first word to try and maximize information gained. Neither one is gonna be perfect since not all clues are equal (a yellow Q might be worth ""more"" than a green E for example).', 'Why not use the Wordlebot, which assigns you a skill score and a luck score?']",2,2,https://www.reddit.com/r/statistics/comments/12m9duj/question_wordle_tie_breaker/
440,2023-04-15 01:59:18,How do departments view candidates who got in via waitlist? [Q],"This years admissions cycle was a complete wash for me when it came to phd programs in statistics. I applied to 13 PhD programs, and three MS programs. I had received 12/13 rejections, and got acceptances to three MS programs.

The two Ms programs (Columbia, Michigan) offered me no financial aid. The MS program at Miami (oh) provided me full funding for an MS in statistics. I accepted the offer two weeks ago, and visited the campus three weeks ago. For a long time they were my only good acceptance and I just decided I would go there because that’s all I had been offered. I had mentally prepared to go to Miami, and even signed a lease for a place in Oxford Ohio. 

A few days ago I was told by an admissions committee member from a phd program I applied to that i had gotten off the waitlist and was offered admission into the program. At this point I don’t even know what to do now, cause I have already mentally committed to this MS program for the last 3-4 weeks now when I thought admissions were pretty much over, yet now I get an acceptance so late.

However, I wanted to ask you all if it’s worth going to a school which I was originally waitlisted for. In my mind I feel it means that I wasn’t really the departments first choice, and they are kinda just “settling” for me. Ie. The fact that I’m a “backup” candidate feels weird, and they might already have low expectations of me coming into the program since they know I was off the waitlist. 

Am I over thinking this? What do you guys thinks? Is going to a phd program where I was waitlisted originally a bad idea?","['assuming the acceptance is funded, I would strongly consider the PhD program. If you go the MS route, you will be re-applying in two years all over again (and moving, etc.). Not the end of the world but life is simpler that way. \n\nOTOH Miami is a solid school overall and Oxford is a nice little town.', ""Sometimes the best advice is the hardest to follow, but: try as much as possible to take the emotion out of your decision. Focus on your career goals and how best to achieve them. Specifically:\n\n- Don't decide based on mental commitments that you've made over the past 3-4 weeks. I can tell that you're experiencing whiplash and feeling a bit jerked around, but those 3-4 weeks are nothing on comparison to the multi-year commitment of graduate school.\n\n- Don't let the fact that you were waitlisted bother you. Most PhD programs get many, many more qualified applicants than they can accept, and they have a very limited amount of information on which to base a decision. If you are in a personal relationship with somebody who feels like they settled for you then it could affect how well they value and treat you in the future, but in graduate school nobody will know or care how you got into the program.\n\nThe fact that you applied for so many PhD programs suggests that you have judged getting a PhD to be the best way to pursue your career goals. If that is still true, then accept the offer. If your goals have changed, or if you have a concrete reason why this specific program won't help you achieve them, then consider the MS program."", ""Well like, how do you know you'll be interested in Finance continuously for the next 5 years of a doctorate. I didn't know what my research interests were when I got into my PhD (none of my friends did either), and even when I picked the area and advisor, what I've been working on has changed like 3 times as I've gotten a better understanding of the literature.\n\nReading your comments, I think you should do the doctorate. If you get your master's a lot of PhD programs will make you redo all of their core classes for necessary credits. So you run the risk of this doctorate taking 7 instead of 5 years which you should not do under any circumstances and at best, it really limits yourself to departments that let you transfer classes.\n\nNobody cares if you were waitlisted, no one cares if you are uncertain about what you are interested in, what you have to do is make the best decision you can given the information you have access to. It seems like you are dead set on doing a PhD, so go do the thing that get's you a PhD, not an MS."", ""It's unlikely that anybody at the department knows that you were on the wait list, and anybody that knows it now is likely to forget soon. It's important to you but other people are not really that focused on it.\n\nMy advice is to ignore the wait list issue and make the choice according to any other factors."", 'Part of the reason I’m hesitant is cause my interests with regards to what I want to research are literally all over the place. I have “phases” of time when I get interested in a specific subtopic or research area and then nothing sticks. Around the time I applied I felt like was interested in biomedical imaging and thus the PhD program I got into has lots of people doing neural data analysis (neuroscience) related stuff. But now that was 4-5 months ago and fast forward to today I’m interested in time series and quant finance related topics and have been reading about options and derivative pricing theory. \n\nPart of the reason I’m wondering if I should go to the MS first at Miami is so I can get a more narrow vision for what it is I want to research. I mean if this quantitative finance interest persists by the time I graduate with MS I would have probably applied to econometrics/OR focused phd programs. This has also been on my mind.']",1,22,https://www.reddit.com/r/statistics/comments/12m63mn/how_do_departments_view_candidates_who_got_in_via/
441,2023-04-15 01:44:05,[question] I can’t remember the name of a property,"Hi all, so there a property in statistics where if you do for example the following hypothesis testing:

“Does the name of person affect their height” (which is obliviously nonsense)

using 3 names only eg John, jack and James and fail to reject the null hypothesis then you can generalise for all the names. 

What’s the name of that property?","[""I have no idea what you're getting at here, and even your example seems unclear -- I bet Jill, Joan, and Jeanne are indeed shorter on average than John, Jack, and James (to say nothing of Juan, Jamal, or Ji-hu).  Are you asking if there's something special about a sample size of 3 in particular?  It's generally not going to afford you a statistically-reliable conclusion about anything.  You'd at least want multiple examples of each of Johns, Jacks, etc to say *statistically* if there's a difference between guys with those names."", '> What’s the name of that property?\n\nSounds like ""poor reasoning"" is a fair description.\n\nYou have two big flaws there --\n\nThe first is you didn\'t provide a basis to generalize (you\'d at least need to be relying on some form of random sampling of the population of names to make arguments like that, but it\'s not going to be much of an argument unless you sample a large fraction of ""names""; you had neither random sampling of names nor did you look at many names; maybe you just missed the names where the effect was obvious)\n\nThe second flaw is absence of evidence of an effect  doesn\'t establish absence of the effect even in the names you *did* sample. It just means your sample size was too small to find it.', 'I am a bit confused about the question but, \nAre you talking about that in frequentist statistics \n\na) one refers to the underlying sampling distribution, and such the inference that one can make is only available in terms of a long run frequency? \n\nb) The p-value only provides evidence on rejecting the Null hypothesis and not any statements (evidence) w.r.t the alternative hypothesis? \n\nc) correlation is not causation?', 'I agree with you it’s unclear. John, Jack and James in that example are not individual points but categories/factors. \n\nThe Qestion is “does then name of men affect their height?”\n\nAnd the way to answer that is to sample the height \nFrom 100 Johns 100 jacks and 100 James \n\nIf you compare the means using one way anova you will find that the categorical variable “name”doesn’t affect their height.\n\n\nHowever in practice to be true to your results you would need to test all the other names out there as well however it’s obvious that you won’t get different results. And that ability to generalise has a name, what is it?\n\nEdit: 100 is arbitrary n,k and j would do as well', 'I\'ll grant you the random selection (though I might have multiple quibbles about how it would probably be done).\n\n>  Once that happened the professor said that we can generalise our conclusion to all the male citizens of that country\n\nThis is still something that I am taking issue with. Three is so small that if there was a strong effect in say 10% of the population of names (and not much to speak of within the remainder), this would be very likely to miss them all. Even if it were larger (say 100 names), you wouldn\'t use ordinary ANOVA, you\'d want a random effects model.\n\n> There is no way the name would have an effect in the blood pressure, so generalising\n\nThis form of reasoning sounds like a mix of *petitio principii* (begging the question/assuming the conclusion)  -- ""given that it can\'t be true, it\'s reasonable to only check 3 names"" -- and argument from incredulity -- ""I can\'t see how it could be true, so I\'ll proceed as if it\'s false"".  \n\nhttps://en.wikipedia.org/wiki/Begging_the_question\n\nhttps://en.wikipedia.org/wiki/Argument_from_incredulity\n\nAnd lastly, failure to reject a null is still not a demonstration that the effect is absent, even among the three names you looked at. An equivalence test would be a better approach here I think (at least showing the effect is small among the names considered, rather than absent).\n\nIt may have been an interesting motivating example (and so useful from a teaching perspective) but it\'s very weak sauce as far as establishing a fact.\n\nI doubt there\'s any connection myself (I agree with you on that), but if you want to demonstrate something you doubt isn\'t true, you don\'t start by leaning into that bias; rather you should do the opposite. Imagine it *is* true, but hard to find (that you\'re not just lucky enough to pick out some names where the effect is clear). How do you make reasonably sure to find it if it\'s there somewhere?\n\nMy prior thoughts have been demonstrated wrong enough times that for most things I don\'t tend to assume they\'re true, or only hold them pretty weakly.']",0,11,https://www.reddit.com/r/statistics/comments/12m5arq/question_i_cant_remember_the_name_of_a_property/
442,2023-04-14 20:39:55,Sample Group Accuracy [Q],"I have this nagging question and it seems like this group is an appropriate place to ask it. In politics, it always seems like people have statistics to point to to criticize or justify decisions, ""85% of Americans support blah-blah-blah"". Yet, of course, it's certain that not all relevant Americans participated in the poll, nor probably did most of Americans.

So, when pollsters conduct a poll, is there a certain sample group size in proportion to the population where they can be reasonably confident of an accurate poll? If so, how does the theory behind that work?","['Great question!', 'The (quite interesting, perhaps counterintuitive) reality is that statistical accuracy depends entirely on the raw sample size, **not** on the proportion relative to the population size.\n\nThere are a few ways to consider this:\n\n- Some populations are practically infinite but we can understand them fairly simply with relatively few observations. For example, consider the set of all coin flips that will ever occur. We can never sample a meaningful percentage of this population, but it doesn\'t take long to see that the percentage is roughly 50/50. Other distributions may be slightly more ""complex"" in nature, but the same logic still applies.\n\n- Think about the population distribution as a law of governing percentages. ""50% of observations tend to be in this region;"" ""1% of observations follow this abnormal behavior;"" etc. Now, when we take even a small sample, what\'s the probability that ALL of them follow some abnormal pattern? E.g., if I sample 50 people, what\'s the probability that all 50 come from the ""abnormal 1%"" part of the distribution? Basic probability tells us that would be (0.01)^(50), which is a function of the 50 units sampled, NOT the size of the original population. Hopefully this line of thinking can illuminate why sample size is the important quantity for uncertainty, not population size.\n\n- Large population size fortunately has no effect on inflating our uncertainty, but we can actually use *small* population size to *narrow* our uncertainty. For example, if the full population is only 100 units and I sample 99 of them, I can be *extremely* confident in my estimates because there is only 1 single unit of uncertainty left. There is a correction factor that describes the reduction of variance called the [Finite Population Correction](https://en.m.wikipedia.org/wiki/Standard_error#Finite_population_correction_(FPC\\)), but notice that it approaches 1 if n << N (i.e. multiplying by 1, which has no penalty on the estimator). If the population size were much larger, then I would follow the logic of the second bullet point above, which says that variance is directly proportion to n (of the sample).', '> So in that case, a political poll must strive to sample as many people as possible?\n\nThis is as close to ""universal law"" as anything in statistics :) yes, larger sample sizes are always inherently more reliable (though we eventually reach a point of diminishing returns). \n\n> on the other hand, a poll which is seeking to confirm bias can be highly selective, and thus persuade those who are un-critical of its methods.\n\nSmall sample sizes are still unbiased estimates of reality. They simply carry higher uncertainty (variability). So a small sample size on its own doesn\'t help to confirm bias.\n\nHowever, if ""highly selective"" means that they are sampling *non-randomly* from the population, that can quickly and easily bias the results. I would guess that non-random sampling procedure is probably the #1 source of bias in most polling.\n\n> given the diversity of human thought that\'s some kind of magical thinking\n\nAs individuals, humans are incredibly and unpredictably diverse. In larger groups... not as much so :) luckily that helps employ statisticians!', 'The plateau happens when the sampling fraction is significantly close to 1, so for most practical purposes we tend to ignore it.\n\nIt\'s very important to note, though, that all of this is predicated on three assumptions:\n\n1. Every person in the relevant population had a non-zero probability of being selected in the sample; and\n2. We can accurately calculate that probability for anyone selected in the sample.\n3. There is a direct relation between the data we collected from the sample and the thing we\'re reporting on.\n\nAs long as all of these are true (or true enough), then the theory gives us clear ways to estimate not just the thing we\'re measuring, but also some measure of error so we know how wrong we\'re likely to be (so a survey could report that 25% of people approve of policy X with a 3% margin of error, meaning they think the true value is probably somewhere between 22% and 28%).\n\nThere\'s also plenty of theory out there to improve the quality of estimates both by (a) designing the sample to get a better spread of the population (e.g. it\'s usually better to select 100 people in each state rather than picking 5000 people randomly across the country and accidentally selecting the entire population of Lower Upper Trumpville), and (b) adjusting the estimator to correct for meaningful differences between your sample and the population (so if you *do* get an over-representation of people from Trumpville, you re-scale the estimate so that they only wind up representing other Trumpville residents and not people from New Bidensberg).\n\nIf you do all of this, then typically you wind up with an estimate that is either unbiased (meaning that across every possible sample you *could* have taken, the average estimate is exactly equal to the true value), or at least has sufficiently little bias that you can have confidence in it.\n\nThe places where bias can creep into an estimate include:\n\n1. A sample selection process that fails to include certain parts of the population - for example, a phone poll that only dials landline numbers will miss a lot of younger people.\n2. Non-response - i.e. people selected in the sample who do not answer the survey - particularly where there\'s a demographic difference between people who do respond and those who don\'t. For example, people who are very busy might not want to sit on the phone for a 30-minute survey; or people from various minority groups might be suspicious about responding to questions that could be used to target them.\n3. Conceptual differences between the data collected and the thing being reported on. This could be because of poor survey form design (like [butterfly ballot](https://www.smithsonianmag.com/arts-culture/redesigning-the-vote-111423836/) forms), poorly worded or [leading questions](https://www.youtube.com/watch?v=ahgjEjJkZks), or representing the statistics differently (if 20 people say ""I like it"", 10 people say ""I hate it"", and 30 people say ""I don\'t care"", you can say that twice as many people like it as hate it or you can say that only a third of people like it and neither is technically wrong).\n\nSo in reputable survey organisations, a lot of work goes into selecting a good sample and *also* collecting and reporting the right data, and if they\'re really doing their job right then also being honest about the accuracy and limitations of their statistics. Which doesn\'t stop politicians, the media, etc. from using the data to support whatever narrative they\'re interested in, because that\'s human nature.', ""You got me :) I don't conduct the polls myself, but my background is in the nuts and bolts of this stuff (I don't work with any of the form design stuff, but I've hung around those people long enough to pick up the basics).""]",3,7,https://www.reddit.com/r/statistics/comments/12lvwnx/sample_group_accuracy_q/
443,2023-04-14 20:26:25,"[D] Discussion: R, Python, or Excel best way to go?","I'm analyzing the funding partner mix of startups in Europe by taking a dataset with hundreds of startups that were successfully acquired or had an IPO. [Here](https://docs.google.com/spreadsheets/d/1AsFgEXnzId7frEmqSpuAiQV-QbTSI2eJ4DcxCicj1w8/edit#gid=2100307022) you can find a sample dataset that is exactly the same as the real one but with dummy data.

I need to research several questions with this data and **have three weeks** to do so. The problem is I am not experienced enough to know which tool is best for me. I have no experience with R or Python, and very little with Excel.

*Main things I'll be researching:*

1. Investor composition of startups at each stage of their life cycle. I will define the stage by time past after the startup was founded. Ex. Early stage (0-2y after founding date), Mid-stage (3-5y), Late stage (6y+). **I basically want to see if I can find any trends between the funding partners a startup has and its success.**
2. Same question but comparing startups that were acquired vs. startups that went public.

There are also other questions I'll be answering but they can be easily answered with very simple excel formulas. I appreciate any suggestions of further analyses to make, alternative software options, or best practices (data validation, tests, etc.) for this kind of analysis.

**With the time I have available, and questions I need to research, which tool would you recommend? Do you think someone like me could pick up R or Python to perform the analyses that I need, and would it make sense to do so?**","[""You'll be able to achieve what you want with all three tools. Assuming you've never worked with a programming language before, I'd say Excel. That way you won't get bogged down by installation, setup, learning the syntax etc.\n \nR/Python is of course better in the long term and/or if you want to something advanced, but what you're trying to do is pretty straightforward and you have a deadline, so Excel will get you what you want in the least amount of time with the last amount of friction."", ""I couldn't imagine trying to learn a new programming language in 3 weeks"", 'Honestly, if you have three weeks, go for Excel **unless** the dataset is really big. Excel will start to malfunction with very large datasets.', '6000 is small enough for excel', 'I know both R and Python, so I don\'t use Excel ;-) But in seriousness, something like Pearson\'s or Student\'s test should be pretty straightforward to calculate ""by hand"", even if there isn\'t a dedicated function for it in Excel.']",20,55,https://www.reddit.com/r/statistics/comments/12lvjei/d_discussion_r_python_or_excel_best_way_to_go/
444,2023-04-14 14:44:55,[Q] P = .000 But how could such a thing happen... repeatedly?," 

I hope this message finds you well. I ran into the darndest thing, trying to do a bivariate Pearson Correlation through SPSS.

I am conducting various regression analyses on Teleworking,  Income, Education, Race during the 2020-2021 transition. I gathered my data from the US Census Bureau's Household Pulse Survey. After some  brief curation (removing variables that I did not intend to study;  limiting N to \~1,000,000 by removing respondents who were likely  retired), I fed the .csv into SPSS and it delivered a few tables where I  saw statistically significant relationships between Teleworking and the  other 3 variables (not reportable, though, correlation is too weak).  But frequently throughout my Tables, I mostly got .000 for my p value  and the occasional .001.

In the  back of my head, I'm thinking, ""Surely, this is impossible."" So, what do  you think? What did I screw up? Thank you for your time.","['With such a high sample size, estimated error (or any value that depends on n) will most likely be very low, giving significant results even for weak correlation or associations.', 'The p values are not actually zero, they are just rounded. You can report them as p < 001.', ""It means that p<0.001\n\nIt's just how SPSS displays very small p-values. Only the first 3 decimal places are printed, so 0.00083, is truncated to read 0.000"", ""1. Sample size. It's always sample size.\n\n    >  N ~= 1,000,000\n\n    Yeah, that'll do it\n\n     A sample size of a million means that in the usual test of H0: rho=0, the standard error of the correlation under the null is 0.001, so a correlation of even such a tiny value as say 0.005 would produce a z-score of 5, which would have p-value of roughly 0.00000057\n\n2. However, a test makes no sense when you have a *census* of the population (or at least nearly so). The usual calculation is wrong for this case."", 'I think you can double click the value in the output and it will show you all of the decimal places']",3,17,https://www.reddit.com/r/statistics/comments/12ln7jz/q_p_000_but_how_could_such_a_thing_happen/
445,2023-04-14 06:59:26,[Q] Is there a way to test whether maximum likelihood estimation in Kalman Filter reached a global maximum?,"I am estimating the parameters of a state-space model with the use of Kalman Filter. The problem I ran into is that the final parameter estimates are very sensitive to the starting parameter guesses. If I understand it correctly this is a feature of estimating parameters via Kalman Filter, but I was curious whether there is a way to ensure that I am getting correct parameter estimates. Since the model has quite a few parameters and dimensions it is computationally not feasible to do grid searching within reasonable intervals and with reasonable steps.

I would also appreciate any advice regarding this, as I am quite new to state-space stuff.

Also I am doing this in Python with statsmodels.tsa.statespace if that helps.","["">Since the model has quite a few parameters and dimensions it is computationally not feasible to do grid searching within reasonable intervals and with reasonable steps.\n\nIt's so costly that you can't repeat the estimation many times? That's bad for anything but a theoretical investigation of whether your estimation procedure works. Maybe you can stare at the objective function a long time and determine that it is non-convex after all. In which case your optimisation procedure might simply reach a local but not global optimum\n\nAlternatively, you could estimate a simpler model which is not as costly and try with many different stating values. Also, simulate data and estimate your model on it, see if it can recover the parameters. Once this works, make it more complicated again."", 'I am not super familiar with the matter but the answers on those threads might help: \n\nhttps://stats.stackexchange.com/questions/409496/kalman-filter-parameter-estimation\n\nhttps://stats.stackexchange.com/questions/215229/maximum-likelihood-estimation-and-the-kalman-filter', 'It may be that you don\'t have enough data to overcome your prior (conceiving the starting state and state covariance as priors in a Bayesian context). It\'s possible to set things up so that your prior in a KF is less ""informative""; if you have enough data to pin down the whole covariance matrix you will, if you don\'t have sufficient information you may have arbitrarily large variance (and possibly covariance) terms left after you have gone through the data. There\'s not much that can be done about that; if you don\'t have information in the data to estimate all your states, you don\'t. \n\nThere\'s some papers by Robert Kohn and Craig Ansley that address this sort of thing.\n\nHere\'s one relevant paper:\n\nhttps://www.jstor.org/stable/2241356', 'The original paper that proposed the model included parameter estimates for a certain data set, which I was able to recover fairly well (since I knew what starting parameters to choose). The problem is not that I am not sure whether the procedure works per se, but that on a new data set I have no way of ensuring that I am reaching a global optimum. I can rerun the estimation from a variety of starting parameters but that feels unsatisfactory, as I am afraid that I might miss the actual correct parameters by chance. That is why I wanted to know whether I can ensure that global optimum was reached at least in one of my tries.', 'Thank you, these are indeed helpful.']",1,5,https://www.reddit.com/r/statistics/comments/12laaq1/q_is_there_a_way_to_test_whether_maximum/
446,2023-04-14 06:30:09,[Q] Analyzing data of a poll and frequency vs. rank,"Hello,

I hope this is the right Subreddit. I’m going to do a poll with a group of friends. We’re fans of a singer. We’re trying to decide which are our favorite 10 or 20 songs, so I think I’m going to ask the members to rank their 10 favorite songs. Keep in mind that this artist has a huge pool of almost 700 songs and I expect about 30 people voting, give or take. Normally you would award a song ranked #1 10 points, #2 9 points and so on, until a song ranked #10 gets 1 point.

Now, since we are not a huge group, if only one person lists a minor song in the poll and it’s their favorite (#1) it will get 10 points. On the other hand, if 4 people list one of his most popular songs as their #9 or #10 that well-liked song will get overall only 5 or 6 points, which I find unfair since they still like it enough among 700 songs. In other words, with this system it would take 10 #10 votes for 1 #1 vote.

I thought about reducing the scale, so that #1 gets 10 points, #2 gets 9.5 down to 5.5 points for #10. With this scale it will only take a couple of votes to overtake a #1 vote and a song mentioned several times will get rewarded over a song ranked #1 by a single voter.

I’m not sure about the scale though. Is there a formula that I can apply so that it’s balanced in the correct way (maybe based on the number of votes that came in)?

Sorry if I didn’t explain my problem well. I know nothing about polls and statistics, but this is bugging me. The answer’s probably very simple. 😊",[],1,0,https://www.reddit.com/r/statistics/comments/12l93ad/q_analyzing_data_of_a_poll_and_frequency_vs_rank/
447,2023-04-14 05:28:43,[Q] Recommendation for screening/optimization design book,"I know in theory there is supposed to be a screening design followed by a response surface design. In theory, the first design is supposed to screen out variables, and the second is supposed to provide greater resolution on the optimal settings. I just don't know where  to find a good vetted source on the subject. I am trying to establish brand new process parameters, so i would also like to take a wide range and narrow it down to a much smaller range. Does anyone have recommendations on a good book tha walks through methods to address this?","['Look into «Design of Experiments» as a topic. This is a huge field with many different designs, e.g. Plackett-Burman and fractional factorial designs, can be used for screening variables before constructing response surface (e.g via Box-Benhken and Central Composite Designs). \n\nMontgomery’s «Design and Analysis of Experiments» is a good intro text. Alternatively, the OG textbook on DoE is «Statistics for Experimenters» by Box and Hunter.', 'You can check out Art Owen‘s lecture notes on experimental design. Might be a good starter and also has further references.\n\nhttps://artowen.su.domains/courses/363/', '> the OG textbook on DoE is «Statistics for Experimenters» by Box and Hunter.\n\nand Hunter, as there were two Hunters', 'Box and Hunter is a good text and I like it more than Montgomery.  The NIST Handbook of Statistical Methods is online and free, and is pretty good, too.  \n\nUnfortunately, many of the DOE texts are sort of outdated now.  They contain a lot of material about ""cell means"" calculations which in my opinion are kind of pointless when you have the ability to fit models with software.  And they won\'t contain anything about newer designs like definitive screening designs.  You may want to read a review article.', 'Box Hunter and Hunter, for the screening, Box and Draper for RSM. Both of them great overview and good at explaining. Hinkelman Kempthorn feels a bit more technical, less explaining. Most of it is old, you might find them second hand for a small price.']",17,10,https://www.reddit.com/r/statistics/comments/12l5rwf/q_recommendation_for_screeningoptimization_design/
448,2023-04-14 05:23:41,[Q] How do I analyse 2 risk models in spss without ROC?,"
I want to analyse which one of the 2 risk models is better for analyzing an binary outcome. However, the second risk model is just the first risk model with 1 variable added.

If I do an ROC with paired sample, it won’t be significant because it’s almost the same model (just 1 variable is added). Does anyone know how I can analyse The efficacy of my new risk model compared to the old one without ROC?

TLDR: read title",[],3,0,https://www.reddit.com/r/statistics/comments/12l5i0j/q_how_do_i_analyse_2_risk_models_in_spss_without/
449,2023-04-14 04:35:18,[E] Undergrad UCB vs UCLA,"
So I got admitted for both undergrad for stats (Ucla changed the major name to stats and data science) and was wondering which one to commit to. I plan on minoring in data science at Berkeley or Data Science Engineering at UCLA. I've seen on this sub that undergrad really doesn't matter but I wanna work in data science in the future and was wondering if the opportunities for research/internship is similar at both universities. If anyone has been to one or the other can you provide more info on your experiences? Thanks

Also how much does gpa matter to get into grad programs cuz I can prolly get a better gpa at la than Berkeley with its supposedly brutal curves.","['Go berkeley', ""Berkeley has the edge for prestige --- but don't go there just because it is prestigious. Go where you'll fit in better."", ""Berkley's stats program is a beast.  Top 5, at least top 10 for sure."", 'Oh hey, i graduated from UCB in stats and intending on attending UCLA for grad school stats. \n\nPM me and id be happy to share my experience', 'UCB has produced such luminaries as Aubrey Plaza, Ellie Kemper, Donald Glover, the Game Gr- oh wait you’re not talking improv groups.']",27,27,https://www.reddit.com/r/statistics/comments/12l2usx/e_undergrad_ucb_vs_ucla/
450,2023-04-14 04:08:19,[Q] is this spurious regression ?,"Hello,

I am working in credit risk and for some reason I need to translate a default rate in exposure to a default rate in number of obligors.

One option discussed so far is to regress the default rate in number by the total exposure and by the exposure at default at a given time.

It does make some sense as the default rate in exposure is indeed the exposure at default divided by the total exposure.

However, due to newly originated loans, the total exposure is not stationnary and just gaining ~5% every year => it looks clear to me the exposure has a linear relationship with time. This makes me worry on whether my regression is spurious.

On the other hand, it is how the default rate is defined so it makes sense from a business pov.

What do you think about it?

Hopefully we have other options.",[],1,0,https://www.reddit.com/r/statistics/comments/12l20qc/q_is_this_spurious_regression/
451,2023-04-14 03:51:50,[Q] How do I test these data?,"Hi everyone!

I am having difficulty doing a small analysis of a test that I have carried out. The test consisted of sound stimulus tests. I conducted a total of four tests for two groups (n=30 and n=24).

In each of these tests, participants had to respond to exactly 8 sound stimuli (8x4=32 stimuli in total).  I segmented the tests in pairs, meaning that the 8 stimuli from tests 1 and 2 were located in specific positions, while the stimuli from tests 3 and 4 were in different positions compared to the previous pair.  I obtained reaction times for the observation of the stimuli, therefore, I would now like to compare which sounds the participants were able to detect more quickly.  That is, compare the 8 stimuli for the first pair (1-2) and see which one was detected more quickly, and then check the other 8 stimuli in the second pair (3-4).  Should I conduct a two-way ANOVA in this case?"" 

&#x200B;

 Thank you very much for coming in and your help.","[""It's not clear from your description what you're comparing. Are you comparing reaction times for all four stimuli? Or just comparing 1 to 2 and 3 to 4?\n\nIf the first, I think it would be a one-way ANOVA with appropriate post hoc tests to index where any significant differences in mean reaction time lay. If the second, it would be two separate t-tests.\n\nEdit: just remembered ANOVA requires your data to be independent. If my take on your experiment is accurate, you've got a within subjects design (i.e. not independent)."", ""the second! comparing the 8 stimulli for 1-2 and for 3-4. So... i should make two separate t-tests, isn't it?"", 'My apologies, if this is your design then it\'s two one-way ANOVAs. I missed that your stimulus has 8 levels, the ANOVA will compare the means of all 8 levels in test 1 with the means of all 8 levels in test 2 (and so on for test 3 vs test 4).\n\nHowever, as I mentioned, ANOVA requires that the data be independent. Since you are comparing data from the same subjects, this assumption of independence is violated. I\'m not sure then which test is most appropriate, you might try googling ""ANOVA for nonindependent data"".', ""Are you looking to compare individual stimuli against one another (1/32)? Or groups of stimuli against one another (1/4)? Or are you aiming to compare individual stimuli across four conditions (1/8/4), sort of like a 4-factor ANOVA (not that such a thing exists, I don't think: possibly a MANOVA)? Maybe if you described your aims/hypotheses a bit it would be easier to understand your design.\n\nEdit: I'm talking rubbish in that last part: if it was to compare individual stimuli across four conditions it would, as you already suspected, be a 2-way ANOVA (IV1: stimulus, IV2: stimulus group). I'm guessing this is what your design is, then. In which case, you have a mix of between subjects and within subjects design and I'm not sure which version of ANOVA would best handle that: but you're certainly on the right track!\n\nEdit 2: I've just realised you're repeating the entire study for 2 groups of participants and am confused again. Unless you're not looking at individual stimuli at all, but rather at stimulus group (IV1) and participant group (IV2). In which case, it's still 2-way ANOVA, and yes I'd go for repeat measures."", ""thank you so much for your help! maybe it's the anova repeated measures test?""]",3,6,https://www.reddit.com/r/statistics/comments/12l1fut/q_how_do_i_test_these_data/
452,2023-04-14 00:26:56,[Q] Trying to figure out how to test these data - correlation for stratified/range of data?,"Apologies if some of my terminology is incorrect, I'm certainly not a stats expert.

I'm a cardiologist doing some research trying to analyse some data I've collected for some patients to gain a little more insight into mechanisms of disease. I have two groups of patients where I've done some experiments on the electrical conduction within the heart. Within each group, I've got a measurement of the degree of electricity abnormality (fractionation) during different conditions.

Groups A and B

Conditions A, B and C

For each condition, I've measured abnormal electricity (% of signals that were fractionated) and because of the way we've measured, we cannot get specific values, only stratified into a range. The range is 0-24%, 25-49%, 50-74%, or 75-100%. I want to show that the degree of fractionation during condition A correlates or doesn't correlate to degree of fractionation during condition B and/or C.

So for example

Group A

Patient1 - condition A=0-24, B=0-24, C=25-49

Patient2 - A=24-49, B=50-74, C=25-49

etc

Group B

Patient1 - A=75-100, B=75-100, C=75-100

Patient 2 - A=50-74, B=75-100, C=50-74

etc

First I want to do this correlation for all patients regardless of which group they are in, so I can show that the measurements correlate based on conditions. Then I want to compare the measurements for each condition between the groups.

I can't quite figure out which tests to use in order to do this because I don't have specific integers, only a range of numbers/stratified for each data point. Could someone help?

I'm using GraphPad Prism as my stats software. I'm guess very few/noone here would use that, so I just need the name of the test/an explanation of how to test for it, and I'll figure it out within the software.

Many thanks in advance for your help!","[""So for non-parametric ordinal data like yours, \n\nTo assess the correlation between conditions A, B, and C for all patients (regardless of the group), you should use the Spearman's rank correlation coefficient.\n\nTo compare the measurements for each condition between the two groups (A and B), you can use the Mann-Whitney test. This compares the distribution of the ordinal data between two independent groups by testing the null hypothesis that the two groups have the same distribution.\n\nI’m 99% certain as I’ve been studying for grad stats finals the last few weeks. I’m not sure how to find those tests in the software you’re using but hopefully this is helpful."", ""Thanks for your reply! I'm having a few issues with that.\n\nIn order to do a correlation coefficient, I need to plot the data in columns only, so two or three columns of Group A,B,C. But in terms of the values I am allowed to input, it will only allow integers, not a range of values within each cell.\n\nThe problem with trying to do a MW test on this data is that it tells me MW tests are only for data in columns ie. there is only one variable which is group A or B. When I add in a second variable (the degree of fractionation as 4 rows, 0-24%, 25-49%, 50-74%, or 75-100%), and then plot the number of patients in each group that fall within these four categories, it has to be a contingency table in this software, and then it's telling me to do a chi square/fisher test, because that is what you do for contingency table data."", ""Apologies for the confusion. You're right; the format of your data requires a different approach. Since you're dealing with ranges instead of specific values, using contingency tables and chi-square or Fisher's exact test would be more appropriate.\n\nIf I understand the first analysis correctly, it is designed to examine the relationship between the fractionation measurements during different conditions, regardless of the group the patients are in. Basically, It will help you understand whether the degree of fractionation during one condition is related to the degree of fractionation during other conditions.\n\nFor correlation between conditions A, B, and C for all patients, you can create a contingency table between each pair of conditions(6 in total). Then, use the chi-square test (or Fisher's exact test as appropriate depending on sample size and the expected counts in the contingency table. \n\nFor comparing the degree of fractionation between Group A and Group B for each condition, you can also use contingency tables with the chi-square or Fisher's exact test. In this case, create a contingency table for each condition (A, B, and C) with the two groups (A and B) as rows and the four fractionation categories as columns.""]",6,3,https://www.reddit.com/r/statistics/comments/12kuouw/q_trying_to_figure_out_how_to_test_these_data/
453,2023-04-13 23:27:56,[D] Books/References on Linear Mixed Models,"I am looking for a book/reference on linear mixed models which specifically describes how the model parameters are estimated. I have found several books, but these details are often skipped over. Can someone please reccomend something?

Thanks!","['I have three.  ""Multilevel Modeling Using R"" by Finch.  ""Mixed-Effects Models in S and S-Plus"" by Piniero.  ""Data Analysis Using Regression and Multilevel/Hierarchical Models"" by Gelman. Honestly, I think I learned through a combination of Finch, Gelman, examples online for lme4 R package, and just thinking about it for a while in the back of my head.  It was a difficult subject for me to learn and no single source really did the trick for me.  \n\nI wish you good luck.  It\'s actually an amazing subject and now I use it all the time in my work.  The combination of maximum likelihood + multiple levels of coefficients is ingenious. It controls overfitting for categorical variables with many levels, allows for predictions for new levels of categorical variables, describes hierarchical relationships that exist in many real problems, and more.', 'I have to disagree here. The better one understands how and what is being calculated, the better application and interpretation of the statistical technique. There are plenty of reasons to look at actual estimates.', 'Thank you for your replies everyone!', 'While it is software specific, this book does a great job going through the equations and exactly how parameters are calculated. \n\nThis is just the free sample. The whole book is also a doorstop. \n\nhttps://support.sas.com/content/dam/SAS/support/en/books/sas-for-mixed-models-an-introduction/68787_excerpt.pdf', ""I don't think you'll find details about the estimation in books, as this is mostly unnecessary for daily use. Most R packages come from a scientific publication, so you can check this. Iirc there are good publications explaining lme4 for instance.""]",10,10,https://www.reddit.com/r/statistics/comments/12ksvml/d_booksreferences_on_linear_mixed_models/
454,2023-04-13 22:45:54,[Q] Nondeterministic Tournament,"How to calculate the probability of A positioning better than B if that was a nondeterministic event?

Suppose we have a race event with 8 possible placements. Do I have to calculate every possible placement? Like: What are the odds of A being 1st, and if first, what are the odds of B being in a better placement. (In that case, 0%, but if A finishes 2nd, the odds are around 14% for B to be better placed.) Or there's a formula to calculate this?","['We can\'t do anything without a better probabilistic description of the race event.\n\nIn the most basic scenario the performance of each contestant, presumably the (random) amount of time it takes to finish the race, would be independent and identically distributed and there are no ties.\n\nI\'m not sure what you mean by ""placements."" Are you saying that there could be more contestants than placements? For example 15 contestants but only the top 8 get a ""placement?"" I\'m going to consider all positions in the race, not just placements. It makes things easier. You can introduce placements again later.\n\nThe *probability* of a single contestant being in a particular position will be 1/(total number of positions). For example, the probability that A is in first is 1/8.\n\nFor your second question, think about the possible remaining positions that B could be in given that A is in position ""x"". If A is in position 3, then there are 8-1 = 7 remaining positions for B. In order for B to place better than A, then they must be in position 1 or 2. The probability that they are in position 1 is 1/(total remaining positions) = 1/7. The probability that they are in position 2 is also 1/7. The probability that they are in 1 OR 2 is the sum of those two probabilities, 2/7.\n\nYou can use the same strategy given that A is in any position.\n\nThis isn\'t a very realistic model, though. Contestants probably interact during the event which violates the assumption of independence and their performances are probably not all equally distributed. To capture these kind of details one would need to describe a ""richer"" probabilistic model and the calculations would be more involved.\n\nOne final note:\n> if A finishes 2nd, the odds are around 14% for B to be better placed.\n\n\nHere you\'ve not described the odds, but the *probability* that B is better placed than A, given that A finished 2nd. The odds would be 1:6, i.e. 1 success to 6 failures. The probability is 1/7 = (number of successes)/(total possible outcomes) = 0.143 or about 14% as you\'ve concluded.\n\nhttps://en.wikipedia.org/wiki/Odds#', ""In general (if each player has different chances of winning), yes, you'll have to consider every possible permutation of players to get a complete answer to someone's expected ranking. There will be shortcuts if you have several evenly matched players, or several players you don't care about. Similarly if A and B are in the same subpart of a bracket, you'll have fewer matches to enumerate than if you have to find their chances of making it all the way to a final.\n\nThe 'independent chip model' is widely used to predict outcomes in poker tournaments, and equitably divide prize money if the tournament ends early. In that model, A will beat B with probability (A's chip stack)/(A's chip stack + B's chip stack.), but if you care about expected winnings, you have to find a probability of landing in each of the top places. \nBoth very early and very late in the game, there are simplifying shortcuts. Right when you first hit the prize money, it's hard."", 'Are all possible permutations equally likely? If so, then A will finish before B in exactly 1/2 of the races because of symmetry.', 'Yes.\n\nBut again, ""odds"" are not the same as ""probability"". The *probability* for A to be better than B is 50%. The *odds* that A is better than B is *1 to 1* or 1:1, i.e. equal odds.', 'Sorry, I didn’t give all the details. The number of placements in that case is equal to the number of participants. \n\nBut I can imagine that the outcomes are equally distributed, by imagining so, the odds are 50% for A to be better positioned than B, and vice versa. (?)']",1,11,https://www.reddit.com/r/statistics/comments/12krm2u/q_nondeterministic_tournament/
455,2023-04-13 22:17:53,[Q] contingency table; two approaches to measure dichotomous behavior. Which statistical parameter would fit?," Hey,

we measured a behavior with two approaches. Both is a dichotomous variable (Behavior is shown, yes or no) . Example below.

Which statistical parameter wold be the best? I though about interrater-reliability but we did not have raters. Any ideas?

Thanks!

&#x200B;

|||Measure 1||
|:-|:-|:-|:-|
|||yes|no|
|Measure 2|yes|550|20|
||no|10|300|
|||||","['It depends on what question you are trying to answer.\n\nIs one *Measure* the ""correct"" one, and you want to see how often the other measure is also correct ?  (I\'m thinking of a confusion matrix here with the associated statistics like sensitivity and precision).\n\nOr do you want to know if there is a significant shift in in *Measure 2* from ""no"" to ""yes"" relative to ""yes"" to ""no"" in *Measure 1* ?  (I\'m thinking here of McNemar\'s test).\n\nOr something else ?', 'The term you\'re looking for is ""interrater agreement"" or ""interrater reliability"". A classical measure is Cohen\'s kappa. Personally, I\'d recommend[newer measures](https://link.springer.com/article/10.1186/1471-2288-13-61) such as Gwet\'s AC1.', 'What are you trying to find out?', 'Neither is the ""correct"" one. It\'s a behavior that has never been measured and it\'s hard to tell which is the absolute correct one.\n\nComparable, easy example: Measure 1 asks people in a self-report which food they buy regularly and by these information you could conclude if this person is a vegetarian or not. Measure 2 directly asked people in a self-report if they are vegetarian or not. However, our behavior of interest is a little bit more complicated and vague.\n\nMeasure 1 was before Measure 2.', 'Thanks. I wrote it as a possibility in my post. However, I feel like there aren\'t ""two raters"" which is why I thought it is not appropriate']",1,9,https://www.reddit.com/r/statistics/comments/12kqrth/q_contingency_table_two_approaches_to_measure/
456,2023-04-13 21:22:14,[Q] Access to Statista?,"Hello all,

Does anyone have a paid account or university access for Statista? There are a couple of statistics I would like to see but I need a paid account for that. However, I'm not wealthy enough to pay such a sum of money per month and being stuck for a year.

Could anyone help me? 

Hope to hear soon. :)","['I checked my uni but they don’t subscribe to Statista either. \n\nSucks you can’t just pay for one month…', 'For those who need a bit of help, I have a guy who sells genuine Statista subscription accounts for $9.90 a month. I have been using it for months without problems. total life savor. If anyone is interested, his telegram is adobe\\_king', 'Thank you so much for checking !! That was very sweet of you', 'Hi! Thanks for the links, but the data I am looking for is about sex crime rates in South Korea, Japan, and India. I am not looking for statistics regarding the U.S.', 'https://data.gov/\nhttps://data.census.gov/']",0,6,https://www.reddit.com/r/statistics/comments/12kp1bn/q_access_to_statista/
457,2023-04-13 14:45:14,Help for an Autistic Person [Q],"I am currently a transient student who is 18TF and I have 39 total credits (transfer/in-school) from my local university. However, they have not been as accommodating and I have been depressed. I want to pursue a B.S. and then a PhD in Statistics, but due to my anxiety I had to take a mental health break. I’ve been getting discouraging messages lately saying that the degree might be too stressful for me and that I should major in something else. Are there any statistics programs at other universities in the United States willing to accommodate someone with severe anxiety? 

*Note: I apologize if my post is wordy. I just want to know if people have been doing through anything similar. If this is the wrong subreddit, please let me know where I should cross post this.","[""You have my heartfelt sympathy. You are not alone. I don't know that there is any university better than others. All I can say is that, and this may sound trite but I don't mean it that way, but you be You. \n\nOthers opinions have nothing to do with who you are, and for damn sure should not discourage you from pursuing your interests. \n\nAnxiety is NOT abnormal in this. Don't beat yourself up. It's hard, it's supposed to be hard, getting through it isn't nearly as much about a piece of paper, but that you are capable of doing something hard. That's worth more than whatever topic you choose to study. \n\nHope that helps somehow, and I wish you the best. If you want to do stats, go for it. If you want to do something else, go for it. It is YOUR choice to direct your life. If you want to get into hard math and learn it, go for it. You've got to want it and be willing to work for it. It's the willingness to work for it that is the value. \n\nGetting what you want in life is easy. Knowing what you want is hard. Best of luck to you ."", 'Grad school is tough and will be stressful for anyone… thats one of the reasons so few people can get a phd', ""go where ever is cheapest for you to study. instead of finding a university that will accommodate anxiety, go see a mental health professional and get your anxiety under control. think about happens after you finish school, an employer won't be as accommodating, they'll expect you to not miss work and to make deadlines"", 'Then focus on your mental health? Get that in order, because that seems to be what’s hindering your learning. If you can afford an expensive school, you should have no problem affording to see a mental health professional as well', 'I went to a very Christian university, and bear with me, they offer an array of classes and programs for people with anxiety. The first week of school, the new students are assigned a mentor that will reach out to them and basically be a go-to for any questions regarding registration, and all questions in general. It’s basically a bunch of extroverts that adopt the new students and make sure they have what they need for the whole semester. Then, there are “college life” classes that basically prepare you with the skills you need to get through college. The class focuses on maintaining communication with professors and forming study groups with peers, etc. I had pretty intense social anxiety and this helped me a lot. Also, they allow you people with depression and anxiety to drop their classes without penalty to look after their mental health (no refund after a bit of the way through the semester, but i had to do a medical deferment due to depression once and it was a relief to know the classes weren’t counted toward the GPA). Lastly, everyone was assigned a family group that they were to meet with once a week, which guaranteed I had friends even when I wasn’t confident in my ability to make them on my own. \n\nI know there’s a stigma around christian universities but some do pander to people with mental illnesses very well. If you’d like some personal recommendations feel free to DM and we can talk a little more in depth on specific schools I could recommend']",0,18,https://www.reddit.com/r/statistics/comments/12kf384/help_for_an_autistic_person_q/
458,2023-04-13 10:22:57,[Question] What analysis to use for insect behavior study,"Hello all! I'm looking for a little help on how I should go about analyzing data for my master's thesis.

In  my experiment I am recording the calls of an insect over 24 hours. My  aim is to work out what time of day this species is most likely to call.  For each recording so far I have worked out how much of each time is  spent calling each half hour (i.e. dividing 24 hours into 48 half hour  blocks). I have noticed a pretty strong bimodal distribution as to peak  calling times. There is little variation in where these peaks are  between different recordings, but there is variation the total time  spent calling.

I will probably end  up discussing this with a stats professor later, but it'd be handy to  know where to start looking so I have something to bring him.

I can provide more info on my data if needed <3","['Can these recordings be done over many days and then check for a correlation between these calls and environmental factors like temperature or humidity? Check Dolbear’s law for an example of predicting temperature with cricket chirps.  Can you do any spectral modeling of the insect calls and provide some analysis on the composition of the sound waves that make up the call?  Maybe this varies with some information known about the time of day, environment, or specific goal of the insect..']",1,1,https://www.reddit.com/r/statistics/comments/12k8jw0/question_what_analysis_to_use_for_insect_behavior/
459,2023-04-13 08:42:18,[Q] high school senior deciding major,What are the potential internship and job outlooks that can be expected by a statistics major. I have been admitted into uiuc for statistics and I want to know what possible jobs are available as a statistics major. What are internship opportunities that can be expected? I have heard that statistics majors are more valued when they have a masters degree rather than an bachelors.,"[""Plenty of ____ analyst type jobs are hiring stats BA or BS degrees. Data analyst, business analyst, finance analyst, etc.\n\nI would recommend a minor in computer science alongside it. Won't have trouble getting a job"", ""As they say, statisticians can play in everyone's backyard. Stats majors in undergrad pair well with other majors. (Plus econ makes econometrics, plus psych makes psychometrics, plus CS gives you an edge in business analytics or machine learning.) You can pair it in anything and it'll enhance it, especially in STEM. (I wish I took more stats for my undergrad publications.)\n\nIf you want to get a Masters, there are also research positions at lots of institutions. I went to grad school for statistics and currently publish in global health. \n\nSo you can really use it anywhere: academic research, businesses, whatever. Just figure out what problems you want to solve."", 'Second this — I’m headed for grad school but minored in CS and doubled in Math']",0,3,https://www.reddit.com/r/statistics/comments/12k5v2r/q_high_school_senior_deciding_major/
460,2023-04-13 08:10:14,"[Q] How do I find the correlation between two categorical variables from two different samples from the same population? (urgent question, any help is appreciated!)","Hi! To start, I'm a senior in high school and this assignment is quite overdue, so please read and respond as soon as possible 🙏. I am doing a research paper for school (the class is AP Research), and I have (foolishly) endeavored to find out if there is a correlation between the psychological and economic impact of super typhoons on residents of the Philippines. I intend to do so by using data collected after 2013 Typhoon Haiyan. I have very little knowledge of statistics (I took AP Stat last year, but it turns out that wasn't enough), so I'm really lost and overwhelmed. I had planned on finding the correlation by taking data from a study/survey about psychological impact and data from another study/survey about financial impact and correlating the two data sets, but I've realized two issues:

1. the data would be categorical, so the typical linear regression (Pearson's r correlation coefficient) doesn't seem possible, and
2. the data would be from two different samples, so I can't do the chi-squared test for independence (can't do chi-squared test for homogeneity either because there are two variables).

My vague understanding of stat tells me there's probably a way to do this using chi-squared tests, but I'm not able to connect the dots. So if there's any way to do that, please let me know! If there's any other test for correlation that works for the above type of data (two categorical variables from two different samples from the same population), please tell me about that too! If not, and/or if there's something fundamentally wrong with the way I'm trying to do this, please enlighten me lol.

I've tried to be clear and concise, but please ask questions if something is confusing. Thank you!","['You\'re on the right track about having two separate issues here, which I\'d describe as \n\n1.) you need the right analysis method for the data type (categorical/ordinal/numeric) and research question, and\n\n2.) you need the right data source(s) for any analysis to actually be able to address your question \n\nFor the first issue, \n\nIf you had the data you\'re looking for (which would probably be ordinal variables, with each variable measured on the same sampled units), you *could* do something with a chi-square test of independence, but it likely wouldn\'t aim at your particular question -- it would be ""is there *any* pattern of association between these (implicitly unordered categorical) variables?"".  There\'s no concept in that test of different categorical values being bigger or smaller than others, just different.  Your real question is like ""do bigger values of A correspond to bigger values of B?\' (these only collapse to the same type of question if both variables have exactly two categories/levels, where one can be considered \'big\' and the other \'small\').\n\nYou might be able do something with linear regression/ordinary Pearson correlation, but the result would only be meaningful to the extent that the variables are each measured on intelligible numeric scales, which it sounds like they wouldn\'t be.\n\nWhat you\'d probably be aiming at is some type of rank correlation, like https://en.m.wikipedia.org/wiki/Spearman\'s_rank_correlation_coefficient  -- this specifically addresses \'does higher A correspond with higher B\'?, even if A and B are just ordered categories, rather than regular numbers.\n\nFor the second issue, you\'ve noticed your confusion here -- if you want to associate variable A with variable B at the scale of individuals, you\'ll have to have data about A and B on the *same* individuals -- measurements of A for one set of individuals and of B for another independent set of individuals isn\'t going to be any use for conclusions at the individual scale.  If you can\'t get such data at the individual level, you might be able to piece something together by aggregating different data sources to the same shared scale, (e.g. by geographic region), like u/good_research says, and treating those aggregates as your data points.  However, in that case, (a) your sample size at that level of aggregation might be too small for useful inference, and (b) you\'d be opening the door to the https://en.m.wikipedia.org/wiki/Ecological_fallacy.  Still, it might be the best you can do here.', ""You'd probably try to group your participants along some lines that vary in typhoon impact, region might be one."", 'Check out the Mann-Whitney U test', ""You can transform categorical variables into dummy variables and use them in a regression. \n\nIt helps to model an equation before even running a regression.\n\nFor example\n\nlog(income) = c + beta1 \\* typhoon+ other variables of interest  + e\n\nlog is the natural log function,    c is a constant,  typhoon a dummy variable that would = 1 if, after the typhoon or 0 for before, and other variables would be added to create a well-specified model.  The beta1 variable is the regression coefficient assigned to typhoon which would represent the effect that typhoon has on the percentage change of income since in the equation we are taking the log of income.  For example, if the beta1 came out to be -.162 then that would represent that income was 16.2 percent lower after the typhoon. The stats package you use should provide a p-value for typhoon and you can use that p value to determine if typhoon had a statistically significant effect on income.  The  e is the error that we don't observe. It contains all the things not modeled in the equation that affect income, for example, ability or education.\n\nWhat types of variables do you have?  Did you study regression / ordinary least squares (OLS)  in your AP stats course?"", ""Couldn't you treat the categorical data as count data and compare the proportion of those suffering poor mental health outcomes in Population A to the proportion of those suffering poor economic outcomes in Population B using z-test? You'd need to acknowledge they were from different samples, but if both are truly random (you'd need to check the studies for any difference in sampling method and discuss these) they should both represent the population.\n\nDisclaimer: I'm a social scientist not a statistician so there may well be glaring issues with what I've suggested! Hopefully others in the sub will chime in if so.""]",8,14,https://www.reddit.com/r/statistics/comments/12k4z0e/q_how_do_i_find_the_correlation_between_two/
461,2023-04-13 06:11:33,[Q] Eager vs Lazy Learners in Statistical Machine Learning,"Hi! 

Apologies if this is not the community to ask, but in what cases should I use eager learners (like random forest) and lazy learners (like KNN) ? Does it have to do with dataset size? 

Thanks!","['As far as I’m aware there are no *statistical* considerations for picking between eager and lazy learners. Practically speaking there’s going to be differences in actual time taken during prediction and training, which means there may be considerations relevant to applications of the two methods in practice.']",1,1,https://www.reddit.com/r/statistics/comments/12k1d6x/q_eager_vs_lazy_learners_in_statistical_machine/
462,2023-04-13 04:11:24,[Q] Does a related measures Anova test apply here?,"I’ve never used this test for anything before so I’m uncertain, but it seems like the best fit.

I have a data sample of different feeding behaviours expressed by ringtailed lemurs when their food is presented in three different forms. The data only focuses on one group and there are 7 different behaviours expressed. The dataset shows the latency of behaviours and instances of behaviours.

Thanks so much in advance",[],2,0,https://www.reddit.com/r/statistics/comments/12jxgeh/q_does_a_related_measures_anova_test_apply_here/
463,2023-04-13 00:17:34,[Q] Binary Logistic Regression vs. Survival Analysis,"I am modeling the probability of my food products to fail shelf-life testing (giving a ""yes"" failed or a ""no"" failed at the point of testing).  There are a number of covariates I explore, including the food recipe, the temperature of storage, and the lighting conditions.  I have many empirical data points, a number of which don't actually fail because I didn't test upper limits (""right censoring"" in survival analysis).

Historically I've explored this through binary logistic regression, and simply predicted the probability that I would get a ""yes"" as a function of covariates that include time in storage, temperature, etc.  Thus, I can estimate for a given set of predictors what my probability of failure is.  This is useful because I can set a cut-off for my estimated probability of failure (say >= 25%), and thus recommend avoiding any combinations of shelf-life conditions that exceed that prediction.

However, I've recently been reading about survival analysis and while the intent seems much more applicable to my application, I'm not quite clear how different they are in practice.  I understand that there are different approaches, including classically Kaplan and Meier estimation and Cox estimation.  I also understand that the survival function gives me the probability that the sample would survive at time t or greater, and that the function will change as a matter of covariates.  However, that seems pretty similar to what I've already done above, with the exception that you get survival not just at the specific linear combination of time t + other covariates, but also > time t **with the** other covariates.    


Am I thinking about this right and if so does this suggest that survival analysis is really what I should be doing with my type of data?","['Logistic regression seems like the more appropriate choice here because it sounds like all of your test samples have been tested for failure (you know if they did or did not). So in that regard, there is no uncertainty in the outcome. \n\nSurvival analysis is useful when you either observe the event of interest (failure) or right censoring occurred (you don’t know the outcome either way). You could turn the question a bit, and use survival analysis to estimate the time to failure (or some other feature of the failure time distribution). In this case, products that haven’t failed are censored at the time of testing because you know the product hasn’t failed at that time, but you also can’t retest the product later).\n\nEdit to add: since these are food products, there may be better or more appropriate aspects of failure (such as measured bacterial growth) that will give more reliable or useful indications of food spoilage.', 'The notion of all products failing as you test them further down the line / further in time is in perfect agreement with the assumptions of standard survival analysis (in the end, all subjects will experience the event of interest). Since you have a rather rough time scale or at least it appears that you test your samples routinely every x-amount of days, you should take a look into discrete time survival analysis.', ""Are you measuring failure y/n at the exact same time for every item? \n\nSay you are examining egg salad sandwiches. Do examine them all for failure at 10 days? Or do you examine some at different time points, e.g. 7, 10, 14, and 30 days?\n\nIf the former then logistic regression will work. However if you follow items for different lengths of time survival analysis is more appropriate. But survival analysis also assumes that you capture the timing of the failure. So if a sandwich fails at day 3 and you don't examine it for failure until day 7 then you're not getting accurate time-to-event data. \nIf you collect data on the exact time of failure then survival analyses will give you much richer information on products. Because you can say well at 7 days we see a x% failure rate, but at 14 days the failure rate is y%. Also with survival analyses you can look for effect*time interactions. This would help you tease apart which of your covariates is effecting failure as a function of time. For your example you might find that lighting doesn't make much of a difference on failure early on (<3 days) but then after that the effect of lighting increases."", ""Thanks for the thoughts!\n\n&#x200B;\n\nRegarding censoring, I test several time points (e.g. 10 days, 14 days, 21 days) as a matter of meeting a target, but I know that if I tested further time points, they would fail.  In other words, there's an increase in hazard associated with further time points.\n\nI've seen with my regressions that when I only test time points that my product can tolerate, then extrapolating (e.g. to 25 days) will inappropriately predict that my product will be fine.  I'm not sure if this is an aspect of right censoring that survival analysis is more suited to; i.e. the acknowledgement that the study could not continue.\n\n&#x200B;\n\nFor the metric of survival, there are indeed quite a few factors that go into a binary consideration of failure.  They include microbial spoilage, product appearance, and the sensory attributes.  If any fail, we would consider that to fall within the fail bucket.  If none fail, we consider it not failing.   In practice I don't use microbial spoilage, simply because it's costly and time consuming to collect microbial data on all of my samples, as well as it being an uncertain predictor of whether or not the product is acceptable sensory wise (there are situations where microbial counts are elevated but sensory is fine, which would result in false positives for predicted failure if the microbial counts are themselves still tolerable from a food safety, regulatory standpoint but used as a proxy for sensory failure)."", 'I am running a couple studies where I specifically look for visual signs of failure on packages aged out to 45 days.\n\nEg I have package IDs 1 through 45 in several different aging conditions, and every couple days I will inspect them and mark them as 1//0.  I believe different conditions will cause different failure rates for a given time, but I expect that by the end of the study, all packages will have failed (45 days).\n\nYour comment about looking at a covariant’s effect on survival over time is something I hadn’t thought about, but I can definitely see it being apparent with plotting']",16,6,https://www.reddit.com/r/statistics/comments/12jr7ba/q_binary_logistic_regression_vs_survival_analysis/
464,2023-04-12 23:43:38,[C] What is a normal W2 rate after 5 years biostats experience?,"I was making 145k+12% bonus (162k total) at my last full time employee role working remote. Now going into the contract world, is $90/hr reasonable? I figure the loss of PTO is worth 10k, and health benefits another 10k, so $90/hr (180k - 20k) = ~ 162k.","['Typically the calculation is made this way:\n\n2.5 x annual total compensation / 2000.\n\nSo for you, it would be:\n\n2.5 x 180k / 2k = $225/hr', 'Understood. Still the R skills can transfer. Data management is moving to r from SAS. They have now submitted a full FDA eDTC entirely done in r that was accepted. Some of the largest Pharma companies in the world have announced they are all in on r and open source and moving from SAS. Five years in good time to transition.', 'I’m pretty sure for contract work, you charge a lot more than 10% more. Don’t forget that independent contractors pay A LOT more in taxes that are no longer paid by your employer. I’d do at least 30 percent more minimum, so 200k a year, but I’d recommend more. You may want to seek out professionals in contractor negotiation if you don’t know what you’re doing.', 'Are you coming from Pharma or CRO what specifically in biostats where you doing. Clinical trial analysis or data management?', 'Unfortunately, diagnostics. The most rare, easy, and useless field. Hugely regret getting into it.']",0,10,https://www.reddit.com/r/statistics/comments/12jq69o/c_what_is_a_normal_w2_rate_after_5_years_biostats/
465,2023-04-12 21:19:53,[Q] do I use probit or logistic regression?,"

I am dealing with corporate disclosures with binary dependent variables. 1 if company discloses whistleblower policy and 0 if it doesn’t. Independent variables are number of directors on the board, percentage of women to men on the board and percentage of independent directors on the board. This is panel data from 42 firms over 15 years. Do I use probit or logistic regression?","[""Use Logit. \nProbit and logit produce basically identical marginal effects and logit is easier to interpret.  Either way, it doesn't matter too much, just make sure you get the marginal effect and don't use the coefficients directly."", 'OLS is a poor choice given the binary dependent variable.  I know some people do that, but frankly it doesn’t make sense.', 'Of the two, logistic is usually a good default. The probit “flattens out” faster—that is, the normal CDF approaches its asymptotes more rapidly than the logistic function—so it may be a better fit in situations where it makes sense to expect probabilities very close to 0 and 1.', 'What? A logit curve can be approximated with a straight line if *none* of the data points are near 0 or 1, but in most applications *all* of the response data points are 0 or 1. It might be defensible if your model\'s range of applicability is limited to, say, 25-75% probabilities *and* your data is dense and complete enough that the average y-values in each ""bin"" are constrained far from 0 or 1, but your model loses all meaning anywhere close to the ends. It\'s a pretty limited range of applications you could get away with this, it\'s a lot harder to think of a case where it works than one where it doesn\'t.', ""Logistic assumes that the effects are additive on the logit scale. Probity assumes they're additive on the probity scale. If all predictors are discrete and you saturate the model, then reality fits either assumption. To the extent your model isn't saturated in either case means your curve fitting.\n\nThat said, the interpretation of coefficient estimates...in a logistic model they are the logged odds ratio e.g for a binary predictor, \n\n     log( (p(Y=1|X=1)/p(Y=0|X=1)) ÷ \n        (p(Y=1|X=0)/p(Y=0|X=))\n\n\nIf X is continous then replace X=0 and X=1 above with X=x and X=x+1.\n\nFor probit the interpretation is that there is really an unobserved continous outcome which we observe the thresholded version of and the effects of covariates is to shift the threshold around. It's used alot in psychometrics.\n\nI would go with logistic\n\nFinally, good point raised that the observations are most likely not statistically independent. You'll need to use something like a mixed generalized linear model (logistic with mixed modeling capabilities) to add a group effect if you can decide how to break your observations into independent groups.""]",11,35,https://www.reddit.com/r/statistics/comments/12jm1tc/q_do_i_use_probit_or_logistic_regression/
466,2023-04-12 21:14:07,[Q]Testing a difference for statistical significance,"Hey guys, I hope this is the right sub. Basically, I have a Dataset with thousands of observations. Each observation includes a name, a number and is either labeled A, B, C, D or E.
What I have done so far that I calculated the mean for every subgroup, I.e. the mean for all observations labeled A is 0.372, for all labeled B it's 0.264 and so on.
I was wondering if it's possible to test whether the differences among all 5 groups are statistically significant and if so, how is it done?","['Sounds like you might be looking for an ANOVA.', 'Another way to test for the significance of the differences among the means of the five groups is to use a set of pairwise t-tests to compare each group to every other group. This approach is known as multiple comparisons or post hoc testing.\n\nTo perform pairwise t-tests, you would calculate the t-value and associated p-value for each pair of groups. The null hypothesis for each pairwise comparison is that there is no significant difference between the means of the two groups. The alternative hypothesis is that there is a significant difference between the means of the two groups.\n\nThere are several methods for conducting pairwise t-tests, including Tukey\'s HSD (honestly significant difference) test, the Bonferroni correction, and the Scheffé method. These methods adjust the p-values to account for the fact that multiple comparisons are being made, which reduces the risk of making a type I error (rejecting the null hypothesis when it is actually true).\n\nIn general, the ANOVA approach is preferred over pairwise t-tests because it provides a more comprehensive and efficient way to test for differences among multiple groups. However, if the ANOVA results indicate a significant difference among the means of the groups, post hoc testing can be used to determine which specific groups are significantly different from each other.\n\nMany statistical software packages, including R and Python, have functions for conducting pairwise t-tests with various correction methods. For example, in R, you can use the `pairwise.t.test()` function to perform pairwise t-tests with the Tukey HSD, Bonferroni, or other corrections. Here is an example code in R:\n\n```\n# create a data frame with your data\ndata <- data.frame(\n  name = c(""observation 1"", ""observation 2"", ""observation 3"", ...),\n  number = c(0.372, 0.264, ...),\n  label = c(""A"", ""B"", ...)\n)\n\n# perform ANOVA\nmodel <- aov(number ~ label, data = data)\n\n# perform pairwise t-tests with Tukey HSD correction\nTukey <- TukeyHSD(model)\nTukey\n``` \n\nThe `pairwise.t.test()` function takes the form `pairwise.t.test(data$number, data$label, p.adjust.method = ""method"")`, where `data$number` is the response variable, `data$label` is the grouping variable, and `p.adjust.method` is the correction method. The `TukeyHSD()` function provides the Tukey HSD correction for pairwise comparisons.', ""The ANOVA that other people suggest only works if you **explicitly don't care which of the five groups are different**, and just want to know whether **any of them differ**\n\nThat's almost never the case, but if it is what you want to know, fair enough\n\nOtherwise you need the tests which are usually called post-hoc tests because often done after an ANOVA. But they don't depend on the ANOVA and you can just do them."", 'Good point about not needing to do the ANOVA first. I think it is unfortunate that the convention is now to call tests such as the Tukey hsd “post-hoc” since they should be specified a priori but there is no going back.', 'One way ANOVA.  The number is your dependent variable and A thru E is ‘group’ or independent variable.  Excel can do this, as well as R, Minitab, etc.']",4,8,https://www.reddit.com/r/statistics/comments/12jlwcy/qtesting_a_difference_for_statistical_significance/
467,2023-04-12 19:47:36,[Q] How to convert effect size (derived from meta-analysis) to another unit for downstream analysis (cost-effectiveness analysis)?,"Hey everyone,

I'm trying to conduct an Indirect Treatment Comparison (ITC) between two treatments for pain. I would then like to conduct a Cost-Effectiveness Analysis (CEA) between the two treatments based on the findings of the ITC. There is a recent meta-analysis of randomized controlled trials (RTC) for one of the treatments that I think is suitable for the ITC analysis. However, they report their findings in effect size instead of EQ-5D values (which from what I understand is the necessary measure required to then conduct CEA).

My question is:

From my understanding, I can perform the ITC between the two treatments using effect size and standard error values only. I do not need to convert effect size to EQ-5D values, for example, in order to conduct the ITC. Please correct me if this is wrong!

Then, I would somehow need to derive EQ-5D values from the effect size derived from the ITC. How do I go about this? Do I simply multiply an EQ-5D value (derived from the literature) for one treatment by the effect size to then obtain an EQ-5D value for the other treatment? I don't think this is the correct way but I have no idea.

Thanks for any help you can provide with this--really appreciate it!!","['>Do I simply multiply an EQ-5D value (derived from the literature) for one treatment by the effect size to then obtain an EQ-5D value for the other treatment?\n\nAbsolutely not! \n\nYou would need to determine if the outcome from your meta-analysis is predictive of quality of life, and if so, the appropriate functional form. Only then can you think about applying a treatment effect.', 'There is a meta-analysis for different injectable treatments for knee pain. They used a disease-specific outcome measure that is distinct from EQ-5D. However, there is a conversion factor between the disease-specific measure and EQ-5D that can link the two. The problem is that the meta-analysis reports their findings in effect size . Is there a way for me to convert the effect size to the disease specific outcome?\n\nThanks a lot for your help!']",4,2,https://www.reddit.com/r/statistics/comments/12jjjze/q_how_to_convert_effect_size_derived_from/
468,2023-04-12 14:28:54,[Q] Calculating mortality rate from survival rate," I'm working on a meta-analysis, and one of the studies has the survival rate given. Can I calculate mortality from the survival rate? For eg if survival is 35/50, then can I do 50-35=15, so 15/50 is the proportion of people who died?",[],4,0,https://www.reddit.com/r/statistics/comments/12jcn71/q_calculating_mortality_rate_from_survival_rate/
469,2023-04-12 12:35:56,[Q]Difference between rejecting the null hypothesis and accepting the null hypothesis,"I have been thinking about how we do not accept a null hypothesis if we reject it, and I am not sure if i do not understand it well enough, what  I think is that we do not accept the null hypothesis because when we fail to reject the null hypothesis we are only saying that the alternative hypothesis is incorrect but that does not make it impossible to another alternative hypothesis to appear and this one be correct. Please let me know if this is correct

&#x200B;

In case that the last paragraph is correct then I do not know why we say that we do not accept the null hypothesis if this is based in how we think things are, would it not be more appropiate to say that the null hypothesis is correct when we compare it to the the alternative that we just reject, because we do not know which alternative hypothesis might make us reject the null

Thank you","['The wording I have consistently learned (and taught) has been\n\n\\> we do not have evidence to reject the null hypothesis.\n\nThat, regardless of what the alternative hypothesis is, we are speaking in terms of rejecting the null hypothesis (or not). \n\nThe reason for this is that for common statistical tests like t-tests we are proposing a distribution with known parameters as the null hypothesis so that we can test how likely it is that the observations we have made came from that distribution. The alternative hypothesis is often simply ""the distribution has different parameterization"". In the case of a t-test that is to do with the mean of the distribution, but just simplifying here,', 'So the reason we cannot say that the null hypothesis is correct/incorrect is because the basis of hypothesis testing is making an assumption. When we state a null hypothesis, we are assuming that it is true (or equal). Therefore, the results of our hypothesis test is saying: based on our testing, we do not have enough evidence to reject the null hypothesis OR we have enough evidence to fail to reject the null hypothesis. \n\nI think i’m rambling, but what i’m trying to say is our main goal is to see if we have enough proof to reject the null hypothesis, so either we had enough proof and yay we rejected or we didn’t have enough proof and we failed to reject it. \n\nHope this clears it up a bit', ""Imagine if I have a coin. I want to test if it's a fair coin. If I don't flip it, there is insufficient evidence to reject the null hypothesis of fairness, but it doesn't make it a fair coin."", 'Suppose the null hypothesis is that a given coin is fair. We flip the coin 100 times and it lands heads up 98 times. Can we reject the null hypothesis? Yes. If the null hypothesis were true, such an outcome would be extremely unlikely.\n\nBut suppose instead the coin lands heads up 53 times. Certainly we can’t reject the null hypothesis. But can we *accept* the null hypothesis as true? Can we conclude that the coin is fair? Well no, we don’t know that the probability of landing heads up is exactly .5, of course. I mean, maybe P(heads) is .51 or .53, who knows.\n\nAll we can do is retain, or fail to reject, the null hypothesis.', ""To me, the main reason you (generally) cannot accept the null hypothesis is that the likelihood that you reject the null hypothesis when there is no effect present (type I error rate/alpha, almost always set at 0.05) and the likelihood that you will fail to reject the null hypothesis when there *is* an effect present (type II error rate/beta, set variably, but often at 0.2) are not the same.\n\nThat is, consider that you have set your effect size of interest and you have enough statistical power to detect an effect at least 80% of the time. If you fail to detect an effect, there is still a 20% chance that an effect was there, you just did not observe it. This is still well above the generally accepted error rate of 5% for type I errors. \n\nOverall, the point is that asking whether a difference exists between two groups is a different question to whether no difference exists between two groups, so you can't really use the one test to answer both (unless you are a Bayesian ;D)""]",32,20,https://www.reddit.com/r/statistics/comments/12ja2p4/qdifference_between_rejecting_the_null_hypothesis/
470,2023-04-12 12:31:07,[Q] Differences between the likelihood ratio test and the power function,"I have been reading about the likelihood ratio test and how this allows us to compare two different parameters to decide wheter we reject the null hypothesis or we do not reject it. But I also know that the power function allows us to know the probability to reject the null hypothesis. So I do not understand well enough why would we need the LRT if we have the power function and viceversa, what is the diffence in the purpose of both of these methods","[""The power function is not a method. It's an intrinsic property of *any* given hypothesis test. \n\nOn the other hand, the LRT for a given set of hypotheses is just one, amongst many, possible way of choosing a test. It has a power function, just as do any of the others. Other potential choices are score tests, Wald tests, sequential tests, or even ad hoc tests imagined up by a tester. They all have their own particular way of defining a rejection region, which inherently determines the power function."", 'The power function is a *property* of a test. It is NOT a ""*method*"". Once you fully specify your rejection rule and the assumptions (along with relevant assumptions you want to compute power under, and a sample size), you should be able to compute your power function. \n\nA likelihood ratio test is one way of obtaining a test -- more specifically, it defines a way to obtain a *test statistic* (and consequently, to obtain a rejection rule), based on the ratio of two likelihoods. LRTs have some attractive properties. The power function of an LRT follows from the rejection rule of the test (and the assumptions you\'re applying under H1) -- the power function doesn\'t tell you when to reject. \n\nRather than just read *about* these things, you will understand better if you learn enough to perform some basic computations -- (a) given a test statistic, and some assumptions to apply under H0, to be able work out how to obtain a rejection region that  yields a desired significance level alpha\\*, and (b) to compute the power at any given effect size (given some set of assumptions under H1), and hence obtain a power function. This doesn\'t have to be for an LRT specifically -- the concepts of test statistic, rejection rule and power are quite general. Once you\'ve actually gone through that process a few times, the concepts should be crystal clear.\n\nIndeed, the power computation can be done using a computer -- at any point under the alternative, you can simulate from the situation the assumptions you make (under H1) specify, and see what the proportion of rejections is (along with its standard error which should be quite small if you\'re doing enough simulated samples at that effect size). If you do that across many effect sizes, you will see the power function as a function of the effect. \n\nHere\'s a comparison of power curves for three (paired-differences/one sample) tests:\n\nhttps://i.stack.imgur.com/UEAaD.png\n\n... there being compared under the assumption that the distribution of the pair-differences had a t distribution with 3 degrees of freedom.\n\nThere are several different power curves (power functions) here:\n\nhttps://stats.stackexchange.com/questions/367125/how-to-graph-wilcoxon-test-power-r/367128#367128\n\nconsdered in each case for some specific set of assumptions (sufficient to simulate) under which the power of some tests were being compared.\n\n\nYou might also be interested to look at the power function as a function of n given some effect size, but it\'s usually not as interesting to look at because of the way standard errors of statistics nearly always scale in proportion to 1/√n (which I refer to as the ""root-n"" effect), at least for sample sizes that aren\'t too small. Once you have one point (or in some situations, perhaps two), you can usually work out the power at other values of n immediately from that. By contrast the power function considered as a function of effect size can be a at least slightly more ""interesting"" to look at (though the broad properties may still be pretty obvious in most cases, even if not quite as numerically tied down).\n\n---\n\n\\* i.e. so the test doesn\'t exceed a given type I error rate when H0 is true -- though you\'ll want it to be as high as possible while respecting that constraint', 'Ah ok this really make sense, but then can i say that the power function, rejection region, etc are a consequence of the test, in this case LRT. \n\nLRT(or any other method)->creates rejection region, power function, etc', 'Yes. A hypothesis test consists of a set of hypotheses and a decision rule. This decision rule determines all of the properties of the test, with the most relevant property being the power function. How you choose is somewhat arbitrary, in the sense that there are an infinite number of choices, but the typical route is to set a significance level and find a decision rule which gives the best power at certain alternatives. The LRT is nice in that it provides a general framework in choosing this decision region in a wide variety of testing situations. It generally has no guarantee to be the ""best"" choice, but in situations where it isn\'t the best it\'s typicality an intractable problem to determine what that best decision rule would be (if it even exists).']",1,4,https://www.reddit.com/r/statistics/comments/12j9ym5/q_differences_between_the_likelihood_ratio_test/
471,2023-04-12 05:00:53,[Q] How to think about power if my interest lies in in not rejecting H0,"I don't have experience in designing AB tests such as a price elasticity one, where **I want to see whether an increase in the price of my digital product causes sales numbers to drop**. In this scenario, a good result for the team would be to conclude that *""data does not suggest that price B generates less sales than price A, go ahead and raise its price"".*

I intend to use **conversion % as a binomial success metric** (sales / number of trials started). Considering this is a product that doesn't sell a large amount of units per day (let's say 4-5 sales on average, from something like 15 trials started).

Here, if I accidentaly underpower my test, I risk actually hurting company revenue, just because my data were too small. 

So,

**tldr: Are there specific good practices that must be taken into account if the ideal outcome of an AB test revolves around not finding significant differences between two groups? How many days are too many when stretching test duration to achieve a given minimum sample?**","[""Sounds  like what you really need is a nonferiority test (or perhaps an equivalence test,  but the description  sounds more like noninferiority). \n\nThen you'd care about power, because low power wouldn't establish what you require."", 'You’re hitting on an important issue with NHST which is that it can provide an absence of evidence but not evidence of absence. Absence of evidence could be due to issues such as being underpowered. To draw inferences about evidence of absence, you would likely want to implement something such as an equivalence test to test whether your confidence interval is practically equivalent to a region around zero, which suggests it’s approximately equivalent to 0', 'Compute a confidence interval. Then the uncertainty due to a low sample size will be evident.', ""This sounds like an economic question, I also don't see how this is applicable to using binomial distribution. The change in conversion % per change in price is what you want to know, is it not?"", ""after the write up, I realize calling this a price-elasticity study is not 100% accurate, for the reason you're stating there, I apologize.\n\nIn the end, we just want to see if a small bump in product price is expected to hurt conversion or not, not so much in the conversion change by any monetary unit.""]",7,5,https://www.reddit.com/r/statistics/comments/12ixijr/q_how_to_think_about_power_if_my_interest_lies_in/
472,2023-04-11 23:47:36,[Q] Help with (I think) binomial probability calculation,"I've tried looking this up online but can't quite find what I'm looking for. Would appreciate any assistance.

I have a deck of 20 cards, each is unique. I am drawing 5 cards from the deck. There are 3 cards I'd consider successes in the deck. No replacement. 

How do I calculate the odds of specifically drawing all 3 on one draw of 5 cards? Of drawing. 2 of the 3? 1 of the 3? None?

Appreciate the help. All the examples I found didn't involve both unique cards and multiple cards drawn without replacement. Cheers.","[""> How do I calculate the odds of specifically drawing all 3 on one draw of 5 cards?\n\nI presume you want *probability* rather than odds (they're not the same thing). \n\n\nYou can use the hypergeometric distribution\n\nhttps://en.wikipedia.org/wiki/Hypergeometric_distribution\n\nWhich in this case is easy enough to do by hand (well, maybe with a calculator) after a bit of cancellation.\n\nOr you can just use a computer ...\n\nIn R:\n\n    dhyper(0:3,3,17,5)\n    [1] 0.39912281 0.46052632 0.13157895 0.00877193\n\nIt can also be done in Excel\n\n     =HYPGEOM.DIST(0, 3, 5, 20, 0)\n\ngives\n\n     0.399122807017544\n\nand\n\n     =HYPGEOM.DIST(1, 3, 5, 20, 0)\n\ngives \n\n    0.460526315789474\n\netc"", 'This is a helpful way of conceptualizing it, I appreciate you writing all this out for me to digest. Thanks!', 'The odds are the number of ways you can get a success divided by the number of ways you can draw 5 cards.\n\nFor the denominator (number of ways you can draw 5 cards), you do (20 choose 5) = 20!/(5! * (20-5)!). \n\nFor drawing all 3 success cards, there are (3 choose 3) * (17 choose 2).\n\nSo you get (3 choose 3) * (17 choose 2) / (20 choose 5).\n\nFor 2 of the 3, it’s (3 choose 2) * (17 choose 3) / (20 choose 5).\n\nFor 1 of the 3, it’s (3 choose 1 )* (17 choose 4) / (20 choose 5).\n\nFor 0 of the 3  it’s (3 choose 0) * (17 choose 5) / (20 choose 5).\n\nA little trick is that for these kinds of problems, all of the numbers before “choose” in the numerator should add up to the number before “choose” in the denominator.  Same with the numbers after choose.  So for the “2 of 3” case, 3+17= 20, and 2+3=5.', ""This is very helpful, especially the Excel formula, since that's where I'm running the calculations (which I should have specified up front, first time asking stat questions on reddit).\n\nFollow-up: I'm making a probabilty tree (I think that's the right term?) for a sequence of several different draws. If I'm calculating the probability of drawing one specific card *without the other two cards that are considered 'successes' generally* from the deck (still when drawing 5 cards, no replacement), I originally thought the formula would be\n\nHYPGEOM.DIST(1, 1, 5, 20, 0)\n\nbut that yields 0.25. So then I tried \n\nHYPGEOM.DIST(1, 3, 5, 20, 0) / 3\n\nand that seems to (along with the rest of the formulas) average out to 1, meaning it checks out. Is that the right way to structure the formula? Thanks again!"", 'So the “choose” function is nice if you think about it in words.  Sometimes on a calculator it’s written “nCr”.  It’s the function that’s n! / (r! * (n-r)!).\n\nIn your head, you can think of this as a function that counts “the number of different ways you can select r things from n total things.”  So “20 choose 5” is short for “the count of different groups of 5 I can choose from 20.”  Thinking of it in words makes it easier.\n\nIt’s also may be a bit unclear why this works. \n\nImagine that when you choose 5 cards from 20, you do it like this:  First, shuffle the whole deck and lay it out in order front of you.  Second, separate the deck into the first 5 cards and the last 15 cards. \n\nThe number of ways you can shuffle 20 things is 20!.  So that’s the starting point for step 1.\n\nHowever, a lot of these shuffles lead to the same hand of 5, just in different orders.  A hand that’s ABCDE has the same cards as one that came out in the order CABDE, ACDEC, etc…  in fact, if we were to count the number of ways to rearrange those 5 cards, it would be 5!.  They’re all redundant and the same thing.  For the remaining 15 cards, they could come out in 15! orders.  \n\nSo, there are 5! * 15! different ways the two piles could get scrambled up and it would result in the same set of 5 cards being in our hand.\n\nWe want the total ways you can choose a unique set of 5 cards from 20.  When we shuffle the deck, there are 20! different orders. But only 1 out of 5!*15! is a unique set.  Or, rather, each unique set of 5 has 5! * 15! other shuffles that result in the same set of 5.  So we divide by that to get the number of unique sets of 5 cards we can draw from 20.\n\n20! / (5! * (20-5)!) = (number of ways to shuffle 20 cards) / (number of duplicates per unique outcome) = (number of ways to shuffle 20 cards) / ((number of ways to shuffle 5 cards )*(number of ways to shuffle 15 cards))']",1,6,https://www.reddit.com/r/statistics/comments/12io0dl/q_help_with_i_think_binomial_probability/
473,2023-04-11 22:13:51,"[Q] For an intention-to-treat analysis, what to do if new-users cannot be identified to prevent selection bias?","In observational (real-world evidence) studies about drug effectiveness, it's encouraged to restrict the study population to new-users. This can be e.g. done by including a washout period in which the drug has not been used (like non-use for 180 days before study inclusion). This is done to prevent selection bias (or prevalent user bias) that can bias the effect estimate. 

I am doing a RWE study, in which we want to do an intention-to-treat analysis because I am interested in the effects of *initiating* a drug vs. control. I however don't have pre-study inclusion data about drug use (or other covariates), so I cannot identify new-users.

If I include prevalent & incident users, I'm really biasing my results, so I was looking if there is anything I can do to mitigate this. I have follow-up data at 3-month intervals, and restricting the analysis to new-users at 3-months is also not possible since I don't have information on confounders beyond baseline. 

Does anyone know if there are any methods or new proposed techniques that can somehow deal with this at the analysis stage?","['I don\'t have any advice for you, I just wanted to object in the strongest possible terms to the misappropriation of the term ""intention to treat"" (ITT). It\'s not your fault, you didn\'t start it, but the concept is from RCTs and using it for so-called RWE is all part of the effort to pretend proper evidence is not really needed even when it\'s perfectly possible to do a pragmatic (""real world"") RCT.\n\nITT means that everyone randomised is analysed in the group to which they were randomised regardless of compliance or crossover. It is done to preserve the integrity of randomisation (because randomisation is done to ensure that the groups only differ by chance). ITT doesn\'t have any meaning in an observational study and you should find a different term because it is misleading.', 'I was confused about the term “intent to treat” population. Like you, I was thinking of the usual definition for RCT. \n\nI think you’re too hard on real-world evidence (RWE). I agree it shouldn’t replace an RCT, it’s but it’s useful when RCTs aren’t feasible or when used as a supplement to RCTs.', ""It's a technical term with a very specific meaning. It should not be co-opted."", "">This can be e.g. done by including a washout period in which the drug has not been used (like non-use for 180 days before study inclusion). This is done to prevent selection bias (or prevalent user bias) that can bias the effect estimate.\n\nHuh? How does this prevent selection bias? There can be all sorts of reasons people end up being new users of a drug that correlate with whatever outcomes you look at, and are not the effect caused by taking the drug,\n\nIf you don't have tons of covariates, so you can adjust for some of those reasons, it seems very unlikely you get to estimate the effect of the drug."", 'indeed, it’s embedded in the causal inference literature. ITT is used as a term to describe the causal contrast of interest that comes from the hypothetical target trial when we do trial emulation. There is ofcourse no such thing as ITT when there is no randomization, but when we emulate a trial with big data, the closest thing coming to ITT is iniating a drug & having no confounding by indication. \nIn observational data you cannot fully prevent confounding (ie unmeasured confounding), so you cannot 100% copy/emulate ITT. But it’s used to make the whole trial-thinking explicit.']",19,8,https://www.reddit.com/r/statistics/comments/12il53q/q_for_an_intentiontotreat_analysis_what_to_do_if/
474,2023-04-11 02:07:46,[Q] How to visualize correlation matrix over time?,The only technique I can think of is a heatmap as a gif. Other methods like a line graph won’t work because I have too many variables and it would get crowded quick.,"['Heatmap as a gif sounds like an interesting idea to me.', 'Just spitballin, but depending on how many timepoints you have and how big the visualization can be, you could potentially make a matrix of graphs (arranged like a correlation matrix) where instead of just having a correlation value in cell of the matrix, you could put a line graph of the correlation values over time.', 'What do you want to convey with the visualization?  \n\nIf you have a simple message, then you can devise a simple visualization.  \n\nThe inclination to show everything often comes from not knowing what you want to say.', ""If you aren't willing to just do a bunch of line plots (understandable -- 100 variables means 4950 correlations each being tracked across time), I think it's best if you look into something like PCA for multivariate time series - there's lots of approaches out there (dynamic factor analysis, etc.), mainly in the area of econometrics and financial time series, AFAIK. The idea would be to plot changes in the underlying (lower dimensional) components.\n\nIt does complicate the interpretation somewhat, but with that many variables, it may be the only solution."", 'Maybe some 3D grid with one axis showing “time,” and each cross section being a timepoint?']",43,22,https://www.reddit.com/r/statistics/comments/12hqtxs/q_how_to_visualize_correlation_matrix_over_time/
475,2023-04-11 00:01:01,[Q] How do I set up my data to perform Wilcoxon Signed Rank test?,"Sorry in advance if this is confusing, but I'll try my best to explain the layout of my data and what I am looking for.

Firstly, my data is set up like this:

I have about 100 participants and 6 speakers who produced 3 variables (let's label them a, b, c) which each have 3 variations within them. Participants each heard one variable from each speaker (for example participant 1 would hear Speaker 1 variable a, Speaker 2 variable b, Speaker 3 variable c, Speaker 4 variable a... etc.).

I've already checked the data and I have a non-normal distribution within the data. 

I now want to test the statistical significance of the variables from one another using the Wilcoxon Signed Rank test, and I have been advised by my supervisor, I should be using the difference (subtracted from the baseline - in this case, variable a) to calculate. So basically he wants a - b, and a - c. 

Now where my problem lies... if my participants only heard one variation from each speaker and they were balanced (participants hear 2 a conditions from different speakers, 2 b, and 2 c) how do I set my data frame up to calculate these differences? In a past experiment, my participants heard all variations, so I could easily calculate the difference and run the test since each participant heard the baseline condition, however in this structure of experiment that is not the case. 

&#x200B;

Any advise would be extremely helpful!!","['If each row/observation has a separate column for each condition, that should be the appropriate dataframe for a Wilcoxon signed rank']",2,1,https://www.reddit.com/r/statistics/comments/12hn3ha/q_how_do_i_set_up_my_data_to_perform_wilcoxon/
476,2023-04-10 19:30:42,"[Q] Newbie to Statistics, I have 109 sample stdev is that normal?","For a context, I am trying to do data visualization on count of product by category, there are 6 category and grandtotal of 1500 prosucts. The calculated mean was 214 and stdev.s was 109.
As i'm trying to do visualize this on a bell curve, is this stat normal? Or am i doing something wrong here","['This sounds like a chi square test, which you can use to see if the categories were sold an equal number of times. (or if they are significantly different from a prior distribution of the amount of products in each category, to see whether they are over/undersold compared to the base frequency of a category).', 'Are you calculating the mean and standard deviation per category ?  That may make more sense than calculating the grand mean and standard deviation.', ""So you have 1500 products, each of which is part of 1 of 6 categories?\n\nIn that case, the sample mean of the count of products per category is constrained at 1500/6 = 250. Something went wrong if you get 214, or the explanation you gave doesn't paint the full picture.\n\nThe count is likely not going to fit a bell curve, if anything counts are more suited to a Poisson-type model. However in this case there are some dependencies that might mess that up too. There is for example a degree of freedom lost in the last category. If you know the counts of the other categories and the total number of products, the last count is not a random variable anymore."", 'Before you even worry about bell curves and normality and things, I recommend you always just look at your raw data - in this case, a simple table of counts by category. For visualization, a simple [dotplot](https://core.ac.uk/download/pdf/153214994.pdf) works well for this kind of data (categorical on one dimension, and quantitative on another). Or alternatively a simple bar chart of counts or proportions.\n\nNow, to answer your other question of whether a 109 standard deviation is normal, I personally wouldn\'t put much trust in an estimate based on 6 numbers. (The magnitude doesn\'t look odd, or anything, though, fwiw -- if there was a huge positive skew you might have a SD that is ""inflated"" relative to your mean. For example, if you had a 200 SD with a mean of 200, that would be a red flag -- such a distribution would put 16% of the distribution below 0, which can\'t happen in a distribution of counts.)\n\nIt\'s also not a metric I would expect to perform well for this type of question in general (although it may not be bad for this particular set of numbers).\n\nFor ""counts of things"", something like a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution) is usually a better starting point (it will approximate a normal for large counts, so may not be better in this particular case, but will tend to be more appropriate in situations where counts are not large).\n\nAnother approach frequently used for this kind of data -- if you are looking for information about the uncertainty in the numbers of products within categories) -- is a [multinomial](https://en.wikipedia.org/wiki/Multinomial_distribution) model. In that approach, you would be estimating not the mean and standard deviation, but the proportion of the total (1500 in this case) allocated to each categories, and the uncertainty in those proportions. (And then you might use some other approach, like a Poisson, to model the total counts across different time periods, etc.)\n\nBut perhaps more importantly, I simply wouldn\'t (necessarily) assume counts of products across different categories to have a normal distribution in general. In fact, it\'s not unusual to have some kind of non-normal distribution when it comes to product counts. (See, e.g., things like [Pareto](https://en.wikipedia.org/wiki/Poisson_distribution) analyses, or various [""product quantity"" analyses](https://en.wikipedia.org/wiki/ABC_analysis) more generally -- note that I know nothing about conducting the latter, only that they exist!)\n\nEdit: saw your update that you want to test whether product sales are balanced across categories. For that, as someone else said, a simple chi-square test of counts will work.', 'Hi no im trying to calculate mean and stdev of numb of products by category.']",0,16,https://www.reddit.com/r/statistics/comments/12hfztv/q_newbie_to_statistics_i_have_109_sample_stdev_is/
477,2023-04-10 14:47:08,[question] 1-way ANOVA testing with incomplete data sets," 

Hello statisticians!!

I am analysing data from a health and QOL questionnaire for 20 patients who have received surgical treatment at different times pre-procedure, post-procedure and at 3 month intervals for 2 years.

I am looking to assess whether there is a statistically significant difference between the different time intervals. Not all patients completed each questionnaire so responses at each time interval vary from 16 to 20.

I initially performed a GLM ANOVA test on SPSS however due to incomplete data sets this only analysed 16 data sets.

Is there a way to perform ANOVA test to include the patients who did not complete every questionnaire or should I be looking at a different model of statistical analysis.","['Do you have dependent data? Like are patients measured multiple times, or are the observations independent/does a different patient belong to each timepoint? \nIt’s quite unclear to me what you are doing.\n\nIn case you do have dependent data (one-way ANOVA can’t be done), but GLMM (mixed models) are an alternative if you have missing data. ANOVA can only do complete case analysis, unless you impute the missings. But GLMM is quite annoying in SPSS. \n\nIf it’s not dependent data, there is no alternative apart from imputation. But this depends on why values are missing.']",3,1,https://www.reddit.com/r/statistics/comments/12h9ygv/question_1way_anova_testing_with_incomplete_data/
478,2023-04-10 10:35:03,[Q] Test for likelihood from a spec distr?,"If I have a normal distribution (known mean and SD), and a known sample, I have an idea how to estimate the probability the data point came from the distribution (look at either the quantile or do some kind of t-test?).

But what if I have three samples? This might be a tool issue -- I don't know of an R, JMP or excel function that takes N-values and checks likelihood they came from a defined normal distribution. 

For example, let's say I drew an 11, 14, and a 20 -- and I want a p-value for null hypothesis they came from a bin with mean = 12 and SD = 3?","[""> Test for likelihood from a spec distr?\n\nBeware using the term 'likelihood' as a synonym for probability (based on the switch to 'probability' in the next sentence, I'm assuming that was your intent). Likelihood has a very specific meaning in statistics, distinct from probability though connected to it. \n\n>  I have an idea how to estimate the probability the data point came from the distribution\n\nThe *probability* that any specific value came from *any* given continuous distribution (within its support) is 0.\n\nIf you're looking instead for a p-value, you need to specify a test statistic and a way to define your rejection regions (equivalently, a way to define the meaning of 'at least as extreme', e.g. 'larger absolute value of the statistic').\n\n> But what if I have three samples? This might be a tool issue -- I don't know of an R, JMP or excel function that takes N-values and checks likelihood they came from a defined normal distribution. \n\nIf you actually *do* want to use the likelihood as a statistic, then just literally use the likelihood (or a simple function of it), or base a statistic off the likelihood or log-likelihood.\n\n> For example, let's say I drew an 11, 14, and a 20 -- and I want a p-value for null hypothesis they came from a bin with mean = 12 and SD = 3?\n\nWait ... bin? as in binomial? i.e. with parameters n=48, p=0.25? \n\nYour opening sentence specified that you have a *normal* distribution with known mean and s.d. and now you're asking about a binomial? Was the normal just meant to be a different example? Why abandon it rather than pursue it? And why the shift from a continuous to a discrete example? That changes the considerations slightly.\n\nIf you really want to use the likelihood as a test statistic (it's not necessarily the best idea, depending on what sort of alternatives you are interested in), then we can pursue that but I want to be sure we're really talking about the same thing now. \n\nPresumably also you intend that the observations be treated as independent. You should clarify.\n\nPlease avoid edit to avoid any change between normal and binomial, though. Just stick with one or the other."", '> I think I meant ""bin"" as in ""urn"".  Let\'s say we have an urn containing marbles\n\nOkay, that\'s even worse than just the change to a discrete distribution, because now we\'re adding the issue of whether you\'re sampling with or without replacement.\n\n> So yes, it\'s a continuous variable\n\nIt can\'t be. There\'s a finite number of possible outcomes, \nthe number of marbles in the urn.\n\n> marbles are independent, \n\nWell, no. Back to the sampling without replacement issue. If you take a marble out, you can\'t sample that marble again, because it\'s no longer in the urn. If you don\'t mean that and you want to replace drawn values you\'d need to say so.\n\n>  want to know if the null hypothesis is non-significant that they came from that urn.\n\nOh, okay, you\'re definitely not sampling with replacement then, because all three are out of the urn at the same time (not possible if you\'re sampling independently) and they\'re *certainly not in the urn* at the end, so you\'re not replacing them.\n\n\n[Even if you made the number of marbles infinite (avoiding the distinction between replacement and not), it still couldn\'t be continuous, because it\'s still *countable*.]\n\n... this is more of a confusion of ideas than the thing I thought I was reading.\n\nUrn, die and coin models don\'t work for representing ways to obtain samples from continuous distributions.', '>  I am interested in the process. \n\nOkay, sure, that works. \n\nAs mentioned previously, what statistic you use depends on what sorts of alternatives you\'d like power to detect. There are various omnibus (in effect ""any alternative"", or at least nearly so) goodness of fit tests, but each will have more power or less power against different collections of alternatives -- you literally can\'t do the best at everything, there\'s no uniformly most powerful test.\n\nI don\'t follow the reasoning in your mention of a CI. What quantity is this a CI for, and if it\'s a CI for the mean, how would it pick up the possibility that the SD was too low but the mean was correct? How would you pick up correct mean and sd but wrong distribution-shape?\n\nWith the likelihood itself, you can construct a statistic from that, but as I mentioned it\'s not necessarily very satisfying. In this case it boils down to being equivalent to computing the sum of squares of z-scores (hence, a chi-squared test with n d.f., rejecting low-likelihood cases means rejecting large chi-squared values); the problem (yet again) is that low-sigma alternatives will not get picked up, but it will have good power for the mean being wrong or for the sd being larger than hypothesized. You could reject in both chi-squared tails of course, but that\'s rejecting for the likelihood being *high*(!)\n\n---\n\nA reasonable omnibus choice for the fully specified mu and sigma case is probably the ordinary\\* **Anderson-Darling** test. The Kolmogorov-Smirnov is another possibility, though there are many others. All have their advantages and disadvantages. \n\nOn the other hand, the most commonly used statistic for the unspecified mu and sigma situation is probably the Shapiro-Wilk test. It\'s a fine test, does pretty well in general and it\'s easy to find implemented in plenty of packages. It\'s a bit hard to do if you\'re trying to self-implement. \n\n---\n\n\\*  Beware: not to be confused with  the variant for the general normality case.', 'Hi! Sorry for confusion. I think I meant ""bin"" as in ""urn"". Let\'s say we have an urn containing marbles made from a process that generates them with mean = 12g, SD = 3g (or whatever I said before). So yes, it\'s a continuous variable, marbles are independent, and process outputs are normally distributed.\n\nI find 3 marbles in a baggie on a shelf. I want to know if the null hypothesis is non-significant that they came from that urn. One of them is pretty different (20g), but the other two aren\'t. Let\'s just say I know for sure that all 3 were all made together (but are independent samples of whatever urn they came from).', ""Ok, forget the urn.  I am interested in the process. If the marbles came from the same machine. And the machine's process is characterized to produce marbles with the given mean and SD.  There is a bucket of other marbles that confirm that characterization, but let's not worry about them.  There is no sampling w or w/o replacement. \n\nAnd the measure at hand is weight. Just assume test method variance isn't an issue and we know the weight exactly.""]",1,8,https://www.reddit.com/r/statistics/comments/12h4d72/q_test_for_likelihood_from_a_spec_distr/
479,2023-04-10 09:46:09,[Q] How can I conduct simple Indirect Treatment Comparison?,"Hi everyone,

I'd like to conduct a simple Indirect Treatment Comparison (ITC) between two treatments. There is no head-to-head data available in the literature. However, there are a few (5 or so) randomized controlled trials (RCT) comparing one of these treatments to placebo, and there's a single RCT comparing the other treatment to placebo. The patient populations and study designs between these RCTs is similar enough for comparison.

I know having 2 RCTs is not a lot of data, but from what I understand, it should be enough to conduct this ITC analysis. (Although, I have been told from different people that it's enough and that it's not enough. If it's not enough data, I'd just like to ask for an explanation as to why?)

Because my analysis is relatively simple, I believe I can use the following:

*The effect of intervention B relative to intervention A can be estimated indirectly as follows, using the direct estimators for the effects of intervention C relative to intervention A (effectAC) and intervention C relative to intervention B (effectBC):*

*effectAB = effectAC – effectBC*

*The variance of the indirect estimator effectAB is the sum of the variances of the direct estimators:*

*varianceAB = varianceAC + varianceBC*

*The corresponding two-tailed 95% confidence interval can thus be calculated as follows:*

*\[effectAB - Z0.975 x sqrt(varianceAB); effectAB + Z0.975 x sqrt(varianceAB)\]*

*Z0.975 here refers to the 97.5% quantile of standard normal distribution, which gives a rounded value of 1.96.*

Source: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4678383/#BX1](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4678383/#BX1)

I'd just like to ask if the above equation correct and applicable for my analysis.

My understanding is that if the ITC is more complex, then network meta-analysis using either a frequentist or a Bayesian approach.

I appreciate any help you can provide with this!!","['What you have described above as a ""simple ITC"" is commonly known as ""[Bucher\'s method](https://pubmed.ncbi.nlm.nih.gov/9250266/)"" in comparative effectiveness research.  (I know, I know, it\'s a bit crazy that applying a basic property from Stats 101 warrants this type of credit). The formulas you have summarized above are correct in general. For one comparison, you have 5 trials comparing treatment A vs placebo (C) and the other comparison of treatment B vs placebo (C) has one trial available. Typically in each trial the difference of each active treatment vs placebo would be studied (e.g., hazard ratio on OS for treatment A vs C). The indirect comparison for treatment A vs B is simply the difference of differences: i.e., mean(A vs B)=mean(A vs C) - mean(B vs C) and variance var(A vs B)=var(A vs C)+var(B vs C).\n\nYou state above you have 5+1=6 RCTs but then later go on to say 2 RCTs. I think you mean you have 2 general comparisons (A vs C and B vs C) and 6 RCTs (5 RCTs for A vs C and 1 RCT for B vs C). You will need to think about how to pool the 5 RCTs for A vs C if you plan to conduct Bucher\'s ITC. One approach could be to pool the results for A vs C using standard meta analysis approaches and then conduct a Bucher ITC. Alternatively, a network meta-analysis could be conducted. Depending on how the analyses are conducted, the two approaches may yield very similar results.\n\nNote, this is all assuming you have conducted a proper systematic literature review to identify all relevant trials (and publications) and a feasibility assessment to confirm a Bucher ITC / NMA are feasible and appropriate.']",19,1,https://www.reddit.com/r/statistics/comments/12h3651/q_how_can_i_conduct_simple_indirect_treatment/
480,2023-04-10 02:43:37,[Q] Logistic Regression : Classification vs Regression?," I have noticed that Logistic Regression ([https://en.wikipedia.org/wiki/Logistic\_regression](https://en.wikipedia.org/wiki/Logistic_regression)) is a model that used significantly for both Regression problems and Classification problems.

When used for Regression, the main purpose of Logistic Regression appears to be to estimate the effect of a predictor variable on the response variable. For example, here are some examples in which Logistic Regression is used for Regression problems:

&#x200B;

* **Modelling of binary logistic regression for obesity among secondary students in a rural area of Kedah** : [https://aip.scitation.org/doi/pdf/10.1063/1.4887702](https://aip.scitation.org/doi/pdf/10.1063/1.4887702)
* **A logit model for the estimation of the educational level influence on unemployment in Romania** : [https://mpra.ub.uni-muenchen.de/81719/1/MPRA\_paper\_81719.pdf](https://mpra.ub.uni-muenchen.de/81719/1/MPRA_paper_81719.pdf)
* **A logistic regression investigation of the relationship between the Learning Assistant model and failure rates in introductory STEM courses** : [https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-018-0152-1](https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-018-0152-1)

When used for Classification, the main purpose of Logistic Regression appears to be to estimate the probability of the response variable assuming a certain value given an observed set of predictor variables. For example, here are some examples in which Logistic Regression is used for Classification problems:

* **Using logistic regression to develop a diagnostic model for COVID‑19: A single‑center study** : [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9277749/pdf/JEHP-11-153.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9277749/pdf/JEHP-11-153.pdf)
* **Logistic regression technique for prediction of cardiovascular disease** : [https://www.sciencedirect.com/science/article/pii/S2666285X22000449](https://www.sciencedirect.com/science/article/pii/S2666285X22000449)
* **A Study of Logistic Regression for Fatigue Classification Based on Data of Tongue and Pulse :** [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8917949/pdf/ECAM2022-2454678.pdf](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8917949/pdf/ECAM2022-2454678.pdf)

**Based on surveying such articles, I noticed the following patterns:**

* When Logistic Regression is being used for Regression problems, the performance of the Regression Model seems to be primarily measured using metrics that correspond to the overall ""Goodness of Fit"" and ""Likelihood"" of the model (e.g. in the Regression Articles, the Confusion Matrix is rarely reported in such cases)
* When Logistic Regression is being used for Classification problems, the performance of the Regression Model seems to be primarily using metrics that correspond to the ability of the model to accurately classify individual subjects such as ""AUC/ROC"", ""Confusion Matrix"" and ""F-Score"".

The interesting thing being that regardless of whether you working on a Regression problem or a Classification problem - if you do decide to use Logistic Regression, in both cases you can calculate Classification metrics such as the Confusion Matrix. Based on these observations, I have the following question:

**My Question:** Suppose if I am using Logistic Regression in a regression problem (e.g. estimating the effect of predictors such as age on employment vs unemployment) and the model seems to be performing well (e.g. statistically significant model coefficients, statistically significant overall model fit, etc.). Even though I technically still able to calculate Classification metrics such as the Confusion Matrix, F-Score and AUC/ROC - am I still obliged to measure the ability of this Regression model to successfully classify individual observations based on metrics such as ROC/AUC? Or am I not obliged to this since I not working on a Classification problem?

I feel that it might be possible to encounter a situation/dataset in which the goal was to build a Logistic Regression model for a Regression problem - and the resulting model might have good performance metrics used in regression problems, but might have poor ROC/AUC values. In such a case, is this a good Logistic Regression model as it performs well for the regression problem as intended - or is it a questionable model as it is unable to perform classification at a satisfactory level?

Thanks!","['It is the same technique: it does one thing, estimates the probability of some binary response as a function of a set of predictors.\n\nSometimes you are more interested in the responses, and sometimes you are more interested in the slopes (describing the strength of relationship with each of the predictors.) But you can\'t have one without the other.\n\nNotice that it *is not a classifier.* Read that again. Logistic regression never outputs classifications. It always outputs probabilities. (It can be the probability of belonging to a class, if that\'s the response variable you modeled.) \n\nPeople sometimes *use* those probabilities to classify things. Doing so throws away information, just like calling everyone who makes over $100,000 a year ""rich"" and everyone who makes less ""poor"" throws away information. \n\nIt\'s ok to do that, if you really only care about exceeding a threshold, and not about how close to the threshold you were. It is important to remember that the model does not seek to maximize number of correct classifications (it can\'t- that classification rule was a post hoc thing you stacked on top of the regression model.)\n\nIf you evaluate a logistic regression model with a classification metric, you are evaluating only a part of the model\'s performance. I will stop short of saying ""that\'s the wrong way to evaluate a logistic regression model"" - but at least be wary of using that as the *only* way you evaluate it.', '> \\- am I still obliged to measure the ability of this Regression model to successfully classify individual observations based on metrics such as ROC/AUC?\n\nHow would such an obligation arise? \n\n(Even in a classification setting, who is creating *obligation*?)', ""There's one additional detail you haven't mentioned.  If you want classifications from logistic regression, you have to choose the probability threshold.  You might assume it should be 0.5, but it depends on your appetite for sensitivity/specificity.  After fitting the model, you will have to make this additional choice.  There is an R function ROCR::performance specifically for this step."", 'Lots of things vary with the terms. If I had to guess, ""classification"" mostly occurs in machine learning context, where we want to make predictions, whereas ""regression"" is mostly used in the context of inferential statistics. I would also assume that a lot of logistic-regression-as-classification cases actually use penalized glm, not maximum likelihood (iirc that\'s actually the default in sklearn). Otherwise, it should be the same model, and you\'re always free to test the metrics you want, possibly on unseen data.', 'A statistically significant coefficient or model fit doesn’t really tell you whether the model fits the data well either. Its like with linear regression, you could have something really nonlinear like y=x^3 and if you fit a linear function to the data, the coefficient/model will still be significant, but the fit is not good. Same applies to logistic.']",10,6,https://www.reddit.com/r/statistics/comments/12grros/q_logistic_regression_classification_vs_regression/
481,2023-04-10 00:09:44,[Q] I'm a poker pro with a straightforward sample question for how to calculate the significance of a value derived from a small sample,"I'm hoping if I can learn the formula once, then I can use it to establish a mental framework for future estimations. I'll provide the sample question first, then go into more detail for those who are curious.

**The question:** Group A makes an action 10% of the time. Group B 20%. Group C 30%.

A player can be in only one group.

Player 1 has made the action 4 of 5 times. Player 2 has made it 6 of 30 times.

What are the odds that each player resides in each group?

**The backstory:** In poker, decisions should be based partly on the tendencies of your opponents. In live games this is done through observation and memory. Online, pros typically use permitted software that records all data & displays it in a customizable HUD. There are hundreds of possible values to display because there are 4 rounds of betting, seat positions, preceding actions, etc.

My primary HUD currently has 58 values plus the number of hands I've played with each opponent. When you hover your cursor over each value, it displays the sample size (i.e. (13/171)) (this denominator is of course different than the number of hands you've played with them because many actions are only possible in a minority of hands, e.g. no data for the 4th round of betting if everyone folds in the 1st round). Once I've played thousands of hands with someone, most of the values are sufficiently fleshed out to be mostly useful. However, for less common actions or for opponents I've only played dozens or hundreds of hands with, the values are less useful & even likely to be misleading.

I've made very effective use of HUD data, but I often have to make a quick assessment of how much weight to assign a value in swaying my action one way or the other. Sometimes you're completely on the fence & can use anything to sway your decision (the way someone put the chips in live, or a minuscule sample size online); whereas other times you're inclined to take an action 99% of the time, but an extreme data point (or combination of data points) over a significant sample will allow you to make that non-standard decision. Point is, it's not optimal to think, ""Eh, I'll wait till I have a robust sample on this opponent to apply any of the data."" It's a sliding scale from the first hand you play together.

Let's say the example above is for a player raising on the flop (2nd round of betting), & in a specific situation I would fold to a player in group A, call if group B, and re-raise if group C. If the data for that opponent is 63/198 I would comfortably assume they were in group C & re-raise. However I only have a vaguely intuitive sense for smaller samples. Sometimes it may be (4/4) so I'll think, ""ya, good enough chance they're a maniac"" & re-raise, but sometimes the night goes on & the value is (6/42), & I realize they were probably just on a hot streak when they first sat down.

I *might* be able to calculate the grouping odds when the value is 100%, but even then I could be wrong: 1% chance a (4/4) player is in group A, 16% group B, 83% group C. (take each group's tendency to the 4th power, & divide that value by the sum of the 3 values). But I don't know where to begin in calculating my example at the top.

Thank you for any help & further insight into the statistics at play here!","['This is straightforward bayes. But you need a prior distribution, what percentage of people are in each a,b and c groups. If 99.999 percent of people are type A, even if you see it done 4 out of 5 times, the person is still likely a type A person.\n\nThe way to do the update quickly is to look at the bayes factor https://en.m.wikipedia.org/wiki/Bayes_factor\n\nOr slightly less dry: https://youtu.be/lG4VkPoG3ko\n\nIn the poker setting, getting the base rates right will depend on your player pool. If you are a pro and making an investment into this, then buying hand histories for the site and stake you play can give you an idea of this. The “advantage” of gto style play is that it is theoretically indifferent to player pool.', ""Take a look at the beta distribution. In reality, there aren't really discrete categories like you're using. Instead, there's some likelihood that an individual's decision will be A or B. Beta allows you to put confidence around a point estimate of the individual's likelihood choice."", ""The prior distribution thing is a fundamental component I hadn't even considered! It makes so much sense though.\n\nI think in the HUD program I can generate a histogram for a given stat. I'll just filter the player pool down to a range of stakes & players with >x hands.\n\nAll of this is just to generate samples to get an idea of how to make better mental approximations on-the-fly. I won't be referencing anything directly to make a specific decision, so it doesn't need to be super accurate, but using groupings & prior distributions from those histograms will give me realistic study material.\n\nMy own database is approaching a million hands overall & should be sufficient. Buying & selling hand histories may be frowned upon, if not mentioned somewhere on a user agreement or restricted programs page, because the HUD won't even let you collect data by just observing a table you aren't being dealt into. Though I've heard of pros sharing data to develop strategies for major heads-up rivalrys. Anyway that won't be necessary here."", ""That is the Bayes Theorem:\n\nX=Player Raising in the flop    \nY=Player is in an aggressive table.    \nP(X|Y)=P(Y|X) \\* P(X) / P(Y)\n\nIn your example you are looking for the probability of raising in the flop given the player is in a table of high raises (this is your mental model and models the idea of having 3 groups, high, mid and low raising type of tables).\n\nP(Y|X) is tricky, it's sometimes called the likelihood, it is the probability of the player being in an aggressive table given he rises in the flop.    \nTo have this, you need some knowledge of how the player behaves in different tables. Let's assume you know this player was in an aggressive table 40% of the times he raised in the flop in the past.    \nTherefore,    \nP(Y|X)=40%    \nP(X)=32% the one coming from 63/198. Sometimes called the prior.    \nP(Y)= assume 20% of tables are this aggressive. (depending on your mental model).\n\nThen the probability of raising in the flop given the player is in a table of high raises=64%.\n\nWith this value you may now contrast it against your current observations in the table, let's say you observe him raising in the flop 80% of the time in your aggressive table, you know this is higher than your 64% expectation and you might think he has a very strong hand (or he might think he is in a more aggressive table than you think.)\n\nThere are a lot of assumptions made so be mindful to incorporate all information you can to come up with a conclusion."", ""Yeah, for mental calculations, there is a simple rule for obtaining a beta posterior from a beta prior.    OP could do it dirty on the fly, since it's easy to visualize variations of beta distributions (after training with feedback).  \n\nAlso, probably less useful for mental calculations, but there is a multi dimensional variation of the beta distribution called the Dirchlet distribution.... If OP really wanted to get the finest resolution possible, then program some Dirchlet inference into the HUD.""]",37,13,https://www.reddit.com/r/statistics/comments/12gnknx/q_im_a_poker_pro_with_a_straightforward_sample/
482,2023-04-10 00:05:12,[Q] How is Pascal random variable sum of k independent geometric random variable?,"My self study question asks me to find the expected value of Pascal(x, k) random variable distribution.

The  answer sheet says: ""because X\_k is essentially the sum of k independent geometric RV: X\_k = sum(Y\_1...Y\_k), where Y\_i is a geometric RV with  E\[Y\_i\] = 1/p. Then E\[X\_k\] = k \* E\[Y\_i\] = k/p.""

I  understand how we find expected value after converting Pascal to  geometric but I can't see how we convert it. I tried to search online  but the two results I found were an inductive proof and a Moment  Generating Function, both of which are out of topic for this lecture. I  would appreciate any help.","['You will get more hits for ""negative binomial distribution"" than for ""Pascal distribution."" But the proofs will be about the same. If you are really good with infinite sums you can directly evaluate the sum of x * f(x) for x from 0 to infinity, but I think most everyone finds induction or MGFs easier.\n\n""How we convert it"" (from pascal to geometric) is simply, if the Pascal distribution is the time it takes to observe k successes, we can write that as the time until the first success + the time between first and second successes + ... + the time between the k-1th and kth successes. Since expectation is linear, you can always find the expectation of a multi-step process by adding the individual steps.', 'A Pascal distribution models the number of trials until the k^th success, where each trial has success probably p. You may also find more information by searching for the ""negative binomial distribution"".\n\nThink of it this way: the number of trials until the first success is one geometric RV. Then from the first success to the second success is another geometric RV. So on and so on, until the total number of trials until the k^th success is the sum of the trials until each individual success, which is the sum of k geometric RV\'s.', 'I get it now, thank you.']",0,3,https://www.reddit.com/r/statistics/comments/12gngd1/q_how_is_pascal_random_variable_sum_of_k/
483,2023-04-09 23:52:32,[Q]can a non-random sample produce an unbiased estimate of population proportion.,,"[""Clearly, a biased sample -- one where not every member of the population has an equal chance to appear in the sample -- *can* still turn out to have the right value on average.\n\n*Guaranteeing* that it will, across all the possible population distributions you might have, though, and all the various kinds of non-random sampling you might be dealing with -- that's going to be *hard*. \n\nEssentially impossible, unless you restrict your non-random sampling to very specific kinds of designs built around making the expected values of particular quantities of interest work out correctly (which is not what most people are doing when they engage in non-random sampling).\n\nIn most cases, *random sampling* is much easier to engage in than nonrandom sampling that works. Some forms, though do work and some of those are commonly used (like stratified sampling, which is basically random sampling of identified subsets of the population; it still relies on random sampling, but attempts to make sure that the strata each get sampled at some fixed fraction of the total sample size). \n\nOutside deliberately-designed nonrandom sampling, you're pretty much relying on luck. For sampling that's done in some haphazard/happenstance way (like convenience sampling, say), this is really not going to work out, in a similar sense that all the nitrogen (N2) and oxygen (O2) molecules in a small balloon *could* by chance briefly sort themselves into nitrogen at the top and oxygen at the bottom -- but they don't."", 'Yes but not necessarily. Suppose any subset of your population is representative of your population. Then choosing any subset nonrandomly would suffice.', ""If by non-random, you mean that not everyone in the population has an equal chance of being in the sample, you can still get an unbiased estimate if the probability that each person is selected is unrelated to the thing you are trying to estimate. For example, suppose people born on Sunday are more likely to be in the sample than people born on other days. If you are estimating average height, your estimate would be unbiased, because height is unrelated to one's birth day of the week."", '> You need a representative sample to get an unbiased estimate. \n\nUsually but not automatically correct. By way of counterexample  ... imagine you have some symmetric distribution (say on the values 1,2,3,4,5,  like a Likert item), and your sampling was nonrepresentative in the following way: values above (/below) the mean got more likely to be in your sample by their distance from the mean (your approach led to better chance of seeing the more polarized opinions), but without regard for direction. \n\nThen the sample mean (and median etc) would be unbiased for the population counterpart.\n\n(edit: lost the ""below"" part in editing while composing the above on my phone -- I put it back)', '**Bias**, in statistics, is the difference between the expected value of the estimator and the population parameter it is trying to estimate. And the **expected value** of a random variable is the average value it takes, where you calculate the average by summing all possible values it can take, weighted by the probability it takes that value.\n\nSo, for example, if I have a random variable that takes the value 1 with probability 10%, the value 2 with probability 40%, and the value 3 with probability 50%, then its expected value is 1x.1+2x.4+3x.5 = 2.4.\n\nWhen we talk about the expected value of a sample estimator, we are looking at the possible values it can take across every different possible sample, weighted by the probability that we draw that particular sample. If, by ""non-random"", you mean the sample is completely deterministic, then there is a single value the estimator can take so its expected value is whatever that value is - so the only way for the estimator to be unbiased is for that value to be exactly equal to the population value. That\'s basically impossible except under very specific circumstances (for example, if whatever you\'re trying to measure is perfectly determinable from the information you get from the sample).\n\nIf by non-random you just mean ""not a simple random sample"", for example if you select people from the population with unequal probabilities, then we would actually still call that a random sample and there are plenty of ways to construct unbiased estimators as long as (a) every person in the population has a non-zero probability of being selected in the sample, and (b) you are able to calculate the probability of someone being in the sample either beforehand or during the selection process.']",2,8,https://www.reddit.com/r/statistics/comments/12gn44w/qcan_a_nonrandom_sample_produce_an_unbiased/
484,2023-04-09 21:45:08,[Q] Have some questions about the feasibility of a Research Project I am doing on social media Echo Chambers. Any help would be greatly appreciated!!!,"So I'm currently doing a descriptive research project titled ""Which demographics are in social media Echo Chambers within the US""

I will be using the ANES 2020 Social Media Study available here: https://electionstudies.org/data-center/2020-social-media-study/

I am planning on creating an index of variables which are associated with a typical user in an echo chamber that align with a social media account in an echo chamber, and then using answers from this survey such as ""Do you get your news from x platform"", ""how many of your Facebook friends are Dem/Republican"" amongst others to assign each respondent with an ""echo chamber score"", before regressing these against binary demographic variables to see which are associated with a higher echo chamber score. I just have a couple of questions:

- The variables in this datadet are mostly associated with Facebook. Would it be better to limit the scope of the project to Facebook?

- Are there any better datasets out there that anyone knows of that would be better for me to use? (I am not allowed to scrape twitter/Facebook due to ethics concerns according to my uni unfortunately).

- Is it okay for me to arbitrarily assign weighting to each of the variables in my index that make up the ""echo chamber score""?

- Does anyone know of any existing literature about which demographics are in echo chambers? I've tried looking but I can't find anything.

- The ANES dataset has post-election and pre-election (2020 presidential vote) data. Is there any way I can use this? Should I remove data that is from one half? I really am not sure what the best way to approach this would be.","[""From a quick look, it looks like the data is only about those that use Facebook: 'The ANES 2020 Social Media Study will allow direct linkage of survey responses with data from participants’ Facebook accounts', so that's your scope unless you have some other data.\n\nYou can have arbitrary weighting in creating the indexes, but you need to defend it and it has to make sense.  If someone say, why didn't you make the weighting like so and so, you need to have an answer for it.  Your inferences will be limited to the index that you create.  Really, all indexes have some arbitrary components to it. \n\nPre and post election would be a useful stat to see if the election have any effect on your echo chamber effect.  At the minimum, you can make some nice summary tables and graphs from it to see if there are any obvious differences;  compare your index and important exgenous variables pre and post election.\n\nFor your regressions, you can use difference-in difference or maybe a simple F test to see if the election have any impact."", ""Thank you so much for your response!! There are variables in the dataset containing participant responses to questions on twitter but yeah I may just have to limit the scope of the project.\n\nThank you as well for your point regarding the arbitrary nature of the index. I'm glad to hear that it's okay to be relatively arbitrary; I intend to justify the weighting using the existing literature if and where I can.\n\nAlso will look into using a difference-in-difference test to see if the election had any impact, although I would imagine that I will go with F testing seeing as I'm more familiar with that.\n\nThanks again for taking the time to respond to me, really helpful and just what I needed to hear. Have a lovely rest of your day!""]",3,2,https://www.reddit.com/r/statistics/comments/12gjvi7/q_have_some_questions_about_the_feasibility_of_a/
485,2023-04-09 16:41:00,[Q] How do I state the hypothesis rejection after insignificant results?,"So, the hypotheses  suggest that x and y  intervention causes people to donate more. (as two different hypotheses)  There were created one control and two treatment groups for that.  Based on my data analysis, the treatment groups showed no significant effect on the donation.  Does that mean the hypotheses were rejected or there is not enough evidence to support them. How should I mention that in my thesis?","[""I think you are confusing the expectations that you as the researcher have about the effect of the two treatments, and the concept oh hypothesis in the hypothesis testing framework.\n\nWhile testing the effect of a treatment with Control and Treatment groups, we want to estimate the Average Treatment Effect (ATE), that under certain assumptions can be obtained by subtracting the average score of the treatment and the average score of the control group.\n\nWhat you want to test is whether if the ATE is significantly different from zero. As your results suggest that the treatments didn't have an impact on donation, then you do not have enough evidence to reject the null hypothesis."", ""You can either reject or do not reject the null hypothesis. If you reject the null hypothesis, this means you are 'accepting' the alternative hypothesis, which means there's an effect. If you do not reject the null hypothesis, this does not mean you are 'accepting' the null hypothesis. It simply means that there is no evidence. Absence of evidence is not the same as evidence of absence."", 'One way to say it is there is not enough evidence to draw a confident conclusion about  the directions of the differences or whether or not the differences are 0. Confidence intervals on the differences could be helpful to see what differences are not inconsistent with your data.']",2,3,https://www.reddit.com/r/statistics/comments/12gdlqb/q_how_do_i_state_the_hypothesis_rejection_after/
486,2023-04-09 15:46:58,[Q] Data presentation tools?,Anyone know of any neat data presentation tools? I tried looking around for private desktop versions of Gapminder or Google Data Explorer platforms but came up empty.,"['Different tools for different jobs. Most of the time, R is perfect for producing graphics. I tend to find the simplest graphics are the most effective. I tend to scorn bells and whistles, things like 3-D barcharts and pie charts will get heckled by other statisticians. You can also incorporate the use of tools like [Highnote.io](https://Highnote.io) to make your presentation more appealing.', 'What do you need to accomplish?', ""Depends what you're trying to do. Jupyter notebooks lend themselves well to presentations about data."", ""Ggplot2 and Rmarkdown (Quarto now, I suppose)\n\nIf you need to present a table, there's a lot of options to export them. I'm not above using the Snipping Tool.""]",16,5,https://www.reddit.com/r/statistics/comments/12gclr9/q_data_presentation_tools/
487,2023-04-09 13:01:27,[Q] How do I improve my approach towards this feature selection and model building for large multiclass dataset?,"I know this is technically assignment question.

The training data contains 9000 observations and 900 features. I have to build the model to predict the testing data, which contains roughly 5,000 observations and same number of features as the training data.

I am wondering that since there are so many features, we use PCA for feature selection? I tried random forest, lasso regression, but they are so slow. I am not hitting the good accuracy using the features given by PCA.

Should I use random samples for random forest or lasso regression?

For modeling, I notice that SVM overperforms compared to naive bayes, neural network, and linear discrimnant analysis. Should I just use SVM over the ensemble method, because of multiclass data? I am getting roughly 88 percent correct on hold one leave one out of training data, but 25 percent correct on testing data.

For the binary data, I was able to get good accuracy(not perfect) by lasso for feature selection and doing ensemble method of logistic regression, neural network, svm, qda, and lda.","['To improve your approach, try:\n\nAlternative dimensionality reduction techniques like t-SNE, UMAP, or LLE.\n\nEfficient tree-based algorithms (XGBoost, LightGBM) for feature selection.\n\nFeature engineering using domain knowledge.\n\nSmaller random samples for computationally expensive methods.\n\nEnsemble methods (bagging, boosting, stacking) with multiple models.\n\nRegularization and cross-validation to address overfitting.\n\nHyperparameter tuning with grid search, random search, or Bayesian optimization.', ""Don't forget for any feature selection methods you use, that you must do some internal validation (eg nested cross-validation) rather than using the whole dataset, otherwise you'll massively over-fit. This is a really common mistake in the literature and is discussed well [here](https://stats.stackexchange.com/questions/64825/should-feature-selection-be-performed-only-on-training-data-or-all-data).\n\nIn addition to the methods in others' comments, I've had good experiences with [Boruta](https://www.jstatsoft.org/article/download/v036i11/417)."", 'Thank you will do hyperparameter.\n\nQuick question: after i tested my model on leave one hold one out on training data to see the accuracy.\n\nWhen it is time to predict the testing data, do i use the whole training data or cross validation of training data.', ""Thank you, splitting the training data and using boruta on each of the splits turn out better.\n\nQuick question: after i tested my model on leave one hold one out on training data to see the accuracy.\n\nWhen it is time to predict the testing data, do i use the whole training data or cross validation of training data.\n\nI will try hypertuning, but it doesn't seem to give out a lot of improvement""]",15,4,https://www.reddit.com/r/statistics/comments/12g9ewh/q_how_do_i_improve_my_approach_towards_this/
488,2023-04-09 12:17:04,"[R] Moving estimators (agnostic) vs ARMA-ARCH-like philosophy (arbitrary dependence) on example of evolution of all parameters of Student's t-distribution: mu, sigma, nu","Plots: mu, sigma, nu evolution for Dow Jones Industrial Average time series: https://i.redd.it/e5c6kw62mlsa1.png , article: https://arxiv.org/pdf/2304.03069

For nonstationary time series there are dominating ARMA-ARCH-like models in hundreds of variations, assuming some arbitrary dependence type.

I wanted to discuss alternative more agnostic approach - shift local (EMA) estimators instead, e.g. allowing to estimate also evolution of nu for [Student's t-distribution](https://en.wikipedia.org/wiki/Student's_t-distribution) here, also  leading to better log-likelihood evaluation.

Which philosophy is more appropriate in which cases? Is there a literature for such moving estimators?

Also, this looking novel nu behavior is very interesting, e.g. heavy tails rho~|x|{-nu-1} nearly always but ~1970s when it goes to Gaussian (why???), the tails become much heavier just after WWII ... is it discussed in literature?","[""Just made a table comparing probabilities of extreme events with Student's t for DJIA series below ... something comparable for S&P500 can be found in https://www.sixfigureinvesting.com/2016/03/modeling-stock-market-returns-with-laplace-distribution-instead-of-normal/\n\nhttps://i.postimg.cc/v890LsJK/obraz.png""]",7,1,https://www.reddit.com/r/statistics/comments/12g8inu/r_moving_estimators_agnostic_vs_armaarchlike/
489,2023-04-09 11:02:06,[Q] t-test for ANOVA when testing treatment against positive & negative controls,"So in my experiment I have several treatment groups along with a positive control and a negative control.

I'm just wondering what kind of test / tests would be optimal to assess these data. When I run a one-sided ANOVA, the individual p-value produced between my negative control and one of my treatment groups (A) does not line up with what I get with a one-sided t-test between those same groups (neg control and A).

Should I maybe do two separate ANOVAs with our treatments vs our + control and our treatments vs our - control?","['I assume you mean ""one-way"".\n\nYou should drop your negative control from the analysis. You expect there to be no effect, but lack of statistical significance cannot mean there was no effect, so there\'s no point in including it.\n\nTest your treatments against the other control with ANOVA and then use Dunnett\'s post-hoc test to compare all treatments to control.\n\nIf your samples consist of single measurements for each condition (i.e. tbey are the average response of cells in a dish or the like) you should use random-block ANOVA with one factor being treatment and the other being experimental replicate. It can have much higher power than one-way ANOVA, and never has very much lower power.']",11,1,https://www.reddit.com/r/statistics/comments/12g6vux/q_ttest_for_anova_when_testing_treatment_against/
490,2023-04-09 07:14:21,[Q] Requesting Step-by-step Tutorial to Conduct Indirect Treatment Comparison or Network Meta-Analysis,"Hi everyone, I'd like to conduct a comparative analysis of 2 different treatments for a medical condition. There's no available head-to-head randomized controlled trial (RTC) between the 2 different treatments, so I'd like to conduct an indirect treatment comparison (ITC)/network meta-analysis. I tried looking for step-by-step tutorials to conduct this analysis on YouTube and also various webpages but was unable to find one openly available. I'd prefer to use WinBUGS or OpenBUGS, but I'd be open to trying on other open-source statistics software too, such as R.

Thanks for any help with this!",[],0,0,https://www.reddit.com/r/statistics/comments/12g1nuj/q_requesting_stepbystep_tutorial_to_conduct/
491,2023-04-09 05:28:25,[Q] How does error propagate across multiple different functions,"Hello, 

Since it's been difficult to find resources on error propagation, I'm having difficulty understanding more complicated scenarios. In many online examples I see:

    x,y --> f(x,y)
    error of f(x,y)=[df/dx df/dy][covariance matrix][df/dx df/dy]^T

Now variances and covariances can be calculated using x,y and df/dx and df/dy can be calculated using finite approximation (assume direct derivation of function not possible). But how about more complicated example?

    x,y --> f(x,y) --> g(f(x,y)) --> h(g(f(x,y)))

So now you have 3 transformations of the original data. How would one go about calculating the error of  h(g(f(x,y)))? Is it just

    h(g(f(x,y)))
    error of h(g(f(x,y))) = [dh/dx dh/dy][covariance matrix][dh/dx dh/dy]^T

Is dh/dx and dh/dy just simple finite approximation? How bout the variances and covariances? How are these calculated?","['The error seems to be the quadratic term in a tailor series expansion from the looks of it. Do the same with g first ie write the formula for error with dgdx etc and then apply the chain rule of partial derivatives. If the function is not linear you might not have a closed form. In that case you can try to use some Lipschitz continuity approximations maybe. This way you can bound derogated of h by derivatives of f I think.', 'Think about your errors as falling on a hilly surface and then ""shifting"" around at random.  You view the result directly from above.    You can imagine the shifts of a given magnitude are more likely in some directions (i.e. those with a shallower slope can move farther with less ""cost"").     This helps me visualize the way error propagates through layers of a model.  \n\nTo move away from the analogy and into analysis, the dimensions of your surface are the functions in your composition.    The gradient has the same dimensions.  The product of the gradient represents an application of the chain rule... So the gradient of your log-likelihood taken with respect to the functional composition is now related to the components of your gradient by either summation or product.      Once you\'ve done this exercise, you should see how the differentials in your post lead to the error propagation matrix.    If not, try again using a normal likelihood function / sum-of-squares for cost.   It all comes down to the derivative of that sum-of-squares taken with respect to the model parameters.']",2,2,https://www.reddit.com/r/statistics/comments/12fyziv/q_how_does_error_propagate_across_multiple/
492,2023-04-09 04:16:17,"[Question] How to handle multiple overlapping, incomplete time series for regression (forward curve data)","Hello, I am looking for some methodological help.

I have FFR futures contracts probabilities as independent variables (well, they're not really independent, but i'm not inferring). I suspect that the FFR futures forward curve has a very similar relationship to treasurys (especially bills). I want to regress the current FFR and FFR forward curve on a dependent variable, like s&p500.

I think it would also be good to include other factors, like prime age epop, dxy, liquidity measures, M2, etc. But leaving that alone for now:

How the hell do i run this regression with overlapping and incomplete time series??

Here's a handy little image to explain what's going on:  
[https://imgur.com/GvFB0CQ](https://imgur.com/GvFB0CQ)

And the FFR forward curve, as a treat (i like it at least):

[https://imgur.com/EaeN2Jw](https://imgur.com/EaeN2Jw)

So how do i regress these overlapping, incomplete time series on my target? Thank you so much for reading and pondering my orb with me.

Edit: I had thought of cohorting it as this is my usual lazy approach, but i don't think that works here since contracts are fixed to specific dates (so i can't generate a rolling x-day feature). I suppose i could just generate a ""days from hike"" value for each contract for each observation though - i will try that approach while i await feedback!","['Hey I am stats/trading enthusiast only and be talking bs, but thought a little on ur problem. This is what I would think of:\n- days to maturity for each given date, instead of thinking in overlapping contracts. Also, In my view , investors would approach similarly the 85th or 92nd day maturity, but would look way more differently the 180 or 360 days contracts, so you could join similar maturities together in some groups by some metric (mean?).\n- what is that is changing s&p500, really? Changes in the curve? so your focus variable should be it. Then you would have variables of changes in rate for each grouped days to maturity for (say) monthly data, and regress it against changes in s&p500\n- btw, I would double check but I think futures of interest rates are just current rates adjusted by the cost of money until maturity, so it may have an almost 100% correl with changes in current rates.\n\nI didn’t understanding ur approach of cohosting/rolling-x, if you could share an example I would appreciate!', 'Hey, I recently had a professor using the slope as simply the difference between the 10y with the 3 month bonds. Maybe that is enough. He also correlated stock returns with inflation and consumption as measured by the BEA.\nDid your analysis yield any results?', ""Thank you - yes I did do dtm. That's cohorting. I just won't have a continuous time set but now I no longer need it, so I no longer need to deal with time series which is nice.\n\nAnd also agreed - the shape of the curve is what matters. \n\nAnd to your final point about causality / correlation etc - yes it's very interesting. If markets are perfectly efficient the ffr curve should look like the front end of the treasurys curve, but i have not checked and since the function of the fed is step on the scale (especially since ~91) so i would not be surprised if interest rate expectations (viewed through ffr futures) and inflation expectations (treasurys)."", ""my dude we are way past any 2-point slope, let alone the 10-2 or 10-3. This is like adjusted volatility skew. I'm looking at the entire shape of the curve - but it turns out most of the information is in the skew within each contract rather than any differential itself"", 'Ok, hope u find something! About the ffr vs current fr curve, I think it’s not a question of efficiency, more of arbitrage. I will study it a little in time to properly discern and post here, but that was my rationale in the answer.']",11,7,https://www.reddit.com/r/statistics/comments/12fx59e/question_how_to_handle_multiple_overlapping/
493,2023-04-09 04:10:41,"[C] We have a psychometrician position (full time and internship) open. Someone who is into stats would be a good fit for it. 100% work from home, but you must be based in the US. Full time, salaried, and we're 100% employee owned.","I'm a dev turned manager here and handle all our IT, but I've hired most of my teams from posting on reddit and so I am just trying to help a co-worker out with hiring. As I mentioned, we're 100% work from home (we don't have a physical location) and have been since 2005.

I can probably get you a salary range if you're interested. 

Feel free to ask in here or IM me with questions.

https://www.alpinetesting.com/careers/psychometrician-assessment-operations/

https://www.alpinetesting.com/careers/psychometric-summer-internship-2023/","['Wow. So I have a B.S. in engineering with a minor in mathematics. Self studied and currently trying to break into the Psychometrics industry. Not currently enrolled in a PhD program but wondering if you have roles for non PhDers. Side note. I’m considering everything from an Eds in school psychology to an MD in psychiatry for my future path.', 'We do have an internship open and that might be a good way to get your foot in the door. I think if you did well with that and were working on your PHD we could probably make something happen. Make sure to apply!']",6,2,https://www.reddit.com/r/statistics/comments/12fx01s/c_we_have_a_psychometrician_position_full_time/
494,2023-04-09 02:52:42,[R]Which is the most effective treatment?,"Statisticians of Reddit! Here's a challenge for you. I have a dataset with responses from physicians about their preferred treatment for headache in migraine. I have grouped the data under various headings such as drugs therapy, surgical therapy, behavioral therapy, calculated the means and standard deviations for each group. But how i go about analyzing the most effective treatment? Please help!","['Im not a statistican but how do you want to find out what’s effective if all you have is data about what’s used the most ?', 'So the dataset basically has various pharmacological and non-pharmacological treatments with their frequency, as answered by physicians through a cross-sectional worldwide survey. It’s around 400 entries. \nI don’t mind the scut work; I just wanna know if there any easier way to go about it?', ""You can't find what treatment is the most effective from a survey where physicians selected their preferred treatment. You can only figure out which treatments your population of interest is most likely to prefer.\n\nAnd I would probably fit a poisson GLM."", ""Depends on what your response/outcome is. You can fit a multivariable model, if it's count, use poisson/negative binomial, if it's normally distributed, use linear, if it's binary yes/no, use logistic. \n\nAlso consider the potential confounders to adjust for in the model. Since it is a worldwide survey, is it possible for some treatments to not be available in some countries and ultimately, not a preferred treatment? What about the level of severity or pain scores of patients at point of seeking treatment?""]",1,4,https://www.reddit.com/r/statistics/comments/12fuxik/rwhich_is_the_most_effective_treatment/
495,2023-04-09 01:08:53,How do I fix this ? Spss [Q],"Hi, I’m using SPSS for my dissertation trying to test for collinearity but when I try to run it I get a pop up saying ‘ there are no valid cases for models with dependent variable (dv name) statistic cannot be computed) I’m using dummy variables as instructed by my lecturer. Please help what have I done wrong?","['Check if the variables are coded as numeric or some kind of string or character. Models often treat strong data as missing.', 'Worked out this is only happening when a specific two of my variables are put into it, but both of these are coded numerically']",0,2,https://www.reddit.com/r/statistics/comments/12fryfc/how_do_i_fix_this_spss_q/
496,2023-04-09 00:43:40,[Q] Question about choosing null and alternative hypotheses,"I teach a probability and statistics course in a university but I'm teaching outside my field so I'm definitely not an expert. I have a question about choosing the null and alternative hypotheses and haven't been able to resolve it via googling. I teach in an engineering department so examples about drug testing aren't as relevant.

Question: does the choice of Ho and Ha depend on which ""side"" of the claim you're on, ie if you want to prove or disprove it?

Let's say a lightbulb manufacturer claims their bulbs last on average at least 800 hours. If I work for the manufacturer, I want to conclusively demonstrate via my hypothesis test that my claim is true, so it seems that I would want Ho : mu <= 800 and Ha : mu > 800 so that I could reject Ho with a certain level of significance and be confident in my claim.

However if I'm a consumer and I don't believe the manufacturer's claim, it seems that I want Ho and Ha to be the reverse, so I could conclusively determine that their claim is false and that the true lifespan is less than 800 hours, so that I'd have evidence that they're being dishonest.

Can anyone confirm if the above logic is correct, that sometimes the choice of whether the stated claim is Ho or Ha depends on if you want to prove or disprove the claim?

Thanks in advance!

Edit: here's an example from the textbook, for an idea of the types of problems I'd like to be able to write:

>A manufacturer of a certain brand of rice cereal claims that the average saturated fat content does not exceed 1.5 grams per serving. State the null and alternative hypotheses to be used in testing this claim and determine where the critical region is located.  
>  
>Solution: The manufacturer’s claim should be rejected only if μ is greater than 1.5 milligrams and should not be rejected if μ is less than or equal to 1.5 milligrams. We test  
>  
>H0: μ = 1.5,  
>  
>H1: μ > 1.5.  
>  
>Nonrejection of H0 does not rule out values less than 1.5 milligrams. Since we have a one-tailed test, the greater than symbol indicates that the critical region lies entirely in the right tail of the distribution of our test statistic Xbar.

To me, this problem seems to be written from the perspective of a test engineer at the FDA who wants to try and prove the company's claim wrong. If I worked for this manufacturer, wouldn't I want to switch H0 and H1, so that I can reject the claim that mu>1.5?","['The null is ALWAYS the opposite of what you want to prove. It is related to modus tollens. If A then B and Not B therefore not A.', 'Yes, if you have a one-tailed hypothesis there is a distinction between a right tail hypothesis and a left tail hypothesis.', ""Yes. Easy way to think of it is that the alternate hypothesis is what we usually think of as *the* hypothesis. So if you're trying to prove the bulbs last more than 800 time units, the null is that they last less. If you're trying to prove that the bulbs fail faster than 800 time units, the null is that they last more."", 'Makes sense, thank you! Strange that of all the websites I visited, none explained it so clearly. Seems rather straightforward!', 'The attempt of a ""probabilistic modus tollens"" is indeed the motivation behind null hypothesis significance testing. But as statisticians and logicians have been pointing out for decades, it is also [fundamentally misguided](https://doi.org/10.1177/0959354395051004).']",35,26,https://www.reddit.com/r/statistics/comments/12fr958/q_question_about_choosing_null_and_alternative/
497,2023-04-09 00:27:39,[Q] Interpreting log10 transformed variable coefficients,"Hello,

I am trying to interpret some independent variable coefficients which were transformed using log base 10 and I just want to check I am using the right formula. Please note only the independent variables have been log transformed and I am using multiple linear regression.

To understand what impact a 50% increase in my independent variable will have on the dependent variable I am using the formula:

coefficient beta \* log10(1.5)

Is this correct?

Many thanks",[],1,0,https://www.reddit.com/r/statistics/comments/12fqtfu/q_interpreting_log10_transformed_variable/
498,2023-04-08 20:24:36,[Question] Not correlated but significant. How do I interpret it?,I found that X is not correlated with Y (r = 0.19) but is statistically significant (p = 0.02). What does it mean and how should I interpret the result?,"['The test you are probably doing is whether the correlation is significantly different from 0. 0.19 is different from 0, with a large enough sample size.', 'Everything but 0 is statistically significant with a large enough sample size. \n\nFolks often forget the difference between statistical significance and practical importance though.', 'They are correlated.', 'Yes, the following R code looks at the t-statistic and p-value for a correlation of 0.19 with sample sizes ranging from 3 to 1000.\n\n    r <- .19\n    n <- 3:1000 \n    t <- r*sqrt(n-2)/sqrt(1-r2) \n    p.value <- pt(t, df=n-2, lower.tail = FALSE)\n    plot(n, t) \n    plot(n, p.value)\n    abline(h=.05, col=""red"")\n    \n\n&#x200B;\n\n&#x200B;\n\nPlots at\n\n[https://imgur.com/jpitgQt](https://imgur.com/jpitgQt)\n\nand\n\n[https://imgur.com/HUhYxTC](https://imgur.com/HUhYxTC)', ""Sounds like homework...\n\nShort answer: whenever you have a small effect that's significant, the cause is essentially always the same... *sample size*.\n\n> Not correlated but significant.\n\nBut ... there's no basis on which to claim it's *uncorrelated*. The correlation is small, not absent.\n\n> What does it mean \n\nIt means that the standard error (under H0) was a bit smaller than half the estimated effect size, and it was that small due to the sample size not being small (looks like you had somewhere in the ballpark of n=144).""]",3,12,https://www.reddit.com/r/statistics/comments/12fkeq1/question_not_correlated_but_significant_how_do_i/
499,2023-04-08 19:58:02,Welch's confidence interval the same as Students. [Q],On jamovi it gives me a confidence interval for students but not Welch's. Was wondering if the answer is the same for both.,"[""Your image is showing the confidence interval for the effect size. If you want the confidence interval for the mean difference, you click *Mean difference > Confidence* interval.  It shows for both Welch's and Student's."", '> Was wondering if the answer is the same for both.\n\nNot usually, no. For example:\n\n    > t.test(x,y,conf.int=TRUE)\n    \n            Welch Two Sample t-test\n    \n    data:  x and y\n    t = -3.8088, df = 22.952, p-value = 0.0009059\n    alternative hypothesis: true difference in means is not equal to 0\n    95 percent confidence interval:\n     -22.473614  -6.652683\n    sample estimates:\n    mean of x mean of y \n     19.83943  34.40257 \n    \n    > t.test(x,y,conf.int=TRUE,var.equal=TRUE)\n    \n            Two Sample t-test\n    \n    data:  x and y\n    t = -3.5334, df = 23, p-value = 0.001777\n    alternative hypothesis: true difference in means is not equal to 0\n    95 percent confidence interval:\n     -23.089296  -6.037002\n    sample estimates:\n    mean of x mean of y \n     19.83943  34.40257', ""The correct calculations don't depend on the package.\n\nYou're not doing what you asked about, but note that homework is not allowed"", ""I assume you did that in R, for this assignment I have to use jamovi. When I click on both Welch's and T test I only get results for CI for T as seen [here](https://imgur.com/a/rBY0jL3)\n\nWhat is your opinion on this?""]",6,4,https://www.reddit.com/r/statistics/comments/12fjqno/welchs_confidence_interval_the_same_as_students_q/
500,2023-04-08 07:55:16,[E] Box and whisker vs bar graph,"
I’ll try to keep this simple and can expound if necessary.

Background: 
I’m a veterinary surgery resident relatively new to research and statistics.


I was told that if data is normally distributed you should not graphically depict that data as a box and whisker and instead as a bar graph and vice versa. However, my recent paper (and others I’ve seen) was accepted for publication in a top veterinary surgery journal with box and whisker plots for our normally distributed data. 

Is this recommendation about appropriate figure choice accurate or is it more nuanced than I’m making it out to be?","['Box plots are better in my opinion if you want to compare distributions across groups. However, if you’re showing just one box plot, it would be more informative to have a histogram.', ""Graph choice doesn't hinge on normality. It just doesn't.\n\nBox plots, dot plots, violin plots, and combinations of those should be the default choice when Y is continuous and X categorical.\n\nI'd only ever tolerate bar graphs if Y is counts. Bar graphs suck in general."", 'I love it that makes so much sense to me and I agree. It was a paired t test looking at cadaveric fracture models repaired with different fixations and the box and whisker made much more sense to me to more clearly see the distribution of results rather than just showing the mean with a variance.\n\nThanks for your response.', 'No problem. And yeah your intuition was correct. If you have some time on your hands, I strongly recommend ""Basic statistics in cell biology"" by David L Vaux, 2014. It\'s a very short read and it\'ll arm you with rock solid arguments for both modelling and plotting when discussing with collaborators or replying to reviewers.', ""> should not graphically depict that data as a box and whisker and instead as a bar graph and vice versa. \n\nThis sounds like nonsense\\* to me. \n\nI presume they meant a dynamite plot showing mean and sd or mean and se of mean rather than a true bar plot\n\nWhat was the reason they gave for doing so?\n\n(In addition I bet your data weren't actually from a population that was normally distributed -- and that we could assert that for sure before seeing any actual data, just based on the definitions of the variables -- not that it's important for any of this.)\n\nIf youre using a t-test a display of mean sd and se of mean isn't silly but I wouldn't use it *instead* of a boxplot. If the n's were middling (*say 9ish to 90ish per group*)  I'd try to use a display that showed all the data, along with means, sds,  se of means, medians and quartiles all on one plot. Not hard to show\n\n---\n\n\\* For a more nuanced reaction; I think I probably get *why* they might have said this but if I do correctly guess at their reasons, those will be based on a series of misunderstandings (misunderstandings common in non-stats textbooks on stats) combined with some less-than-ideal traditions in your area.""]",13,9,https://www.reddit.com/r/statistics/comments/12f4cpf/e_box_and_whisker_vs_bar_graph/
501,2023-04-08 07:41:05,[Q] Two-Sample T test Valid?,"I'm comparing data sets between age in months as the independent variable vs presence of symptoms as the dependent variable (coded as 0 or 1). Because the independent variable is quantitative and the dependent variable is categorical, I thought a two-sample t-test would be best. However, I feel like this test is often used when the independent variable is categorical and the dependent variable is quantitative in most cases. Is there a better test to use than the two-sample t-test in this situation? 

&#x200B;

I can provide more information as needed. Thanks in advance :)","['Logistic regression', ""> thought a two-sample t-test would be best. \n\nDV and IV are the wrong way around for that. You condition on the wrong variable. It should still work if what you're interested in is the p value but the amount of discussion required to justify it to readers of your research may be off putting\n\nWhat's wrong with a binomial GLM, such as logistic regression?"", 'Sounds like you might looking for a chi squared test.', ""Nothing wrong with binomial GLM, just that someone else did some analysis on the data before I touched it, and they did a multiple linear regression. It was kind of a mess. It sounds like the general consensus is chi-square or logistic regression, and I'm leaning towards logistic regression. I genuinely just didn't think of logistic regression because I was fixated on the fact that the previous analysis was done w/ linear regression, and it didn't seem like the best representation of the data."", 'Thing is, there are 36 different age groups so i wasnt 100% certain about chi square, which is why I thought of the t test. Would a t test not be valid in this case, or would the chi square test just be more fitting?']",2,6,https://www.reddit.com/r/statistics/comments/12f3y6w/q_twosample_t_test_valid/
502,2023-04-08 04:10:17,[C] Career tip requests- how to maximize my experience with CRO?,"I got my masters in stats 2 years ago. Currently I work with a CRO, and have 1.5 years experience. So far, I have been just programming safety tables, and listings using SAS. 

Im getting better at it but feel that my value is still low to any employer and I’m looking for ways to make myself job-secure through developing skills and knowledge. 

I don’t have a mentor, and I went into this career bc it was the first job that got offered to me, and didn’t know what I wanted to do. There’s very little stats but eventually I will move onto the efficacy tables. 

I keep hearing that knowledge of CDISC is the way to go if I want to increase my value, and I’d like to know how true that is based on others’ experience. Also I heard studying CRF design and CSR’s is important if I want to be a lead someday. I want to hear other people’s thoughts. 

Follow-up questions,
What are some tips for how to study CDISC outside of work?

What are some things I can do improve my value to get picked up by pharmaceutical companies instead of CRO? Also are pharma companies generally better to work at?

What is the job outlook on being a statistician with a masters only/ or a statistical programmer? Do we have to worry about recessions?

What is the outlook on SAS? Do you foresee it being taken over by open source tools soon?","[""> CRO, ... CDISC, ... CRF, ... CSR’s \n\n#  🤯\n\nI at least recognize what area of work you're talking about ... but gee, help readers out a bit there, so that the ones that can't help you at least realize sooner."", 'I recently left my first job out of school (MS) doing Stat Programming at a CRO for 4 years and accepted a job Stat Programming at a Pharma company \n\nOne of the biggest things I’ve learned is that many managers in this industry are not good at managing people (at least at my old company). They may be really nice and talented programmers or Statisticians but often they’re pulled into managing a team due to seniority rather than any actual interest in leading and helping others to develop. To that end, you might need to take your professional development into your own hands since your manager might not even have it on their radar to help you grow in your role at all. \n\nFor that, I recommend talking with them and clearly voicing your desire to take on new tasks (ADaM programming, efficacy analyses, figure programming, TFL shell development, etc). At one and half years of experience, you should be ready to take on new types of work beyond safety tables and listings. For me, I basically learned to do my simpler assignments as efficiently and reliably as possible (writing macros to speed up my TFL programming, learning from senior analysts on how to program more efficiently, creating templates for common programs, etc) so I had more opportunities to take on new types of work (offering to write first drafts of TFL shells or analysis plans, helping draft responses to sponsor comments, side projects using R, helping with submission packages, etc). If your job can’t provide opportunities to grow, that’s probably a sign to start looking for other jobs. \n\nEven if your job is giving good opportunities to grow, don’t be afraid to job search while you’re happy at your current job. The best way to learn what skills are in demand is to see what other employers are asking for in job postings. The best way to see how marketable your current skills are is to get some offers. You’ll either find your experience is lacking and learn where you need to improve, find a better job, or get some new insight that will make you happier in your current role. Either case, it can’t hurt to see what’s out there— and you’ll like be surprised how marketable you are with an MS and some industry experience. \n\nFor learning CDISC, I’d recommend just pushing for opportunities to program CDISC datasets at work and you’ll learn everything you need to on the job. No need to study outside work. You probably already have a good start having used them in your programming.\n\nFor Pharma vs. CRO, I’m sure others with experience in both can answer better. I’d say one isn’t necessarily better than the other and your experience will vary drastically company to company. My biggest advice is just look for roles where you’ll be on an established team with lots of opportunities to learn from experienced people. \n\nJob outlook as a SAS programmer in Pharma seems pretty stable in my opinion. I don’t see SAS going away anytime soon, but as a current SAS programmer, I’m more excited than anyone for the industry to make the switch. Your knowledge of statistics and clinical trials is really the value you bring, even if programming is your day to day.', ""Learn everything related to the projects you work on. Read the protocols in detail and google anything you don't understand. What statistical methods are used? What rules are used for deriving efficacy endpoints. Study the SAP until you understand those well. Read the CDISC guidance documents: ADaM 2.0, ADaM IG, ADaM OCCDS and TTE, STDM IG. Ask questions and learn about the other non-stats areas, who designs the trial, how do sites enter the data, how does the EDC system work? Picking up all that sort of info will help you get promoted faster or get hired elsewhere."", '[deleted]', 'I’d say learning ADaM programming would be the biggest next skill for where you’re at, but it’d still be good to keep pushing for exposure in each of those other areas too. Programming ADaM datasets yourself will make you a stronger TFL programmer since you’ll get to really dive into the specifics of how the data goes from CRFs to raw database to SDTM to ADaM to analysis outputs and all the challenges in between.\n\nI never had the option of personally programming SDTM datasets at my old job because that was handled by a separate working group (from my recent job search, this seems to be a pretty common setup for other Biostatistics departments as well)— either way, it certainly couldn’t hurt learn, but I’d personally steer clear of it since my interest  is in the analysis itself. I think a lot of companies will have MS level analysts programming ADaM datasets + analyses and BS level analysts doing SDTM datasets (though my experience is pretty limited in that regard). All I can really say is that while applying for various Biostatistician and Senior Stat Programmer roles recently, a lack of hands on SDTM programming experience did not hurt my prospects.']",5,6,https://www.reddit.com/r/statistics/comments/12exocn/c_career_tip_requests_how_to_maximize_my/
503,2023-04-08 02:36:07,[Q] significant regression weight but not overall model - what to do?,"Hello,

&#x200B;

I would like to calculate a **simple linear regression**. I have fulfilled all the prerequisites for this. In RStudio I get the following result: 

&#x200B;

MODEL FIT:

F(1,142) = 3.804, **p = 0.053**

R² = 0.026

Adj. R² = 0.019

&#x200B;

\---------------------------------------------------

Est.    S.E. t val. p

\----------------- -------- ------- -------- -------

(Intercept) -0.000 0.083 -0.000 1.000

meanGPSK 0.162 0.083 1.950 **0.053**

\---------------------------------------------------

&#x200B;

I have previously made a directed correlation hypothesis, so to my knowledge **I may divide the p-value of meanGPSK by 2**. So I would have a significant predictor, but the model as a whole would just not be significant. What exactly am I doing now? 

&#x200B;

I am actually primarily interested in the (standardised) regression weight that I calculated above. The fact that it is small is not a big deal. May I still interpret the regression weight, especially because I am so close to significance? 

&#x200B;

This is ""only"" a bachelor's thesis, so my statistical knowledge is rather limited and English is not my first language. Please explain to me in simple terms whether this is a total disaster or whether I can overlook the overall non-significant model.","[""What you do is to report the results of the model and discusses them. I wouldn't get hung up on significance - the more important considerations are your very low r2 value and your b1 coefficient. In other words, the model does a poor job at explaining the relationship between the variables and would not be very effective for predicting your dependent variable based on your independent variable.\n\nWithout more information about your data and model, that's all that can be said.\n\nI'll add that your intercept seems to be effectively zero which is surprising. Did you specifically force the y intercept to pass through zero? That's usually not a good idea."", ""It looks like you regressed outcome Y on 'meanGPSK', which will just give you the mean of 'meanGPSK'.  Not much to interpret.  You should add more variables for a model since this is just saying the mean of ''meanGPSK' is 0.162\n\nThe default p value for R is for 2 tails, so essentially, what your result is saying is that there is a 5.3% chance that 'meanGPSK' is not different from 0.  Usually p value of 0.05 is the standard maximum for hypothesis testing.  Ideally it should be < 0.01. Any higher isn't wrong per say, just not great.\n\nedit: ok I'm feeling like I'm missing something, what exactly is this meanGPSK variable? and what is the weight that you're talking about?  If you want to do weighted regression, there's an option for it in lm(): weights= your data weight""]",0,2,https://www.reddit.com/r/statistics/comments/12eusb7/q_significant_regression_weight_but_not_overall/
504,2023-04-08 02:03:23,[E] What are the top UK universities for applied maths?,"I assume at the top is Cambridge and Oxford, closely followed by Warwick and Imperial because everyone says those are the best for maths. But how do they compare for applied, and what other unis are best for applied?

I'm asking you guys at r/statistics and statistics is the main part of most applications of maths but I'm interested in stuff like economics too.

So far it seems like Oxford allows you to specialise in statistics more quickly and I remember hearing somewhere that Cambridge focuses a little more on pure while Oxford has good applied.

Warwick seems to have more applied focused courses like MORSE.

Other than that I really don't know anything.

Thanks!",[],2,0,https://www.reddit.com/r/statistics/comments/12ets83/e_what_are_the_top_uk_universities_for_applied/
505,2023-04-08 00:56:02,[Q] Anyone knows a simple algorithm to find the start and end point of a peak?,"I have a room impulse response (RIR) sample audio file and trying to find the start and end point of said impulse. This type of sample has relative silence except for the single impulse.

I can find the init point of the impulse by looking at the maxima, but then, how do I find the last from the tail.

When taking the absolute of the RIR, this looks like a time series for which I'm trying to find a peak.

&#x200B;","['Your options here are going to be extremely highly dependent on problem specifics. This could be as simple as checking if change is greater than some threshold, or it could be an almost entirely ill defined question where you’ll have to make some arbitrary definitions of what start and end mean.\n\nWe would need a lot more information to help you.', 'Thanks.\nGive that context you would probably be better suited to ask this question in a DSP subreddit (digital signal processing).\n\nThat said, here’s a first idea. Presumably you have some minimal known time in the signal before an impulse could have possibly occurred (maybe 1 second). Use this portion for background estimation. If you can’t make this assumption, and even if, it might be valuable to make a dedicated background measurement. With the background measured you can find an construct a stopband filter that attenuated out the background signal giving you a relatively clean impulse signal. Then you can use some sort of simple threshold rule on the relatively deterministic impulse signal.', 'You can fit smoothing splines and find where the derivatives are zero and the 2nd derivative is zero.  Check out these R functions: smooth.spline, splinefun, ksmooth.', 'Any function to look at derivatives and their trends and when they go positive to negative and vice versa is a good indicator, as well as anytime second derivative is 0 will be a peak', 'Imagine an impulse function (or function that models the peak you want to find) centered at time *T* laid over your time series.     Your goal is to find the value of *T* that makes the impulse as similar to your time series as possible.     If you choose covariance as your measure of similarity, then this becomes an application of cross-correlation analysis.']",7,9,https://www.reddit.com/r/statistics/comments/12eroo9/q_anyone_knows_a_simple_algorithm_to_find_the/
506,2023-04-07 19:10:37,[Q] How to conduct Indirect Treatment Comparison (ITC) and subsequent Cost-Effectiveness Analysis (CEA)?,"Hi all,

I'm trying to compare the cost-effectiveness of two treatments against each other. However, there are no randomized control trial (RTC) data that directly compare the two. However, there are RTC data of each of these treatments from separate studies comparing each to sham treatments. Therefore, from my understanding, I can implement an indirect treatment comparison (ITC) to compare these two treatments and then perform a cost-effectiveness analysis (CEA) from the ITC results. Please correct me if I'm wrong about this.

Can anyone provide resources/citations that will explain to me how I can go about doing this?

Also, is it adequate for me to use the data from just two RCT (one for each treatment) to conduct this analysis?

Thanks so much for any help with this!","[""2 RCTs is definitely not enough. You're probably looking at conducting a systematic litt review and a bayesian network meta-analysis.\n\nThe cost-effectiveness analysis would be derived from that."", ""So the terminology is different from what I'm used to but this sounds a lot like average treatment effect/average treatment effect on the treated.  \n\nOne way of going about this is using difference in difference regression.  Regression outcome on fix effects, time (in this case dummy variable for before and after treatment), and fix effect * time dummy.  Read Card and Krueger (1994) for the foundational paper on this method\n\nOr just google difference in difference, this is a pretty common method.  Wikipedia have a pretty good overview, although it's a bit technical.\n\nhttps://en.wikipedia.org/wiki/Difference_in_differences""]",0,2,https://www.reddit.com/r/statistics/comments/12ehn7o/q_how_to_conduct_indirect_treatment_comparison/
507,2023-04-07 19:07:04,[Q] [D] Best textbook for advanced biostatistics/survival analysis?,"Hi r/statistics, I'm a PhD student in epidemiology with an undergrad in maths and stats. I find the survival analyses very interesting, and I'd be interested in a biostats postdoc. The thing is, my knowledge of the maths underlying all the stats is still at an undergrad level.

1: What textbook would be best to bring my biostats understanding up to a level competitive for a postdoc?

2: Also (this might just be impostor syndrome talking), is an epidemiology PhD likely to count against me, or would it probably be viewed as close enough to biostats?","['I love Frank Harrell’s book: “Regression Modeling Strategies with applications to linear models, logistic regression, and survival analysis”\n\nI’m not qualified to answer the second question since I haven’t been in your shoes, nor have I been on the other side of the interview table, so take this with a pinch of salt. From what I’ve seen though, many people perceive biostats as an easier and more hackable subfield of statistics. This is absolutely not true. Biostatistics often has just as rigorous training in statistics as statistics PhD programs. However, it really depends on the supervisor. With your background, I suppose you’d be seen as more suitable for applied work with advanced existing statistical methods than developing statistical methodologies, so maybe aim for that? I’m sure other people will have better advice, but in general, it never hurts to highlight your strengths, compatibility with the field you’re applying to. Wish you best of luck!!', 'Although you mentioned your math background I am going to recommend two basic but very well written references. I enjoy reading Kleimbaum https://link.springer.com/book/10.1007/978-1-4419-6646-9. When is something I struggle to understand the easiest reference was Allison https://www.amazon.com/Survival-Analysis-Using-SAS-Practical/dp/1599946408. \n\n2. Depends on your experience during your PhD and your dissertation topic', ""Kleinbaum is good but a bit dated (he's an epidemiologist btw), I'd suggest Moore's book:\n\nhttps://link.springer.com/book/10.1007/978-3-319-31245-3\n\nWhat's wrong with an epi PhD? Feel free to come over to r/epidemiology and ask us."", 'With regard to 2) I think it really depends what you are interested in doing --- if you want to do theoretical work in survival analysis, it could be hard. If you want to engage on the application-side (or more applied methods development), then I think it could be a great fit! I think there are a lot of biostatistics faculty who are engaging with collaborations (and more applied methods work) involving the analysis of time-to-event data, and would love to have a postdoc who is interested in thinking deeply about the data and study design. My suspicion is that it would also be important to be very comfortable with programming.', '1: Bendix Carstensen, Epidemiology with R, https://bendixcarstensen.com/EwR/']",39,11,https://www.reddit.com/r/statistics/comments/12ehjvd/q_d_best_textbook_for_advanced/
508,2023-04-07 18:23:16,[Q] How to represent large categorical data?,"I've 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category. The category is not distinct thus other row may refer to any of the 3 categories.

e.g.

&#x200B;

|Date|Value|Category|
|:-|:-|:-|
|1/1/2010|1.11111|Alpha|
|2/1/2010|2.11111|Beta|
|3/1/2010|2.00009|Alpha|
|4/1/2010|0.00000|Charlie|

But the 10 datasets have different volume of data. E.g.  dataset A may have 10K rows, dataset B around 100K, Dataset C 1 million, etc.

I couldn't process all the data as its too large.

What would be the best way to sample each dataset? I'd like the sample containing a fair representative of the 3 categories.","['Perform stratified sampling based on 3 categories?', ""[Q] How to represent large categorical data?\n\n> I've 10 numerical and large datasets where each has 3 generic categories. Each row contains unique data. The end row of each dataset contains the labels for each category.\n\nI don’t quite follow what you mean here. My best understanding of this also doesn’t seem to agree with what your example data looks like. Is your issue here about reading in data from this format?\n\n\n> I couldn't process all the data as its too large.\n\nWhat type of processing did you run into problems with? Answers are going to be completely different if this means you can’t even load the data into memory, or if instead you mean you can’t fit models to these data."", ""For each10 dataset, we need to get representatives.   \n\n\nWe are using a new algorithm to process them but we can't process all data. So we would like a representative for each dataset.""]",3,3,https://www.reddit.com/r/statistics/comments/12eghmf/q_how_to_represent_large_categorical_data/
509,2023-04-07 14:14:43,[Q] Probability symbol confusion,"Hi all, I am new to statistics and am confused with the ""<"" and "">"" symbol.

Say if ""The probability that an individual score is above 22..""

Which one is correct?  P(X < 22) or P(X > 22).

Thanks.

kel","['The symbol ‘<‘ itself is larger on the right than on the left. So it means that the number on the right is the larger of the two. It’s opposite for ‘>’, which is larger on the left :)', 'We always drew it as an alligator mouth that is hungry for the larger number.', 'Pac man eats the larger number', ' ""<"" and "">"" are not probability symbols, they\'re inequality symbols.\n\nhttps://en.wikipedia.org/wiki/Inequality_(mathematics)#Properties_on_the_number_line\n\n(also covers ≤ and ≥)\n\n They should normally be well covered in mathematics classes before you\'re about 14 or 15. When I went to school they were first discussed in 6th grade, and then covered in more depth a couple of years later.\n\nspecifically, ""<"" is ""less than"" and "">"" is ""greater than"". For both symbols, the small side of it points to the smaller value, and the  wide arms encompass the greater value.', 'I think he knows \n\nHes just understandably confused as it the same as whats up and down \n\nFor left to right languages it is simple enough \nBut with right to left languages ( although it doesnt matter much ) it is not that intuitive \n\nThere is no standard as to put higher or lower numbers first ( i.e on the left in Left to right languages )\n\nLike 3< 5 and also 5>3']",0,14,https://www.reddit.com/r/statistics/comments/12ebyc6/q_probability_symbol_confusion/
510,2023-04-07 13:37:14,[Q] Scoring higher on a test by virtue of taking the test more than once - what is this called?,"For example, a group of students takes a test at 8AM. At 12PM, these same students take the same test. There was no education between these two times, and the 12PM average was higher than the 8AM average.

Is there a term for this?","[""It's a testing effect or a practice effect in my books."", 'The learning effect is defined as a significant increase occurring in cognitive test scores as the number of repetitions increases until the score no longer changes and achieves stability.', 'Usually that\'s called ""waking up""', 'This is a paired sample and we use the wilcoxon signed-rank test or sign test to determine if there is an actual effect and the effect size. I love me some nonparametric statistics.']",2,4,https://www.reddit.com/r/statistics/comments/12ebafg/q_scoring_higher_on_a_test_by_virtue_of_taking/
511,2023-04-07 09:42:11,[Q] What data can I use for a cost-effectiveness analysis (CEA)?,"Hi all, thanks for taking a look at my post!

I'm trying to conduct a cost-effectiveness analysis (CEA). To keep it general, I'll try to describe my study with a generic example.

Say I'm trying to determine the cost-effectiveness for a new treatment for back pain (treatment ""A""). I would like to compare this treatment with the gold standard, treatment ""B"". I would like to use data that is already published in the literature. There are no studies that directly compare the effectiveness of treatment A vs. treatment B. Instead, there are case series, randomized control trials, meta-analyses, etc. that compare one of these treatments to either a different treatment, no other treatment/sham treatment. Also, all these studies have different follow-up periods. 

Because there is no published data directly comparing treatments A and B against each other, can I use all of those individual studies (where treatments A and B were analyzed separately from each other) to conduct a CEA?

If so, how do I go about combining all of these different studies together to generate quality-adjusted life year (QALY) values? Do I need to weight the numbers from these heterogeneous groups of studies somehow?

My last question is, can I use Visual Analog Scale (VAS) in order to calculate QALY? I believe I can, but I just wanted to double check with y'all.

I really appreciate any help you can provide. Even if you don't have the answers, just a point in the right direction would be very, very helpful (and appreciated!)

Thanks so much for your help!","[""You need to find, or conduct, a network meta-analysis. Of RCTs only, non-randomised trials will be chock full of unavoidable biases.\n\nThen you'll need to build a decision-analytic model, which can be quite simple or very complicated, depending on the clinical context.\n\nQALYs are constructed in all sorts of ways, some justified, some less so. You may need a patient panel to help you weight the different outcomes if the work has not already been done well.\n\nThis isn't really something you can do alone. You need patients, clinicians, information specialists, reviewers, and health economists at a minimum, possibly also psychologists, mathematical modellers, etc. It's a six figure funding job, with the first number depending on how complex the question is.\n\nThe [HTA series](https://www.journalslibrary.nihr.ac.uk/hta/#/) has a lot of monographs reporting this sort of research. You could have a look through to find out more about methods."", ""Oh dear. There's issues here, but you're definitely a student so I'll be helpful.  \n\nI'll give you one term to help you on your way: indirect treatment comparison."", ""> We're not looking to submit it for government review, etc.\n\nIt doesn't matter. You still need to do the work properly and you need to have collaborators who already know the answers to these questions. The team doesn't need to be huge, it does need to have all the skills required to do the work. It's not something you can do with advice from strangers on the internet."", '100% agree with D-Juice here. If you work at academic institution and are planning to published this research, then consider reaching out to health economists, statisticians, and clinicians affiliated with this university.', ""I really appreciate the response--very, very helpful. This is supposed to be a clinical research study for a new treatment for pain management. We're not looking to submit it for government review, etc. I work in healthcare and am at an academic institution, so we're trying to put together something to submit for publication. There are a handful of other studies in the same realm and though they seem complex, it seems like they were able to do the analysis without a huge team. What do you think?\n\nAlso, are you able to recommend any open source/free software that is capable of conducting these analyses?\n\nThanks so much for your insight so far!""]",8,5,https://www.reddit.com/r/statistics/comments/12e5fh5/q_what_data_can_i_use_for_a_costeffectiveness/
512,2023-04-07 08:32:52,"[Q] Creating a scoring system with different weightings…the closer a number is to 0, the more weighting it should be given…How do I do this?","Sorry if this is a silly question. I come from a non math background but have to do something for work.

Essentially, I have data that tracks the number of maintenance events an item/equipment underwent. The best score the item can get is 40, whereas the worst is 0. (It is 40 because the other 60% of the score is made up from something else)

The MORE maintenance events an item underwent, the LOWER it should be.

BUT the weightings shouldn’t evenly distributed. For example, if the maximum number of maintenance events an item can go thru is 12, I can’t just do 40 - (40/12 * [number of events for that item]).

Ideally, the MORE maintenance events an item goes thru, the FASTER it will approach zero. (Logarithmically approaches 0).

I really like doing this sort of task and creating a scoring methodology with my data, but have no clue how to do this (or even what to search - things I tried don’t seem close at all to what I am talking about).

Any tips on how to achieve this? I think I have included the relevant information but just ping me if not.

[I think I basically need a formula for this…maybe. But also not sure if there is a better way, hence the post](https://i.imgur.com/lvYwMYR.jpg)","['Something similar to this\n\nx=40+ 40\\*log(1/(12-y))\n\nIn other words,  the more maintenance, the higher the score, but it logarithmically grows, rather than linearly.\n\nMin score is 1  \nMax score is 12', ""Just gona rewrite this stuff I wrote before in case if anyone else is interested:\nFormula for this particular problem is essentially a  production possibility frontier curve:\nY^2 + (x*40/12)^2 = 1600\n\n1600=40*40. what ever your maximum is, square it. 12 is the maximum number of maintenance event. \n\nY does drop faster as X -> 12. Slope as X->12 approaches infinity. You can increase this 12 to make the oval flatter.\n\nIf you want Y to drop faster:\n\nY^2 + (x*a)^b = 1600. Change 'b' to a higher number and solve for 'a' such that x=12 and Y=40. As b increases, the relevant portion of the curve would start to look more like an edge/ horizontal then vertical line. Y^2 fixes the Y axis at maximum of 40."", 'https://en.wikipedia.org/wiki/Exponential_function', 'A simple equation that meets your criteria is a parabola.  \n\nY = 1 - a*x^2\n\nAdjust the constant *a* to match your taste.  \n\nYou could also just code your graph into a spreadsheet and use it.', 'That could work! Is there a science to picking the right a coefficient (if that\'s what we should call it)? Or is it random?\n\nIn other words, is there a ""more"" correct way to do this?']",21,27,https://www.reddit.com/r/statistics/comments/12e3m3p/q_creating_a_scoring_system_with_different/
513,2023-04-07 07:24:52,[Q] blme package and p-values,"I recently tried to run a binomial mixed model using lme4 but couldn't because some of the levels had complete separation so I adopted the approach recommended by Ben Bolker [here](https://bbolker.github.io/mixedmodels-misc/ecostats_chap.html#digression-complete-separation) by using the package blme and fitting a bayesian binomial mixed model. It worked well but I get p-values as one would when adopting a frequentist approach. I am so confused by this! Does anyone know why this is a bayesian mixed model but the output is identical to a frequentist one, i.e., to one generated by lme4?",['See [this](https://stackoverflow.com/questions/58065947/extract-p-values-and-estimates-from-blmer-list-of-regression-models) stack thread for a comment on the p-value reported by blme.'],3,1,https://www.reddit.com/r/statistics/comments/12e1uc5/q_blme_package_and_pvalues/
514,2023-04-07 06:48:18,"[E] Comparing Funded MS Programs, R1/R2 and Other Factors, MS","Hi all,

Hope you're all doing well. I've found myself in an extremely fortunate predicament, and was hoping this sub might have some advice for me. I'm going for a MS in Statistics this autumn, and have two fully funded offers, one from a R1 and one from a R2 University.

The offers themselves are fairly similar (tuition waiver, stipend, health insurance, etc.) for TA-ing 20 hrs/week. I'm currently undecided on a PhD. I'd imagined going into industry after an MS and am still leaning that way, but want to leave the door open in case I really get enamored with a topic while in school.

The R1 has a PhD program and seemed to be pretty open to letting people continue on (obviously assuming good academic/scientific standing) and have a traditional qualifying exam after the first year. The R2 does not have a PhD program.

The R1 is quite a bit larger (\~31k grad/undergrad) than the R2 (\~9k grad/undergrad.)

I should be clear that I'm not asking for ""Which Program Should I Pick?"" more, which factors should I be considering in making a decision?

Any additional information I'm happy to provide, and thanks again. I'm feeling extremely excited and fortunate with whatever happens, I just want to ensure I'm considering the right variables & factors.",[],1,0,https://www.reddit.com/r/statistics/comments/12e0uwu/e_comparing_funded_ms_programs_r1r2_and_other/
515,2023-04-07 04:33:26,[E] Thinking about Masters in Statistics with focus in Quantitative Finance,"Hey all,

I’m a currently graduating senior at FSU and I will be obtaining degrees in Economics, Statistics, and Political Science. I also ran a business (pseudo hedge fund) where I had friends who had similar backgrounds investing with me and creating AI trading strategies to generate alpha. I got As in really anything involving statistics application but I only really got Cs for Calc 1-2 and linear algebra. Is this bad if my focus is application and not necessarily mathematical theory? I’m also worried if it will hold me back from what I’ve accomplished up until this point when it comes to applying to masters programs.","['How can someone study 3 degrees and run a business on the side? (and then consider to add another degree)', ""I have a bachelors in pure math and decided to do a Masters in Statistics. I am graduating in a few months, and did just fine with some C's in my bachelors. (However, linear algebra is extremely useful, so maybe refresh this knowledge) I found that I did better in the Master because it was closer to my interest, so I studied more and worked harder. If you are prepared to do the work, you will be fine."", ""It's a pain in the ass but some of us are dumb. \n\nI'm currently doing a PhD in criminology, a college program to be a research analyst, and just took a mental health break from a 45 hour work week compromising of 5 jobs (bunch of different contracts). \n\nCan't wait till I finish college at the end of the summer, and finish my data collection for uni....then I just have to try working till I can save enough to buy property lol"", 'Evidently, by sacrificing grades in some areas. (Impressive nonetheless)', 'Business grew out of my interest in investing from a young age. My dream job is to be a quant and wanted to build my own experience if I couldn’t get internships. Started with Economics and Statistics but added poly Sci since I felt that it would help with modeling seasonality eventually when I code my strategies as well as other reasons.']",1,13,https://www.reddit.com/r/statistics/comments/12dx1kj/e_thinking_about_masters_in_statistics_with_focus/
516,2023-04-07 00:56:30,A priori Power Analysis for GLMM? [Q],"Hey, I am a MSc student in Psychology and a little overwhelmed by the a priori power analysis for my experiment. The experiment is repeated-measures within-subjects design. The outcome variable is binary, so are two predictor variables. Based on that, I am planning to conduct a GLMM in the likes of glmer(response \~ variable1 \* variable2 + (1 | subject) + (1 | stimuli). Since my supervisor is currently working on a similar analysis, he provided me with a script he came up with. It basicially creates a dataset and simulates for every trial the outcome based on probabilities set in the beginning of the script. Then it sets the model and runs a power simulation based on the powerSim() function. Now my issue is, that I get the weirdest results when plotting the outcome: Basicially I get a power spike for n= 10 before dropping until n = 17 and then rising again at n = 20. I can not find a rational explanation for this effect, however, I do also not see a flaw in the logic of the script. Therefore, I wanted to ask wether there is any possible explanation for this?","['[This](https://i.redd.it/ix8wacrzkasa1.png) makes my issue a little clearer, I guess.', 'I don’t see how it’s possible for power to decrease as n increases, all else being equal.', ""I've been using simr and the powerSim() function without any issues for a while so I doubt this is related to a bug in the function. That said, I've never used it for a glmm. If you don't get an answer here, you could try over on github: https://github.com/pitakakariki/simr"", 'It can happen with very skewed datasets. https://garstats.wordpress.com/2017/11/28/your-power-is-lower-than-you-think/', 'Right? I am so confused by this? But then - even with simulated data - this would not be possible, so I assume the error to be in the power analysis function.']",11,10,https://www.reddit.com/r/statistics/comments/12dqfb8/a_priori_power_analysis_for_glmm_q/
517,2023-04-06 21:36:36,[Q] Optimal pseudo-random algorithm for mutation in a Genetic Algorithm,"Hey. So lately I've been working on a problem for which I'm trying to apply a genetic algorithm to find the optimal solution.

The thing is the (binary) chromosome is very large (hundreds of thousands of bits at least, millions maybe) so some aspects of the algorithm can become a problem if they don't scale.

One bottleneck I found is mutation - so generally the algorithm is to go over each bit, generate a random number between 0 and 1 and if the mutation\_rate (e.g. 0.05 = 5%) is less than that we flip the given bit.

Of course this for such long chromosomes becomes a computationally heavy operation so I've been thinking about replacing it with maybe something less truly random but faster.

The approach I currently have in mind is to simply pick the number of genes to mutate at random (random number in *\[0, chromosome length)*, and then generate that random number of random numbers between *\[0, chromosome length\]* which would be the indexes (genes) to flip.  But I'm not 100% sure how to incorporate the mutation rate - so I would need the number of genes to mutate to be picked from some normal distribution with *mutation\_rate \* chromosome\_length* being the average (maximum chance)?

Does anyone have an interesting idea how to replace the naive O(n) algorithm with something better but still being in line with the idea behind mutating the chromosome so it doesn't get stuck in a local optima?","['The approach you suggested has merit, and I can suggest some further refinements:\n\nIncorporating the mutation rate with your approach can be done by using a binomial distribution to determine the number of genes to mutate. The binomial distribution allows you to model the number of successes (mutations) in a fixed number of independent Bernoulli trials (flipping bits), each with a constant probability of success (mutation_rate).\n\nTo implement this, you can follow these steps:\n\n- Determine the number of genes to mutate by sampling from a binomial distribution with parameters n (chromosome length) and p (mutation_rate). In Python, you can use `n_mutations = numpy.random.binomial(n, p)`. \n- Sample the selected number of unique indices from the chromosome without replacement:  `mutation_indices = numpy.random.choice(range(chromosome_length), size=n_mutations, replace=False)`.\n- Flip the bits at the chosen indices: \n```python\nfor index in mutation_indices:\n        chromosome[index] = 1 - chromosome[index]\n```\nKeep in mind that this approach has an average-case time complexity of O(k), where k is the expected number of mutations. Since k = mutation_rate * chromosome_length, the complexity is O(mutation_rate * chromosome_length). However, in practice, this should be significantly faster than the naive O(n) approach, as the mutation rate is typically small.\n\nYou can do the same thing in r with `rbinom(1, n, p)` and the `sample()` functions too, if that’s your preference.\n\nGood luck with your project!', 'Thanks for the detailed answer! Will try out doing it like you said, hopefully the rand libraries for the above are present in Rust :)', 'Just a side note: the optimal algorithm depends on your environment/data, as well as your mutable structure. Check out the no free lunch theorem.', "">  so I would need the number of genes to mutate to be picked from some normal distribution with mutation_rate \\ chromosome_length* being the average (maximum chance)?\n\nAssuming you want each gene to have the same chance, and the genes mutate independently of each other, the total number would be binomial, not normal.\n\nIf the number of genes is very large, so that the expected number to mutate is not too small, then you can use a normal approximation for it, but why not simply generate the binomial directly? I assume you have access to a decent stats library in your language of choice; the binomial can be obtained via an inverse regularized beta function if you don't have a binomial quantile function.\n\n> then generate that random number of random numbers between [0, chromosome length] \n\nIn R that could be done by something like \n\n     Ng=100;p=0.05;sample(Ng,rbinom(1,Ng,p),replace=TRUE)\n     [1] 41 67 85  5 47 25\n\n\nOne problem is that you have a chance of choosing some of the same numbers more than once (with 100 genes, this happens about 1/9 of the time); sampling without replacement would avoid this doubling up of some genes. \n\nAgain in R:\n\n     Ng=100;p=0.05;sample(Ng,rbinom(1,Ng,p))\n     [1] 26 34 23 25  2 13\n\nwould suffice; there's approaches to coding sampling a vector of indices without replacement that are easy to implement that would probably do for your purposes."", 'Thanks, ChatGPT.']",18,10,https://www.reddit.com/r/statistics/comments/12dkm49/q_optimal_pseudorandom_algorithm_for_mutation_in/
518,2023-04-06 19:44:21,[Q] Is it okay to 'split' a 3x2 table where you would normally do a chi square test into three 2x2 tables?,"Hello,

I have a question about chi square tests. I want to compare some nominal variables, with 2 answer options, between three age groups. For example: I want to compare the 30-day mortality (Yes or No) between group 1 (ages < 70), group 2 (ages between 70-79) and group 3 (ages 80-89). This gives a 3x2 table that I wanted to compare with a chi square test.

Fortunately not many people died but this means that I have multiple expected cell counts lower than 5. This is the table for the 30-day mortality: 

||< 70 years old|70-79|80-89|
|:-|:-|:-|:-|
|30-day mortality NO|41|40|39|
|30-day mortality YES|2|3|5|

I can not group the data more than this as we really need to compare these 3 groups.

&#x200B;

What is the best way to compare these groups?

I had some ideas but I don't know if these are correct.

1. Dividing the 3x2 table into three 2x2 tables (compare group 1 with 3, compare group 2 with 3 and group 1 with 2). This would also give me the answer to between which groups the significant differences are but I fear that I won't be able to extract all the information from it as if it were a 3x2 table. Is this a correct way to analyse this data or will I miss things?
2. Use a chi square test anyway. Since some posts on the internet say that the rule of cell counts having to be above 5 is not necessary and that statistical programs nowadays are good enough to calculate the significance even with lower numbers.
3. Use a different kind of test. I don't know which one as there are many different opinions to be found online.

Are any of these options correct? And if so, what would be the best option?

Thank you for the help!","[""I would recommend using Fisher's Exact Test to analyze your data, as Chi-squared is likely inappropriate given your low cell counts. \n\nThis test should provide an accurate assessment of the relationship between age groups and 30-day mortality rates, allowing you to make valid inferences from your data. \n\nRough steps: \n\n- Create three 2x2 tables for pairwise comparisons between age groups:\n- For each 2x2 table, calculate the Fisher's Exact Test p-value. R/Python/SAS have built-in functions for Fisher's Exact Test - in R, it’s `fisher.test(table)`.\n- Apply the Bonferroni correction to control the family-wise error rate. To do this, multiply each p-value by the number of tests performed (in this case, 3).\n- Compare the corrected p-values to your desired significance level (e.g., 0.05). If the corrected p-value is less than or equal to your significance level, you can reject the null hypothesis and conclude that there is a significant difference in 30-day mortality rates between the age groups being compared.\n\nGood luck!"", ""Fisher's test would be most appropriate here - it's like a chi square test but more appropriate when you have low cell values like this.\n\nYou also might be able to try a logistic regression on mortality - I expect your event rate is too low, but it's worth a shot."", 'Thank you for the help!', ""Thank you! I'll try both""]",4,4,https://www.reddit.com/r/statistics/comments/12dhlw2/q_is_it_okay_to_split_a_3x2_table_where_you_would/
519,2023-04-06 19:10:24,[Q] Assign predictor to extreme measures in Bayessian Regression,"Hello,

I am using Bayesian regression to model the distribution of a certain quantity depending on some predictors. This is a quantity that is supposed to be 0, but it is not due to measurement errors of the predictors, so the purpose of the study is to identify abnormal measurements and where could the problem be among all the predictors.

So far I have the regression model, and I have a prosterior calculation to obtain the likelihood of a new measurement given the model. However, the step of assigning the abnormal measurement to a certain predictor has me stuck. This is for work, so right now we would like to at least get a ""toy"" method to show the clients what we have is on the right track (they do not have the background to understand regression models sadly so I need to come up with some examples to make them see what we are going for).

My only idea/hope right now is to do tests on some simple datasets the client can provide, and then if a extreme measurement happens, try to manually adjust the predictors and rerun the model to see if the measurements are more plaussible with this modified model. However, since this is just my intuition I am still not completely sure if modifing the model like that would indeed show what I expect.

Any comments, discussion or just references is greatly appreaciated, I am learning a lot of new tools on this project and with Bayesian regression I am discovering a lot of things I didn't know you could do. Thanks in advance!

&#x200B;

edit: (wrote this on a comment too)

To give more detail:

I have a system with a material that goes in the system, out the system and sometimes the system itself uses some of the material for mantainance or other reasons. Then, in theory, if I measure all the material that goes out, goes in and gets used and then add it, it should be 0. However, it is not, due to measurement errors in the measure stations (or sometimes it could happen that there is a leak and then there needs to be immediate action). Abnormal measurements are interesting because it means some measurement device is working out of the expected errors and a check up will be needed, or that there is a leak and it has to be repaired. So the are the measurements I will look at, not just outliers.","['Apologies if I’m missing something, but if you already have your posterior distributions, then can you not just take any given “abnormal” sample, and input this data into the posteriors to see which predictor gives the lowest likelihood?', 'What do you mean by ""the quantity is supposed to be 0""?\n\nIs an ""abnormal measurement"" interesting by itself or just an outlier to be discarded?', 'Am I understanding the model correctly. It is, essentially, p(y+e(y)-x1-e1-x2-e2-x3-e3)+(1-p)(y+e(y)-x1-e1-x2-e2-x3-e3-leak)=0? Where each e is a measurement error and p is the long run probability of no leak. You need to know P(Leak>0ly, x1…3)?', 'I am the one that is not an expert on Bayesian regression, so I might be missing something, but my understanding is that the prior is just the probability of a new measurement of the quantity given the model and the new predictor values, how would you work from that to assigning the uncertainty to a specific one?', ""To give more detail:\n\nI have a system with a material that goes in the system, out the system and sometimes the system itself uses some of the material for mantainance or other reasons. Then, in theory, if I measure all the material that goes out, goes in and gets used and then add it, it should be 0. However, it is not, due to measurement errors in the measure stations (or sometimes it could happen that there is a leak and then there needs to be immediate action). Abnormal measurements are interesting because it means some measurement device is working out of the expected errors and a check up will be needed, or that there is a leak and it has to be repaired. So the are the measurements I will look at, not just outliers. \n\n&#x200B;\n\nMaybe I was not too clear about it so I'll edit this to the post.""]",2,13,https://www.reddit.com/r/statistics/comments/12dgrom/q_assign_predictor_to_extreme_measures_in/
520,2023-04-06 17:28:06,Smashing my head against Kruskal-Wallis [Q],"Smashing my head against Kruskal-Wallis

So I'm doing a masters thesis which includes 1 dependent sample taking 2 questionnaires, which I am comparing. A biostatistician at work told me that I should use Kruskal-Wallis to replace the one-way ANOVA (which I thought was for independent group) for this type of data but stats is far from my area of expertise so I've hit a bit of a wall with this one. 

Can anyone give me any guidance?","[""The Kruskal Wallis is a non parametric test to check if two samples originated from the same distribution, while the one way anova is a parametric test of equality of means. \n\nTo use anova you have to check your assumptions first: are response variable residuals normally distributed? \nAre the variance of the populations equal? \nThe third assumption independent and from identicall distribution i am not sure if it holds anyway, (if the same person answers two questioners are the questioners independent? I think not but i need to look it up).\n\nLets say that the above assumptions dont hold so you are forced to use Kruskal Wallis.\n\nWhat exactly is the issue?\nYou don't know how to implement the test? How to interpret the results?"", ""What's the research question you're trying to investigate?"", 'I’m not sure about Kruskal-Wallis being appropriate given you have paired samples.\n\nSince you are working with paired data (1 dependent sample taking 2 questionnaires), you need a test which accounts for this. If your data is normally distributed and meets the assumptions for parametric tests, you can use a paired t-test, which compares the means of two related samples. If your data does not meet the assumptions for parametric tests (e.g., not normally distributed), you can use a non-parametric alternative like the Wilcoxon signed-rank test.', 'What is the scale of the responses? Are they binary (correct/incorrect), ordinal/Likert ( disagree, neutral, agree), or something else?', ""I asked the statistician again to clarify and he stated that I would split my sample into 2 by the documents the questionnaires are based on. So, 20 volunteers for doc A and the same 20 for doc B, is this correct?This guy has a PhD in stats but every comment here says otherwise so no idea what I should do. I've already wrote 1000 words on Kruskal-Wallis and the more I wrote the more confused I got as it didnt seem to match.""]",4,10,https://www.reddit.com/r/statistics/comments/12dejek/smashing_my_head_against_kruskalwallis_q/
521,2023-04-06 12:46:23,"Why is learning statistics so ""annoying?"" [Q]","Dear All,

I am currently a student learning statistics. it's nothing advanced in the field, just a prereq for other courses I need to take.

however, as I take this course, I'm noticing more and more issues overall with statistics. many things are intuitive and good, such as some equations. others, not so much.

perhaps it is simply due to how I am being taught, but I feel as though many parts of this field are incredibly obtuse to learn. things that already have names are given new ones to fit the field, or sometimes an equation will use 30 different greek symbols instead of using a normal latin alphabet (which the majority of chemistry equations use, and they're easy to process to me).

I consistently ask myself ""why did they make this that way? why did they explain it in such a horrible way? why can't they just say what it is, instead of using so much jargon every time they need to explain something?"" every time I crack open the textbook to solve a single HW question.

I was wondering if anyone else had these sentiments about the redundant and annoying nature of statistics as a whole.

like, why do equations use the same symbol in very different contexts, instead of using a similar method to chemistry (such as the equilibria constants having different subscripts based on the type, or why don't we just use a single letter to always mean a certain thing, like p=population or r=radius)

why is a left skewed graph on the right side, when skewed means to swerve in a certain direction? would it not make more sense for a left skewed graph to have the data skewed to the left, rather than to the right?

why do they use explanatory/predictor variable in the same way as independent, despite them not meaning the same thing?

there's many questions I ask about this field, and it's increasingly annoying to learn as they never seem to have any set way to solve something...

maybe I'm just too used to chemistry, which mostly uses X, Y, Z, A, B, alpha, omega, and delta.

&#x200B;

edit:

another example:

I am being taught about Center of Population Distributions. The question is not dealing with geographic centers of population distributions. it then goes on to immediately say that "" The center of a population distribution is the mean of the population."" Why in all that is holy would it not just say ""population mean"" instead of using a term, then defining the term in an application that is not meant for that word?","[""It probably depends on your background whether you find the language statistics uses intuitive or overly complicated. I've always appreciated the nuance it allows for"", 'I am glad that you find statistics intuitive. Frankly, not many people enjoyed statistics and usually found it counterintuitive (such as probability theory). In terms of Greek letters and some jargon, you just have to know the language we speak here (the same goes for math). When I was an undergrad, I sorta had the same issues with statistics - why are the wordings in statistics so convoluted? Why not just accept something instead of failing to reject it anyway.\n\nBut as long as you pass that invisible ""doorstep"", you will appreciate how rigorous the statistics are. Statisticians can reject an idea, but we never provide a ""concrete"" answer - it always ends with ""It depends"" or ""We need more data"" - We adore the beauty of uncertainty.\n\nOne day, you will find yourself speaking statistics and you will realize ""Darn, now why am I turning into this pain in the neck person"" when you see your colleague scratching her/his head and trying to process what you just concluded based on the results you provided.', '>  things that already have names are given new ones to fit the field\n\nThis is true of every field. Your problem seems to be that you\'re used to certain conventions in *chemistry*, and are having to learn new ones in statistics. This isn\'t statistic\'s fault, it\'s just hard to have to learn new conventions in general\n\n> why can\'t they just say what it is, instead of using so much jargon every time they need to explain something?""\n\nStatistics, like all mathematical fields, work with objects and concepts that have extremely precise definitions. Jargon is inevitable, and saying what something ""is"" generally does involve a lot of specialized terminology.\n\n> I am being taught about Center of Population Distributions. The question is not dealing with geographic centers of population distributions [...] Why in all that is holy would it not just say ""population mean""\n\nThis is like complaining that a ""group"" in mathematics isn\'t the same thing as a collection of people, or that a mathematical ""ring"" doesn\'t have anything to do with benzene. Words mean different things in different fields. The ""population"", in statistics, is a distribution, and the mean is a measure of central tendency. All of those terms have perfectly clear definitions, and it\'s perfectly fine to say that the mean is the center of a population distribution. It\'s just a matter of becoming comfortable with what those words mean in statistics.\n\n>  why do equations use the same symbol in very different contexts\n\nIt\'s hard to say without you giving an example, but there are far more contexts than there are symbols, so it\'s natural that some would get reused. There is still a fair bit of consistent terminology, though.\n\n> would it not make more sense for a left skewed graph to have the data skewed to the left\n\nThat\'s exactly what left skewed means (at least, intuitively, and this will be correct most but not all of the time). A left skewed distribution has a longer left tail, which pokes off to the left.', 'nothing is intuitive when you are first learning about it. Knowledge builds upon itself over time. I’m sure a seasoned statistician with no exposure to chemistry would be saying the same thing about the naming convention in chemical compounds. Or why is the chemical symbol for gold not G. Etc.', 'I feel you, as a student this can be rough. I am a QA and Food Safety professional where stats is only a part of what I need to apply to my job so as you end up applying the same small part over and over you lose some knowledge. Hell half the time it is better for me to use reference materials provided by ANSI than it is to try to apply them myself. That being said its no reason to hate on the words for the class. Knowing them for the class will help you pick them back up if you ever have to relearn some things you dont use every day.']",0,8,https://www.reddit.com/r/statistics/comments/12d9d5d/why_is_learning_statistics_so_annoying_q/
522,2023-04-06 07:11:08,[Q] Statistical significance of a data set from a poll I conducted,"I recently ran a poll online where I asked if you agree or disagree with some statement. I asked respondents to indicate ""yes"" or ""no"" and then their age. I want to test a few hypothesis, and do this in Excel. I think I would use a T-test but I am unsure, it's been a while since I did this. 

E.g. data is like this:

Age 59+: Yes - 23 No - 41

Age 43 - 58: Yes - 33 No - 74

Under 42: Yes 53 No - 106

I wanted to test the theory that ""older folks"" would be more inclined to say yes than younger folks, and that overall, more people would say NO than YES (sorry, in this case for example, old is the 43 and over group). 

How would I do this in Excel?","['Generally, I think it’s a really bad idea to split continuous variables like age into arbitrary categorical levels. Lots of lit on this topic. Probably avoid that if that was how you were gonna analyze', 'Totally fine if your inferences are about the population that would take a voluntary survey.', 'I would do a logistic regression predicting Yes or No dichotomous outcome from continuous age IV', 'CHISQ.TEST() function will test if the probability changes with age window using the 3 by 2 table you have above.  \n\nIf you have the responders’ actual age you could also do logistic regression; this would have more power and get at the direction of the affect with older subjects.', ""You have to assume that your sample is representative of the population, or your inference is only applicable to this one sample.  Anyway, read this:\nhttps://stats.stackexchange.com/questions/113602/test-if-two-binomial-distributions-are-statistically-different-from-each-other\n\nor you can use Fisher's exact test :\nhttps://en.wikipedia.org/wiki/Fisher%27s_exact_test""]",0,9,https://www.reddit.com/r/statistics/comments/12d1bbh/q_statistical_significance_of_a_data_set_from_a/
523,2023-04-06 05:11:43,[Q] Question about regression analysis with numeric variable as Dependent and ordinal as Independent Variable,"\[Q\] Hello there! I'm trying to conduct  a mediation analysis with mixed variable types (ordinal and numeric)  and I'm at my wit's end. This these are the three regressions I am making and the first one is the procedure in question:

1. **Variable 1 (numeric) <- Variable 2 (ordinal) <= This is the analysis in question.**
2. Dependent Variable **<-** Variable 2 (numeric) + Variable 2 (ordinal) <= Already performed.
3. Dependent Variable <- Variable 2 (ordinal) <= Already performed.

**Variable 1**  is  a numeric variable (factor from factorial analysis, range: \~ -3 to  1) and the Dependent Variable and Variable 2  are ordinal variables (scale: 0 to 3, ordered).Previously, I've conducted partial proportional odds models successfully, but this particular  procedure (1) has left me puzzled.

* Should I  transform the numeric **DV** **(V. 1)** for this analysis? / Is dummy coding a viable option?
* Can I convert the ordinal **IV (V. 2)** to numeric for the sake of completeness?
* Or is there a procedure / package I've overlooked?

Any suggestions on how to measure the impact of IV2 on IV1 are appreciated. Thank you!

Edit: Clarification of the question.","['In brms you can wrap the ordinal predictors in the monotonic mo() function.  I think Paul Buekner has an accompanying paper explaining but too lazy to look it up.', 'Structural equation models to the rescue?\n\nYou should provide more context why you conduct this analysis.', ""Found it! I'll look into it."", 'Thank you for your response! The research question is ""do values affect the relationship bettween childhood adversities and violent behaviour"". I have questionnaire data und there was a problem with mulitconlinearity so i did a factorial analyisis with the 10 Items and got 2 well-fitting factors out of it (values). After the FA they are numeric and range from negative to positive values.   \nCurrentyl I am thinking to just do last two PPO and measure their difference with and withouth the potential mediating/moderating value. With that I can circumvent the modelling of the first IV1 <- IV2.']",17,4,https://www.reddit.com/r/statistics/comments/12cy3sm/q_question_about_regression_analysis_with_numeric/
524,2023-04-06 01:16:02,[Q] Sample vs Population - Do I have this right?,"I am trying to solidify my understanding of sample vs population. For the most part I think I understand. I get that a sample is just a subset of the population I am looking at. For example, If I wanted to know whether or not there is a relationship between the height and weight of people it would be really hard to get accurate data on every person on earth. Instead, I would take a random sample and infer that my sample is a representation of the whole population.

&#x200B;

My question is when I wouldn't take a sample because I can get the whole population. For instance, let's say I work at a company and I wanted to know within that company if there was a relationship between a person's gender and whether or not they leave our company. I wouldn't use a sample I would use the whole population because I have access to that data. 

Would this change the way I would do statistical testing? 

Would I be doing inferential statistics on a population?

Do I have to reframe my thought on what a population is?

&#x200B;

Just trying to nail down the difference and my thoughts on the conclusions of running statistical tests.","['> For instance, let\'s say I work at a company and I wanted to know within that company if there was a relationship between a person\'s gender and whether or not they leave our company. I wouldn\'t use a sample I would use the whole population because I have access to that data. \n\nIn that case, you are almost certainly interested in whether your companies culture/administration/policies affect the tendency of a particular gender to stay/leave. You\'re probably not interested *only* in the specific people who happen to work at the company right now, or in the past. Even if you examine everyone who has ever worked at your company, you\'re only observing a sample of people who have experienced the specific factors at work in your company.\n\nThe ""population"" is the data generating process. If your research question is about a specific group of people, then that set might reasonably constitute the population, but for questions of the form ""Is X related to Y"" (e.g. does my company discriminate based on gender), even the entire group of people who have ever experienced X is still only a sample of the underlying mechanism relating the two variables.\n\nAs an example, if your company flipped a fair coin in order to determine who to fire, this process is absolutely non-discriminatory. But it is very unlikely that the *empirical* distribution of firings would be exactly equal -- e.g. if 1000 people worked at your company, by chance it is likely that slightly more of one gender have seen heads than the other. Your interest is in the *data generating mechanism* (the coin flip), not the sample of people who happen to have worked at the company.', '> I wanted to know within that company if there was a relationship between a person\'s gender and whether or not they leave our company.\n\npresumably you want to know that because such knowledge could influence some kind of company policy (e.g. \'women leave because they feel we aren\'t promoting women, we should see what we can do to address that\'). In that case your population - the one you want to make conclusions about - is not ""people in the company at the time of sampling"", but across a sequence of times where the policy were to be in place.\n\n(Of course you can\'t sample those populations, but they\'re the ones you want to carry your conclusions over to.)', 'Thank you for the comments. The main thing that I was hung up on was the population through time. It makes sense that we would still consider the data as a sample of what could be in the future.']",3,3,https://www.reddit.com/r/statistics/comments/12cr9z6/q_sample_vs_population_do_i_have_this_right/
525,2023-04-05 22:52:34,[R] t test for ratios,"I am a biologist. I have some data that fits the description of this website.
https://www.graphpad.com/guides/prism/latest/statistics/stat_paired_or_ratio_t_test.htm
As a result I want to use a t test for ratios. But I don't really find a lot about t test for ratios online except on this Graphpad website. Is it a commonly used method? Is there any alternatives? Thanks a lot, people.","['I have never encountered the term ""T test for ratios"", but perhaps some other redditors have. \n\nThe name Is tricky, because there are many tests regarding ratios, but the test you\'re talking about would be generally seen as a T test for transformed samples. That might be why you\'re having trouble searching for it.', 'That term isn’t commonly used but transforming to logs and testing differences in geometric means and computing confidence intervals on ratios of geometric means are well understood. [This webpage](http://www.jerrydallal.com/LHSP/ci_logs.htm) has a good explanation.', ""I've also noticed that Prism is the only place this test is described this way. u/dmlane is correct though that the underlying logic is common.""]",2,3,https://www.reddit.com/r/statistics/comments/12cmwew/r_t_test_for_ratios/
526,2023-04-05 21:35:24,[Q] Interpreting Coefficient of Logistic Regression with Independent Variable Expressed as Percentage,,[],0,0,/r/AskStatistics/comments/12c555g/interpreting_coefficient_of_logistic_regression/
527,2023-04-05 19:20:58,[Q] How does the hypothesis formulation influence the statistical analysis?,"I have a few questions regarding the formulation of hypotheses.

For example, here are 2 hypotheses:

1. Age will influence the choice of food individuals order at the restaurant.
2. Younger individuals are more likely to order fried food at the restaurant.

These 2 are almost the same, yet different. Does one have an advantage over the other? How does the stats analysis change, if at all? Which one would I be better off choosing?

Another example could be a hypothesis on gender and safety.

1. Gender impacts the individuals' feeling of safety when walking home at night.
2. Women are more likely to feel unsafe when walking home at night.

Thank you for your help.","['The way you state the hypotheses will define the nature of the data: age could be ratio, ordinal, interval or nominal. That will influence the type of test. It also matters if the hypotheses is stated directionally or not. The second one is directional, I guess. So it determines whether you should be considering one tailed or two tailed test.', 'Thank you']",0,2,https://www.reddit.com/r/statistics/comments/12cgyym/q_how_does_the_hypothesis_formulation_influence/
528,2023-04-05 18:10:41,[Q] What are your favorite data visualization packages in r?,"Just like the title says! I’ve experimented a ton with everything ggplot and plotly has to offer, and I’m familiar with a lot of other libraries, but what are some libraries that you guys have been using, whether with or without shiny, that display data pretty well?

Side quest: what are the best ways to display regressions on top of existing data? I’ve only ever used base r for lm visualization","['Ggplot2 is really the end-all be all for me. Combine with GridExtra and ggally for anything that’s missing. Specific packages often come with specialized visualizations for model outputs like LASSO regression and I often use those rather than reinventing the wheel. But anything other than that is ggplot2. \n\nAs far as plotting, simple models can be fit with geom_smooth. More complicated models, I fit separately and store the predictions in a new data frame and plot separately.', ""Man this is a much easier question than the Python equivalent.\n\nBesides ggplot2, I like:\n\n* ggforce.  It has some nice miscellaneous functions.  I use facet_zoom a lot, which shows a plot with a zoomed-in portion of an existing plot.\n\n* ggrepel.  Creates readable labels that automatically dodge each other, which is more readable than what geom_text and geom_label do by default.\n\n* ggridges is great for showing a bunch of distributions in a relatively small amount of space.\n\n* gganimate is great for demos.  It lets you easily add animation components to a static ggplot object.\n\n* leaflet is great for 3D mapping.  You can use OpenStreetMaps or similar as a background.\n\n* ggsurvplot for plotting survival objects more flexibly than R's default plotting."", 'https://r-graph-gallery.com/\n\nEnjoy the rest of your day trying these out :)', 'You have the esquisse package, they have an interactive ggplot builder, where you can drag and drop variables in x and y axis, choose plot types, colors, adjust legend... and then export the code.\n\nTo make an interactive ggplot like in plotly, just call the ggplotly funtcion on your plot made with ggplot2.', 'I use base R.']",64,22,https://www.reddit.com/r/statistics/comments/12cfbkr/q_what_are_your_favorite_data_visualization/
529,2023-04-05 17:20:31,[Q][D] A problem that requires your creativity! Can it be solved?,"Let's assume you are a platform (called X7) that connects distributors to supermarkets. 

You have two services: 

1) X7-orderNow: as a supermarket you order a basket of goods from different distributors. 

If a supermarket uses this services, only a fraction of the supermarket's purchases are being captured, many purchases are done outside the platform.

2) X7-PointofSale: as a supermarket you sell to retail customer, and all your purchases from distributors are automatically registered.

If a supermarket uses this service, all its purchases are captured.

Therefore, we have 4 types of supermarkets:

1) uses X7-orderNow only (73% of all the supermarkets)

2) uses X7-pointofSales (2%)

3) uses both (2%)

4) uses None (23%)

&#x200B;

which means:

4% of the supermarkets have there purchases being fully captured.

73% of the supermarkets have a fraction of their purchases being captured.

23% of the supermarkets have none of their purchases being captured.

&#x200B;

Statstically, scientifically, practically, hypothetically, whatever approach you want to take, would I be able to answer the following question:

What were the total purchases of a given product (i.e. across all supermarkets) in a given month?

If yes!!! How would you approach this?","['Okay, well you can never know the total purchases of a given product exactly, but you can develop a pretty good estimate with statistics if you’re willing to make a few assumptions. \nHowever, it is not known whether the fraction of the 73% using X-7 OrderNow includes this product, ideally it does. And I also don’t know if this fraction changes store-to-store such that it sometimes includes the product, and sometimes doesn’t. So, I’m going to assume it sometimes includes the product. \n\nSome assumptions:\n1) there’s no systematic difference between stores that use this program and stores that don’t (I.e., the 4% is representative of the population of all stores).\n\n\nLet’s start with what we do know:\n1) We know all of the sales of this product from at least 4% of the stores, and at most 77% of the stores. \n\nMy approach would be to find out how many stores we have sales-data on for this product, and to also know this as a proportion of all of the supermarkets (e.g., 4%?).\nThen I’d record how many sales of this product each of those supermarkets made within the month. \nI’d find the mean-average of the distribution of sales for this product, and then I’d calculate the standard error of the mean (using the finite population formula). \nUsing this, I’d then find the margin-of-error, and then the 95 and 99% confidence intervals for my mean. \nFinally, to get an estimate of the amount of this product that would’ve sold, you would multiply the mean by however many supermarkets there are altogether (even the ones not using the programs). However, I’d also do the same with the lower-bounds and upper-bounds of the confidence intervals to gauge how confident you can be in your estimate.', '""find out how many stores we have sales-data on for this product"", regardless what type of store, i.e. a store using the service orderNow, poitOfSale or both?', 'In my analysis, I assumed that there was no systematic difference between stores that use one program, and stores that use another. \nSo, this assumption means that it would make no difference. \nHowever, if you expect a difference, then just do the exact same thing, but break the data down by the program the store uses. Sadly, you would also be left with no information about stores that we have no data on though. \nAs well as this, other factors like region and population density would be important to consider, but weren’t considered here.']",0,3,https://www.reddit.com/r/statistics/comments/12cedgj/qd_a_problem_that_requires_your_creativity_can_it/
530,2023-04-05 13:33:12,[Education] Excited to learn more statistics,"Excited to learn more Statistics

Hello,

I’m an adult student studying math and computer programming. I’m a total beginner in the computer stuff, but I took Calculus in high school 25 years ago (damn I’m old).

I’m remediating my math skills, working up through algebra and trig, and starting Calculus again this summer. I also took Elem statistics last fall and enjoyed the real world aspect of it. It might be the first real applied “math” I’ve ever taken. 

I’d like to learn more stats, but The Internet™️has told me that learning more is going to take a lot more math skill first. I’m planning on taking Calculus 1-3 and linear algebra at my community college. What should I master so that I can learn more stats, and what stats should I try to learn once I can do it?

Thanks.","[""> I’m planning on taking Calculus 1-3 and linear algebra at my community college. \n\nThose are just the right things to start with, if you want to learn the basic theory (which will get you quite a way) that will suffice to get you through something like the harvard probability course\n\nhttps://projects.iq.harvard.edu/stat110/home\n\n(The book there is free as a pdf, there are video lectures and exercises etc there)\n\nThat will be enough probability to tackle a typical math stats subject (but you can probably leave the later chapters until a bit later). There are many suitable texts that cover basic theory of inference (e.g. Larsen and Marx, or Mendenhall, Wackerly and Scheaffer ... or any number of others). From there you can pick up many other topics in statistics, but I'd suggest a regression text next, one with a theory component as well as coverage of the practical side (I don't have a particular recommendation for that)."", 'Thanks for your help. I have Mendenhall, and I also have a book called Statistical Inference by Casella. Which is better?']",9,2,https://www.reddit.com/r/statistics/comments/12c9qkf/education_excited_to_learn_more_statistics/
531,2023-04-05 12:22:33,[Q] Most suitable test to compare changes in word frequency in Twitter after a treatment,"Hi, I'm having real hard time finding the correct test for statistical significance.  I'm trying to measure if a statement by an influential figure had an effect on a particular discussion on Twitter, one of the ways I do this is by comparing frequencies for certain keywords before and after the statement, i.e. the treatment in my case. Keywords are also grouped under certain themes, such as violence, ethnicity,rights-based etc. So my data looks like this

&#x200B;

|Keywords|Group|Pre Treatment - Group Frequency|Post-Treatment Group Frequency|
|:-|:-|:-|:-|
|\[indian, arab, white\]|race|150|100|
|\[killing, beating\]|violence|120|140|
|\[civil rights, human rights, law, legal\]|rights|50|80|

&#x200B;

So far I've been reporting the simple percent change for group frequencies, which feels lacking. I've been suggested to take the means of each keyword group and use dependent/paired samples t-test, but I'm hesitant as I can't claim pre-treament and post-treatment tweets include the exactly same users. Is it fine to do that, or is there a better way to test the treatment?

I've read that ANOVA is also not recommended as the independence assumption is violated ([https://stackoverflow.com/questions/66423466/anova-test-on-time-series-data](https://stackoverflow.com/questions/66423466/anova-test-on-time-series-data))","['Do you have placebo (non-treated) users in the “post” period? Without them it’s difficult to distinguish whether the changes in word frequencies come from the treatment or just trend.', 'Hi, you could also try doing counterfactual inference. So you we expect that there will be bias in doing the straight comparison using a t-test between your pre and post treatment groups because we believe that tweet keyword frequencies change over time. So we try to remove the change over time bias by fitting a time series model to the data up until the treatment time. Then you use your model to forecast the group frequencies from the start of your treatment time to the end of your treatment time. This counterfactual answers the question what would the keyword frequencies have been if the treatment never happened. Once you have this counterfactual prediction you can simply take the difference between the true post-treatment group frequency that you measured and the predictions that the model made and this gives you your causal impact of your treatment. Now to determine ""significance"" you have two options, you can either do all of the above in a Bayesian framework (recommended) where you will have uncertainty estimates for your causal impact estimate or you can generate a bootstrap confidence interval. If your confidence interval/highest density interval does not contain 0 (or some value of relevant equivalence that is reasonable) then your results are not due to random chance.']",2,2,https://www.reddit.com/r/statistics/comments/12c86li/q_most_suitable_test_to_compare_changes_in_word/
532,2023-04-05 10:43:55,[R] I need textbook or resource recommendations to learn more about more advanced statistics,"I’m getting deeper in my thesis work and I need to be more than just familiar with Generalized linear mixed model and Multivariate statistical analysis. Are there any textbooks that cover these topics that you can recommend. Most college textbooks just stop at Anova.
I’d appreciate textbook recommendations or any other form of resource. I don’t have a very strong stat background but I happen to enjoy statistics a lot and I understand the topics fairly easily. YouTube videos haven’t been as intuitive for me to understand.","[""Statistical inference by Cassella and Berger should cover everything you'll need or at least provide the bridge to understanding stat theory if you'll need to dive into that."", 'You may benefit from checking out this [reading guide for an independent study program in statistics](http://stanfordphd.com/StatisticsTutor.html) (2nd half of the page).', 'Wow. Thanks for this list. Would be very helpful indeed.']",1,3,https://www.reddit.com/r/statistics/comments/12c5tso/r_i_need_textbook_or_resource_recommendations_to/
533,2023-04-05 10:23:31,Use cases for Bayesian time series? [Q],"I’m currently in a time series course where we are learning about classical models: white noise processes, AR, MA, ARMA models etc. I was wondering when one would use the Bayesian version of these models, or in general when does Bayesian time series come about? I’ve heard of “Bayesian structural time series” is this a Bayesian way of decomposing a time series into things like seasonal, trend components? What do we put priors on etc?","['The use cases for Frequentist and Bayesian are different. They are not substitutes for one another. They are orthogonal to one another, literally. \n\nFrequentist probability works in the sample space. The sample is randomly chosen from the sample space. That is not true in Bayesian probability. The parameters are fixed, the sample is random in Frequentist probability. \n\nBayesian probability does not, strictly speaking, use the parameter/sample split. Instead, it speaks of observable and unobservable things. Parameters, generally, are unknown, so are random. Randomness here is in the sense of uncertainty not chance. However, if a parameter is known, it is treated as observable. Data is generally known and has no randomness to it. However, missing data is no longer observable, so it is treated as random. Likewise, future data is unobservable at this time. So a known parameter is treated as data and as missing value is treated as a parameter. \n\nYou place prior distributions on unobservable things. Priors, generally, should be proper and informative when information about their location exists, such as prior research or theory. Priors are actually important, and SHOULD bias results. That bias is desirable, in this case because, if done correctly, should contain information about the unobservable thing. \n\nBayesian methods are generative, not sampling based. The best Bayesian model will match nature’s way of creating data. If the best Bayesian model is AR(2), given a large enough sample size, then you can be reasonably certain that the Bayesian model is the true model if there are no omitted variables. \n\nFrequentist methods are about recovering as much information in the data as is possible, subject to the conditions and constraints in the model, such that it has good properties on repeated sampling. Frequentist time series tend to be lossy. It is impossible for a Bayesian method to lose or double count information. A Frequentist model can have equal or less information in it than a Bayesian model. It can never have more. The Bayesian likelihood function is always minimally sufficient. \n\nA Frequentist method is superior when you need large scale social agreement as to the results. Everybody will have a different prior so everybody’s Bayesian results will differ. A p-value has an agreed upon meaning. \n\nA Bayesian result is superior if you need to gamble or if you need to find something for noncollective reasons. \n\nFor example, let us imagine that you are a COVID researcher back near the beginning. You will want unbiased estimates of its rate of spread because the results will be politically consequential. \n\nNow imagine that you are, instead, running a hospital and you are seeing a wave hit New York City and you want to know how long before you are overwhelmed in your hospital in Wyoming. You can capture all the information from other areas in your prior. It is as if New York gave you a Wyoming sample before it happened. You can greatly improve the quality of your predictions. In effect, you are getting a larger sample size. \n\nIf you are placing money at risk in a competitive situation or need to find the value of something that you cannot see, use a Bayesian method. If WE need a result from you as a community in a manner WE can all agree on, use a Frequentist method. \n\nBayesian methods cannot be stochastically dominated, minimize the average loss created from getting a non representative sample, they cannot be arbitraged and their predictions minimize the K-L Divergence. They are ex post optimal methods. \n\nFrequentist methods minimize the maximum amount of risk, can be unbiased, and give 100% weight to the data and nothing to outside information. They have an agreed upon meaning. They are ex ante optimal methods.', 'you can easily build expressive models with latent variables and put priors on them\n\neg HMM, DLM and state space models more in general', 'In macroeconomics bayesian vector autoregressions are huge and DSGE models are often estimated using bayesian methods.', ""Filtering. The Kalman Filter and its variants are Bayesian inference on a time series.\n\nYou have a hidden state x\\_t, x\\_t+1,.. and observations y\\_t, y\\_t+1.. . There is a model for the evolution of the hidden state (e.g. physics of an asteroid moving through space) and a model for the observations given the hidden state (how an image of the asteroid is produced given its position and velocity). There is also a prior for the hidden state, which comes from the previous time step.\n\nBayes' theorem will give you the distribution of the current hidden state given current observations. Continuing the example, the posterior is a distribution over the position and velocity of the asteroid, which gets updated given new observations of the asteroid as time evolves."", ""Take a look at fb prophet. It's a bayesian ts framework and it's open source as well. It's fairly well documented so you may be able to absorb some of the motivation as well as practicalities of bayesian ts modelling by going through the docs and then the source code.""]",18,6,https://www.reddit.com/r/statistics/comments/12c5azg/use_cases_for_bayesian_time_series_q/
534,2023-04-05 08:39:26,[E] Youtube recommendations,"
Hello! 
I am looking for some cool educational YouTube channels on stats. 

For example, 3blue1brown is great for math, Essentials of Linear Algebra is elite for me.

What are some channels with similar quality that teach stats related topics that you'd recommend?","['StatQuest', 'Bam!!', 'DOUBLE BAMM!!!!', 'I recommend [statisticsmatt](https://youtube.com/@statisticsmatt). He has been helping me in my statistics courses (probability, inference and regression).', 'there is an channel that posts their lectures for MBA stats and analytics I know of.  https://www.youtube.com/@OnlineBusinessAnalytics']",37,8,https://www.reddit.com/r/statistics/comments/12c2l7c/e_youtube_recommendations/
535,2023-04-05 08:12:01,[D] Need help creating a formula using multiple variables,"Hello. I have a bunch of data. I think the easiest way to explain is that I have a bunch of possible independent variables (like, 15<) and 1 dependent variable. I don't know for sure if all of the independent variables even have an effect on the dependent variable. I'm looking for a way to develop a formula that has a very strong relationship with the dependent variable. Imagine not knowing that velocity is the change in speed divided by change in distance. Is there a way to compute/process columns of speeds, distances, and velocities that would output the equation v = ds/dt? But in a way that could process 15+ different independent variables rather than just two?


I was thinking, since all of this data is on a spreadsheet that I could find the P-value between each individually independent variable and the dependent variable? Is that a first step somewhere? I have no idea, maybe this is useless.

Please help!","['Look up LASSO regression.', 'So you want to get this out of the spreadsheet and into a statistics environment like R, Matlab/Octave or use some Python. \n\nThey have built in functions that, to put it simply, just consume the data and spit out all the answers you are looking for. Keep in mind those answers are technical and you need to provide some information about the variables which you may are may not be aware of; if you provide the wrong info the answers that you will get will be wrong. \n\nYou just can’t do that with excel. And even if you could, you would still get technical answers and require knowledge of the variables you are using.']",2,2,https://www.reddit.com/r/statistics/comments/12c1v4b/d_need_help_creating_a_formula_using_multiple/
536,2023-04-05 04:42:24,[Q] Determining statistical significance,"I'm conducting a market research project where I have obtained satisfaction ratings (1 - not satisfied at all, to 10 - very satisfied) from the general population across a series of brands. Respondents can provide ratings for multiple brands if they have experience with each.

I have summarised the response data into means and distribution plots by brand but am having difficultly deciding which statistical test to apply to determine which brands have an adequate sample size to be confident in its aggregation.

My summarised data looks something like this:

Brand | Responses (n) | Mean Rating
:--:|:--:|:--:
Brand A | 854 | 7.43
Brand B | 18 | 7.26
Brand C | 1,034 | 7.11
Brand D | 228 | 6.43

What would be an effective test for determining if ""Brand B"" received enough responses for their mean rating to be fairing compared against the others. I'm not sure if I'm overthinking things by reading into ANOVA scores or if it's just a simple p-test? Just need someone to point me in the right direction.

I'm using R for my analysis if there's any recommended functions to apply to the observation sets. 

Many thanks!","[""If by fairly rated compared to the others, you mean there are no significant differences between the means, then an ANOVA will suffice if you can't reject the null (which would imply not enough evidence for any differences across all means). You could also do t tests if you want in a pairwise fashion, but you ought to do multiple comparison corrections. So you could compare category B pairwise to all the others one by one."", ""(long discussion removed for now because I missed some of the information in your question)\n\nOne thing that worries me is that you don't have information for all brands from all people. That's going to make it a little bit harder to do well, but it won't stop you doing something, as long as you can reasonably assume that it's missing at random (rather than in some way connected to ratings, for which, unfortunately, there are some potentially plausible mechanisms). \n\nPresumably you would want to treat the individuals as having  random intercepts in a linear mixed model.\n\nFailing that you could treat the individuals as blocks I guess, but that's less suitable."", 'Thanks for your help. \n\nTo clarify what I mean by ""fairly rated"" - I want to establish if there are enough responses per brand to determine if a single brands mean response rating is a valid rating.\n\nThink of it like looking for a pizza restaurant on google maps, if there\'s a pizza place with 300 reviews and a 4.7 rating, you know that place is probably pretty good, but what about a pizza place with 8 reviews and a 4.8, I might not trust that rating because the sample size is too small. If I wanted to say which is the best pizza place in my town, what would be the minimum number of reviews per restaurant to be considered in the top 10?', 'Thanks for the long original response - and for re-clarifying.\n\nI do have all the original response 0-10 ratings for each respondent for all brand. For this single question I would have ~3,000 responses across something like 25 total brands.', ""Conceptually what you are talking about is a simple t test of means, whether there is a significant difference is a function of the difference of means and the (sample) variances. In your pizza example, I would do a test with mu = 4.8 with the null hypothesis that it is equal to 4.7. Statistically speaking, there is no sample size that is too small to detect /any/ difference; the smaller than sample size, the greater the variance in this case, which will make your t test value smaller as it is on the denominator. Hence, to detect a significant difference, you would need a larger difference in mu vs the null hypothesis. The idea that you're close to is the power of your test, the ability to detect differences given a sample size, and you absolutely can calculate the power of tests you're going to do, but it is a little more involved.""]",5,5,https://www.reddit.com/r/statistics/comments/12bvvb1/q_determining_statistical_significance/
537,2023-04-05 02:57:02,[Q] Which stats test to use when I have unequal variance & skewed distribution?,"Can’t do a students t-test, or welchs, nor can I do a wilcoxon. What to do?

Thanks!","[""1. What is you response variable? What values can it take? (NB not what values are in the sample, but what is able to happen)\n\n2. What is the working hypothesis or research hypothesis? (/what *exactly* are you trying to find out?)\n\n3. Why can't you do the things you say you can't do?"", 'Anova is generally conservative with skewed distributions although it is often the case that a transformation such as the log (to reduce positive skew) increases power. One alternative is a randomization test which can be done on [this website](https://www.lock5stat.com/StatKey/).', 'Try some nonparametric tests....order statistics should be robust.', '\n>to accompany\xa0Statistics: Un*lock*ing the Power of Data\nby Lock, Lock, Lock, Lock, and Lock\n\nTIL: this book exists and these authors must have had lots of fun coming up with that book title', 'Yes, one of the greatest book titles ever and from a family of statisticians.']",4,5,https://www.reddit.com/r/statistics/comments/12bsqev/q_which_stats_test_to_use_when_i_have_unequal/
538,2023-04-05 02:53:49,[Q] Breaking up a dataset into groups based on frequency of occurrence. How to determine what the groups are.,"Background: I am a data analyst dealing with a dataset of about 33,000 chemicals which are broken up into about 800 classifications. My task is to break those up into the least and most represented classifications. I am using R.

Problem: The top 15 classifications range from 500-1200 occurrences, the next 50 from 100-500, the next 300 from 10-100, and the rest are below 10 with 300+ occurring once or twice in the dataset. Those ranges/groups are rough and I do not expect them to fit.

For representation, I was going to make a log bar plot displaying the top vs bottom x but that is obviously not a good idea since the bottom 100 are all the same value (1). Creating groups based on similar frequency may point me in the right direction. How do I figure out what ranges/groupings are statistically significant in order to organize the data?","[""My first approach wouldn't be to do this computationally; it would be to get a more qualitative sense of which kinds of chemicals are in certain ranges. It may be the case that they share something in common. Maybe make a stacked bar plot of which chemicals occur at which frequencies (bin them, perhaps, if that helps)."", 'First question is are you sure you need to group them. It is probably fine to give/show examples of classifications with few occurrences. If they are all at 1, it’s not your fault, you cannot say that 1 is less important than the other.\n\nIt would be possible to group classifications together, but the grouping can only be done based on the business context ie. you need to decide which classifications can be grouped together because you know they share some intrinsic similarities, not because of a statistical reason. You can then count the number of occurrences in each ‘new’ group and order them.']",1,2,https://www.reddit.com/r/statistics/comments/12bsmwo/q_breaking_up_a_dataset_into_groups_based_on/
539,2023-04-05 01:29:56,[E] Should I take Mathematical Statistics before applying to grad school?,"I’ve heard that it’s harder than real analysis at my school, and I don’t want to tank my GPA. I also want to be prepared for graduate school. Are there similar courses that go into stats theory? Or should I just take it and hope for a B.","['You would be shocked at how much easier math stats will be easier to you than real analysis truthfully. Most people who take math stats don’t have any proofs classes before it. Math stats doesn’t require proofs, but seeing something as rigorous as real analysis helps to an extent.', 'Having gotten an A (amazingly) in a Math Stats course, yes. It’s not easy, but my prof made the grade attainable — be prepared to work REALLY hard, use outside resources (papers sometimes… very, very rarely), and repeat the mechanical aspects of the class a LOT.', 'Grad-level mathematical statistics is likely a lot harder than undergraduate. My advice is to take both real analysis and undergrad mathematical statistics -- when you end up in grad-level mathematical statistics, it will be a lot of work but it will be familiar and your chances of success are much higher.', 'I’m confused by this comment. I’m trying to get into stats and all of the textbooks are chock full of proofs. Especially at the grad textbooks I’ve looked at. \n\nWhat books do you recommend?', 'Have you taken probability already?']",8,17,https://www.reddit.com/r/statistics/comments/12bq5jl/e_should_i_take_mathematical_statistics_before/
540,2023-04-05 00:22:24,"[Q] In hypothesis testing, how can we just reject/accept the Null Hypothesis by taking the mean only once for example from a sample and then calculating it to get the p-value?",Doesn't it make sense to calculate the mean from the sample multiple times to know if that mean value that was calculated initially actually occurs repeatedly?,"[""We don't generally perform experiments multiple times -- it's too expensive and too time consuming. So we're stuck with one sample, and the best we can do is ask if the sample mean we have observed is consistent with a specific population mean."", 'That’s the whole point. In a perfect world yeah I’d just measure the entire population but that’s not practical so we take a sample and draw conclusions. That’s why a power analysis is done prior so you know how large your groups need to be.', ""It's all about a middle ground between observing every instance (which gives you the true mean) and inferring from just an observation (which may not be that different from a random guess). \n\nSo yeah, it could be better to have a bigger or better sample, or multiple samples, but are the resources spent gathering that extra samples worth the marginal confidence you gain in your assessment?"", ""It might be helpful to play with some toy examples.\n\nFor example, if you sampled the weight of apples from an orchard, with a good sampling procedure, and the weights are (10, 11, 12, 13, 14, 15, 16, 17), how likely is it that the mean weight in the orchard population is 5 ?  It's pretty unlikely.  It's *possible* that you just happened to pick 8 really large apples, but you can have some confidence in the conclusion."", 'What you\'re asking about is already, to some extent, baked into the construction of the hypothesis test.\n\nFirst, assume that we\'ve collected as much data as is reasonably possible - because of course, if you can get more information then you should. Then using that data, you want to work out whether the evidence points to ""nothing special is happening"" or ""something special is happening"".\n\nAs a matter of course, the default assumption is ""nothing special is happening"", because we\'re usually trying to prove that something *is* happening, meaning the burden of proof lies on us to demonstrate it. We call the ""nothing is happening"" case the null hypothesis, and the ""something is happening"" case the alternative hypothesis.\n\nSo we build our test statistic. And we feed our data into the test statistic, and it tells us one thing:\n\n* Assuming the null hypothesis is true, are these results weird? How weird are they?\n\nAssuming we\'ve constructed the test statistic properly, the ""how weird are they"" part includes a measure of how likely we are to be wrong, which is a function of whether we\'ve collected enough data to have any confidence in our results.\n\nSo you *could* have a test statistic that involves splitting the data into parts, calculating individual means (or whatever is appropriate) from each part, and then looking at whether the spread of those values suggests that they give a consistent result. But you can take that to the logical extreme of considering each data point individually, and then finding a statistic that summarises the whole spread of the data into a single measure, and that\'s pretty much what most standard test statistics already do.\n\nFor example, if I\'m doing the classic ""test whether this coin is weighted"" test, my test statistic is likely to be something like ""the proportion of heads flipped"". And when we perform the hypothesis test, we calculate the probability that we would observe a proportion at least as large on a fair coin - and that probability is inherently dependent on the number of flips.\n\nIf I flip a coin 10 times and get 6 heads, there\'s a better than 1 in 3 chance I\'d get a result like that from a fair coin and that\'s pretty reasonable so we wouldn\'t reject the null hypothesis. But if I flip a coin 100 times and get 60 heads, there\'s only a 1 in 35 chance I\'d get a result like that from a fair coin, so I should probably be pretty suspicious that something\'s happening. And if I got 600 heads out of 1000 flips, then that\'s something like a 1 in 10\\^10 chance that it would happen with a fair coin, which seems like pretty damning evidence against it.\n\nSo yes, clearly more data > less data, but assuming we only have the budget (or patience) to flip the coin a limited number of times, we set up our hypothesis test in such a way that we have a clear measure of how ""weird"" our results are under the null hypothesis, and we only reject the null when it seems hard to justify doing otherwise. That\'s also why the language is usually ""reject the null / fail to reject the null"" - when the results are not weird enough we\'re not saying that the null is unconditionally true, just that there isn\'t enough evidence to say otherwise, in a sort of ""innocent until proven guilty"" situation.']",0,15,https://www.reddit.com/r/statistics/comments/12bo5s9/q_in_hypothesis_testing_how_can_we_just/
541,2023-04-04 23:31:18,[S] SPSS/R and Manual Calculation yields different Chi-square Results,"Checked many times that there is no data entry. Compared the Expected Counts in my manual calculation (abbv to MC) and SPSS. They yield different chi-square result. SPSS is 12-ish and MC is 13-ish. 

Downloaded R and loaded the same data. SPSS and R yield the same chi-square value. 

I am using the formula provided by my Statistics book: Summation of (o-e)^2 divided by e. 

What am I doing wrong with my MC? I want to prove in my thesis that even when MC-ing the data, it will yield the same results. 

Advice?

EDIT: SUCCESS! Rounding-off was the issue. Thank you very much to u/SalvatoreEggplant and u/bubbleberry1!","['The difference is due to rounding. \n\nFor instance in the first cell (male,column1) the expected frequency is exactly 26.1610169491525\n\nIt may look like a small difference but when squared and then summed over all the cells, it adds up. *(chi-square joke)*', ""Can you give an example of the contingency table and what you're manual calculation is ?"", ""The expected values look correct.  \n\nIn R,  I'm getting \n\n*X-squared = 12.821, df = 3, p-value = 0.00504*"", '*(O - E)\\^2 / E* \n\nlooks like\n\n*\\[,1\\]      \\[,2\\]     \\[,3\\]     \\[,4\\]*  \n*Male   0.05152553 1.0702234 4.250872 4.025962*  \n*Female 0.01876363 0.3897345 1.548005 1.466102*\n\nAnd the sum of these is 12.821', 'Yes. I also manually acquired those Expected Counts and they check out with the E. Counts in SPSS. But upon acquiring Chi-square, it yields a different result.']",11,7,https://www.reddit.com/r/statistics/comments/12bmlxg/s_spssr_and_manual_calculation_yields_different/
542,2023-04-04 22:02:57,[Q] What design is a 1 way between subjects ANOVA?,I have a one way between subjects ANOVA 1 IV (3lvls) and 1 DV. Can i call it an experimental 1x3 Between Subjects factorial design?,"[""No. It's just called a one way between subjects ANOVA. Not that it would make sense, but 1x3 would be two way with a 3 level and 1 level factor."", 'Thank you very much! :)']",3,2,https://www.reddit.com/r/statistics/comments/12bk4gq/q_what_design_is_a_1_way_between_subjects_anova/
543,2023-04-04 21:41:21,[E] Why the Cross-Lagged Panel Model Is Almost Never the Right Choice,[https://journals.sagepub.com/doi/full/10.1177/25152459231158378](https://journals.sagepub.com/doi/full/10.1177/25152459231158378),[],15,0,https://www.reddit.com/r/statistics/comments/12bjjpg/e_why_the_crosslagged_panel_model_is_almost_never/
544,2023-04-04 07:13:21,[Q] Analysis of factorial design experiment - how should a discrete numeric predictor be treated?,"Hey everyone,

I'm  looking for some literature on (or a somewhat simple explanation of)  the proper way to treat predictors which are continuous (e.g.  temperature) but manipulated at discrete values (e.g. 10C, 20C, ...,  40C) as part of a experiment with a factorial design, where this  predictor is one of the factors. My hunch is that this predictor should  be treated as discrete when fitting a linear regression or GLM, because  (1) it's not known a-priori whether the effect is linear, and (2) the  model with a linear effect would have ""too many"" degrees of freedom, and  would therefore be likely to lead to an inflated probability of type 1  errors.

My PI wants to fit these  as linear, because he's interested in testing a specific hypothesis  about the effect of e.g. temperature, which is that it has e.g. a  negative, linear effect, but I figure I can just test the linear effect  by using polynomial contrasts and post-hoc tests, e.g. with the  \`emmeans\` package in R. Intuitively, this seems like a better approach,  but I haven't been able to find any papers or posts explaining why  specifying a model of experimental data in the way my PI suggests is a  problem, or even whether it is a problem.

Thanks!","['It makes sense to do polynomial contrasts for linear and perhaps quadratic as well. There will be essentially no difference between a linear regression and a linear component of trend. Keep in mind that a significant linear component of trend just means the function has a linear component and if the function is monotonic the test of the linear component of trend will typically have the most power. Incidentally, I think the question about the predictor is categorical versus numeric rather than discrete versus continuous even though most software requires you to specify it in terms of the latter.', ""You seem to be conflating 'discrete' with 'categorical'"", ""Hmm, so there isn't a problem with fitting an equally spaced ordinal experimental factor as continuous in terms of statistical power - it's just a choice of which hypothesis you want to test? Seems like the linear model would be more likely to reject the null than the linear component of a polynomial contrast, no?\n\nAnd I'm using R / glmmTMB to fit a Binomial GLMM, so unfortunately the syntax for specifying the model is the same regardless of how the predictor is coded, and the terminology is none of the above, but numeric, factor, or ordered factor :/"", ""No yeah, I know the distinction between categorical / ordinal  vs. continuous predictors, and discrete vs. continuous random variables, even if I'm being loose with the terminology in the post... the question is whether treating an ordinal experimental factor as continuous in a (G)LM(M) is a bad practice, and if so why."", 'By definition, an ordinal variable cannot be assumed to be equally spaced. In a regression analysis (OLS) without higher-order terms, there is no practical difference between the null hypothesis that b=0 and that there is no linear component of trend. I would guess the same for the power would be the same to about 2 decimal places. I suspect the same for your analysis but I’m not certain. I would choose “numeric” but some purists may prefer “ordered factor.”']",2,6,https://www.reddit.com/r/statistics/comments/12b1go6/q_analysis_of_factorial_design_experiment_how/
545,2023-04-04 02:46:46,[Question] How to set in-process control limits for the mean?,"We have a manufacturing process in which the finished products have the following requirements: individual unit must have a weight within ± 5% of average weight (test with 10 random samples).

The in-process controls tests so far is:  
Test 1: Collect 10 products. Determine the weight of each product and the average weight. Individual unit weight must be within ± 5% of average weight (so same as finished specs).  
Test 2: Collect 10 products, determine the average weight (not the individual weight). The average weight must be within ±??% of theoretical weight.

Test 1 and Test 2 are done separately.

My question is: In test 2, which % should we set the control limits? From my limited understanding of statistics, I'm thinking since we're dealing with averages, should it be the standard error? (i.e 5%/sqrt(10) ~ 1.58%). If anyone can point me to any related documentation I'm be very thankful.",[],1,0,https://www.reddit.com/r/statistics/comments/12atpam/question_how_to_set_inprocess_control_limits_for/
546,2023-04-04 02:07:40,"[Discussion] How seriously do you take errors, in general? (sorry if this isn't the right tag)","I took a statistics class in DOX methods this semester to ""feel out"" the field of biostatistics. I was considering a medical degree as an undergraduate but hated the culture for various reasons, like elitism, caring more about their image than the patient, prioritizing memorization over critical thinking, among others.

I also hate when doctors make mistakes because they're too dismissive or egotistical to care. When my dad was a resident, he warned his supervisor that a patient seemed to have a lot of symptoms of this really rare disease, and that a biopsy, while it was the recommended route for more common diseases with similar symptoms, would be deadly in this case. The supervisor chewed out my dad, and the older residents laughed at a first-year resident correcting a seasoned physician. The next day, the whole hospital floor was quiet, because they did a biopsy on the patient, and her room was splattered with blood, empty blood transfusion packets everywhere. She was dead because of that doctor's arrogance. No apology or admission of poor judgment followed.

So I wanted to do biostatistics because with metascience is emerging as a field, we've observed that over half of scientific studies have statistical errors, and a majority cannot be replicated, with some fields like psychology approaching an 80% failure rate. Doctors go off of faulty data, and then their mistakes and biases are amplified, giving patients bad care but will treat them like idiots if they question a professional.

Thing is, my professor has made a few mistakes on the whiteboard this semester, and I also found three misprints in the book, confirmed by my professor. She also gave us a study guide for the first exam, and said that I should focus on learning Minitab than worry about what's going on in the equations. Then she changed gears completely, saying I should do more handwritten work, and then the day before the exam she changed the tested chapters from 1-3 which I'd been studying intensely, to chapter 4. I found two misprints in this section, but only after I bombed the exam. Everyone else also bombed, unless they asked the professor so much that she caved and gave them the answers.

I'm just really confused about the work ethic and culture here. I scored in the 97th percentile for college entrance exams and am attending a school that's in the mid-50s, but I'm still the village idiot in the class because I literally cannot change my professor's mind about my intelligence, and I'm not a math major. My motivation is completely shot because it doesn't matter how hard I work, she'll just change the goalposts. She accused me of cheating once because I just did too well to match her expectations of me. To compare, I'm a model student in Calc II and have a 101 average and my professor really wanted me to do an MS in math instead of what I wanted to do.

I'm so frustrated. Is this at all typical? I don't want to go into a field that's this chaotic.","[""The professor sounds like a worry.\n\nNot sure what to tell you, but I will say that errors in lectures  are very common (though often not too serious  if the person teaching is ready to correct them), even among people that know their stuff. \n\nMany  books are quite poor but even good ones will have a few typos. When learning stuff I like to supplement a book with several others so I'm not relying too much on any of them\n\nI can tolerate obvious typos (for all that I'd rather they were not still present 3 editions later!) It's the books that just flat out state superficially plausible nonsense that are the bigger worry.\n\nWhat text do you have?"", ""\n> I had a professor who used my class as guinea pigs to proofread a book he was writing \n\nI've had a similar experience though the class was postgraduate, so less of an issue"", ""Misprints in books and lectures are a nuisance. I had a professor who used my class as guinea pigs to proofread a book he was writing (and he did not write well.) But I don't think it is routine.\n\nIn every field there are conscientious researchers and sloppy researchers. But the great thing about statistics is we are the field that *studies proper management of error rates* --- if you want, you can attach a cost to a false positive and a false negative, and design the analysis method that minimizes the expected cost of errors.  That's the sort of thing I *wish* my doctor might do when deciding whether or not to run expensive tests on me, rather than just ordering every test in the book."", ""Hi, thanks for the response! I'm getting back to you late, but it's Design and Analysis of Experiments (10th edition) by Douglas Montgomery."", ""Montgomery? It should mostly be good, but damn you'd hope that almost all errors were gone by the tenth edition, since an author should only have to be very worried about new sections by that point.""]",2,6,https://www.reddit.com/r/statistics/comments/12asj2g/discussion_how_seriously_do_you_take_errors_in/
547,2023-04-04 01:34:52,Why don’t we always bootstrap? [Q],"
I’m taking a computational statistics class and we are learning a wide variety of statistical computing tools for inference, involving Monte Carlo methods, bootstrap methods, jackknife, and general Monte Carlo inference. 

If it’s one thing I’ve learned is how powerful the bootstrap is. In the book I saw an example of bootstrapping regression coefficients. In general, I’ve noticed that bootstrapping can provide a very powerful tool for understanding more about parameters we wish to estimate. Furthermore, after doing some researching I saw the connections between the bootstrapped distribution of your statistic and how it can resembles a “poor man’s posterior distribution” as Jerome Friedman put it. 

After looking at the regression example I thought, why don’t we always bootstrap? You can call lm() once and you get a estimate for your coefficient. Why wouldn’t you want to bootstrap them and get a whole distribution? 

I guess my question is why don’t more things in stats just get bootstrapped in practice? For computational reasons sure maybe we don’t need to run 10k simulations to find least squares estimates. But isn’t it helped up to see a distribution of our slope coefficients rather than just one realization? 

Another question I have is what are some limitations to the bootstrap? I’ve been kinda of in awe of it and I feel it is the most overpowered tool and thus I’ve now just been bootstrapping everything. How much can I trust the distribution I get after bootstrapping?","['Bootstrap distributions for statistics don’t always converge (quickly) to their true distributions. For example, consider bootstrapping for the sample maximum of a uniform distribution. You can show with some simple calculations that as your bootstrap samples approach infinity, the bootstrapped sample max is not a good estimator for the sample max', ""I've wondered the same thing - bootstrapping is kind of a cheat code. Over time I've concluded:\n\n1. Historically, statistics as an academic discipline evolved in an environment with low compute power, so a lot of theory was built to construct probability distributions from first principles. Now all this theory is sort of taught out of habit, even though lots of practitioners will just go straight for more compute intensive approaches like bootstrapping.\n\n2. The core idea of bootstrapping shows up in disguise more often than you think: for instance, you can think of the random forest model in machine learning as a sort of bootstrapped decision tree. It's a similar story for lots of other ensemble models.\n\n3. There are a lot of cases where it's not appropriate: if your data is skewed or biased in some way, bootstrapping can give you a false sense of security."", 'You didn\'t want ""quickly"", the bootstrap distribution does not converge to the sampling distribution of the estimator *at all*.\n\nUpvoted anyway.', 'The big one computation efficiency.\n\nIf you have an analysis that takes 24hr to run, 10k bootstrapped samples is not feasible.', 'Added quickly because in general, bootstrap estimators may converge, but at a slow rate. You’re right that in the case of the sample max, it doesn’t converge']",118,74,https://www.reddit.com/r/statistics/comments/12arj39/why_dont_we_always_bootstrap_q/
548,2023-04-04 00:28:32,[Question] Should I use two-way or repeated measures ANOVA,"I am trying to determine whether I should analyze my experiment using two-way or repeated measures ANOVA but am not clear on the definition of a repeated measures ANOVA. I ran my agricultural experiment as follows:

A population of 20 individuals of the same plant was subjected to two factors of treatments: temperature treatment (non-stress control vs. elevated temperature) and chemical treatment (untreated control vs. chemical).   
To break this down, here were the treatments:  
\- Non-stress control temperature + untreated control without chemical (5 plants)  
\- Non-stress control temperature + chemical treatment (5 plants)  
\- Elevated temperature + untreated control without chemical (5 plants)  
\- Elevated temperature  + chemical treatment (5 plants) 

Measurements were taken every 7 days for a duration of 35 days, which means that there were 6 data collection dates.

I am specifically looking to determine the significant effects of the chemical on plants under each temperature condition; therefore, I am comparing non-stress control temperature + chemical treatment to the non-stress control temperature + untreated control (no chemical). For the other group, I am comparing the elevated temperature + chemical treatment to the elevated temperature + untreated control (no chemical). \*\*\*I am not comparing all four groups to one another\*\*\*

I originally evaluated my data through two-way ANOVA using the general linear model procedure in SAS with the intention of defining the significant differences between chemical treatments for each temperature condition. Someone who reviewed my work wants to know why I did not use repeated measures ANOVA. Would repeated measures ANOVA be the proper way to analyze this experiment, or did I already analyze my data correctly?","['The repeated-measures ANOVA will let you test Day and the Treatment x Day interaction. The tests of the effects of your treatments will be the same.', 'Sounds like you need repeat measures ANOVA', '2 x 2 x 6 with temperature, chemical, time as factors', 'Seems fine. At 20 reps you have 480 df.']",1,4,https://www.reddit.com/r/statistics/comments/12apkrg/question_should_i_use_twoway_or_repeated_measures/
549,2023-04-03 22:49:16,[Question] Parametric modulation in fMRI GLM - log transformation?,"I'm getting more confident with statistics but still very unsure of myself, so would really welcome any input anyone has here. Thanks in advance for your answers!

I have a task-based fMRI dataset which includes ratings for each trial, where participants ranked how they felt about the stimulus they saw. These are 1-5, and I'm interested in whether the brain activations 'map on' to the rankings across the two conditions (https://andysbrainbook.readthedocs.io/en/latest/PM/PM_Overview.html). One condition is likely to receive a pretty low ranking and the other higher, but more variable. This tracks with how the histograms are looking for each individual, with a very aggressive right skew for one condition and a slightly less aggressive but much more individually variable left skew for the other condition. 

If we want to include these ratings while including as much variance as possible (ie not doing a median split and doing cross-condition comparisons that way) is the best thing to do to log-transform these? And then add them as a parametric regressor to the GLM? I've never done log-transformation before so not sure if this would be correct.","['The participant responses are the parametric regressor for the HRF in your fMRI GLM. Regressors don’t need to be normally distributed in a GLM. That’s a common misconception. It’s about whether the residuals are normally distributed. I would not log transform', 'This is not fMRI but just in general keep in mind median splits is generally not a good idea\n\nLot of lit on this topic but here’s one\n\nhttps://people.duke.edu/~gavan/bio/GJF_articles/McClelland_et_al.pdf', 'What glm? Why do you want to transform?', ""Thank you, that's really helpful to hear. So would plotting the residuals to check if those are normal be the next step, rather than the plots I have for the responses themselves? And that would be at the single-subject level as well?"", ""My supervisor wants to transform so that the scores in response to the two conditions are more normally distributed, I think because they're going to be included as parametric modulators/regressors and he thinks they need to be normal for that to be the case. The GLM would be the standard for fMRI, so including the sections of the tasks, the other regressors (heart rate variability in our case), and then the nuisance motion regressors. Does that make sense?""]",1,12,https://www.reddit.com/r/statistics/comments/12amr4p/question_parametric_modulation_in_fmri_glm_log/
550,2023-04-03 22:16:02,[Question] Measuring effects between variables (Binary/Likert),"Hi all,

I have a question about what tests to use to analyze the data of my experiment.

My main question is: How can I estimate effects between variables that are measured on a Likert scale. And effects of binary variables on Likert scale variables. I am somewhat familiar with OLS regressions, but I am not sure if I am allowed to use those with Likert-type data.

Your help would be much appreciated!

P.S. I am not allowed to add an image, so please dm me if you want me to send an image of my conceptual model if that helps.","['By ""Likert scale"" you mean single Likert-type items, not several items averaged into a ""scale"" per se ?\n\nIn general, the results of Likert-type items can be treated as ordinal responses.  The general approach is to use ordinal regression.  \n\nThere are also several tests that can be used with an ordinal response in simple designs.  Like Wilcoxon-Mann-Whitney, Kruskal-Wallis, Friedman, Kendall correlation, and others.', '\nSome are single-item Likert and others are averages of multiple-item Likert scales. Should they be treated differently? \n\nCan I use ordinal logistic regressions also if the IV is binary or also measured on a Likert scale?', ""When composing a scale, since you've already summed or averaged the responses, the result can be considered interval in nature.  (Although some will argue that it's still ordinal in nature. I don't really understand this argument.)\n\nResponses from single Likert-type items are probably best considered ordinal. (Although I could make arguments that they could be considered interval in some cases).\n\nYou can definitively use ordinal regression if the IV is binary.  Or for any combination of binary, nominal, continuous IV's.\n\nHow ordinal IV's will be handled really depends on the software.  For simplicity, often people treat them either as interval or as nominal.  But some software, like R, will explicitly treat them as ordinal, basically equivalent to treating them as nominal categories and testing if there are linear or quadratic trends across the categories."", ""Routinely in behavioral science, averages or sums of Likert scales are treated as interval data. \n\nJust do a t-test. Or 2x2 anova if there's 2 factors.""]",3,4,https://www.reddit.com/r/statistics/comments/12altgl/question_measuring_effects_between_variables/
551,2023-04-03 18:36:17,[Research] Need help analysing survey data,"Hi everyone,

I am currently attempting to explain how I will analyse my survey data and I am struggling with what method to use and why.

I am creating feedback forms for sessions. There will be a feedback form for every participant after every session (10 sessions in total with up to 30 participants).

The feedback forms have been made using the Likert scale (strongly agree to strongly disagree). The aim of the research is to see if the intervention as a whole as helped participants with their numeracy skills (completely made up topic).

So, on the feedback form there are a range of questions. Some are specific to that session (e.g the learning material of session 1) and others are standard questions that we are using to see a trend across the sessions. For example, ""I feel confident in my numeracy skills"" will be on every feedback form in hopes we will see a change in answers across the number of sessions (participant starts with a ""strongly disagree"" and by session 10 is a ""strongly agree"").

How should I analyse the results to see the change in responses over time? What is the best method and why? How should it be conducted?

Any help would be appreciated thank you!","[""If you want to look at the impact of an an intervention, you could do a before and after survey and use a paired t test to see if there's any significant difference."", 'Thank you! This was very easy to understand as you can see from my question statistics isn’t my strongest point! I’ll definitely take this into consideration when discussing the final analysis tool!', 'What if you take the mean & standard deviation of the scores prior to the sessions, then do the same after & compare the two numbers? Simple but effective. Then say that the scores after the session were “x% higher/better”. Maybe use a line graph & map out one line with the “before” numbers & the other with the “after” to really highlight the impact it had on the scores (hopefully they will be significant enough to look more impressive if you choose this route!)\n\nSorry if my answer is super simplistic, but I think this could be pretty straightforward & you won’t have to work too hard to interpret the data this way :) good luck, sounds fun & interesting!', 'Hey there!\n\nIt sounds like you have a well-designed survey and a clear research goal. To answer your question about the best method for analyzing the results, one approach could be to use a repeated measures ANOVA. This would allow you to test for significant changes in responses over time, while controlling for individual differences among participants. \n\nAnother option could be to use a linear mixed effects model, which would also account for individual differences and allow for more nuanced analyses of the relationships between variables.\n\nUltimately, the best method will depend on the specific research questions and data at hand. It may be helpful to consult with a statistician or research advisor to determine the most appropriate approach for your study. Good luck with your analysis!', 'hi,\n\nyou could combine both approaches from Haleodo & from Regina to utilize all the insight from designed questions. I presume the key metric under evaluation here is ""I feel confident in my numerical score"" (dependent variable) and there are few other questions (materials, length of session, etc.) (independent variables). To utilize all the questions designed, you could analyze step by step:\n\n1. To see change in responses over time: approach as suggested by Haleodo  \n\n2. After that, there\'s a high chance you\'d receive follow-up questions like ""What factors/elements from the session that drive participants confidence?"". This could serve as findings for improvement. At this point, you can run linear regression (cuz the likert scale gives numerical variables) or logistic regression if the questions are binary ones. Then pick up elements with high coefficient to the dependent variable to highlight in your report. Eg. if Length of session and Confidence score has significantly negative coefficient then you could recommend reducing the length of the session to improve confidence score.\n\nhope this helps!']",8,7,https://www.reddit.com/r/statistics/comments/12ag8mb/research_need_help_analysing_survey_data/
552,2023-04-03 06:58:14,[Question] Binary Logistic Regression Alternatives,"Hi all, I have data from my survey/cross-sectional study and I want to look at some questions (with binary responses) with predictor variables (mostly categorical, a couple continuous). Now I have been able to run the BLRs okay but my R² values are really low for each model (<.1).

Should I just simply report this or look at alternative models from: [https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-3-21](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-3-21)

Thank you!","['I wouldn’t say low R2 is a good reason to try alternative models if you think logistic regression was the right model for the data. That seems like phishing or phacking', ""Are you performing a prediction study or an association study? In other words, is your goal to predict who gets what outcome or to see if the outcome is associated with some particular predictors? If the former, there are a lot of strategies you can use to maximize 'accuracy' in whatever metric makes the most sense. We could maybe help if you give more details on your study. If the latter, (psuedo) R² and other similar metrics are all irrelevant because your goal is to get an unbiased estimate of a particular effect and R² won't tell you anything about that. A reasonable strategy for the latter question would be to think about all confounders for the relationship to your particular predictor and outcome and then use logistic regression with your predictor and those confounders. That would give you the (hopefully unbiased) log odds ratio for that predictor. If you have several predictors that you are interested in, this may require several different models based on which potential confounders should actually be included for each predictor.\n\nHopefully that at least helps you get started!"", 'You can use a linear probability model which is ols with a linear target variable.  Just be aware your residuals will be heteroscadastic so you’ll want to enable “robust” errors in your stats package.  A low r2 would only be of concern if you were making a predictive model.  If your goal is casual analysis it really depends on what you’re doing.', 'What are you using for (pseudo) r-squared for logistic regression ?', ""R square means nothing.\n\nWhy don't you to a 2x2 table in which you like at observed data vs. predicted, and check which % you are correctly predicting? That's a lot more useful than looking at R-squared.""]",12,10,https://www.reddit.com/r/statistics/comments/12a1bp6/question_binary_logistic_regression_alternatives/
553,2023-04-03 01:55:55,[Question] Can anyone explain to me how the concepts of Standard Deviation and Coefficient of Variability are used to interpret data in the archaeological context?,,"['same as in the other contexts', ""\n\n*Dammit Jim, I'm a statistician not an archaeologist.*\n\nWhy would we know about how archaeologists choose to use statistics? It's not like we'd be regularly reading their journals, I've never seen what textbooks archaeology students might be taught their 'stats' from (I bet it's none of the many intro books I have read), and no archaeologist has come to me to ask to collaborate on any research. Aside from popular science articles I haven't had a lot of contact.\n\n...  is this homework?"", 'I mean, there’s a lot going on in archaeology (I assume). Do you mean like “something is this old, plus or minus X number of years?” Because that’s something they’re probably getting from carbon dating which is more of a physics question. I imagine other phenomenon like that are similar.', 'I don’t even know what quantitative data archaeology generates, tbh']",0,4,https://www.reddit.com/r/statistics/comments/129sog8/question_can_anyone_explain_to_me_how_the/
554,2023-04-03 01:49:17,[Question] Event study with GARCH model," Hi, I have to do an event study, I analyze the US inflation and the returns of the biggest market indexes. Could you help me, I used GARCH modelling to forecast my estimated returns in the event window, than I got CAR, but how should I calculate SCAR? I am a bit confused, I learned it with only market model, but sure here it is not working :( Can anybody explain it?","[""That is the answer of ChatGPT:\n\nTo calculate the SCAR (Standardized Cumulative Abnormal Return), you can follow these steps:\r  \n\r  \nCalculate the cumulative abnormal return (CAR) as you have done already.\r  \n\r  \nCalculate the average (mean) and standard deviation of the normal returns (not the abnormal returns) over the same time period as the CAR.\r  \n\r  \nDivide the CAR by the standard deviation of the normal returns to obtain the SCAR.\r  \n\r  \nHere's the code to calculate the SCAR:\r  \n\r  \npython\r Copy code:\r  \nimport numpy as np\r  \n\r  \n\\# Step 1: Calculate CAR\r  \nWindow\\['CAR'\\] = Window\\['Epsilon\\_star'\\].cumsum()\r  \n\r  \n\\# Step 2: Calculate mean and standard deviation of normal returns\r  \nmean\\_return = Window\\['Return'\\].mean()\r  \nstd\\_return = Window\\['Return'\\].std()\r  \n\r  \n\\# Step 3: Calculate SCAR\r  \nWindow\\['SCAR'\\] = Window\\['CAR'\\] / (std\\_return \\* np.sqrt(Window.index.size))\r  \n\r  \n\\# Print the results\r  \nprint('Mean Return:', mean\\_return)\r  \nprint('Standard Deviation of Return:', std\\_return)\r  \nprint('SCAR:', Window\\['SCAR'\\])\n\n\r  \nNote that in Step 3, we are dividing the CAR by the standard deviation of the normal returns multiplied by the square root of the number of observations (i.e., the size of the window). This is done to standardize the CAR and make it comparable across different time periods.\n\n&#x200B;\n\nWhat do you think, correct?""]",4,1,https://www.reddit.com/r/statistics/comments/129shjs/question_event_study_with_garch_model/
555,2023-04-03 00:54:29,[Question] A question about justifying time-series analysis,"This question concerns the application of time-series analysis for the social sciences. Any insight, thoughts, or feedback is greatly appreciated.

Is there any rationale, besides theory, for determining if it’s worth examining the relationship between two series? In other words, is existing theory the only rationale justifying time-series analysis? Or is examining a scatter-plot of two series that appear related a prima facie justification to begin a more formal analysis, even when there is no existing theory to go on?

An example might be useful. Say we are interested in examining the relationship between two series, A and B. Both are monthly data with 100 observations. You plot each series over time. Both series seem non-stationary and appear to follow a deterministic trend. Without knowing any theory about the relationship between these two series, wouldn’t the fact that they both appear to follow an increasing trend over time be enough justification in and of itself for a more formal analysis?

With that said, I recognize that it’s problematic to correlate two non-stationary time-series since they are not independent observations. A random time-series with a trend component will appear related and correlate very highly with another random series with a trend, and simply adding a trend induces one time-series to be related to the other. This is the spurious regression problem that Yule (1926) identified nearly 100 years ago. Indeed, one would need to determine if the series are non-stationary using unit-root tests, and then difference them until they are stationary. These are the beginning steps of a more formal analysis to determine if there is truly a relationship between the series, if they are co-integrated and have a long-run relationship, etc.

But is examining a scatter-plot of the two series really “not telling us much”? Is there any value in doing this? Say that you plot series B and it appears to be stationary instead. When you examine the two series over time, they appear unrelated. Without existing theory, couldn’t the fact that the series appear unrelated be justification for not conducting a more formal analysis, especially since a stationary series cannot be driven by the same data-generating process as a non-stationary series? I do recognize that one cannot simply deduce if a series is stationary just by plotting it over time, as a series that appears stationary could in fact be non-stationary (unit-root tests are needed to verify this). But my point is that by plotting the series to get a sense of their patterns over time, doesn’t this provide important information in and of itself that can be used to determine if future analysis is warranted?

Besides existing theory, what else do researchers have to go on when determining whether to conduct a more formal time-series analysis between two series? If theory is all that matters, and we really cannot infer anything from series plotted over time due to the possibility of serial dependence of the error terms, then how are novel areas of research discovered when there is no theory to go on? As a final caveat, I do recognize that in practice, there is always at least *some* prior theory to go on. So perhaps theory is a necessary but not sufficient condition to begin formal analysis, where you need a plausible theory that explains the mechanism, but also a preliminary examination of both series to determine if there is a potential relationship between them. Thoughts?","["">Is there any rationale, besides theory, for determining if it’s worth examining the relationship between two series? In other words, is existing theory the only rationale justifying time-series analysis?\n\nOf course not. You can look for patterns and use statistical methods to help your find patterns that might continue to hold in the future, without knowing why.\n\nBut thinking about why some relationship comes about is an additional, and in practice extremely important, check against falsely concluding the relationship will continue to hold.\n\nIt may seem like statistical methods can tell you this (whether the pattern will extrapolate) for free, but they can't. You invariably need to assume something about how the data comes about (and will continue to come about in the future) for the methods to have their advertised properties. Theory is a key source for motivating such assumptions."", '>  wouldn’t the fact that they both appear to follow an increasing trend over time be enough justification in and of itself for a more formal analysis?\n\nThe fact they they both increase with time is a good reason to exercise caution. \n\nhttps://en.wikipedia.org/wiki/Spurious_relationship#Examples\n\n(... Otherwise you might conclude that global warming is due to there being fewer pirates, or that increasing priests in a town leads to more fires.)', 'You can regress time series A to time series B. The trick is, you have to analyze the residuals. If the residuals look like random white noise, then it’s okay. If the residuals have autocorrelation or are non-stationary, then you have a problem with spurious correlation. Look at the Engle-Granger method of cointegration', ""Great reply. I agree with you, it's helpful to hear it explained by someone else. Thank you!"", ""I already mentioned spurious regression in my post.\n\nEdit: I understand your point. But your examples...\n\n>global warming is due to there being fewer pirates, or that increasing priests in a town leads to more fires\n\nboth use *theory*, which is exactly my point. What if we don't have any theory? What if we have no information about the series? Can a scatter-plot demonstrating their over time patterns be used as prima facie evidence and rationale to begin more formal time-series analysis (i.e., unit-root tests)?""]",3,6,https://www.reddit.com/r/statistics/comments/129qx2m/question_a_question_about_justifying_timeseries/
556,2023-04-03 00:49:37,[Q] Basic question about assumption of normality for error terms in OLS,"Hi all, I’ve asked this question on SE but i’m still confused.

I had this question from a quiz in my stats class:

Which of the following is NOT a correct way to specify the assumptions needed for inference about the parameters of a Simple Linear Regression Model?

A. The experimental units are randomly selected, the responses are Normally distributed, and there is a constant variance of the points around the line.

B. The responses are random and Normally distributed with a mean of zero and constant variance.

C. The errors are independent and normally distributed with a mean of zero and constant variance.

D. ε∼iid N(0,σ)

Now of course B is the correct answer since those assumptions are wrong. But, would C and D not also be correct answers? I was under the impression that the error terms do not need to be independent nor normal due to Gauss-Markov Theorem?","[""The question specifically asks about inference, which may be intended to describe the traditional set of hypothesis tests (though it should really be more specific). If that's the case, then the assumptions about the residuals are required.\n\nAlso note that option A is false too - the responses don't need to be normally distributed. There is practically no assumption about the distribution of the response, or any of the predictors."", ' You don\'t need any of those assumptions (nor any of the assumptions in Gauss-Markov) if all you\'re doing is estimating a least squares line; the relationship doesn\'t even need to be linear.\n\nThe assumptions here presumably relates to hypothesis tests (or possibly CIs, both count as inference, but then so does estimation). Specifically, for tests, what is intended by ""assumptions"" is usually the basis on which the null distribution of the test statistic is obtained, since that\'s the basis for getting the desired significance level (and consequently, also p-values).\n\n\\* though they should definitely have specified what inference they meant, since it can certainly matter.\n\n(edited for clarity)', 'Conducting a test with the correct asymptotic level does not require Gaussian errors. You are correct. They need only be mean 0 with finite variance. In finite samples type 1 error probability can be inflated without normality, but the same is true of the test of a single mean, using a t-test.', 'C and D say the same thing!', 'Oh I missed that but thank you.']",1,5,https://www.reddit.com/r/statistics/comments/129qs94/q_basic_question_about_assumption_of_normality/
557,2023-04-02 23:22:26,[Q] How to best handle ordinal independent variables in causal mediation analysis?,"I'm writing a statistical analysis plan, and one of the RQs aims to estimate the effect of a treatment (binary) on Quality of Life that goes through a specific mediator, which is a symptom and is ordinal in nature (1-5 Likert scale). 

Since it's not an RCT, I'm using causal inference; and I made a DAG (using expert knowledge) which has 5 causal pathways I need to block. Some of the variables I need to include in the analysis to disentangle the effect of interest, are other symptoms; which are also ordinal in nature.

I'm aware about some options to deal with ordinal (likert) IVs, such as treating them as continuous or as categorical variables. However, I don't want to assume going from 1-2 is e.g. the same as going from 2-3 and I don't want to lose the ordinal nature either. And I don't know if looking at model fit is the right choice either.

What is the best way I can deal with ordinal IVs in case I'm behind causal inference? 

I would appreciate if someone can push me in the right direction. Many thanks in advance.","[""I am not an expert in causal inference, but I've learned that ordinal IVs should be entered as monotonic variables, which takes into account their ordinal nature:\n\n[https://cran.r-project.org/web/packages/brms/vignettes/brms\\_monotonic.html](https://cran.r-project.org/web/packages/brms/vignettes/brms_monotonic.html)"", 'The key with ordinal variables is that you cannot necessarily assume that a 1 point increase has the same meaning across the levels of the ordinal scale. \nBecause of this representing the ordinal variable as a continuous term with only linear effects is particularly unlikely to be reasonable, and so it is especially important to model the effects of the ordinal variables flexibly. You have a lot of choices for doing this. One nice choice is the default in R which is to encode them as integers, but consider high order polynomials of those integers. This allows the effects to vary nonlinearly across levels.', 'Very helpful, thank you so much', 'Thank you!']",1,4,https://www.reddit.com/r/statistics/comments/129ocqf/q_how_to_best_handle_ordinal_independent/
558,2023-04-02 16:41:31,[Q] Standardizing between 3 measured variables,"For data like below, would it make sense to take the average of the triplicate measurements as one x value, while using the average and stdev of all 9 values for standardization?

424 = average of 24hrs, 348 = total mean, 89 = total stdev

Standardized x for 24hrs = (424 - 348) / 89 = 0.85

Sample 1

&#x200B;

|24hrs|463|394|418|
|:-|:-|:-|:-|
|48hrs|414|343|347|
|72hrs|171|260|326|

I am planning to graph the standardized measurements of each variable's x average for a total of 4 samples (if it even makes sense to do so)","['I don’t really know what your research question or design is, but I’ll just eyeball it. \nIt looks like you have 3 groups of time measurements (24hrs, 48hrs, 72hrs), if these measures are correlated (I’m guessing they are) then you’ll want to work with the mean and standard deviations of each group, respectively. So, no don’t use the grand mean and grand std deviation. \nIt will still be fine to graph the standardised measures altogether, as of course, they are standardised, so you can interpret between sets of data.', 'See cross-posted question: https://www.reddit.com/r/AskStatistics/comments/129f5nh/standardizing\\_between\\_3\\_measured\\_variables/']",5,2,https://www.reddit.com/r/statistics/comments/129f7wu/q_standardizing_between_3_measured_variables/
559,2023-04-02 16:25:58,[Q] How can I know the mean and the variance of the population before even doing a sampling on all the population?,"Hi! I'm a student in Aerospace engineer and I'm trying to get a grasp on statistic.

I saw [this](https://www.youtube.com/watch?v=F2mfEldxsPI) video and there's one thing I don't get.  
In the video it's showed a code that is used to demonstrate why unbiased variance is used instead of biased variance, but that is not the focus of my question. For the question we can just focus on the biased variance (graph on the top left): the code produces a casual sample of 50 new elements at every click, so at every click our ""total"" sample increase by 50, at the sime time for every click the code calculates the variance of the total sample (red dot), and a pseudo variance (black dot) which is calculated using the population mean instead of the total sample mean. Then the code plots the results with the ""true variance"" (The horizontal line) for the comparison.  
I know that the creator assumes a population with flat distribution.

My question is: how the heck does he know the true mean value (used to calculate the pseudo variance) and the true variance (horizontal line), in other words the mean value and the variance of the population? Isn't the population composed of all the samples, so first the codes needs to know all the samples?  
In my head I immagine that if I assume that the population is 500, all the samples before 10 clicks will be less than 500 elements so the variance will not coincide with the true variance, finally when I reach the 10th click I can really calculate the true variance because only now I have all 500 elements that compose my population.

Hope I made it clear. If you know what it's wrong in my view please let me know, I really want to understand this. ç.ç","['You have stumbled upon the difference between real world statistics and simulations. In simulations we assign actual values to unknown parameters to show that the methods we are working with actually do what we claim they do. You’re right, in practice population parameters will never be known, otherwise there would be very little reason to be doing any statistics.', ""He is sampling from a distribution with known mean and variance -- i.e. using a random number generator with specific mean and variance. What that distribution *is* is impossible to say, since he doesn't show his code."", ""He's sampling from a discrete uniform distribution over the integers 0:200. The variance is (n^2 - 1)/12 = 3366.667. See [here](https://en.wikipedia.org/wiki/Discrete_uniform_distribution)."", 'https://en.wikipedia.org/wiki/Expected_value#Random_variables_with_density', 'But he did calculate the ""true"" value of variance. Is there a way to calculate the variance assuming a mean value, a range of data and that the distribution is flat?']",17,6,https://www.reddit.com/r/statistics/comments/129ewm0/q_how_can_i_know_the_mean_and_the_variance_of_the/
560,2023-04-02 14:20:07,[D] Does scale invariance explain Benford’s Law?,"There   are a handful of proposed explanations for why Benford’s law crops up   in so many natural datasets. One of the explanations listed by [Wikipedia](https://en.wikipedia.org/wiki/Benford's_law#Explanations)   and other sources is scale invariance, meaning that the law applies   regardless of what units the data are in (pesos or dollars, meters or   inches). They note that *if* the distribution of the leading digits in a dataset are scale invariant, *then* they must follow Benford’s law.

My   question is - how is this an explanation for why we see Benford’s law   so often in the world? The main argument I have been able to find goes   like this: “Wouldn’t it be weird if the leading digits were very   sensitive to units? These are ground truths about nature, so we should   expect them to be scale invariant (and thus, expect them to be   Benford).” However, I don’t think this appeal makes sense, because I   don’t agree that we should have this intuition that the leading digit of   our data should be insensitive to units. Indeed, there are plenty of   datasets where scale invariance doesn’t hold.

I have one idea about how scale invariance might actually *explain*   the preponderance of Benford’s Law and I haven’t seen it put forward   elsewhere. I'm curious what you all think: For all we know, there is a   really striking distribution of leading digits that emerges from a   common natural process, but the distribution is only visible when we   measure in yards and base 7. Maybe there are many such distributions   that if only we measured in certain units and bases, we’d be in awe of   them and they’d have their own Wikipedia entries. Since Benford’s Law is   scale (and base!) invariant, it happens to be the one we see all the   time because it doesn’t require that we measure in a particular way.

What do people think of the above explanation? Are there other ways in which scale invariance **explains** the preponderance of Benford’s law?","[""The best explanation I have heard of Benford's law has to do with logarithms:\n\nConsider data of the form y=a^x. with x spread somewhat uniformly. The lead digits of x will be relatively evenly spread (x is uniform), but what about the lead digits of Y?\n\nNote that x=log(y). log(20)-log(10) is bigger than log(30)-log(20) etc.\n\nNow, note that Benford's law only works when you have data that spans many orders of magnitude: What kind of data does that? if a is 10 and x goes from 2 to 20, then y goes from having 2 digits to having 20 digits. So X only spans one order of magnitude and Y spans 19 of them.\n\nHence, this kind of phenomenon shows up in situations where you have some relatively tame distribution A and another distribution the logarithm of which is A. This is often seen whenever effects are multiplicative rather than additive, which is what causes the eccentric distributions where you will see Benford's law."", ""Scale invariance in statistics is a property of some probability distributions that makes it so the *distribution itself* doesn't change if you change units. This is only true for a very small family of power-law-like distributions like Zipf's law, where X ~ X^-d. (neglecting the constant of proportionality). \n\nYou can see how this would lead to scale invariance while something like a Gaussian distribution (X ~ e^(-X^2)) would not: let's say we have house prices measured in either dollars or pesos. If the prices are distributed like a power law with parameter *d*, then the number selling for $10k divided by the number selling for $100k will be ($10k)^(-d) / ($100k)^(-d) = (0.1)^(-d). Now let's translate the prices to pesos instead of dollars. One USD is about 18 Mex$ at the moment, so we should be comparing Mex$1.8M houses to Mex$180k houses. We don't need to do anything at all to our distribution to do this comparison: (Mex$180k)^(-d) / (Mex$1.8M)^(-d) = (0.1)^(-d), still! If we tried to do this with Gaussian-distributed prices, it would not work without changing the distribution: e^(-$10k^2) / e^(-$100k^2) is not equal to e^(-Mex$180k^2) / e^(-Mex$1.8M^2).\n\nBecause scale invariance implies a power law distribution, the power law and Benford's law are closely related. Whenever you see a power law distribution, Benford's law will hold (if the data covers enough orders of magnitude). So the question then is **why are power law distributions so ubiquitous?** As /u/tuerda said in their reply, this seems to be a consequence of multiplicative effects. More specifically, power laws seem to emerge from the combination of exponential growth with normally-distributed growth rates. (And there are very good reasons for things like growth rate to be normally distributed!) To me, this is a satisfying explanation for why Benford's law appears everywhere: it's a consequence of power law-distributed data, and power laws appear everywhere because they are a consequence of random, exponential growth.\n\nSo to your explanation:\n>I have one idea about how scale invariance might actually explain the preponderance of Benford’s Law and I haven’t seen it put forward elsewhere. I'm curious what you all think: For all we know, there is a really striking distribution of leading digits that emerges from a common natural process, but the distribution is only visible when we measure in yards and base 7. Maybe there are many such distributions that if only we measured in certain units and bases, we’d be in awe of them and they’d have their own Wikipedia entries. Since Benford’s Law is scale (and base!) invariant, it happens to be the one we see all the time because it doesn’t require that we measure in a particular way.\n\nI think this makes sense, but kind of misses the point. Yes, any combination of a specific data distribution & specific units should yield a specific distribution of leading digits. But Benford's law is interesting because the only things it requires (power law-distributed data covering several orders of magnitude) are actually very common."", 'This gives some good intuition:\n\n\nhttps://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/', ""I'm not sure I buy your proposed argument. The universality of Benford's law is the interesting part, not the specifics of the function log(d+1) - log(d).  (Well, the fact that this function is not constant is also part of the interestingness.) If you found some first digit distribution that only holds for specific datasets in base 7, then that is inherently less interesting than the fact that Benford's law holds for a wide variety of diverse datasets in any base.\n\nBut you are correct to observe that scale invariance is not by itself an explanation: it is not obvious that any universal pattern should exist at all, let alone that it should be scale invariant.\n\nI think the most convincing explanation has to do with the statistics of sampling from families of distributions. A given normal distribution does not itself satisfy Benford's law, but if you repeat the experiment of choosing a random normal distribution and then randomly sampling from it over and over again, the resulting numbers will satisfy Benford's law. You might be able to find a proof of this fact which emphasizes scale invariance; I'm not sure."", 'I think the relationship to power functions establishes the scale invariance.... Notice your proof remains valid for any choice of *a* coefficient.   \n\nI think scale invariance emerges as this feature of mathematical proofs where they remain valid after any linear transformation of the parameters.  In the physical domain, results are valid whether you measured in degrees Celsius or Fahrenheit.']",20,11,https://www.reddit.com/r/statistics/comments/129cbe4/d_does_scale_invariance_explain_benfords_law/
561,2023-04-02 08:08:56,[Q] What does it mean when two random variables are stochastically independent ?," I don’t understand the difference with linearly independent… Also, how does stochastic independence show on a plot ?","['Colloquially, independence between the rvs X and Y means that knowledge of an event about one of them does not affect the probability we would assign to events with the other.\n\nFormally, P(X&isin;A, Y&isin;B) = P(X&isin;A)\\*P(Y&isin;B) for all events A and B.\n\nYou can\'t really plot random variables, but if you have two random samples of them (realizations of the random variables), a scatter plot of them would look like a ""cloud"" of points, with no obvious relationship between them.', '> What does it mean when two random variables are stochastically independent ? \n\nhttps://en.wikipedia.org/wiki/Independence_(probability_theory)\n\nSpecifically, you probably want this:\n\nhttps://en.wikipedia.org/wiki/Independence_(probability_theory)#For_real_valued_random_variables\n\nin short -- the joint density is the product of marginal densities; the joint cdf is the product of the marginal cdfs.\n\n>  Also, how does stochastic independence show on a plot ?\n\nThe conditional distributions are all the same. Or, equivalently, the conditional distributions match the corresponding marginal distribution.\n\nRoughly speaking, take a thin slice (vertical or horizontal) anywhere, and the distribution you get\\* is the same no matter where you slice it. One way to think about it is to consider transforming the marginal distributions so that the margins are uniform, you have independence when all the conditional distributions are also uniform; equivalently, (for two variables) the bivariate distribution becomes uniform when you transform the marginal distributions to be uniform. (Mathematically, independence implies that the [copula](https://en.wikipedia.org/wiki/Copula_%28probability_theory%29) is uniform.)\n\nThese notions are illustrated in the plot here:\n\n https://i.stack.imgur.com/IM5si.png\n\nOf course there are some small differences (most noticeable in the histograms) but those differences just due to random sampling; the two variables are actually independent here. \n\n---\n\n\\* assuming you have a large enough sample that Glivenko-Cantelli ""kicks in"", i.e. that the sample cdf is essentially the population cdf', ""The image in this wikipedia article is quite informative. Only the center top image reflects a random sample from stochastically independent variables. The horizontal and vertical cross-sections have basically the same distribution regardless of where the cross-section occurs. Also kind of the center-center image, but the variance of y being zero causes some mathematical problems.\n\nLinear independence means there's no linear relationship. That's true for the entire bottom row as well as the center top image. There is a clear relationship between x and y for the bottom row, but none of them can be captured with a linear equation.\n\n[https://en.wikipedia.org/wiki/Correlation](https://en.wikipedia.org/wiki/Correlation)""]",2,3,https://www.reddit.com/r/statistics/comments/1293qd3/q_what_does_it_mean_when_two_random_variables_are/
562,2023-04-02 01:40:51,"Help Needed, Appropriate Test for a Research Study [Q]","I am conducting research on diet changes in a population of mammals. The crux of the study is comparing data gathered over a 10 year period in the 80s and 90s to data recently collected in 2020-2022.

I am comparing counts of plant species eaten in the old study and those in the new study. Some species have been phased out of the diet or new species have been added. 

I initially did a t-test but my mentor told me to do a chi square test, and wilcoxin signed rank test instead. But the data is largely numerical so I'm concerned about the validity of the results. 

I am not confident enough in my reasoning for using certain stats tests so it is difficult for me to make suggestions with my mentor who has been doing this work for decades. 

What do you think would be the best statistical test for the type of data I have below. This is an example of the raw data.

 

|Plants|Recent Data Collection|Old Study Data|
|:-|:-|:-|
|Banana Trees|650|0|
|Coconut Trees|0|3000|
|Mint Trees|750|5000|

or 

&#x200B;

|Parts of Plants|Recent Data Collection|Old Study Data|
|:-|:-|:-|
|Coconut Leaf petioles|300|600|
|Mint Pods|500|450|
|Banana Leaves|500|250|","[""What type of data do you think a chi square test would be used on? \n\nWhat you've laid out here are contingency tables, which typically are exactly what you want for chi square tests. That's testing the null that time of collection and plant (or part of plant)  are independent of each other.""]",2,1,https://www.reddit.com/r/statistics/comments/128t6vb/help_needed_appropriate_test_for_a_research_study/
563,2023-04-01 22:14:00,"[E] Is ""Probability and Statistics - by DeGroot and Morris"" a good text book to learn statistical theory from?","Coming from a background in Psychology and Molecular Biology, I've learned about how to use off the shelf statistical tests - in SPSS; and to an extent -  building my own models in R. However, I'm working as a data analyst now, and I would really like to understand the fundamental concepts of statistics, sort of from first principles, I guess.

I've had Probability and Statistics by DeGroot and Morris on my PC for a while now. And while I can sort of keep up with the examples and tasks. I do wonder whether it is even a good book to be using? If not, could I get some suggestions? I hear Casella's book is good.","[""Honestly, if you're coming from a psychology background, reading [this classic paper by Edwards, Lindman & Savage](https://doi.org/10.1007/978-1-4612-0919-5_34) will build up invaluable intuition about probability before diving into the mathematical details with texts like DeGroot & Morris."", 'statistically speaking, it probably is.', ""I use Walpole & Myers, but I don't know if it's exactly what you are looking for. It's pretty common on science and engineering courses"", ""DeGroot is fine.  Theres dozens of other texts at a similar level if its not to your taste. \n\n\nYes Casella &  Berger is a good text  on the theory of inference but I would not start there unless you're mathematically pretty comfortable"", 'Okay, thanks, I will definitely take a look.']",13,8,https://www.reddit.com/r/statistics/comments/128nn19/e_is_probability_and_statistics_by_degroot_and/
564,2023-04-01 22:08:05,[Q] What to do if residuals are not normally distributed (linear regression)?,"Hello folks,

I would like to calculate a simple linear regression in RStudio. To test the assumption of normally distributed residuals, I have calculated a Shapiro-Wilk test. Unfortunately, it shows a significant result, so that the prerequisite is not fulfilled.

Due to my sample size (N = 144), can I invoke the central limit theorem at this point and still reliably interpret the results of the hypothesis test? In principle, my hypothesis could also be tested using a simple correlation, for which I would not need normally distributed residuals, but the supervisor of my thesis seems to be a fan of using regressions.

English is not my first language, but I hope that one can still understand what I am trying to ask.  


Edit: Thank you to everyone who wrote a comment! Nowhere do you get advice on statistical questions as quickly as here! :)","['Generally speaking, significance tests of whether an assumption is exactly met are not very informative because the assumptions are never  exactly met. Often a Q-Q plot is more informative. Incidentally, the CLT applies to means not individual data points. As pointed out  by General-point801 you could try transforming the data. Tukey’s ladder of powers is sometimes a good simple alternative to Box-Cox.', 'OLS is still the best linear unbiased estimator regardless of non-normal residuals as long as the Gauss Markov assumptions are met: https://en.m.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem\n\n(Note: the Gauss Markov assumptions + normally distributed errors gives you the Classical Linear Model assumptions)\n\nIn practice, these assumptions are rarely met, but OLS tends to be relatively robust to modest violations of these.\n\nAlso, OLS never requires the _residuals_ to be normally distributed but instead that the _errors_ are normally distributed (under the CLM). We do often inspect the residuals though as they are our estimates of the errors.\n\nJeffrey Woodridge suggests that at sample sizes as low as n=30 we can rely on the Gauss Markov assumptions to guarantee the asymptotic normality of the test statistics. We can use a t-distribution instead of a normal distribution to compute p-values to be extra careful.', 'Of more concern than non-normality is heteroskedasticity for normal standard errors. At a sample size of 144, you’re probably reasonably well off with normal standard errors unless the distributions are extremely heavy tailed or highly skewed due to the central limit theorem. I would suggest looking at residual diagnostics to assess rather than a Shapiro-Wilks test, which is generally not particularly useful.', 'I believe running a box cox transformation test should give you some options for transformations.', 'Subtle point here, if errors are normal the residuals **will also be normally distributed**. Non-normality of residuals is sufficient to falsify normal errors.']",52,33,https://www.reddit.com/r/statistics/comments/128nhhi/q_what_to_do_if_residuals_are_not_normally/
565,2023-04-01 20:42:38,[Q] Univariate vs multivariate analysis,"Hey there, novice here.

I'm doing a cohort study about a certain medical procedure that has fail/success outcomes, and its risk factors for failure. 

When I did a univariate analysis for risk factors for failure I used Chi-square. After that, I used a multivariate regression model to analyze the effect of these risk factors and to account for confounding effects.

Some of my results did not reach significance in the univariate analysis, but they did reach p<0.05 in the multivariate. 

Did I do something wrong or biased the analysis when I included the non-significant factors of the univariate analysis in the multivariate one?","['Side note: When you say ""multivariate"" you really mean ""multiple"" or ""multivariable"". Multivariate means that there are multiple dependent variables (think PCA).\n\nGenerally, you shouldn\'t do the univariate analyses at all and certainly not for screening variables to be included in subsequent multiple regression models. A paper that goes into detail is [here](https://www.jclinepi.com/article/0895-4356\\(96\\)00025-X/fulltext). Ideally, the selection of variables is based on subject matter knowledge condensed into a DAG.\n\nFinally, it\'s no surprise at all that the p-values in the multiple regression model do not ""match"" those in the univariate analyses. After all, the reason to use a multiple regression model is to asceratin the effects of variables after taking the effect of other variables into account.', 'I understand why it makes sense to not univariate screen for significance. But what if you are discussing topics in which you cannot predict based on subject material knowledge. For example predicting prolonged stay after hospital. The reality is for each procedure their may be different patient demographics that predict versus some surgeries it may be more intraoperative or postoperative factors. \n\nIn a more general sense truly most the variables we know from subject matter to be potentially important are ones we have previously determined usually via single variate analysis to be important just in past studies. \n\nJust something I struggle with at times as it relates to this.', ""If your goal is prediction instead of explanation/confirmation, then I'd apply a technique that is known to perform well for the task, such as LASSO, ridge or elastic net (or some even newer ones).""]",15,3,https://www.reddit.com/r/statistics/comments/128la81/q_univariate_vs_multivariate_analysis/
566,2023-04-01 11:40:27,[Research] Attrition Rate - Critical Appraisal of an Article,"How do you guys tally the drop outs? Do you include those subjects who withdrew or got excluded before the group randomization process or only those who were removed after the randomization process when the experiment/intervention is already commencing  

Any help would be appreciated. Thank you!","['The study that I’m appraising totally did what you stated. But I’m trying to determine if the study has high or low attrition rate. The study has 64 subjects consented but 26 of them either withdrew or got excluded, so 38 went on with the randomization process. At the middle of the intervention phase, 2 withdrew, so only 36 subjects were included in the final analysis\n\nIf I only count those 2 that withdrew after the randomization process, then I could say that they have low attrition. But if I include those 26 who withdrew or got excluded before the randomization, then I could say that they have a higher attrition rate. That’s where I am stuck up right now 😅', 'That’s what I am leaning on as well, that those who got excluded before randomization should not be considered as dropouts. Thanks a bunch!', 'When I’ve done projects/studies where sample sizes have decreased based on inclusion/exclusion I usually state those inclusion or exclusion criteria and then also the n (number of people) excluded because of them. Almost always report the largest sample we stared with and then say X was our final (reduced) study sample and here’s who fell out and why.', 'Oh wow! That is high attrition if you could the study starting with consenting phase. Nearly half! Overall though, low attrition because the “study” phase and data collection phase was “started” after randomization and then intervention. Or that’s how I’d see it']",2,4,https://www.reddit.com/r/statistics/comments/128a20a/research_attrition_rate_critical_appraisal_of_an/
567,2023-04-01 07:22:44,[Q] - Odds of a baseball card existing based on sale data of other cards in the set.,"# Summary

I am putting together a set of baseball cards from 2022 Topps Inception. In the set there is a base autograph card, which is white and ranges from 75 copies per player, to 299 copies per player. While searching for them over the last year, I have been able to find 80 of the 83 players for this specific card. **In that time, I have not seen a single base card for 3 players: Juan Soto, Sammy Long, and Ian Anderson.**

# Set Information

There is a white base card and 7 colored parallel cards. Green, magenta, aqua, red, orange, blue, and black. Black data is excluded since only 1 black card exists per player. If a white base card is rarer than the other “parallels”, the parallel does not exist. Example: Shohei Ohtani has only 75 white base cards, so the green, magenta, and aqua do not exist. The others are Fernando Tatis Jr (/125 – so no green exists), and possibly Wander Franco (/130 – no green sales have been seen).

Topps sells these in cases or boxes. Each Box includes 1 pack with 7 cards in it. Each case has 16 boxes. There are different types of card in the Inception set. See here for details on other cards:

[2022 Topps Inception Baseball - Beckett](https://www.beckett.com/news/2022-topps-inception-baseball-cards/?utm_term=&utm_campaign=Performance+Max-BGS&utm_source=adwords&utm_medium=ppc&hsa_acc=5407491096&hsa_cam=18643815275&hsa_grp=&hsa_ad=&hsa_src=x&hsa_tgt=&hsa_kw=&hsa_mt=&hsa_net=adwords&hsa_ver=3&gclid=Cj0KCQjwiZqhBhCJARIsACHHEH8qbCwjGvSasuQHKGFrnGiEE8zwOuOc7t44pQ5GmWPT6fEy6u4YYjAaAmm3EALw_wcB)

# Data

The data provided below is from eBay’s Terapeak, which gives insight into sales over the past 2 years. Since this set was released in early 2022, it only shows data from April 2022 until now. The data provided shows how many sales exist for each colored card for each player.

The search used looked for generic terms about the set, including the player and the number of the parallel. Common terms that would return incorrect listings were specifically omitted. Example Search:

    2022 inception auto -break -silver -signings -patch -relic -hat -rpa -glove -digital -sock Gavin Sheets 299 

# Question

**I am trying to figure out if Topps omitted these cards in the set, or they exist and just happen to have 0 sales. Is there any statistical evidence they did not include them in the sets?**

I think it is nearly impossible that these cards exist, but all have 0 sales when compared to the other sale data.

&#x200B;

Assumptions that were made:

1. All players have a white base card
2. The same card can be sold more than one time. (data reflects this)
3. Soto is somewhere around a /99 since there is an aqua color, but no magenta.
4. Sammy Long is likely around a /299
5. Ian Anderson is likely around a /299
6. The seller’s listings were accurate and included what the card was numbered to. (A scrolling QC of the pictures was done, and any photos that differed from the players card was excluded.)

# Data

||Number of sales per card||||||||
|:-|:-|:-|:-|:-|:-|:-|:-|:-|
||**BLUE**|**ORANGE**|**RED**|**AQUA**|**MAGENTA**|**GREEN**|**BASE**|**Notes**|
|**Player**|**/10**|**/25**|**/50**|**/75**|**/99**|**/125**|**/299**\*|\***unless otherwise noted**|
|Sammy Long|1|3|6|7|10|19|**0**||
|Juan Soto|2|8|0|19|N/A|N/A|**0**|magenta and green do not exist since there is a low print run. See Ohtani& tatis|
|Ian Anderson|0|4|10|3|19|12|**0**||
|Aaron Ashby|1|1|3|8|12|22|37|/249 base|
|Alec Bohm|6|4|8|9|25|19|29||
|Adolis Garcia|1|9|10|75|24|26|30||
|Andre Jackson|0|7|6|5|8|19|14||
|Alejo Lopez|1|5|2|5|6|15|20||
|Austin Riley|2|9|10|20|28|25|34|/199|
|Alfonso Rivas|2|1|5|5|8|13|26||
|Angel Rondon|1|3|4|11|7|11|17||
|Alex Verdugo|2|10|10|14|7|17|37|/249|
|Andrew Vaughn|1|2|8|11|15|29|18||
|Alexander Wells|0|2|3|6|10|8|19||
|Bryan De La Cruz|3|2|1|13|13|27|26||
|Brandon Marsh|1|0|0|12|0|0|14||
|Charlie Barnes|2|2|5|7|8|16|26||
|Casey Mize|1|7|5|15|15|18|16|/150|
|Cristian Pache|1|0|3|8|7|18|27||
|Cal Raleigh|0|2|13|9|18|22|33||
|Curtis Terry|5|2|1|3|8|9|15||
|Dylan Carlson|1|1|5|14|18|31|30||
|Drew Ellis|2|1|7|19|9|14|5||
|Deivi Garcia|1|3|3|2|18|7|22||
|Ernie Clement|2|3|4|12|7|4|16||
|Emmanuel Rivera|2|6|5|7|2|7|20||
|Freddy Peralta|0|6|7|15|3|15|28|/225|
|Fernando Tatis Jr.|2|3|15|21|18|N/A|20|/125 base (no green exists)|
|Greg Deichmann|1|2|4|4|8|9|22||
|Griffin Jax|0|5|6|7|13|13|19||
|Gavin Lux|1|2|7|9|6|11|23|/225|
|Glenn Otto|1|1|9|15|6|6|26||
|Gavin Sheets|0|8|12|6|15|6|25||
|HOY Jun Park|2|1|2|16|8|10|24||
|Jake Burger|2|5|6|11|20|26|22||
|Joe Barlow|0|2|10|13|13|14|18||
|Jazz Chisholm Jr.|4|1|14|13|17|12|32|/225|
|Jake Cousins|1|3|6|8|14|13|34||
|Jarren Duran|1|3|2|1|0|1|19||
|Josiah Gray|2|4|4|8|11|9|31||
|Jarred Kelenic|0|1|15|12|18|21|28||
|Jackson Kowar|1|3|1|9|10|11|36||
|Josh Lowe|0|2|9|12|13|8|46||
|Jake Meyers|1|4|4|9|10|18|34||
|Jake McCarthy|3|4|14|11|15|20|35||
|Joe Ryan|2|2|7|10|24|26|41||
|Jose Siri|0|9|3|5|15|22|26||
|Kyle Muller|1|5|14|14|18|15|28||
|Logan Gilbert|3|5|9|21|21|23|37|/249|
|Luis Gil|3|3|3|8|7|13|40||
|Lars Nootbaar|3|7|8|2|12|6|39||
|Luke Williams|3|3|6|3|15|24|28||
|Marcos Diplan|2|4|6|5|3|0|39||
|Max Kranick|1|1|8|1|6|3|10||
|Matt Manning|1|3|4|11|9|15|20|/225|
|Zach Thompson|1|4|2|3|8|9|8||
|Matthew Vierling|4|2|2|6|14|14|23||
|Nick Madrigal|1|4|11|15|10|10|30||
|Pete Alonso|1|9|9|8|22|34|15|/150|
|Randy Arozarena|2|8|3|11|24|28|36|/225|
|Riley Adams|1|4|12|8|13|13|32||
|Reid Detmers|1|10|15|15|8|19|45||
|Romy Gonzalez|0|3|3|5|18|16|19||
|Reiss Knehr|1|1|2|10|13|6|33||
|Ryan Mountcastle|0|5|20|21|23|22|36||
|Ryan Vilade|0|3|3|10|1|4|14||
|Seth Beer|2|5|6|10|23|13|47||
|Shane Baz|3|1|15|12|14|21|48||
|Shohei Ohtani|0|9|7|N/A|N/A|N/A|16|/75 (no green, aqua, magenta)|
|Stephen Ridings|2|4|11|8|13|19|26||
|Sixto Sanchez|2|1|9|15|10|8|32|/249|
|Spenser Watkins|5|2|9|9|6|6|40||
|Trey Amburgey|0|2|9|5|4|20|31||
|Tyler Gilbert|1|2|5|10|7|11|15||
|TJ Friedl|0|5|2|9|6|13|23||
|Tylor Megill|0|4|8|17|9|14|30||
|Trevor Rogers|1|4|6|8|11|13|9||
|Tony Santillan|1|1|3|7|12|10|25||
|Vidal Brujan|3|4|0|4|18|8|30||
|Wander Franco|0|1|3|4|5|0|3|/130|
|Yordan Alvarez|4|3|10|20|24|28|45|/150|
|Yonny Hernandez|1|1|8|5|7|6|17||
|Yohel Pozo|1|4|3|9|12|7|15||","['You doing bunt too?', ""Edit: Wait, had a mistake in my spreadsheet. I'll fix it. Edit2: done\n\nIf every existing card has the same chance to show up in your dataset then it's very likely that these three cards were left out somehow, but your data sample itself is suspicious in some aspects.\n\nAmong the colored cards you expect 31224 cards in total (83 players for blue, orange, red, 82 for aqua, 81 for magenta, 80 for green). Out of these you have 3871, or 12.7%.\n\nIf you expect 12.7% out of 125 cards (lowest for Sammy Long and Ian Anderson) then the chance to find 0 cards for one player is just 40 in a billion.\n\nHowever: The chance to find 45 or more base cards out of 150 (Yordan Alvarez) is just 20 in a billion. If we use the maximal 299 base cards then the chance increases to a reasonable 13%. We still have the very unlikely 20 out of 75 aqua (0.03%), 4 out of 10 blue (0.5%), 24 out of 99 magenta (0.05%) and 28 out of 125 green (0.09%) all for the same player. Something seems to make their cards much more likely.\n\nIf something can make some cards more likely to show up in the dataset than others then we can't tell anything without a quantitative understanding of what causes this bias."", ""Tangent question: I amassed a massive baseball card collection in the 80's and early 90's.  I quit after the strike.  I recently saw packs for sale for $10-20 while standing line at Walmart and was shocked.\n\nJust curious, how much did this experiment cost?"", 'Bunt as in digital cards? No', 'Let me dig into yordan and see what numbers have been sold.']",17,8,https://www.reddit.com/r/statistics/comments/1283ic5/q_odds_of_a_baseball_card_existing_based_on_sale/
568,2023-04-01 03:06:22,[Q] Methods for handling offset variables,"I have built a BRT model with about a dozen input variables of mixed types and a binary outcome variable. There are 105 observations in my 80% training set (trust me, I wish it were higher, but that’s the reality). 

I am getting promising results in terms of P/R and AUC. However, one of my variables is an exposure measure in the form of minutes during each observation. 

When I try to shift this from an input variable to a log-transformed offset variable, the model essentially breaks. 

Precision goes from 0.9 to 0.7

Recall goes from 0.9 to 1

AUC goes from 0.9 to 1

The model does not predict any positive cases, even with a decision boundary of 0.1. 

Using the raw value (not log-transformed) yields similar results but the AUC is 0.6 instead of 1. 

I don’t think it’s valid to use an exposure variable as a predictor, but I’m having trouble figuring out how to best incorporate it. Any help appreciated. 

Thank you.",[],1,0,https://www.reddit.com/r/statistics/comments/127vmvm/q_methods_for_handling_offset_variables/
569,2023-04-01 02:24:56,[Q] Inference: Which explanatory variables should I include in the model?,"Hi all!

I need your help. Suppose that the aim of the analysis is hypothesis testing, point and interval estimation of the unknown regression parameters. I am not interested in making predictions. Which explanatory variables should I include in the model? For the sake of simplicity, suppose that there is no multicollinearity (both perfect and approximate multicollinearity).  Moreover, suppose that there is no prior information on the phenomenon. If I include irrelevant variables in the model, I will get wider confidence interval for the parameters. If I exclude some relevant variables, I will get biased estimates. Is it correct to start with the full model (the model with all possible explanatory variables) and iteratively remove the most insignificant variables according to the p-value until all variables left in the model are significant? Can anyone point me to a good reference on this subject?

Thank you a lot!","[""> Is it correct to start with the full model (the model with all possible explanatory variables) and iteratively remove the most insignificant variables according to the p-value until all variables left in the model are significant? Can anyone point me to a good reference on this subject?\n\nNo! No! No! No!\n\nFirst, p-values are a terrible way to prune features/variables.  That's not how you use p-values.   Secondly, after you prune features your p-values will be biased.  Finally, what you describe is called backward stepwise selection, and is now considered a terrible way to do feature selection (e.g. https://www.stata.com/support/faqs/statistics/stepwise-regression-problems/, https://statmodeling.stat.columbia.edu/2014/06/02/hate-stepwise-regression/, https://twitter.com/f2harrell/status/1276575455982825473?lang=en).  A better choice is something like lasso regression, but if your goal is hypothesis testing then lasso might not be the best choice because the selection procedure of the lasso will also bias the p-values.  \n\nIf your goal is hypothesis testing, then you need to either choose through domain knowledge the variables to include, or include everything and be careful with your interpretation.  Or maybe someone else has other ideas.  But under no circumstances should you do what you proposed."", ""You're thinking about this completely the wrong way, and it seems like you're confused about what the parameter hypothesis tests are actually testing.  The parameter hypothesis tests test whether the parameters are significantly different from zero, not whether they are useful in the model.  That's a completely different question.  Hypothesis tests are *not* the way to choose a model.  There are much better choices out there such as AIC, BIC, lasso, ridge, etc.  \n\nAdditionally, if you had read the links I provided above you would see that \n> It yields p-values that do not have the proper meaning, and the proper correction for them is a difficult problem.\nIt gives biased regression coefficients that need shrinkage (the coefficients for remaining variables are too large; see Tibshirani [1996]).\n\nSo by doing your proposed procedure, you're biasing your coefficients and rending your final hypothesis test useless."", 'By far the most important reason to include or exclude variables depends on something you haven\'t said anything about. What are you analysing? And with what? \n\n&#x200B;\n\n>Suppose that the aim of the analysis is hypothesis testing, point and interval estimation.\n\nIf you *really* care about the hypothesis test achieving nominal coverage, standard errors and intervals not being misleadingly small etc,  you have to either do no model selection (pre-specify the model and the variables included, without doing any analysis of how the variables predict the outcome whatsoever), split the data, doing model selection on one and estimating the selected model on the other half (and once you have done it, you cannot change it anymore). Or google advanced ""post model selection inference"" papers and follow their approach', ""It doesn't matter, stepwise regardless of the method is bad.  Even when I was taught it in the early 00's people knew that stepwise with p-values was bad and AIC was preferred.  But then later the field realized that stepwise in general is a bad idea."", 'if a linear combination of two highly correlated predictors explains the output then in stepwise regression you’d likely drop at least one of them and end up with the “wrong” model']",14,20,https://www.reddit.com/r/statistics/comments/127udio/q_inference_which_explanatory_variables_should_i/
570,2023-04-01 01:56:00,[Q] measuring user persistence,"I am working with a data set that tracks a daily distinct count of user_ids that meet a specific criteria. There is some noise in the data and I want to differentiate groups that have high and low persistence.

An example:

Let's say that one group has an average daily unique count of 100 IDs with relatively low variance. After 30 days I have a total unique count of 200. If I use DAU/MAU I get a stickiness of 50% but that feels like a poor measurement of how persistent each user is. If over the month I had the same 100 users every day I would see a total monthly count of 100 vs if it were a different 100 users I would see 3000. I normalized over this range and came up with 1 - (monthly_count - daily_average)/(daily_averagex30 - daily_average) (not simplified on purpose)

In the example above that means that every day on average, ~97% of my daily users are repeats. The remaining 3% is either noise or net growth.

Is this calculation commonly used anywhere and are there pitfalls to relying on it?",[],5,0,https://www.reddit.com/r/statistics/comments/127thqk/q_measuring_user_persistence/
571,2023-03-31 23:46:18,Help Needed [Q],"I’ve never had to take a stats class, and I’ve run into an issue that I think someone with stats knowledge can help me on.  

I’m having a competition at work between my teams, and I don’t know how to apply equal weight to the points.  The winning gets 2 days off work, so I’m trying to make it fair.  

I have 5 groups, all of different sizes (ranging from 2-21).  Each individual is given a set number of points in various categories.  I am trying to identify which group has the highest totals.  I can easily determine the average number of points per group, but because the groups are different sizes and some people have zero points, the overall averages seem skewed.     For example, the group of 2 has one person with 60 points and the other has zero, their average for the group is 30, but the group of 21 has 2 people with points  10 points each and the Rae have zero, making the group average .95.   

How to I make it more equally weighted?  I’m using an excel file.  What type of formula would I use?  What term(s) can I Google to teach myself? 

Forgive me if my terms are confusing or incorrect.","['If the purpose is to determine a winner, I’m curious as to why you’re trying to average at all. Wouldn’t total points make the most sense?', ""In general, an approach is to trim the means.  That is, say, discard the highest and lowest scores for each group, and then use the mean.  This obviously won't work if your group is as small as 2.\n\nTo figure out what statistic you want to use, you might create some toy data, and see how you ***want*** to rank the groups.  That may give you insight into what algorithm you think you should be using."", 'Good points.  Thanks for your input.', 'Can\'t you give the days off to the individual people with the most points instead of doing it by team? Doing it by team is extremely unfair because (a) difference in sizes, (b) your example of the 2 person team! One person is among the one with most points and the other has no points! So person with many points has no chance simply because the coworker is lazy or doesn\'t want to participate.\n\nThe whole thing was set up very poorly. No way you can come up with a fair ""formula"" or anything.\n\nAre individual people\'s points dependent on support they are getting from their team? Or are they truly individual points? If they are truly individual, why give anything to the team?', ""So in my humble opinion is the game was unfair. There is also no fair way to weight/handicap teams with more people vs less people. If I were you id probably not give something as major as days off at work over a silly game either. If you are going to do something this big you'd probably need to design the game to be fair. Equal teams, equal departments, etc. Anywhoo this seems like you are begging for a lawsuit if its real.""]",0,14,https://www.reddit.com/r/statistics/comments/127pkt5/help_needed_q/
572,2023-03-31 23:04:36,[E] Post Hoc analysis literature suggestions,"Hello! Could someone recommend literature on post hoc analysis? Specifically, on Bonferroni, Tukey's, and Scheffe's method.

I'm struggling with my Variance Analysis course and it will be my fourth time taking the final exam. We are using the book ""Applied Regression Analysis and Other Multivariable Methods"" Fifth Edition by Kleinbaum et al. However, the post hoc analysis section of the book only covers the basics and easiest types of problems, nothing more advanced. The exam problems are far more complex than what is taught.

Thank you in advance.","['There may exist better books, but when looking through my the texts this is what I found. \n\nA thorough review on Tukey can be found in Modern Mathematical Statistics (Devore and Berk). Moreover, Statistical Inference (Casella and Berger) provides alot of detail on both Bonferroni and Scheffe. Also, Applied Linear Statistical Models (Kutner et al) contains some brief info on all of them in its 5th edition. All of these discussions are in the (One-Way) ANOVA sections.', 'Following!', ""Thank you for your suggestions! I'll have a look at them.""]",6,3,https://www.reddit.com/r/statistics/comments/127ob6l/e_post_hoc_analysis_literature_suggestions/
573,2023-03-31 13:13:08,[Q] Help me with a problem that I've been thinking about for years,"Let's say I'm going in my car at 50 mph and I crest a hill and spot a red traffic light 100 yds in front of me. I know the light is red for 1 minute and green for 1 minute. There's a limitation on how fast I can accelerate and decelerate my car.

How should I accelerate and decelerate my car in order to maximize my speed through the whole interaction.

I used specific numbers but I really want variables.

I know that on average there's 30 seconds until the light turns green, but should I decelerate linearly? That's not clear to me.","[""The ideal strategy will depend on what exactly you want to optimize, but it's likely that an ideal strategy will be very difficult to find.\n\n- minimize the total time loss, measured when you accelerated back to full speed after the intersection?\n- maximize the speed at which you cross the traffic lights?\n- maximize the average speed until you reach the traffic lights?\n- maximize the lowest speed you reach, to reduce fuel consumption?\n\nFor all options you have to further specify if you want to optimize the best case, the worst case, the median, the expectation value, or something else.\n\nThe second option has an easy mathematical answer, although it's probably violating traffic rules: Stop well ahead of the traffic lights, so you can accelerate back to full speed ahead of them once they turn green."", 'Maximising speed (without direction) might suggest no gradual decreases. If necessary break hard and reverse direction at speed repeatedly until the lights change, or until a minute is nearly up.', 'The variable is “when did the light turn red as you crested the hill. This is going to change all variables. If you hit the apex, still in full motion, as you saw the light turn, then the simple-ish answer would be to increase to at least 65mph to make the light just as it’s phased over. (60mph = 1 mile per minute or 5280ft per min, which is 88ft per second.) I say 65, because I don’t know how long it could take the car to reach 5 more mph. One vehicle could take 1.5sec, while another could take .5 sec. If you wanted to ensure you made it, then increasing to 70 would ensure you make the light. But, if you hit the apex and the light was already red, you now have to wait until it’s green to start your timing. While not extremely important, one of the other variables is the slope angle of the hill. Now with knowing the slope, I would want to know the car. 50mph cresting one hill could leaving you soaring with little to no control for many feet, placing a great strain on the braking system, while another slope could leave end the most modest of vehicles cruising about. I could add in aerodynamics, but while it could help with acceleration/deceleration, I figure that it would be on extreme levels of variation that deal more in long linear equations than this one. Lastly, are you being monitored (police/intersection camera) at the light or on the street. If so, then decreasing is the only option. This then goes back to the variables of when the light changed to red to gauge whether to cruise or continue to brake.', 'Unfortunately you would be riding your brakes, but that would decrease enough for 1min to roll through the light.', 'I want to get to my destination as quickly as possible.']",1,9,https://www.reddit.com/r/statistics/comments/127ayur/q_help_me_with_a_problem_that_ive_been_thinking/
574,2023-03-31 12:49:00,[Q] Any spatial stats person here? Need help understanding how to choose a model for variogram [R language],"I’m new to spatial statistics. How do I decide which model to use to fit my variogram (vgm function).
Do I just visually look at the experimental variogram and decide on the model or use or is there a more concrete way to make the decision like the way I can use Anova to decide between linear models in traditional stats.","['A statistician might answer that you should select your variogram model based on some theoretical considerations about processes, not based on the data themselves. But this is usually unrealistic, and you have to use a criterion for model fit such as Akaike.', 'Common practice in applied geostatistics is to simply fit a variety of variograms models to the experimental variograms and pick one that looks best. This has the downside of underestimating uncertainty in the resulting model predictions, and also has an element of added subjectivity, but often does work reasonably well in practice.\n\nAlternatives do exist though. AIC or cross validation methods can be used to select from a candidate variogram model, or alternatively a flexible variogram that accommodates several others (especially the matern model) can be used.', 'Sorry, not to answer, but more out of curiosity -- for variogram, do you use coordinates as inputs?', 'There is the eyefit function in a package that’s escaping me now… an alternative is to use maximum likelihood to compare the fits of different covariance functions and skip the variogram', 'library(automap)']",35,5,https://www.reddit.com/r/statistics/comments/127ahpn/q_any_spatial_stats_person_here_need_help/
575,2023-03-31 08:54:51,[Q] Any good sources for treatment of errors and error propagation? Like a for dummies version with more discussion on concept and basic math?,"I've been having a difficulty time understanding error propagation and covariance matrices. I've looked online for resources for these, but they're scattered university websites and clips, or they're designed for math/stat major students with just lines of derivations and equations. I'm looking for more like maybe a course textbook, that starts out with simple stats and error analysis, and gets to more complicated topics later, but with plenty of discussion on what the math is doing rather than just pure lines of derivations. I wouldn't mind more advanced math like linear algebra down the line (I don't know if it's possible to describe more advanced stats without linear algebra terminology), but I'd like the book to also break down the more complicated math and explain it (rather than just assume you know what the terminology means).

Any suggestions?","['A good place to start with error propagation is reviewing your Calculus I, especially the Related Rates section that the professor always blows through because he\'s done teaching new derivatives rules and is anxious to get started on integrals. That, combined with remembering the basic rule for combining variances, will take you far: Var(aX+bY) = a^(2) Var(X) + b^(2) Var(Y) if X and Y are independent (add an extra 2ab Cov(X,Y) term if not.) If you know the rate at which one quantity changes with respect to another, you can estimate how much error in your output will be caused by an error in your input.\n\nSimple example: the volume of a sphere is V = 4pi/3 r^(3). dV/dr = 4pi r^(2). A sphere with radius 10cm has volume 4189 cm^(3). If that 10cm radius has a ±1cm uncertainty on it, put in 10 cm for r and 1 cm for dr and get dV = 4pi(10)^(2)(1) ~ 1257 cm^(3).\n\nNow Check your answer by seeing that when r=9cm, V=3054 cm^3 (1135 less than 4189), and when r=11, V=5575 cm^(3) (1386 more than 4189). You can see that, on average, your estimate of the uncertainty in V was close to correct, but that the nonlinearity begins to matter when you have as much as a 10% error in r.\n\nSame general idea with two variables: the area of a rectangle is xy. What if x and y are both uncertain measurements? dA/dx = y; dA/dy = x. Var(A) = (dA/dX)^(2) Var(X) + (dA/dY)^(2) Var(y) = y^(2) Var(X) + x^(2) Var(Y). \n\nSuppose we have x = 20±1cm and y=50±2cm. Var(A)=50^(2) 1^(2) + 20^(2) 2^(2) = 4100: sqrt(4100) ~ 64. We estimate our area is 1000±64 cm^(2). That\'s a narrower bound than you\'d expect from looking at 19x48=912cm^(2) and 21x52=1092cm^(2) since we\'d be unlucky to have both errors be large and in the same direction -- but a wider bound than if there were only one source of error: 19x50=950, 21x50=1050; 20x48=960, 20x52=1040.\n\nSave the linear algebra on giant variance-covariance matrices until after you can solve the simple questions of the ""I measured the height of that flagpole with trigonometry, but I had a ±0.2° error in my angle measurement and a ±5cm error in how far I was standing from the flagpole"" variety. \n\nWhen you are comfortable there, you can tackle the general form. Given some big ugly function f(w,x,y,z), and a variance-covariance matrix describing the uncertainties in w, x, y, and z, you multiply:\n\nVar(f) = [df/dw df/dx df/dy df/dz] [V-Cov matrix] [df/dw df/dx df/dy df/dz]^(T)\n\nThere will pretty much always be a string attached, that the errors in each input must be ""small"".', 'Honestly, I have been in this field somewhat for years (propagating model errors and uncertainties through time and coupled domains) , and I wish I had a more fundamental understanding of some concepts. I hope someone has a good resource suggestion for us. \n\nI think the field of GD&T (geometric dimensioning and tolerancing) could have the right fundamentals. I have tossed around the idea of exploring that field anyways.', 'Thank you for this breakdown, but any recommendations for a book that goes through this?', 'I wish! It is often skimmed over very fast. A phrase to look for the index of a stat book is ""delta method."" Rice (the book my grad program used) spent about 5 pages on it. Bickel and Doksum give it nine. Wasserman\'s *All of Statistics* gives it two. Rossi gives it one (plus a problem set.) Freund, a popular undergrad book, does not cover it explicitly at all. \n\nBut at least if you work on industrial/practical problems you\'ll probably use it more often than half of the rest of the book put together.']",3,4,https://www.reddit.com/r/statistics/comments/1275292/q_any_good_sources_for_treatment_of_errors_and/
576,2023-03-31 08:25:51,[Q] Question about Medical statistics,"Hi guys, I am stuck for past 2 days on following problem.

I need to figure out treatment and period variable. any help would be appreciated

I have dataset with 3 variables Group Air Co A clinical trial was designed to assess the impact of low levels of exposure to carbon monoxide (CO) on exercise tolerance in patients with ischemic heart disease. A total of 30 nonsmoking patients with documented obstructive coronary artery disease and a history of exercise-induced ischemia were enrolled in this 2-period crossover study. The study period consisted of 3 days : a training day and 2 exposure days during which patients were exposed to either air or CO in an environmentally controlled chamber. On all 3 days, patients followed a bicycle exercise regime in which exercise was conducted at increasing work loads until angina, fatigue, or hypertension occurred. On the first (training) day of the study, all patients conducted a training exercise on the bicycle. Patients were then randomly assigned to one of two exposure sequences, exposure to air followed by carbon monoxide (Air:CO) or the reverse (CO:Air). The outcome variable measured was the difference between the duration of exercise (seconds) after the exposure condition and the duration of exercise recorded on the training day. The data for each patient are provided, Group 1 was exposed to CO first and then to Air, and group 2 was exposed to Air first and then to CO.","['> I need to figure out treatment and period variable. any help would be appreciated\n\nIt looks like you already identified the treatment variable. Which consist of two conditions. (Although I\'m surprised they didn\'t include a control (only air) condition): \n\n> Patients were randomly assigned to one of two exposure sequences, exposure to air followed by carbon monoxide (Air:CO) or the reverse (CO:Air).\n\nAs for ""period"" variable, I assume that is defined by the 3 study days you refer to:\n\n> The study period consisted of 3 days : a training day and 2 exposure days during which patients were exposed to either air or CO in an environmentally controlled chamber.\n\nSo you measure your outcome at day 1 (baseline), and then have 2 days of outcomes after treatment.', 'Review your class notes.', 'Would this study benefit from 4 conditions: co:air, air:co, air:air and co:co? I guess it depends on the sample size?', ""I have, I am stuck so that's why asked. thanks anyway""]",2,4,https://www.reddit.com/r/statistics/comments/1274dzu/q_question_about_medical_statistics/
577,2023-03-31 06:36:47,[Q] Panel Regression,"I’m doing panel regression where y is the correlation between bitcoin and 11 cryptocurrencies and x is market capitalization. I’m trying to figure out whether market capitalization effects the correlation between cryptocurrencies. As data is over 5 years, 5 increments of market cap is used (55 overall), however there is only 11 correlation coefficients. Should I measure the correlation between each year? Additionally, can the regression tool on excel be used for accurate results specifically to this?

Any assistance would be appreciated, many thanks 🙏","[""Let me try and rephrase this to see if I understand.\n\nYou have 5 time periods, so T=5. Within each time period you have 11 observations, one for each cryptocurrency, so N=11. Your outcome measure for each is the correlationn bewteen their... what, value in dollars? and bitcoins value in dollars or something... is that right? It does seem a little weird that bitcoin - a crypto currency itself - is singled out for calculating the outcome. \n\nthe model sounsd like it's y\\_it=apha+beta\\*x\\_it+epsilon\\_it. \n\nWith panel data, people usually assume epsilon\\_it are not independent of each other, because within currencies there's likely correlation over time. There's lots of ways to model that correlation and try to account for it. One way is to assume epsilon can be decomposed into a a currency-specific term plus another term, as in epsilon\\_it = c\\_i +phi\\_it, where c\\_i is constant within currency over time and then you assume phi\\_it is independent over time. In that model, there are a couple estimators you can use, such as the fixed effects estimator or a first-difference estimator. Both can done in excel with some transformations of your data. For example, for the first difference estimator you'd transform your data to be the change in these things from one year to the next:\n\n(y\\_it - y\\_it-1) = beta \\* (x\\_it - x\\_it-1) + (phi\\_it-phi\\_it-1)\n\nWhere the c\\_i and the constant drop out. You'd now have 4 years, because the change is not observed in year 1. \n\nThere remain several challenges though: you have a small sample and your outcome measure - a correlation - is by definition not normally distributed. You therefore really can't trust any standard errors you get out of this model. You would want to have many currencies per year - over 50 would start to make people more comfortable about the standard errors. \n\nAdditoinally, x is not randomly assigned, so it's probably a leap to say you would estimate the degree to which market cap actually effects the correlation. Rather than estimating a causal parameter, you'd simply be describing the correlation between market cap and this correlation measure.""]",8,1,https://www.reddit.com/r/statistics/comments/1271m71/q_panel_regression/
578,2023-03-30 23:59:36,"[Q] I’m a novice R user. I have a dataset of events of varying duration that take place over the course of a year. I’d like to predict the likelihood of an event occurring after one days, two days etc. Can anyone suggest tutorials or methods for doing this?",,"['To make your life so much easier I would suggest to get some basics on R first', 'https://rpubs.com/riazakhan94/arima\\_with\\_example', 'Remove the escape slashes on the underscores BTW, otherwise link is broken', 'Thank you!', 'no just clicking on the link']",2,10,https://www.reddit.com/r/statistics/comments/126r715/q_im_a_novice_r_user_i_have_a_dataset_of_events/
579,2023-03-30 22:11:04,[Q] Sources for Self-Study of Application?," I'm looking for sources to gain knowledge in applying statistics. I am in finance in the manufacturing industry, and my work includes corporate finance as well as operational/integrated forecasting models. I finishing up my Masters but most of my exposure to statistics has been formula based and the application and method are already matched for you in the problem. That doesn't help me in the real world when I need to create a non-financial model and then measure its behavior vs. expectations. I have plenty of experience modeling at established companies with mature processes, but my current employer is very green so I am starting everything from scratch.

I have full access to Cengage and plan on renewing it for a year past graduation. Is there a textbook anyone could recommend for self-study? Or another learning source, paywalled or not? I am specifically looking for how to apply statistics, how to choose the correct statistical method for a situation, etc.",[],14,0,https://www.reddit.com/r/statistics/comments/126obem/q_sources_for_selfstudy_of_application/
580,2023-03-30 17:47:18,[Q] Need help with looping/repeating syntax in SPSS,"Hi all.

I'm looking to create a loop for the following syntax, so the syntax will not only create a new variable for the year 2013, but for the years 2010 through 2022. Everything with \_2013 should thus be replaced in the loop by the correct year for the loop (i.e. 2010 through 2022'. I'm using SPSS 25.

compute position\_2013=0.

if any (var1\_2013,90,93,99) or var2\_2013=999 position\_2013=6.

if any (var1\_2013,31,32,33,34,61) position\_2013=5.

if any(var2\_2013,200,310,320,400,500) position\_2013=4.

if var2\_2013=100 position\_2013=3.

if any(var3\_2013,-2,-1,0,1,2,3,4,5,6,7,8,10,11,12,13,14,15,20,21,22,23,24,25,26) position\_2013=2.

if any(var3\_2013,9,16,17,18,19) position\_2013=1.

exe.

value labels position\_2013 1'test1 2 'test2' 3'test3' 4 'test4' 5'test5' 6 'test6'

Thanks in advance!",[],1,0,https://www.reddit.com/r/statistics/comments/126hz2i/q_need_help_with_loopingrepeating_syntax_in_spss/
581,2023-03-30 14:03:27,[Q] Masters Statistics Berkeley vs UChicago,"I got into both Masters programs in Statistics.  
I plan to take the PhD level courses regardless of which program I go to.  
On one hand, I want to go to UChicago because I believe there are a lot more course options and I could learn more statistics. If I go to Berkeley, there's only around 4 thereotical courses and 2 applied courses. Chicago has tons of optimization, applied math, and thereotical statistics courses.

  
I love learning statistics and doing thereotical math, but I don't know anything about research, so I'm afraid that the thesis at Chicago will be a pain in the ass because I hate writing. I don't care too much about the time or cost differences. What program would you go to? Is it really true like I believe that Chicago has better courses?","['UChicago', ""If you got into both programs, you must be a pretty strong student to start with. My advice is to go for the PhD -- try to switch gears once you get there. After you graduate, the first and very possibly last thing people look at on your resume is the highest degree completed. A master's from a strong program might not outweigh the lack of a PhD when you apply for jobs -- you don't want to chance it. \n\nI know you said you don't like writing, but once you've done the work for a dissertation, the writing is a lesser problem. Most people just muddle through it, and once you're done, you'll be glad you did."", ""They are both very good programs. You could go either way and it'd be a good decision. I think you need to focus on the other part of this, which is:\n\n(a) level of support in the department; is there a potential mentor? Are grad students happy? If you stayed for the PhD program directly, how is the funding? (I'm assuming UChicago would be much better stipend). Do you have an opportunity to work with a professor while you do the masters?\n\n(b) living; would you prefer to live in Chicago or the Bay Area? Berkeley is expensive and you might have to live further from campus and commute, while in Chicago you can take the train and it'd be an easy commute if you don't live close to campus."", 'I did USNews, looked at the T20, but this is because I had good stats. I applied to 7 programs which matched with me by either location and theoretical phd coursework available to me as a masters. I looked at the coursework, and chose what I thought was best. I didn’t do Stanford as I was too late.', '169 gre quant when I applied. 162 verbal. \n4.0 math and stat double major. \nInternship in big bank. T100 Public state university.']",1,15,https://www.reddit.com/r/statistics/comments/126dm6b/q_masters_statistics_berkeley_vs_uchicago/
582,2023-03-30 10:59:48,[E] Undergrad grades for Stat PhD,I’m a 3rd year stat undergraduate student taking differential equations even though it’s not a requirement for my major. I’m on track to get a C in the class unless something changes dramatically but the highest I could get even if I try really hard is a B. Is it worth trying to improve my grade or should I put the class on Pass/Fail. Does that look worse than the GPA hit? I’m involved with research and I have a good relationship with my university’s former graduate admissions director for stat (I do research under him).,"['You really don’t want a C in a math class when you’re in stats, especially if you’re going for a PhD', 'This is a lie OP. Take my advice as someone who just got rejected from all 12 phd programs in stats with a 3.5 gpa with no Cs and mostly As and Bs. I had Bs, and was told the same shit about how I would be fine, but well here I am.', 'I would REALLY try to get a B. I have two Bs on my transcript from when I was a freshman/sophomore (aka an idiot), but turned out just fine applying. \n\nDiff Eq can be tricky — this sounds like bad advice, but do textbook problems till you’re blue in the face.', 'I failed an app cycle for PhDs but people who gave me real responses instead of a canned email were people my advisor knew. So I will say you can take the C and be unscathed but you have to make it up in an equally difficult class like real Analysis, or a graduate course that uses Diff Eq heavily (stochastic Optimization Courses come to mind). \n\nWhen it comes to apps rely on professors your advisor has connections to as they will get honest consideration and not get GPA filtered as easily.', 'Take a W']",6,8,https://www.reddit.com/r/statistics/comments/1269ew6/e_undergrad_grades_for_stat_phd/
583,2023-03-30 08:55:34,What job titles am I looking for?? [C],"Hopefully will be a PhD in Statistics, on track to defend in November. I like experimental design and I’m a statistical consultant at the university currently. I enjoy statistical modeling, inference, and helping translate clients’ research questions into statistical processes. I don’t want a job where one of the main responsibilities is scripting or using SQL to interact with databases, etc. Obviously I’m aware of jobs with the title “Statistical Consultant,” but what other job titles should I be investigating that are maybe less known? 

Also for the love of god everyone and their dog are data scientists nowadays.","['Biostatistician and clinical data analyst are a couple that come to mind', 'You’re just oversimplifying the post and being confrontational. Incredibly unhelpful.', 'Follow the money', 'I mean, as far as modern statistics go, I think you get fairly close', 'I do consulting and I’ve worked, and you’re hitting me with the cliché, “well wait for the real world…” you’ve gotta be better than that.']",11,14,https://www.reddit.com/r/statistics/comments/1266k4o/what_job_titles_am_i_looking_for_c/
584,2023-03-30 08:26:33,[research] Help with Confidence Intervals,"I understand the basic idea of confidence intervals and was wondering if you could help me make sense of some data. 

Correlation analyses on the same sample, testing for moderation. So we did a median split on our data, and did a correlation for the ‘high on this’ and ‘low on this’ group using two variables. 

Our output didn’t give us p values, it gave us CIs. Here’s an example of the data:

Low group: r = -.54, 95% CI [-.81, -.16]
High group: r = .11, 95% CI [-.55, .45]

Interpretation: Is it safe to say that this is a significant finding? As in, low group’s r is outside of high groups CI, and high group’s r is outside of low groups CI. 

Is this how to interpret?

Thank you.","['The best way to understand this is to draw it out. Plot your point estimates and the related confidence intervals. How do they look?  \nThen review what a CI means and how you can interpret your data.', ""I'm missing something. The low groups r is 0.54 but the CI is -0.81 to -0.16. Shouldn't the r be inside the CI (and roughly near the center?). Are you sure the low group's r isn't -0.54 ?"", "">  So we did a median split on our data, \n\nsee the links offered by Stephan Kolassa here:\n\nhttps://stats.stackexchange.com/questions/43608/which-test-to-use-for-a-set-of-data/\n\n\nor see p128-129 of:\n\nweb.archive.org/web/20120412171759/http://www.unt.edu/rss/class/mike/5030/articles/makefriends.pdf\n\n(or indeed any of literally dozens of other references which point out problems with this disturbingly widespread notion; I don't have my old references on this, but if you need more I can probably dig a few up)."", 'As a general rule of thumb, when two CIs overlap, you should explicitly test the differences. It’s not sufficient to say “correlation two is outside the CI of correlation one therefore they are different.” The reason for this is that there is uncertainty with both estimates; when you just compare to the CI, you are basically assuming the second correlation is estimated with certainty. \n\nTest it! You can do this in a regression by interacting x with a dummy variable for the high (or low) group.', ""They are not significantly different because their confidence intervals overlap. A confidence interval doesn't give you a rejection region. It says with given probability (95% in this case), the true parameter lies in the interval.  Determining if two confidence intervals overlap is equivalent to a two-sided hypothesis test of two group differences.""]",2,7,https://www.reddit.com/r/statistics/comments/1265ve4/research_help_with_confidence_intervals/
585,2023-03-30 03:56:22,[Q] Anyone have experience with ChatGPT-4 and statistical analysis?,"I'm in the process of performing statistical analysis on a large datasets using spss for the first time. I don't know a lot of statistics as this is my first time doing it, and its a pretty steep learning curve.

In the process of analyzing I got the idea to give GPT-4 the context of my study and just literally copy-paste the data from Spss, and it seemingly was able to analyze the results, in the context of my study..

If GPT does this accurately its a pretty amazing learning tool. I actually find it really fun doing different analysis and chating with GPT about what it means in the context of my study, I just don't know to what degree I can trust it.

Any statistics pros that have experience with this?","['Use GPT as a super-Stack-Overflow. Do not use it as you.', 'Please don’t do this', '> Anyone have experience with ChatGPT-4 and statistical analysis?\n\nOn statistics, my only experience is in correcting its errors when other people  use it; the errors can sometimes be hard to spot, and sometimes deeply consequential - its a confident idiot. \n\nThis repeated experience has been more than enough to convince me not to trust it on anything that needs to be correct.\n\n(edit: minor clarification)', 'it couldn’t calculate pearson’s r given two lists of numbers. I am begging you, do not do this.', '""I want to do something, but don\'t care to understand why or how it works.""']",39,66,https://www.reddit.com/r/statistics/comments/125yvdy/q_anyone_have_experience_with_chatgpt4_and/
586,2023-03-30 03:15:48,[Q] Mediation analysis shows increase of strength between the relationship of two variables...I've only seen relationships reduce. Is there a way to explain this?,"After running a mediation analysis using PROCESS by Hayes, the relationship between two variables increased from .25 to .29.

I'm not really great with statistics in general, and I've only seen relationships reduced after a mediation analysis. Is there a layman's way to explain what this means?

My analysis shows that a specific learning style mediates (in this case, increased) the relationship between a specific personality trait and test scores...",[],3,0,https://www.reddit.com/r/statistics/comments/125xt4o/q_mediation_analysis_shows_increase_of_strength/
587,2023-03-30 01:41:25,[Q] Is coefficient estimate only valid above the intercept value?,"Let's say we have data with age and alcohol consumption. In the data we have age from 12-90, but below 18 consumption is negligible, and so is above 80.
Let's say the intercept is 16, after which consumption increases by 1 glass per year of age. If the intercept is 16, can we say that consumption increases by that much if age is above 16?

Also, let's say people above 80 are not allowed to drink and they generally follow this rule, apart from some that still drink a bit. The scatter plot for this data would look like a triangle (because there is big variability, some 30yo drink a lot while others don't, but in general the older you are the more you drink, until you're 80), with a sharp drop at 80. Is this still a linear relationship? How do we account for this drop?","[""Give more details.\xa0\n\n1. It is apparently clear that age is measured in years. What is the measurement unit for alcohol consumption?\n2. which one of these 2 are the dependent variable? Again, I am assuming it is\xa0alcohol\xa0consumption.\n\nIf assumption 2 is correct, and we assume the usual simple linear regression model (E(Y|Age=x)=b0+b1x) then the intercept is just the average consumption of alcohol for people with 0 years, which is totally useless since you don't have data in that age range.\n\nIf you want a meaningful intercept, you can do a little trick: centering the independent variable (age) around its mean. Your model would look like this: E(Y|Age=x)=b0+b1(x-mean(x))for this model, the intercept would be the\xa0average consumption of alcohol for people with\xa0average age. This would be more helpful and useful.\n\n&#x200B;\n\n>Also, let's say people above 80 are not allowed to drink and they generally follow this rule, apart from some that still drink a bit. The scatter plot for this data would look like a triangle (because there is big variability, some 30yo drink a lot while others don't, but in general the older you are the more you drink, until you're 80), with a sharp drop at 80. Is this still a linear relationship? How do we account for this drop?\n\nNo, this would be outside a linear relation. You may use several techniques for nonlinear regressions. Or simple exclude those people for the analysis since they behave totally different.\n\nEdit: improved notation"", ""If your data is heteroskedastic, then you are violating one of the assumptions for linear regression and may need to find another way of modeling this data. A triangle shaped scatter plot can be an issue. \n\nA good model doesn't just describe the central tendency, it also describes what kind of uncertainty you have. As age increases you become less and less sure how much someone drinks, right? You need a way to describe that.""]",1,2,https://www.reddit.com/r/statistics/comments/125v0tk/q_is_coefficient_estimate_only_valid_above_the/
588,2023-03-29 22:22:40,[Q] What issues/challenges you face in current tools for data science/analytics?,,"['Stan is too slow.', 'I have no budget and a bunch of inane requests.\n\nIt\'s all well and good to say that you want to automatically qualify leads, but when we don\'t pay for API access to the actual data I can\'t do anything.\n\nIf I have to use the same ""user-friendly"" interface that computer-illiterate salespeople do, you may as well just ask me to cut the crusts off your sandwiches, because we\'re trapped in a Fisher-Price ecosystem.', 'chatgpt is too slow', 'my backlog of things I want to play with is growing too quickly', 'Perceived sexiness. \n\nI do some internal stats consulting within a company. Many of the people I collaborate with, particularly associate scientists, view coding as a way towards career advancement. So they are adamant on learning and using Python for basic stats because coding and Python are sexy. \n\nFor some people, it works and it’s useful. But for the vast majority, I think it would be better if they used their time and energy to understand basic stats, and use the most user-friendly tool available (yes, with a GUI) that allows them to do their work with the flattest learning curve.\n\nBasically, people want to learn tooling at the expense of stats fundamentals when it makes no sense, and it bothers me.']",26,12,https://www.reddit.com/r/statistics/comments/125pjix/q_what_issueschallenges_you_face_in_current_tools/
589,2023-03-29 20:07:08,[Q] Cost Benefit Analysis on maintenance of offshore wind turbines with random breakdowns,"Hello everyone, this is my first time posting, so forgive me if I’m breaking any of the rules. 

I’m performing a cost benefit analysis to assist in the decision making process for choosing a boat to buy for the maintenance of offshore wind turbines. 

I have data for; the number of turbines, the assumed yearly breakdown rate for the turbines, the number of transfers for maintenance session per year, the number of days each boat is available to travel per year and the cost per day of downtime due to inability to travel to perform maintenance. 

I am unsure how to model this to calculate the potential benefits of purchasing a more expensive boat that can travel through more days of the year  (which therefore leads to less costs from the downtime of the turbines). 

I am unsure how to assign the days available to travel per year with respect to random breakdowns (i.e flat turbine breakdown rate of 5%, 1 boat can travel 215 days and another 260 days etc etc). 

Can anyone help me with this? Please let me know if I need to provide further info. A point in the right direction would be great!","[""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3657479\n\nI learned about this today, but it seems to work for your problem, but may also be overcomplicating things.\n\nNormal or benefit analysis ranks expected lifetime cost and expected lifetime benefits. You can use probabilistic outcomes in this. For example if there's a 5% chance in a year that a particular wind turbine breaks down, you can just assume that every year you will have to service 5% of the turbines. Run this for multiple years and the randomness all balances out by the end."", ""Seems very intense (with my limited knowledge of this area). But it's all I've got at the moment so will be giving it a try. Thanks for your reply."", 'How would you normally do a cost benefit analysis?\n\n[I discovered Markov decision processes from this video, which is nice and clear.](https://youtu.be/2iF9PRriA7w)']",1,3,https://www.reddit.com/r/statistics/comments/125m6l3/q_cost_benefit_analysis_on_maintenance_of/
590,2023-03-29 18:43:52,[Q] Can I define my own estimator function?,"Since, estimator is a function on a statistical(or any?) data to estimate an **unknown** parameter, will it be valid (or is one allowed to) to define your own estimator, like your own parameter..","['You can define literally any estimator you like.\n\nYou would (presumably) want it to have good properties, but there are tools for examining the properties of estimators - consistency, sufficiency, bias, variance, MSE, relative efficiency etc etc. So you can define one and then investigate how good it is. \n\nOf course, things like efficiency are not the only consideration (e.g. simplicity may be important in some circumstances, or the ability to update ""on-line"", or robustness etc etc). I use very simple but not fully efficient estimators quite often (getting a ballpark estimate or a starting value for something else for example). They can be very handy in a variety of situations.', ""Depends where you live. In some countries it's illegal without a license."", 'You can estimate things however you want, but most of statistics is about defining what estimators are good, which is why people follow the theory.', 'Yes.\n\nWhy would you want to?\n\n\nThink of the following problem: you want to estimate the unknown mean of a distribution.\n\n- you could draw many observations from that distribution, take their average, and use that as your estimate of the mean.\n\n- or, you could draw **one** observation from that distribution, and use that as your estimate of the mean.\n\nNote that both methods give you unbiased estimates of the mean.\n\n\nNow, why would you prefer the second method to the first?', '🤣']",7,9,https://www.reddit.com/r/statistics/comments/125k2mr/q_can_i_define_my_own_estimator_function/
591,2023-03-29 18:16:59,[Q] What are the problems you face in your data science workflow?,"Hi All,

We are a team of data analyst. We are doing a survey on the problems data scientists face while doing their work. Please comment if you have anything to share from your experience.

In our personal work, we have found that sharing our work across our team and saving a history of our work progress is challenging.

Thanks","[""Receiving shoddy data is probably my most annoying and frequent problem. (Shoddy both as in 'not sufficient for what they want me to do with it' and as in 'untidy, riddled with spelling errors and formatting issues, missing or incomplete cases etc')"", 'Low quality data (and a lot of it). Specifically problems with compliance/ownership over data calculations/features. Working data analysis for financial institutions is gruesome work.', 'The description of your problem to track progress and share data in your team sounds like the perfect use-case for [DVC](https://dvc.org/). It is a tool that has helped me enormously.\n\nI like it because it is not opinionated about your workflow. You can integrate pretty much anything in it.']",2,3,https://www.reddit.com/r/statistics/comments/125jcda/q_what_are_the_problems_you_face_in_your_data/
592,2023-03-29 14:33:03,[Q]Chi-square or T-test for finding statistical significance for market share of brands?,"Hi,

As the title suggests, I'm calculating the statistical significance of 2 brands within the same market using their market share. I have the market shares of the two brands for the past 12 months and am weighing the cons/benefits of using either a chi-square or a t-test. 

If anyone has any inputs or other statistical techniques that could be of use, it'd be much appreciated. Thank you!","[""Chi square and t-tests don't work on the same data, so there is no need to decide between them.\n\nYour research question is nonsense anyway.  \nIf you want to do null hypothesis significance testing, you need to start with the basics of knowing what a hypothesis is, what significance is (and more importantly isn't) and only then start looking into tests."", 'If I understand your question correctly, the null hyppthesis is that the two distributions are equal, correct?\nAssuming I got that right, then either a Wilcoxon or a Kolmogorov-Smirnov test are due. Only if both distributions in market shares are normally distributed you could go for a more potent T-test.\nI would leave the Chi Square test out of the whole matter to be honest.']",0,2,https://www.reddit.com/r/statistics/comments/125fd4h/qchisquare_or_ttest_for_finding_statistical/
593,2023-03-29 13:19:35,"[Q] How do I compare lotteries' chances of winning jackpot, when they differ in the maximum number of plays?",">[However, there is one way to boost your chances of winning the lottery, says [Dr. Mark] Glickman: Your odds do improve by buying more tickets for each game.](https://www.cnbc.com/2019/05/31/harvard-prof-on-odds-of-winning-multiple-lotteries-like-these-people.html)


#### 1. How can I deduce which lottery's jackpot is easiest to win, when they differ in the maximum plays? 

#### 2. For example, which has the highest chance of winning : 10 plays of Lotto 649 vs. 5 plays of DAILY GRAND (assume I pick $7 million lump sum) vs. 2 plays of DAILY KENO?

## Data for 3 lotteries from the Ontario Lottery and Gaming Corporation

| | [LOTTO 649](https://www.playsmart.ca/lottery-instant-games/lottery/odds) | [DAILY GRAND](https://www.playsmart.ca/lottery-instant-games/lottery/odds) | [DAILY KENO](https://www.playsmart.ca/lottery-instant-games/lottery-daily-games/odds/)
|:-:|:-:|:-:|:-:|
| **Jackpot** | $5 million | $1,000/day for life or $7 million lump sum | $2.5 million |
| **Odds of winning jackpot** | 1 in 13,983,816 | 1 in 13,348,188 | 1 in 2,147,181 | 
| **Matching numbers required to win jackpot** | 6/6  | 5/5 + Grand Number | 10/20 on a $10 bet |
| **Number pool** | 49 | 49 + Grand Number (7) | 70
| **Number of tickets one person can buy** | [10](https://i.imgur.com/rmjU10P.jpg5) | [5](https://i.imgur.com/5b5PMEM.jpg) | [2](https://i.imgur.com/h43DaJV.jpg)","[""Divide the odds by the number of tickets you are allowed to buy.\n\nIf you're trying to choose which one to play, you would calculate the expected value of each ticket. This is the jackpot x the odds.\n\nFwiw, your expected value is better by putting the money into a savings account.""]",0,1,https://www.reddit.com/r/statistics/comments/125e0bx/q_how_do_i_compare_lotteries_chances_of_winning/
594,2023-03-29 11:55:05,[Q] Looking for advice on what statistical test would best suit a small data set (n=25) with a random grouping factor. LMMs keep resulting in singular fits.,"I'm working on some research examining the impact of different land cover types on bat species diversity in my area. Month was not accounted for in planning for the species surveys and, as some bats are migratory, this may have an unintentional impact on my results. My original plan to account for this was to use a linear mixed effects model using the month the survey was performed in as the random grouping factor. Bat study season in the US only last 4 months, however, so my grouping factors has less than the recommended 5 levels. I've read recently that it is becoming more accepted to use LMMs with grouping factors with as little 2-3 levels as they are still more accurate than fixed-effect models. I did my best to simplify the models to accommodate my grouping factor, but my results are coming back with too many singular-fits to really justify using LMMs to make an argument. At this point, I'm thinking of just using a linear regression and incorporating ""month"" as a fixed-effect factor. Though this is the approach I've read is recommended in my case, I still want to try and assess what, if any, explanatory power the potential influence of the month takes away from different types of land cover. An approach I've been ruminating on is running two regressions, one with month as a factor and one without, then using a chai square goodness-of-fit to assess which model is the best fit for the data. My knowledge of statistics is piece meal, but working through things like this is how I've learned thus far. Open to any advice and/or suggestions.",[],1,0,https://www.reddit.com/r/statistics/comments/125cc9u/q_looking_for_advice_on_what_statistical_test/
595,2023-03-29 09:42:37,[Q] How many rolls in average will it take for a twelve sided dice to roll the same number four times in total.,"I'm working on a tabletop RPG with a less conventional health system where each point of damage is represented by a d12 rolled that has its value recorded and a character dies when they take four points of damage to the same value.

So I wanted to ask if someone knows the answer to how many d12 would have to be rolled in average to roll the same number four times across all rolls.","[""Someone else can chime in with an exact answer, but here's a quick Python simulation...\n\n    import numpy as np             \n                \n    def roll_until_four():\n      rolls = []              \n      maxct = 0\n      while maxct < 4:\n        rolls.append(np.random.randint(low=0, high=12))       \n        vals, counts = np.unique(rolls, return_counts=True)      \n        maxct = max(counts)           \n      return len(rolls)           \n                \n    results = [roll_until_four() for _ in range(1000000)]\n\n[Histogram of results shown here, average is about 17.5](https://i.imgur.com/Ich6hUH.png)"", 'During my computational statistics class, I learned that all of the difficult analytical methods could just be simulated and you can calculate sample properties super easily. Blew my mind! Asked my prof about it and his answer was ""well, if computers would have been around 200 years ago, we would all be doing computational stats instead of analytical stats.""', 'TL;DR E(X) = 17.505 VAR(X) = 23.709 StD(X) = 4.869\n\nThe number is conclusively between 17 and 18 by the following examination of the CDF.\n\nE(X) = the value of x when the CDF(X) = 0.5. By definition this is the expected number of rolls to get 4 of the same kind.\n\nWhen 12 \\* (X choose 4) \\* (1/12)\\^4 \\* (11/12)\\^X-4 = 0.5 the number X will be the number of rolls that will achieve a 0.5 probability of any of the 12 sides appearing 4 times in X rolls.\n\nThis confirms the simulation that I ran to get the exact figures.', 'I haven’t verified this answer, but [here](https://math.stackexchange.com/questions/542200/expected-number-of-tosses-before-you-see-a-repeat) is a stack overflow thread that describes the answer (it’s a nasty integral). You can follow the person’s links to their other answers that show the derivation if you’d like. According to Wolfram Alpha, the answer is around 17.5', ""Lol right. I started trying to math this out exactly with tree diagrams and combinatorics, and then when things started getting hairy I stopped and reminded myself that outside of academia virtually nobody actually needs exact results. Simulations like this don't even have to be *good* code, just any old thing that comes to mind will probably get you the right answer within a tiny margin of error in like five minutes tops.""]",40,22,https://www.reddit.com/r/statistics/comments/1259epv/q_how_many_rolls_in_average_will_it_take_for_a/
596,2023-03-29 07:30:38,[Q] Instrumental Variables in Gompertz Functions,"Hello!  I have but a mere Statistics minor and am currently in my Econometrics Honours, so I'm not the brightest bulb in the Python Studio.  


I don't quite know where I am going with this question, but I've had these neurons firing at each other for over a week wondering if there's ""something here"" and so I thought, maybe people who take apart data for a living can parse what I am asking!  


I am using an ODE model with a Gompertz function to measure oncolytic virus interactions with cancer cells for my dissertation.  For one of my classes, we are using IVs to measure the usual education/income data everyone does every year.  


But I wondered, could we utilise IVs in a Gompertz model to isolate the actual effects of the oncolytic virus vs any ""noise"" or errors?  But if the ODE model DOESN'T INCLUDE NOISE (cause there's just so many variables in cancer) how can an IV be used to figure out the true effects of the oncolytic virus? Additionally, what would that IV even BE?!  


For some reason my brain says ""there has to be a way to use IVs in this ODE model to truly capture the cancer lysis process"" but if the model doesn't model noise is that even possible?  Would this introduce more bias?  Am I on a track that others have been on and can tell me if they know where my brain is going?

It feels like my brain is missing a tiny piece of the puzzle but wants REALLY BADLY to connect these two concepts if not just to see what happens.

Thank you for putting up with this fool. I'm still learning and I was never good at maths to begin with, so it's been four years of straight new-found passion with 0 mathematical background, so I appreciate the help!","['I am not answering your question directly because I don’t have enough experience with IV methods in general.\n\nI look at it as a problem where you want to use Gompertz likelihood instead of Normal likelihood for your outcome.  An easy way to adapt this is to do it using Stan.  Yes, you need to go full Bayesian, which is probably not where you are interested in going.  But it should be the same stan code for the IV case study with a modification on the likelihood on the outcome vector.  Good luck!', ""No no actually a lot of this study is based in full Bayesien so thats not a problem. I mean it is in the sense it's painful and not quite perfect for oncolytic virus models but hey, good enough.\n\nAnd holy shit I did ask my supe and your answer was similar to both her and what I found. Regress w my ivs and then insert into the gompertz function... It seems possible and fine??\n\nI'll check out the Stan, not terribly familiar with that funnily, even tho I am knee deep in Bayesien. \n\nI mentioned the heat of the mouse cage as an IV for if the injection volume is too noisy we could find closer effects for the model... And my supe actually said that was not a stupid suggestion at all and she will see if it could be reasonably applied to our study???\n\nI am NOT an idiot???\n\nThank you so much. It's wild to know my brain worm actually did have something reasonable it was trying to figure out!!\n\nEdit to clarify that I have aphasia so my words are always a little off thank you for parsing what I mean."", 'As you are getting into Stan, there is a wonderful forum https://discourse.mc-stan.org/ for you to ask questions.  I am active on there, too.  Good luck again', 'If I did a 2 stage with regression on the Treatment and IV and then substitute for the endo in the gompertz in the 2nd stage, would that work?\n\nEdit: would this not have a chance of increasing my standard errors tho?']",6,4,https://www.reddit.com/r/statistics/comments/125688k/q_instrumental_variables_in_gompertz_functions/
597,2023-03-29 06:13:09,"[Q] Correlation between a variable that is a %, and one that is a whole number?","I'm trying to find the correlation between population density (per square km) and obesity rates. When i graph these two variables in a scatterplot, they look very correlated, almost like a straight line. But when i calculate the R value, after i convert my % data into decimals, the r value is low, 0.43. When i just leave it as 30%, it's really high, 0.9. which is correct? Am i doing anything wrong? would it be incorrect for me to leave the % value as it is when calculating R? (ex. 30% instead of 0.3) Thank you.. i'm really weak in stats 😭","[""You're doing something wrong.  Given, e.g., the following, the correlation between A and B, and between A and C are the same.\n\nA = (1,2,3,4,5)\n\nB = (0.10, 0.20, 0.25, 0.30, 0.27)\n\nC = (10, 20, 25, 30, 27)"", ""That's odd since (assuming you're computing the Pearson correlation coefficient. i.e. covariance over product of standard deviations) correlation doesn't change under scaling by a positive number. I would guess you're computing it incorrectly."", 'TBH I can\'t tell what you might be doing from the description but 30% is the same number as 0.3. ""Per cent"" means ""per 100"" or ""times 0.01"".', ""LOL a sorry yes. i'm trying to say, i calculated an r value using decimals (like i converted my percent data to decimals) and i got 0.43. but my scatterplot looks VERY  strongly correlated, so i am confused.... I don't understand why this happened""]",1,4,https://www.reddit.com/r/statistics/comments/125475o/q_correlation_between_a_variable_that_is_a_and/
598,2023-03-29 04:39:21,[Q] Sample size estimation," I want to know if patients with a particular disease have a greater likelihood of having a comorbidity if their disease is severe (0/1). I want conduct a cohort study of patients with a particular disease from a single hospital site. All patients with the disease from that hospital will be asked to participate. Those who choose to participate will then be assessed for a comorbidity with a device and stratified by severity of their original disease. 

I want to see whether their comorbidity is significantly different by their original disease severity or not…

What’s the smallest sample size I should enroll before looking at the findings?

What should I integrate/do to adjust if I look at the findings midway and then decide to enroll and collect more patients after?

Thanks for any and all help thinking through this!","['I think [sequential analysis](https://en.wikipedia.org/wiki/Sequential_analysis) is what you are looking for. The link is just an overview, you’ll need more details to perform the analysis.']",2,1,https://www.reddit.com/r/statistics/comments/1251j7o/q_sample_size_estimation/
599,2023-03-29 03:38:28,[Q] Significance of hedges g?,Can someone explain for me the significance of the hedges g values? Are values greater than 1 possible and what would that infer? Can provide more information in comments if need be.,"[""Hedge's *g* is similar to Cohen's *d* (which you'll find more literature about).  It's an effect size statistic.   Conceptually, it's the difference in means between two groups, divided by the pooled standard deviation.  Yes, it can be greater than 1.  Simply, if the difference in means is greater than one standard deviation."", ""Hedges g is an effect size.\n\nHypotheses are about population parameters, like population means. You could potentially cast a hypothesis in terms of population effect size but it would be ... kind of odd.\n\nYes, values greater than 1 are easily possible, unless you're dealing with some unusual situation (which would likely make a nonsense of using Hedges g).\n\n>  what would that infer? \n\nYou mean *imply*. *Inferences* are something *you* do. It would imply something, you would infer.\n\nDirectly, it would imply that the means differ by more than one  pooled standard deviation\n\n> Can provide more information in comments if need be.\n\nPlease don't just put any additional information in comments. If you need to add information, make sure you edit it into your post, so that hapless readers aren't expected to go hunting through what sometimes ends up being lots of comments looking for some essential bit of information, or to ask the same clarifying questions over and over.""]",6,2,https://www.reddit.com/r/statistics/comments/124zq43/q_significance_of_hedges_g/
600,2023-03-29 01:26:25,[Question] What kind of hypothesis testing can i perform on such a table,"Hello everyone , 
Currently on my junior year, after the end of the hypothesis testing chapter , our teacher asked us to lead a study on how people from our country spend their time , the tables  we are working on has 4 columns which constitutes the education level and lines which are the average time spent on each activity : leisure , sleep , religious practice ..etc we have a table for men and one for women . 
My question is what kind of hypothesis testing can be performed on this table , so far i thought of performjng a chi-squared test of independance to see if for example the average time spent sleeping is dependent on the education level , but i don't know how to proceed further since the data i have on hand doesn't have observations but rather average times spent so i don't see how i can perform tests on the mean ,variance ,proportion ..etc","[""The usual chi-squared tests on contingency tables are for count data (while the hypotheses would relate to proportions in any of several ways), not means\n\nI don't see any reasonable basis for inference if people are recording *times* (specifically, durations) and all you have is the average. Further, the note that the times in each of the categories are dependent (you can't do more than 24 hours of activity in a day; e.g. more time sleeping means less time on other activities)""]",2,1,https://www.reddit.com/r/statistics/comments/124vytt/question_what_kind_of_hypothesis_testing_can_i/
601,2023-03-29 00:42:15,[Q] diffusion models - drift rate,"Does anyone have a good example of how one goes from having response times to extracting the drift rate in a diffusion model? How is the decomposition into non-decision time and drift rate done? I have seen it written up in stan code and all, but I was wondering if there is a simpler explanation I could memorise more easily. Thanks","[""There really isn't anything to memorize more easily, and there generally aren't any closed form solutions for the model estimates. The combination of model parameters implies a specific distribution of response times, and the parameters are selected to best match the distribution of the observed data. In certain very specific cases, for very simple models, people have derived closed form approximations for some of the parameters, but these are rarely used in practice.""]",3,1,https://www.reddit.com/r/statistics/comments/124uqh5/q_diffusion_models_drift_rate/
602,2023-03-28 23:14:11,[S] How to find p-value boundary in Minitab,"Greetings,

I have the following situation in Minitab:

I have a reference population with a mean and standard deviation. 

I'm looking to make an area plot with mean and standard deviation on its axes. In the figure I wish to plot areas and the boundaries where a sample with the mean and standard deviation from the axes values are significantly different from my reference population. 

I've done this before in Excel where it's essentially countless different t-tests (for each mean-stdev pair) but that doesn't give me smooth contours, and I feel like this might be built in somewhere but I just don't know the right name.","[""I'm confused by what you write there -- how were you testing for differences in standard deviation using a t-test?"", ""I'm not testing for differences in standard deviation. This is more akin to a sensitivity analysis. \n\nI'm looking to visualize how much change to the sample mean, sample stdev, or both, it takes for the difference to become statistically significant.""]",1,2,https://www.reddit.com/r/statistics/comments/124rx0i/s_how_to_find_pvalue_boundary_in_minitab/
603,2023-03-28 22:34:05,[Q] Help in understanding error propagation,"Hello,

I'm having some difficulty understanding how to propagate errors.

Let's say you solve 2 linear system of equations

    1=A
    0.01=2A+B
    -1=B
    
    AND
    
    1=C
    2=2C+D
    -1=D

Naturally, the above doesn't have an exact solution, so you get an approximate where

    A=1.003
    B=-0.996
    C=1.66
    D=-0.33

With a norm of residuals

    A and B=0.0057
    C and D= 1.154

Say you have some function

    f(A,B) = sqrt(((A-C)^2)+(B-D)^2)

Now I've seen 2 ways of error propagation. The errors can be added,sub,mult,divided 

I.E.

    if addition,  the error in the result of an addition or subtraction is the square root of the sum of the squares of the errors in the quantities being added or subtracted https://faraday.physics.utoronto.ca/PVB/Harrison/ErrorAnalysis/Propagation.html
    errors A-C and B-D=sqrt(0.0057^2+1.154^2)=1.531
    errors A-C^2=2*(A-C)*1.531
    errors B-D^2=2*(B-D)*1.531
    ((A-C)^2)+(B-D)^2) = sqrt((2*(A-C)*1.531)^2+(2*(B-D)*1.531)^2)
    sqrt(((A-C)^2)+(B-D)^2) = 1/2*(1/sqrt(((A-C)^2)+(B-D)^2))*sqrt((2*(A-C)*1.531)^2+(2*(B-D)*1.531)^2)

I've also seen it done via covariance matrix, where the norm of residuals is your variance, and assuming no covariance between the variables, you would get

    errors f(A,B)= sqrt((df/DA)^2*(0.0057)^2+(df/DB)^2*(0.0057^2)+(df/DC)^2*(1.154)^2..

where the derivative of the function can easily be derived

    e.g. df/DA= A-B/(sqrt(((A-C)^2)+(B-D)^2))

Then you can just plug in the values of A,B,C, and D and calculate the uncertantity that way.

These 2 methods give you different results however, and I'm not quite sure which is the proper route or method for propagation of errors. Any help would be greatly appreciated!","['For the physical sciences, the last approach is the ""go-to"". You can find (much) more information from BIPM\'s [Guide to The Expression of Uncertainty In Measurement](https://www.bipm.org/documents/20126/2071204/JCGM_100_2008_E.pdf/cb0ef43f-baa5-11cf-3f85-4dcd86f77bd6).\n\nNote that this last approach is also just an approximation using a first-order (sometimes second) Taylor polynomial.  While you *can* find exact uncertainties, this is often very hard and sometimes impossible. Another cool way to find uncertainties are via Monte Carlo simulations.', 'The first method is not valid. When you find the difference, between two random variables, A and B, then the variance of difference V(A-B) = V(A)+V(B) + 2cov(A,B). The variance is not V(A)-V(B). This should also make sense intuitively as that could provide negative variances very easily.\n\n&#x200B;\n\nAgain, for the first method, you do not actually need to think partial derivatives to handle the problem. It is possible to split the problem up to simpler chunks.\n\nFor example, for the problem\n\nY = (A\\*B - C\\*D)/E,\n\nyou can first calculate the uncertainties of A\\*B and C\\*D. Calculate then the uncertainty of the difference F = A\\*B-C\\*D, then you find the uncertainty of F/E.\n\nFor simple situations like these, you can just apply some [simple formulas](https://www.isobudgets.com/propagation-of-uncertainty-rules/).\n\nUncertainty propagation can be very hard, and many people in my field (analytical chemistry + chemometrics) struggle to do it.', ""The general case of a function of some uncertain variables is handled via the change of variables method; a web search for that will find some resources for that.\n\nFormulas involving derivatives are approximations to the exact result which you would get via change of variables -- essentially just making a linear approximation and then applying rules that are exact for linear combinations of uncertain variables. Whether the linearization is a close approximation or not depends on how nonlinear the function is, and that has to be studied case by case.\n\nMonte Carlo methods are a different approximation, which keep the nonlinearity at the cost of giving up a simple formula, and instead yielding just a histogram. (On glancing at the NIST Uncertainty Machine, it looks like that's what they're doing.)"", ""so are both methods valid then? In many cases the 2nd approach derivatives are harder to determine (I usually use finite differences to approximate it). If so, I presume both should give similar values, but I've found in my experience they can give drastically different uncertainties (which is why I was asking this question)"", 'You are correct, I have subsequently fixed it']",8,9,https://www.reddit.com/r/statistics/comments/124qnl0/q_help_in_understanding_error_propagation/
604,2023-03-28 20:42:14,[Q] someone who understands correlation pls explain this to me,"So I'm doing research on impact of Social Media on Eating Behaviour. Out of 60 respondents, 33 had a social media addiction and 25 of those were at risk of developing an eating disorder. There were only 7 participants with ED risk without SM addiction. But when I used Pearson's correlation on excel, I got the score of 0.3 which signifies low correlation, why is this the case when 75% of people who are addicted to SM are also at risk for ED?","['Correlation is for a linear relationship between two numerical variables. You want to do a chi-square test for independence.', ""Aren't your variables categorical ? You should probably not be using Pearson's correlation in that case !"", 'In response to some of the comments, Pearson correlation for dichotomous variables, treated as numeric, and *phi* \\--- the correlation coefficient for a 2 x 2 table --- will give the same result.', 'But you want to analyze each as *AtRisk / NotAtRisk* ?  Or you want to analyze the scores ?', 'Both *Social Media* and *Eating Behaviour* are dichotomous variables ?  That is, each is *yes / no* ?']",0,14,https://www.reddit.com/r/statistics/comments/124nk38/q_someone_who_understands_correlation_pls_explain/
605,2023-03-28 17:06:13,[Q] Writing model based on model parameters,"Just a quick question, did I write the model right based on model parameters or I made a mistake ? 

Thank you 

[https://imgur.com/TNAxrRY](https://imgur.com/TNAxrRY)","['Yep', 'Yep I made a mistake or Yep I wrote it right :)\n\nThank you for your answer :)', 'A linear equation is of the form:\n\nY = intercept + parameter*predictor + error\n\nSo yes you got it right.', 'Thank you.\n\nAnd how would I write the model for this ? [https://imgur.com/IF1ochU](https://imgur.com/IF1ochU)\n\nDo I look for ar5 and something else ? \n\nThank you again', 'x\\[t\\] = 0.0081 - 0.4075\\*x\\[t-1\\] + 0.0046\\*x\\[t-2\\] - 0.0206\\*x\\[t-3\\] + 0.0303\\*x\\[t-4\\] + 0.0532\\*x\\[t-5\\] - 0.0537\\*x\\[t-6\\] - 0.0040\\*x\\[t-7\\] + 0.5494\\*e\\[t-1\\] + e\\[t\\] would something along these lines be good or not']",0,6,https://www.reddit.com/r/statistics/comments/124iozp/q_writing_model_based_on_model_parameters/
606,2023-03-28 13:33:17,[Q] Are there any good references for linear algebra in statistics?,My class is getting into time series. Would there be any good references for starting in linear algebra in statistics? The reference our professor gave us is really hard to understand. It skips most of the proofs at times.,"[""In our class we used Searle & Khuri - Matrix Algebra Useful for Statistics. Pretty readable and straightforward. While it's not that rigorous compared to other LA books intended for math majors, it's a pleasant introductory material that you can use to get up to speed w/ fundamental LA concepts useful for higher stats."", '>\tLinear algebra is important in classical statistical analysis (I’m excluding ML) for three things \n\nI’m sorry, but I really disagree with your post. Linear algebra is the vocabulary of statistics. What language *other* than a matrix do you have to express something as fundamental as a dataset? \n\nI agree that you don’t need a full understanding of linear algebra to be a good statistician. Most methods will be expressed in terms of matrices and vectors, though. If you want to learn something new, you’ll have to find a non-technical explanation that glosses over the linear algebra.', 'If you have the time/patience to start from scratch, Linear Algebra Done Right is really intuitive and friendly. \n\nMatrix Analysis by Carl Meyer is from a numerical analysis point of view which I think is an underrated  but really useful skill for statisticians (eg knowing about condition number and QR factorizations etc). \n\nEither way I’d recommend to really just hunker down and invest a lot of time in making sure you understand eigenvalues and matrix factorizations and just linear algebra in general because it’ll make the rest of the statistics journey much faster and enjoyable to wade through. And bonus it’s also used all over the place in engineering too so it’ll make you better at understanding that literature too.', 'This is very misguided. Linear algebra is needed for dimension reduction techniques, which are butter and bread for statisticians, and for penalized regression, which is a staple of modern statistics.', 'The Matrix Cookbook has a lot of *super* useful identities. I definitely recommend keeping it as a reference. It can easily be found on Google (or Bing, lol).']",43,16,https://www.reddit.com/r/statistics/comments/124ehl5/q_are_there_any_good_references_for_linear/
607,2023-03-28 06:11:44,[Q] How to straighten qq plot for regression?,"Say residuals are not normal and you want to do some transformations to variables. Is there a way of knowing how to transform them and which? Can that be seen from the gg plot? Say you want to bring down the upper tail that is curved. Is there a specific transformation?
Any way I do it, I always break something else. I get the upper part to be straight, and the lower part just curves more. Or the other way round. So are there any rules on how to know what to do?","['There is an r function (boxcox in the mass package) to help you determine the optimal box cox transformation of the response variable to make residuals as normal as possible. \n\nProf. Frank Harrell advocates making a restricted cubic spline of the predictor variables.', '(made several edits to add information and examples)\n\n> Say residuals are not normal and you want to do some transformations to variables\n\nThere\'s problems with this whole approach, but let\'s start by entertaining the idea.\n\n1. You\'re looking at a Q-Q plot and you see it\'s clearly not linear. Does that of itself mean the conditional distribution of the error term is non-normal?\n\n  No. \n\n  It really doesn\'t. Here\'s two examples. In both cases, each and every error term *was exactly normal*. I know because I generated them that way.\n\n   https://i.stack.imgur.com/MG9Hw.png\n\n   The top row is the Q-Q plot. The bottom row is the residual plot that *shows why the Q-Q plot looked nonlinear*.\n\n  Clearly, we need to impose some restrictions before we try to interpret a Q-Q plot!\n\n  Requirement: the other assumptions of your model have to hold, especially (i) the model specification of the conditional  expectation must be correct (or at least very close to it); (ii) the model specification of the variance term must be correct (ditto).\n\n  Trying to interpret a Q-Q plot without that requirement is a waste of time, you\'ll be chasing phantoms.\n\n2.  Okay, let\'s imagine that we\'re satisfied - by some means - that this is not a problem and that QQ-plots are interpretable. \n\n   Then (speaking very loosely) the apparent relationship in the plot the way around R does it (random quantity on the y-axis, fixed quantity on the x-axis -- i.e. the \'right\' way to plot a relationship) will tend to show how a normal error distribution has been ""transformed"" (and hence, it might seem, how to invert it).  If the relationship looked curved like an increasing quadratic / cubic power / exponential, that might suggest square root, cube root or log, for example.\n \n3. Great, so we at least sometimes have a pretty useful tool, right? If we are satisfied that the other assumptions hold, we can just look at the Q-Q plot and use that transformation and everything should be okay, right? \n\n   No. \n\n   Sadly, everything is not okay. \n\n   It sometimes sort of works ... but it often doesn\'t work (in part for reasons we\'ll see in a moment, but also for other reasons). While it may sometimes give you some sense of a helpful transformation; we\'d also need  to consider additional complicating issues -- like, say the impact of a location shift of the conditional distribution on a transformation - a shift may render the transformation next-to-useless, without impacting the shape in the Q-Q plot).\n\n   Even if you do happen to have a transformation to approximate conditional  normality, your nonlinear transformation *will screw up the residual vs x plot* - the very thing that had to be right in order to correctly interpret the Q-Q plot. Worse, normality is usually the least important of the three (linearity, homoskedasticity, normality) so screwing up the other two things to get approximate normality is likely to be highly counterproductive.\n\n   Take a look at the bottom row of the plot above. You will create *both those issues at once*. Well, not exactly those things, but things like those, you\'ll introduce curvature where you had linearity and heteroskedasticity where you had homoskedasticity. \n\n   Further, typically when you do get a sense of how a normal error distribution might have been transformed, the inverse transformation doesn\'t help the normality either, because the errors are different from the conditional distribution of the data.\n\n   That is, when you *can* interpret the Q-Q plot, transformation is not the right tool to fix your regression.\n\n   Conversely, the only time transformation is of use is when the residuals-vs-x relationship is *wrong* (and it would have to be wrong in just the right way), making the Q-Q plot essentially *uninterpretable*, as we established back in 1. That is, if transformation is to be useful, you should not expect that the Q-Q plot will typically be of much direct help in figuring what transformation to use. It may mislead you because of the interaction between the impact of the non-linearity in residuals, the heteroskedasticity and the non-normality.\n\n   Let\'s look at an example where the linearity and the heteroskedasticity are okay so the Q-Q plot *is* interpretable and indeed, pretty clearly shows us how a normal error was modified before the error was added. \n\n   https://i.stack.imgur.com/zUO3F.png\n\n   Looking across the top row of the plot, we can see that the relationship is linear, so the residuals are flat vs x, and we can see that there\'s no heteroskedasticity -- the spread is constant as we go from left to right. The distribution of errors around the line is skewed, and that shows up clearly in the Q-Q plot. This is a case where the Q-Q plot really does show us the problem with the distribution of the error term. \n\n   The curve in the Q-Q plot in row 1 looks sort of exponential, and indeed that\'s exactly what happened; I took normal errors and exponentiated them before adding them to the line. We can see it fairly accurately. So wouldn\'t we just need to take logs?\n\n   Well, what happens when we try it? As we see in row 2, the y vs x relationship is now curved, and (though it\'s not as obvious) the variance is no longer constant. It\'s actually a bit more spread out on the left end than at the right end. We screwed up the part that was right.\n\n   Worse, we didn\'t even make the residual distribution look normal. We did improve it a bit at the left end but had almost no impact at the right end. This is because the line shifted the noise distribution up and an unskewing-transform is changed when you shift it; in the case the log-transform does less and less the more you shift it up, so the skewness is only a little improved on the left - and hardly affected at all on the right. What we\'d need to do (if only we could) is go back to the exponentiated noise I made and take the log of that, but we can\'t do that. \n\n   Transformation is just the wrong tool in precisely the case where the Q-Q plot most accurately shows you the problem! Conversely, in a case where transformation would help, the Q-Q plot may suggest something like that problem or it might entirely mislead you about it (e.g. you might be able to approximately fix nonlinearity, heteroskedasticity and non-normality all with the same transformation - it\'s not typical but it can happen -- but the Q-Q plot might not show any problem at all, or might suggest a very different-looking problem).\n\n  A better use for transformation is to try to fix residual plot patterns - nonlinearity and heteroskedasticity, though it\'s not the only tool. The Q-Q plot is usually not all that critical by comparison, if sample size isn\'t small (unless efficiency/ power is important)\n\n4. Given that you will generally introduce considerable difficulty in interpretation when you transform y (even when you avoid the problems above), generally my advice is not to use transformation even in a case where it might work quite well, but to choose a more suitable model for the conditional distribution for y from the start, considering the whole thing together. This might be a generalized linear model, for example, or it might be something else. There\'s multiple options depending on what the particular collection of issues may be.\n\n   Wherever possible such choice should be made without reference to the same data you want to use for inference or prediction, for the usual reasons; this means either figuring out a good model without looking at the present data or splitting off part of the data at random in order to make such an assessment -- though that, too can carry some problems of its own.\n\n   An exception to the \'generally, don\'t transform\' advice would be where the transformed scale is particularly meaningful in its own right (e.g. convert times to speeds by inverting, that sort of thing).\n\n\n---\n\nWhy don\'t books, notes, websites (etc) that talk about transformation and regression address these issues more carefully? I don\'t know. I think many people perhaps aren\'t aware of these issues.', 'Not sure internet strangers can recommend a transformation of your variables that would result in a good model fit without more information. Options are: better/more correct statistical model, data transformation, or maybe the data are what they are. Need more info.', 'Just here to say that just because residuals don’t look normal doesn’t mean you can’t use good ol’ linear regression. \n\nThanks for coming to my Ted Talk.', 'Thanks, this is exactly what I had done, but it the messed up the other part of the plot. I then tried transforming an independent variable that has a funneling residuals vs fitted plot, but anything I do, any increase/decrease, makes it worse than no transformation at all']",23,17,https://www.reddit.com/r/statistics/comments/1242kkk/q_how_to_straighten_qq_plot_for_regression/
608,2023-03-28 01:39:42,[Q] Necessary and Sufficient Condition for UMVUE,"A necessary and sufficient condition for an estimator T to be the UMVUE for theta is that T is uncorrelated with every unbiased estimator of zero. 

The prototypical example of using this that I have found online and in books use a uniform distribution. However, I am not really sure how to show this in general. Specifically, I am trying to use this condition to show that for a random sample X1, ... , Xn \~  exponential(theta), Xbar = n^(-1)(X1 + ... + Xn) is the UMVUE for theta. I am using this parameterization of an exponential distribution: 

f( x | theta ) = theta^(-1)exp(-x/theta)I(x > 0)

I know there are easier ways to show that Xbar is the UMVUE, but I want to get some practice using this condition on a distribution other than uniform.","['Okay I think I got it, in case anyone is interested:\n\nLet *T = (X1 + ... + Xn)*, a sufficient statistic for theta, and let *V(T)* be any unbiased estimator of 0. (Yes, this is a function of *T*)\n\n*T* is known to follow a *gamma(n, theta)* distribution, so we can use this to show that\n\n*E\\[V(T)\\] = 0 => E\\[T V(T)\\] = 0* by dividing out constants and differentiating both sides of *0 = E\\[V(T)\\]* w.r.t. *theta*.\n\n\\---\n\nNow, let *U(X)* be any unbiased estimator of 0.\n\nThen,\n\n*0 = E\\[U(X)\\] = E\\[ E\\[ U(X) | T(X) \\] \\]*                           *(by the law of iterated expectation)*\n\nBecause T is a sufficient statistic, *E\\[ U(X) | T(X) \\]* does not depend on *theta* and is unbiased for zero. Lets say that *V(T) = E\\[ U(X) | T(X) \\]*.\n\nSo we have\n\n*E\\[U(X)\\] = E\\[ E\\[ U(X) | T(X) \\] \\] = E\\[ V(T) \\] = 0*\n\nand by the argument above the  ---,\n\n*0 = E\\[ V(T) T \\] = E\\[ E\\[U|T\\] T \\] = E\\[ E\\[ U T | T \\] \\] = E\\[ U T \\]*.\n\nSo *T* is uncorrelated with every unbiased estimator *U(X)* of 0, and therefore \\*Xbar = n\\*\\*^(-1)\\**T* is the UMVUE for *theta*.', 'This post just reminded me why I hated stats grad school so much.']",5,2,https://www.reddit.com/r/statistics/comments/123ubu4/q_necessary_and_sufficient_condition_for_umvue/
609,2023-03-28 01:26:33,[Q] ANOVA Graph,"I have data from a one way ANOVA (3 conditions). I need to create an ANOVA graph to show the mean, variance and standard deviation. Which is the best graph to use? I was thinking box plot? But i am not sure.
And is there anyway i can create this graph in Jamovi? Tia!","[""P.S. In Jamovi, *One-way Anova > Descriptives Plot* will produce a plot of the mean with confidence intervals.  ...  There used to be a couple of good plotting modules, *jjstatsplot* and *flexplot*, but they don't work with newer versions (I guess).  If you find a good module for this, you might share it for others."", 'boxplots don\'t show mean and standard deviation; you can make a display that does of course.\n\nYou can\'t show variance on the same plot as mean or mean +/- standard deviation; it\'s not in the right units (and would be redundant with s.d. there anyway)\n\nWhat\'s ""best"" is more about what you want to achieve', ""I'm not sure about plotting the variance and the standard deviation on the same plot.\n\nBut ignoring that, how about something like this:\n\n[www.biologyforlife.com/uploads/2/2/3/9/22392738/c4ja00347k-f3-hi-res\\_orig.gif](https://www.biologyforlife.com/uploads/2/2/3/9/22392738/c4ja00347k-f3-hi-res_orig.gif)\n\nor this:\n\n[www.biologyforlife.com/uploads/2/2/3/9/22392738/mean-heart-rate-of-participants-with-standard-deviation-error-bars.png?465](https://www.biologyforlife.com/uploads/2/2/3/9/22392738/mean-heart-rate-of-participants-with-standard-deviation-error-bars.png?465)"", 'Thank you! I plotted a bar graph (similar to the first link that you have attached) using Excel. I used SE error bars to depict variance. And the graph has the mean as well. \nI’ve tried searching but can’t seem to tell if the SD is shown on the bar graph. Do you happen to know if the SD is shown somewhere on the graph as well? Sorry for the questions as i’m still new to statistics!', ""There are options for what the error bars show in Excel. It appears the options are sd, standard error, or percentage.  You used to be able to specify a specific number for the error bar limits, defined in a cell, but I don't see it anymore.""]",3,7,https://www.reddit.com/r/statistics/comments/123ty3u/q_anova_graph/
610,2023-03-28 00:22:04,"[Q] When sources about the Mahalanobis distance use the term ""multivariate"", do they actually mean ""multivariable""?","""Multivariate"" means observing more than one outcome (more than one dependent variable). ""Multivariable"" means more than one variable. I don't see how the Mahalanobis distance is related to multivariate statistics, but it's the term that is always used when I see sources describe the Mahalanobis distance. Am I missing something important here?","['Why are you posting this again? Why did you delete the [last post](https://old.reddit.com/r/statistics/comments/123b1se/q_is_the_mahalanobis_distance_only_used_in/)?\n\n> ""Multivariate"" means observing more than one outcome (more than one dependent variable). ""Multivariable"" means more than one variable. \n\nNot everywhere and not consistently. This terminology is reasonably common in the context of a regression model, but not elsewhere. Pick up any textbook on ""multivariate statistics"" and you\'ll find extensive discussion of unsupervised techniques that don\'t have *any* dependent variables.', ""The data that you are using in the distance is the outcome variable, hence multivariate. This is overly pedantic in my opinion and there isn't really a chance of confusion."", 'Have you heard of https://en.m.wikipedia.org/wiki/Multivariate_normal_distribution', 'I think you mean [https://en.m.wikipedia.org/wiki/Multivariable_normal_distribution](https://en.m.wikipedia.org/wiki/Multivariable_normal_distribution)']",0,6,https://www.reddit.com/r/statistics/comments/123s1q0/q_when_sources_about_the_mahalanobis_distance_use/
611,2023-03-27 20:17:37,[Q] How to find an average weekly score in a game I play,"I went to ChatGPT but I probably asked it in the wrong way.

There's a dungeon with 20 sequential rooms, each room has 5 doors. Only one door leads to the next room. Each door has an equal chance of being the right door. If you open a door and it's wrong, you can try another door without that one closing again. (You have a 1 in 5 chance. If you fail, there are only four doors left – 1/4 chance. So you're guaranteed on the 5th try). If you open the correct door, you automatically move to the next room. Once you get to the next room, you face 5 new doors.

Every time you open a door, you get points depending on the room you are in. Room 1 gives you 150 points for each door you open. Room 2 gives you 200 for each door you open. Room 3, 250 and so on and so on going up by 50 for each room. Once you're in Room 20, the doors close after you open them so you can keep farming points even if you make it to the final room.

During a week, you can open 28 doors before the dungeon resets. So what is the formula for figuring out how many points on average I get in a week?  


Edit: Just wanted to say thank you in case any of you read this!","[""This is quite a complex situation, so it's probably easier to stimulate the situation a few thousand times and observe the distribution of final scores."", ""It's kinda tedious. Get a free trial of [Treeage](https://www.treeage.com/). You should be able to build the model in there."", 'I’m bored at work so I’ll take a crack. It sounds like an expected value question so you can multiply all possible total points by their probabilities of happening then summing them all up, but I went a different route. How long to expect to stay in a room x the points associated with X number of doors per room. So you have a 1/5,1/4,1/3, etc chance (1/X) of exiting, 5 as the upper bound. You’ll “expect” to leave when you’ve got the sum (integral) of probabilities of leaving a room =1, so take the integral of 1/x, from 5 to an unknown lower bound, equal to 1.\nLn(5)-ln(unknown) =1, lower bound = 1.839, so 5-1.839 = 3.16, or how many trials you’ll expect it takes to leave a given room. 28 total trials so 28/3.16 =8.85 rooms, so the summation of rooms 1-8 = 150,200,etc x 3.16 + 85% of 3.16*room 9’s value.\n\nTl;dr and I’m not sure if my logic is correct: 9710 points to be expected weekly. Doing a monte carlo sim would be an easy nonmath heavy way to check\n\nAlso: say you make it from room 1->2 on your first roll, do you get 150 or 200 points', ""Makes sense. Thanks for the response. I don't feel as dumb."", ""Interesting! Thanks for sharing. I'm kinda relieved to hear it is tedious because now I don't feel as stupid.""]",4,19,https://www.reddit.com/r/statistics/comments/123lb7p/q_how_to_find_an_average_weekly_score_in_a_game_i/
612,2023-03-27 15:31:05,[Question] Linear Mixed Model with missing data? How can I check if its random?,"Hello! I'm doing a linear mixed model in SPSS and as far as I've understood it missing data in this model is not  really a huge issue as long as the missing data aren't in any way missing for a systematic reason. How can I check if the missing data is random or systematic? Also what should I do if the the missing data is not random? I'm a bit stuck so very grateful for *any* help, haha :)

If anyone knows how this is done in SPSS that would also be a massive help! Thanks :)","[""You can't check it - it either is or is not [missing, missing at random, or missing *completely* at random](https://datascience.stackexchange.com/questions/38138/what-is-the-difference-between-missing-at-random-and-missing-not-at-random-data). It's a condition that is up to you to determine.\n\nDepending on the situation, you can either ignore it, drop cases, or impute data."", ""In my experience, data is rarely missing at random. There's basically always a systematic reason that the data is missing; fewer resources/ability available to measure it, participants who don't want to participate for important reasons, too odd to adequately categorize, etc.\n\nBut then, I'm dealing with social data most of the time.\n\nOur models work far better when the data is missing at random. But that's not a good reason to assume the data is missing at random."", ""Endogeneity is the biggest problem in statistics.  My field is in economics so I can only tell you the popular method of dealing with endogeneity from econometrics.  \n\nIf you know or think that your data is biased, and you can't get any better data, you can use instrumental variable regression to correct for the bias.  Although getting a good instrument can be just as hard.\n\nIf your data is truncated and you suspect selection bias, you can use Heckman 2 stage regression.  Although you should still use an instrument for this as well.  Without an instrument you'd need to assume some functional form for the bias, which is usually a very strong assumption."", 'Thank you for your help! I really appreciate it :)']",13,4,https://www.reddit.com/r/statistics/comments/123f8st/question_linear_mixed_model_with_missing_data_how/
613,2023-03-27 12:33:44,[Q] How to calculate a simple ANCOVA by hand (or what is the step-by-step procedure)?,"I have the csv data below (Gender: Independent variable, Age: Covariate, Height: Dependent variable):

&#x200B;

Gender,Age,Height

M,20,150

M,22,160

M,24,170

F,23,150

F,25,170

F,27,180

&#x200B;

Inputting it in SPSS gives me an ANCOVA, no problem. But I have no idea how it's done. Can someone please give me the procedure? Thanks.","['It’s quite difficult to do ANCOVA by hand, but it’s just an ANOVA combined with linear regression; You have a categorical predictor and a continuous predictor. Instead of only looking at height across gender, you also include age to look how age influences this effect.', 'you might find a practical way  in some of the old books. From the time when a computer was a person who computes and you had a room full.', ""First - linear regression of age and height. Get the predicted value of height for each ages\nSecond - subtract predicted age from actual age to get corrected ages (this remove the marginal effect of age on height)\nFinally - anova with your categorical variable and the new corrected age variable (note that the actual value don't seem mean much now, but they still hold the partial effect of the categorical variable if there is any)""]",2,7,https://www.reddit.com/r/statistics/comments/123bgfv/q_how_to_calculate_a_simple_ancova_by_hand_or/
614,2023-03-27 10:41:42,[Q] Reference request for a quote about residuals,"Hi, this is a bit of a long shot but I am hoping someone here can help me find a quote about residuals. 

If I recall correctly, the quote is an anecdote about a prominent 20th century statistician (perhaps John Tukey). The statistician asks a student why a particular model is useful, the student replies that the model gives a good summary of the data. The statistician says that although that's true, the real benefit of this particular model is that it gives good residuals. Once you take out the simple structure captured by the model, the residuals show the interesting features that were lurking. 

Does this story ring a bell for anyone? I'd really appreciate knowing where I read it. Thanks for your time","[""The concept discussed at the end sounds at least very like things Tukey has indeed talked/written about (but also somewhat like things other people have said). However, the story itself doesn't particularly sound like Tukey to me; I imagine its details are possibly either apocryphal or modified-enough in the retelling as to be merely a rough gist of some interaction.\n\nAcross his talks, papers and books Tukey wrote about residuals *a lot*, so tracking down a specific quote might not be so simple.\n\n\n(multiple edits in what follows as I look around for a suitable reference)\n\n\nIt's rather like some of the notions expressed in books he wrote/edited like *EDA*, *Understanding Robust and Exploratory Data Analysis* or *Exploring Data, Tables, Trend and Shapes* for example, but also in several of his other books (whether lone or co-authoured), and in multiple papers.\n\nThe preface of EDA says something very like it a couple of times (e.g. *regard every description (always incomplete!) as something to be lifted off and looked under (mainly by using residuals)*) but I'm sure he's come closer to the notion than that example; he expresses versions of the notion repeatedly. \n\n---\n\nNot particularly closely related to the present discussion, but in looking for some suitable quote, I stumbled again across a display in *Fundamentals of Exploratory Analysis of Variance* that I had quite forgotten. A collection of city effects (essentially levels in a one-way type of model) is plotted *beside* a display of residuals on a common vertical axis, so you can see the relative size of the modelled effects compared to what is unmodelled. If I was doing one now, I'd change some of the details of the display, but the basic concept is informative, concise and simple. The chapter in question is by Schmid (who himself published a boxplot-like display to summarize a set of distributions shown side-by-side, back in the late 40s -- i.e. predating even Mary Spears' range plot), but the display that's used in *Fundamentals of Exploratory Analysis of Variance* is very like other displays that Tukey uses in multiple books; it looks a lot like some of his other displays. A similar display (but now for two-way type models, among others) shows up in chapters."", ""Thanks for looking. I'll take a closer look at Tukey's book. I may be misattributing the anecdote to Tukey. If you think it doesn't sound like him, it could definitely have been someone else (or made up). Thanks again""]",2,2,https://www.reddit.com/r/statistics/comments/1238p8k/q_reference_request_for_a_quote_about_residuals/
615,2023-03-27 09:58:44,[E] Thoughts on OSU online MS in Applied Statistics?,"[Link to Program](https://osuonline.okstate.edu/programs/graduate/applied-statistics-master-of-science.html)

I have been looking at different Master's programs for statistics delivered 100% online. My top two choices are:

1. TAMU MS Statistics
2. Penn State World Campus - MAS

TAMU would be my top choice because they seem to be a well-known program. Additionally, they offer a variety of electives, and they are quite cost-effective. That said, I applied once to this program last year and was rejected, so I am gunshy about my chances of getting in when I reapply in the coming months. But I digress.


I also came across the program offered at Oklahoma State University, and it doesn't appear to be appreciably different from other programs on the surface, but it is a much cheaper option than my other two choices. However, I haven't found much online regarding the quality of the program. If anyone has attended this program or knows someone who has, I would appreciate any insight and opinions on it. Additionally, if there are other programs you think I should consider, I am all ears.

TLDR: Title","['I’d also consider NCSU if it’s in your price range (especially if your OOS). Looks like I might end up there for a PhD and enjoyed getting to know some faculty a few weeks ago.', ""I had PSU, Ok State, and Colorado State as the three I was interested in. I ended up with CSU because I didn't like the learning style at PSU, it was more rigorous than OkSt, and I especially liked the flexibility with 8 week subterms instead of 16 week semesters.\n\nIf you're okay with the PSU learning style (very notes and reading heavy) I'd pick them over OkSt, they are more rigorous and better known. If you haven't looked at CSU, id highly recommend it too. I had a blast in the program"", 'Right — gotcha! I’m in-state so it works out well. Good luck with your’s!', 'I agree. Their reputation and curriculum are very intriguing, but at $1600 per credit hour for non-residents, they are outside of my price range. PSU is pretty much my upper limit at ~$1000 per credit hour. \n\nGood luck with your PhD! It seems like a great place to study.', ""Thanks for the insight! I liked the looks of CSU, but they only seem to accept applications once a year on April 15th for the fall. Unfortunately, my transcripts won't be ready by then as I am still finishing up Calc III. Should I be in the same position next year, I will happily apply.""]",3,6,https://www.reddit.com/r/statistics/comments/1237k46/e_thoughts_on_osu_online_ms_in_applied_statistics/
616,2023-03-27 09:15:59,[Question] Correlation Coefficient?,"Hello, I'm currently writing a research paper in which I need quantitativly find the correlation between two graphs (same x and y axis). Think GDP or some other economical concept graph which goes up and down a bit. Is a correlation coefficient the right thing to use in that case? My understanding is that pearson and spearman coefficients don't work with graphs that aren't some type of polynomial. How could I go about finding a quantitative relationship for economical type of graphs??","[""I am not sure that I fully understand your question..?\nGraphs are just visual representations of some data. Don't you have the data available that was used to make these graphs in the first place? Because that would allow to do a standard correlation calculation."", ""Can you use pearson for a graph that isn't linear??"", ""What do you mean by a graph? Don't you have the actual data available?"", 'I do yes', 'Then it should be straightforward to calculate a Pearson correlation of the two data series, as it is just a normalized covariance, which should be between -1 and 1.\nJust make sure that the data is time aligned (on the same x-axis).']",5,6,https://www.reddit.com/r/statistics/comments/1236gaf/question_correlation_coefficient/
617,2023-03-27 07:20:58,[Q] What are your favorites statistics/probability articles?,title.,"['[Parachute use to prevent death and major trauma when jumping from aircraft: randomized controlled trial](https://pubmed.ncbi.nlm.nih.gov/30545967/)\n\n(A funny critique of study design, but good to keep in mind.)', '[The Difference Between “Significant” and “Not Significant” is not\nItself Statistically Significant](http://www.stat.columbia.edu/~gelman/research/published/signif4.pdf) Gelman & Stern 2006', 'Leo Breiman - Statistical Modeling: The Two Cultures (with comments and answers by Leo)\n https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full', ""It is funny but it is, irritatingly, often miscited to bolster claims that we don't really need to do RCTs even in situations where there are no ethical or practical barriers to doing them. Because, you know, theory (and/or shareholder returns) are enough to just *know* and there's a (Christmas) BMJ article to prove it.\n\nIts arguments apply to those very rare (in the modern era) treatments which can, in principle, be tested in RCTs but are so obviously effective enough to outweigh any other concerns that it's just not ethical to randomise.\n\nThe list is short. Cisplatin for testicular cancer is the only one I can think of (penicillin would count had it been discovered after RCT methods were developed). When you see a change from 90% of your (typically very young) patients dead within a year to 90% surviving the year, you don't need an RCT to prove that the difference is large, even if we can't ever know how large.\n\nBut very few treatments are effective enough to be able to discount any other explanation for observational evidence. There are many, many people who would rather not have to produce solid evidence, for a myriad of reasons. But they're nearly always wrong."", '[Probability, Frequency, and Reasonable Expectation](http://jimbeck.caltech.edu/summerlectures/references/ProbabilityFrequencyReasonableExpectation.pdf), by R.T. Cox. \n\nThis is a brief exposition of Cox\'s ""logical Bayesian"" probability theory, which takes ordinary true/false logic as its starting point and generalizes to degrees of belief between 0 and 1 (or, it turns out, equivalently 1 and infinity). \n\nCox\'s stuff is the basis for E.T. Jaynes, ""Probability Theory: the Logic of Science,"" which I also recommend.']",88,19,https://www.reddit.com/r/statistics/comments/1233hhx/q_what_are_your_favorites_statisticsprobability/
618,2023-03-27 07:15:37,[Question] Improving random forest model result,"I have a dataset with ~700 rows and 8 predictor variables, 5 of which are multi-class, nominal categorical variables and 3 are continuous. I’m trying to predict a continuous output variable. 

Currently, I’ve tried a random forest model and have achieved an r squared value of ~0.4 on a test dataset using cross validation with a 80/20 train/test split. 

Looking for suggestions to improve the model fit or alternative models/data pre-processing techniques to try out here.",['Hard to say without more information.  This might be close to the best prediction that can be made from the information that exists in the data.  Or there may be some opportunities for feature engineering using insight into the problem and the data.'],2,1,https://www.reddit.com/r/statistics/comments/1233cji/question_improving_random_forest_model_result/
619,2023-03-27 04:43:22,[Question] Formula to get precision of estimates when running experiments,"I saw a YouTube Video about Monte Carlo simulations and how they can be used to derive Pi. In the video, they used the area of a square and 1/4 of a circle to derive the number of pi by randomly dropping dots in the squared field. 

Later, they state that one has to perform 500,000 experiments to get Pi up to 2 decimals. 3,000,000 repetitions to get Pi up to 3 decimals, and 500,000,000 repetitions to get pi up to 4 decimals. 

I do not understand how they calculated the required number of repetitions and its relationship to the precision of the estimate of pi. Does anyone know the formula or can point me toward a concept I am unfamiliar with (is it statistical error?). Thanks a lot!","['Depends on what specific calculation they did -- there\'s a few possible things (bounds/approximations) they might have done\n\nTheir numbers look high but my first thought  would have been that they\'d be relating it to the margin of error with known population proportion (p = pi/4) and some high coverage (roughly 95% say) ... see the links at the end for such calculations. Or they might have done some kind of worst-case bound.\n\nSo let\'s outline a simple margin of error calculation to try to get 2dp - accurate to say 0.01 or 0.005 depending on what they define as ""accurate to 2 dp"", but lets look at the first - you could look to get the margin of error (MoE) below 0.01.\n\nThe MoE under those specific choices is roughly 2.√[pi/4 x (1-pi/4)/n] ~= 0.82/√n  (any MoE will be for the form c/√n, so once you know one, the others will all use the same c).  To make that < 0.01,  you need n> 0.82 / 0.01^2 or roughly 6750ish\n\nThat\'s way less than their 500K, so they must have done something quite different. Even if they used less-informed calculations (not assuming they knew pi, for example), and stricter accuracy (say 0.005 rather than 0.01, and higher coverage probability) they won\'t get that high -- e.g. we could try the upper bound MoE (start with the worst-case p=1/2), 0.005 MoE and 99% coverage, but that will will still be way smaller than the 500K they got. \n\nFor those numbers we would solve √[(2.58 x 1/2 x 1/2)/n] < 0.005 for n ... i.e. n > (2.58 /4/(0.005)^2 ~ 25800\n\nThey have to be doing something else because their numbers are about 20 times as large as that.\n\nIt occurred to me that they might keep simulating it and just stop when some criterion is satisfied, but what exact criterion that could be is not obvious.\n\nNevertheless, the ratio of  accuracies should scale like 1/√n. e.g. to get an extra d.p. of accuracy they should multiply their n by 100. Their numbers are going up way faster than that, which suggests they\'re not using binomial proportions or simulation outcomes or indeed any facts about how variance rules relate to margin of error. \n\nMy calculations follow from the standard error of a  binomial proportion, which can be derived from basic facts about expected value and variance. (see the links at the end)\n\n{They may have used some [bound](https://www.cs.cmu.edu/~avrim/Randalgs11/lectures/lect0124.pdf) / asymptotic bound, or perhaps some other form of [bounds analysis](https://en.wikipedia.org//wiki/Probability_bounds_analysis). It\'s not 100% clear to me which one since the ratio of n\'s doesn\'t seem to correspond to any regular rule I\'d expect. Their number went up by a factor of *just 6* to go from 2dp to 3, and then by a factor of about 170 to go from 3 to 4??!? (170 I could believe, but 6? I can\'t see how that scaling makes sense). I must admit I\'m somewhat mystified by this. Did you copy all the numbers correctly?}\n\nhttps://en.wikipedia.org/wiki/Margin_of_error#Standard_deviation_and_standard_error\n\n\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval\n\nhttps://en.wikipedia.org/wiki/Binomial_distribution\n\nhttps://en.wikipedia.org/wiki/Variance#Properties\n\nhttps://en.wikipedia.org/wiki/Expected_value#Properties', 'Thank you for your response. I am a bit new to the topic. I guess what I don’t understand is how the error relates to the number of decimals. \n\nIs the correct way to think about it that a standard error below 0.1% would give me a pi up to 1 decimal?', 'Thank you for your detailed explanation! It became clearer to me! I double-checked and all numbers were correct. \n\nI suppose the difference is due to the probability of the position of the dots: In the video, the position of a point was determined using a uniform probability distribution, not a binomial distribution.']",3,3,https://www.reddit.com/r/statistics/comments/122yxeb/question_formula_to_get_precision_of_estimates/
620,2023-03-27 04:25:52,[Question] Can I use a Poisson Regression here?,"There is a group of 40 people participating in a Badminton Club. 

They play once a week (Player A vs Player B etc) for 60 minutes. 

I am trying to model the number of points Player X is going to get versus Player Y, for the upcoming week.

I have good reason to believe that the # Points can be modelled by a Poisson distribution, so I thought a Poisson Regression would be a good plan of attack.

My dataset contains every match for every player over the past couple of years (with # Points scored).

Now, I have a few questions here: 

1) Can I use Poisson Regression here? The observations clearly aren’t independent, as different observations will contain the same players.

2) How do I ensure that the most recent observation is far more “important” to the model than an observation 2 years ago? Do I need some sort of exponential smoothing etc?

3) Is there a way I can combine these single observations in to some sort of running average (over the last X game), and then use these averages as features? For example, I could look at average points over the last 10 game for this player, and use it as a feature in the regression? 

If I do this, can I essentially treat each 10 game cycle as a new data point? Or does that contradict the independence?","['> Can I use Poisson Regression here?\n\nThat sounds like a reasonable choice, given what you know already about the game. Another option is binomial (or, equivalently, negative binomial). \n\n> How do I ensure that the most recent observation is far more ""important"" to the model than an observation 2 years ago?\n\nYou might consider including a general ""time"" predictor variable, if you think all players\' points increase over time. If you expected the rates to change differently with each player, you can model that too. I\'m most familiar with hierarchical linear modeling (HLM), which definitely allows you to look at individuals\' growth over time. If you\'re using R, I recommend lme4. \n\nModeling growth using HLM is probably better, since it will auto-assign how much weight to give to your predictions based on when it occurs / players\' specific growth trajectories. However, you can also use whatever weighting scheme you think is best, a priori, and include them as sample weights in your models. \n\n> Is there a way I can combine these single observations in to some sort of running average?\n\nIf possible, I suggest inputting the data you have, and making modeling / design choices that will aggregate the data for you (e.g. as described above). That is, unless you have strong reason to believe that 10 is the right number to use here.', ""1. What is the purpose of this model  . . . What are you using it to do?\n\n  I dont see a suitable way to answer some of your questions well without an idea of what it's for \n\n   > I have good reason to believe that the # Points can be modelled by a Poisson distribution, so I thought a Poisson Regression would be a good plan of attack.\n\n2. Can you clarify what this is? I often see people convinced toward or away from models for mistaken reasons (such as looking at the marginal distribution). I'm not saying you're necessarily wrong - it might well be a reasonable approximation - but it would be good to be choosing it with suitable reason"", 'I am very (very) rusty with stats, but I don’t think this fits a Poisson distribution. Poisson distributions work when numbers are random without any trends, and when the denominator for possible occurrences is either too large or impossible to count.  Since the number of games in easily quantified and the score has obvious trends, it wouldn’t be an appropriate fit. \n\nIf you’re basing it strictly off of previous games played, I would probably look at some type of weighted moving average—games from the last year with games from the last 2 months given extra weight, for example. I seem to recall there’s some way to determine the best time frame and best way to weight the scores for recency to create the most accurate prediction, but unfortunately I don’t remember the particulars! Hopefully someone else here can help you with that.', 'Toward question #2, you might want to try searching “concept drift”. The term comes from data streaming literature. It might give you some ideas about how to regularize stale data.', 'Thanks for this, very useful.\n\nSo for each match/player, I could add a variable which denotes how long ago this certain match was played?\n\nThen my model would infer the importance of the recent observations here?']",10,8,https://www.reddit.com/r/statistics/comments/122yekn/question_can_i_use_a_poisson_regression_here/
621,2023-03-27 02:33:00,[E] What is the bare minimum number of stats classes you need to get a job as an analyst? Data scientist? Data engineer?,"Currently working on a bachelors in math with a minor in computer science. I'm heavily leaning towards getting a masters in math to help me get into a math PhD. By the time I'm done, I will probably have an undergrad probability class, and either one or two upper division undergrad stats class or a grad stats class or two.

My question is, how far will this combo of math major + possible masters in pure math + couple of stats classes + computer science minor get me? I really want to get a PhD in math, but there's a significant chance that won't work out, so I'm wondering what I can do if I don't go in that direction. Are there any data scientists out there with pure math masters? I expect not many, but I'm curious what people say.","['don’t do pure math bro. i was once a pure math person. never again', 'For data engineering roles, it probably doesn\'t matter at all. Recruiters will be a lot more interested in whether or not you have proper software engineering skills. \n\nFor the other two, I also don\'t think there is a ""bare minimum number of stats classes"". Sure, it\'s helpful, but it\'s a nice to have I\'d say.', '0.', 'More than stats classes, make sure you know Python, R and SQL. You can pick up stats/ML fairly quickly']",0,4,https://www.reddit.com/r/statistics/comments/122v40j/e_what_is_the_bare_minimum_number_of_stats/
622,2023-03-26 21:39:36,[Question] Minimum Detectable Effect,"Hello everyone,

I just came across the concept of MDE and I can't understand the rational behind certain things like:

1. If I have a baseline conversion rate of 20% and I decide that in order to change my current setting to the experiment setting I need at least 1% increase in conversion rate, then it leads to a 5% MDE. With a confidence of 90% and alpha of 5%, then I need a sample size of 33k.

However, if I decide that I want at least 2% increase in conversion rate, it leads to a 10% MDE which means that I only need a sample size of 8.5k.

Why is that when I want to see a bigger effect (higher conversion rate in this case) that I need a smaller sample size?

Additionally, let's assume that I do run with the 5% MDE and I see that the conversion rate wasn't 1%, but 0,7%. What would you do in this situation?

Thanks for helping!","['>Why is that when I want to see a bigger effect (higher conversion rate in this case) that I need a smaller sample size?\n\nBecause, quite simply, it is easier to detect a bigger effect than it is to detect a smaller effect. A small effect is harder to distinguish from random noise than a larger effect.', ""Think of it this way: it wouldn't take you that many flips of a coin with two heads and no tails to conclude with a reasonable degree of confidence that your coin is biased. But if your coin is only biased 51-49 in favor of heads, then it will take you many, many more coin flips to detect that bias. \n\nThat's an extreme example but the same principle applies. It takes a smaller sample to detect a larger effect. It's not that you *want* a smaller sample in order to detect a larger effect, it's telling you the minimum sample size that you would need to detect the larger effect is smaller than the one you would need to detect the smaller effect with the same degree of confidence."", ""If I understand the calculation you are performing, the effect size isn't the effect size you are looking for, it's the actual effect size in the larger population you are sampling.  It therefore makes sense that you need a smaller sample to see the effect, if that effect is actually large.""]",4,3,https://www.reddit.com/r/statistics/comments/122mds0/question_minimum_detectable_effect/
623,2023-03-26 20:39:16,"[Discussion] on probability paradoxes in the real world. I.e. Boy or Girl, Monty Hall","There is an average statistical family with two children.We know for sure that at least one of the children is a boy.What is the probability that both children are boys?Correct Answer: 1/3, i.e. 0.33

There is an average statistical family with two children.We know for sure that at least one of the children is a boy and was born on Wednesday.What is the probability that both children are boys?Correct answer: 13/27, i.e. 0.48 (7\*2-1)/(7\*4-1)

There is an average statistical family with two children.We know for sure that at least one of the children is a boy and was born on April 18th.What is the probability that both children are boys?Correct answer: 729/1459, i.e. 0.499657, practically 0.5 (365\*2-1)/(365\*4-1)

There is an average family with two children.We know for sure that the eldest of the children is a boy.What is the probability that both children are boys?Correct answer: 1/2

Note  that the more information or noise in these problems, the faster the  answer approaches 1/2. The answers to these problems range from 1/3 to  1/2. Note that 1/3 is obtained only when we are dealing with ""noiseless""  average values, and with each new piece of information, even a tiny  one, the probability immediately approaches 1/2.

Those  kind of probability problems are not only unintuitive because our  brains have evolved to work with natural numbers, but also because  probability theory works with noiseless mathematics, whit no real-world  noise. Our brains don't solve complex differential equations when  catching a ball or thinking about what to buy. Utility maximization and  optimization problem-solving only work in a sterile world where there is  no noise, all unknowns and their corresponding probabilities are given,  and the rules of the game do not change.

In  reality, where there is a lot of noise and the unknown, our brain uses  simple and fast heuristic methods (rules of thumb) to solve problems of  the unknown. One should not think that in the real world of uncertainty,  the one who does not optimize is not rational. The truth is the  opposite: in a world of uncertainty, success lies in simplicity.

One  of our brain's simple heuristics is 1/n, where under uncertainty, we  simply divide into n cases. Now imagine the following situation: I met  Mr. Smith in real life, he said that he had two children and began to  talk about his boy. What is the probability that both children are boys?  My answer, and the answer of a rational person who does not delve into  probabilities would be 1/2! Why? Because we live in a world of  uncertainty, and the more noise in this problem, the faster the answer  approaches half.

The same goes for  Monty Hall's paradox, which says: “Imagine that you have become a  participant in a game in which you have to choose one of three doors.  Behind one of the doors is a car, behind the other two doors are goats.  You choose one of the doors, for example, number 1, after that the host,  who knows where the car is and where the goats are, opens one of the  remaining doors, for example, number 3, behind which there is a goat.  After that, he asks you - would you like to change your choice and  choose door number 2? Will your chances of winning a car increase if you  accept the host's offer and change your choice?

From the noiseless point of view of probability theory, the answer to this problem is that yes, the probability does increase and become 2/3. And if I play with a computer or with a statistical design with the rules described above, then I will always change the door.

However, this problem is also counter-intuitive, and our inner voice tells us that there will be no change in the odds, and the probability will be the same 1/n (a simple heuristic), that is, the correct answer is 1/2. And our intuition is right. In this problem too, the more information or ""noise"", the faster the answer statistically approaches to 50%.

This task is similar to the previous one. And in the real world, playing with real people, I would not change the answer and say that the probability will be the same. Why? Because we are dealing with unpredictable rather than sterile probabilistic risk. In the real world, the host (Monty Hall) can play any trick on us, and we will think about hints, and not about probabilities, which is more correct.

So in the real world of uncertainty, trust your intuition, rather than trying to remember and solve complex problems of probability theory.

PS. Also, don't trust the charlatans at fairs, who in both problems, using probability theory in a sterile situation, can get probabilities of 1/3 and 2/3, respectively.

edit. problems with * in markdown","['Sir, this is a Wendy’s.', 'Okay.\n\nPlease define\n\n> average statistical family\n\nCan you expand on how you arrived at the probabilities for the following.\n\n1)\n> There is an average statistical family with two children.We know for sure that at least one of the children is a boy and was born on Wednesday.What is the probability that both children are boys? Correct answer: 13/27, i.e. 0.48 (7 x 2-1)/(7 * 4-1)\n\n2)\n\n> There is an average statistical family with two children. We know for sure that at least one of the children is a boy and was born on April 18th.What is the probability that both children are boys? Correct answer: 729/1459, i.e. 0.499657, practically 0.5 (365 **2-1)/(365 **4-1)', 'Those ""correct"" answers in the first three problems really depend on how you got the mentioned information. Let\'s analyze the first one:\n\n**There is an average statistical family with two children. We know for sure that at least one of the children is a boy. What is the probability that both children are boys? Correct Answer: 1/3, i.e. 0.33**\n\nThe reason why 1/3 is supposed to be the answer is because if we list the possible pairs of two children, we get these four:\n\n*A) Boy-Boy*\n\n*B) Boy-Girl*\n\n*C) Girl-Boy*\n\n*D) Girl-Girl*\n\nLet the leftmost one represent the eldest, and the rightmost one represent the oldest.\n\nSo, having said that at least one of them is a boy restricts our sample space to cases A, B and C, from which only in case A there are two boys, and since all of them should be equally likely, that gives us a probability of 1/3.\n\nNow, what some people don\'t realize is that this calculation is only correct if we are sure that when there is a boy and a girl we will be informed that one of them is a boy and never that one of them is a girl. Otherwise, the cases B and C would split in two halves according to which sex we are informed later, and therefore when we are told that one is a boy, we must discard the respective halves in which we would have been told that one of them is a girl.\n\nI mean, if from the start it is guaranteed that they will give you as a clue the sex of one of child, but you never know which one, the possible cases with their probabilities are:\n\n*A) Boy-Boy => 1/4. Here you will be told for sure that one of them is a boy.*\n\n*B) Boy-Girl => 1/4. But it splits in two halves.*\n\n*... B.1) They tell you later that one of them is a boy => 1/8.*\n\n*... B.2) They tell you later that one of them is a girl => 1/8.*\n\n*C) Girl-Boy => 1/4. But it splits in two halves.*\n\n*... C.1) They tell you later that one of them is a boy => 1/8.*\n\n*... C.2) They tell you later that one of them is a girl => 1/8.*\n\n*D) Girl-Girl => 1/4. Here you will be told for sure that one of them is a girl.*\n\nAs in this case you were told that one of them is a boy, that only leaves the cases A), B.1) and C.1) as possibilities. Both children would be boys if we were in case A), that originally had 1/4 chance, and one of them would be a girl if we were in case B.1) or in case C.1), that together also add up 1/4. So both being two boys or one being a boy and the other a girl had the same original probability 1/4, that must represent 1/2 at this point (with respect of the subset of the remaining cases after eliminating those that are no longer possible). So 1/2 is the correct answer this way.\n\nThe probability 1/3 would be correct if, for example, you had asked your informants something like: *""One of them is a boy?""*. That way they would have been forced to tell you that one is a boy even in the cases that there is a girl. But without a restriction like that, the answer must be 1/2.\n\nNow, strictly speaking, the statement ""at least one of them is a boy"" should be calculated as the conditional probability that gives us the 1/3 as result, but in practice I would bet that it is not the intended problem most of the time, because I don\'t think they are pretending that assumption of always being told the same sex ""boy"" whenever it is possible.\n\nThe same reasoning applies to your second problem:\n\n**There is an average statistical family with two children. We know for sure that at least one of the children is a boy and was born on Wednesday. What is the probability that both children are boys? Correct answer: 13/27, i.e. 0.48 (7\\*2-1)/(7\\*4-1)**\n\nThat supposed correct answer is only true if whenever there was a boy that was born on Wednesday, they would have told you that information and never the sex and the day of birth of the other child when those parameters are different. And this can be extrapolated to the third problem as well.\n\nNow, in the Monty Hall problem what occurs is that it is usually assumed as a rule, despite not always well stated, that the host will always reveal a losing door from the two that you did not pick and offer the switch regardless of what you initially picked. It is not his decision to sometimes offer the switch and sometimes not, otherwise is no longer the same intended game.\n\nYou say that in the real world the probability would be 1/2 because the host\'s actions would be unpredictable, but if his desire is to trick the player, he could also use the Monty Hall mechanism in his favor: Imagine that after you make your selection between the three doors you make a bet about if you managed to pick the prize or not. Then the host proceeds to reveal a losing door from the other two, not to offer any switch, but with the purpose that you increase your bet thinking that now it is more likely that you guessed right. But it is a trick, he could have revealed a losing door regardless of if you got it wrong or not, so your chances are still the same.\n\nIn conclusion, all problems that you put here share in common that they start with a list of possible cases, that are equally likely, but later, after we get information, not all of them remain in their whole, but only part of them prevails. That means that the remaining cases are no longer equally likely, which changes their original respective proportions. In the first three problems, that disparity is what ends supporting the intuitive answer 1/2.\n\nIt is worth noting that this has already been discussed previously. For example, The YouTube channel Zach Star has two videos about it, that are the links below. In the first he states the problem, but didn\'t understood it yet, and it is in the second that he explains the solution:\n\n[https://www.youtube.com/watch?v=bDZieLmya\\_I](https://www.youtube.com/watch?v=bDZieLmya_I)\n\n[https://www.youtube.com/watch?v=ElB350w8iJo&t=255s](https://www.youtube.com/watch?v=ElB350w8iJo&t=255s)', 'An important point for the Monty Hall problem is that the host will never open the door that reveals the car. If he could, then switching does nothing because then it means he picks random doors.', '**Boy or Girl paradox** \n \n [Information about the child](https://en.wikipedia.org/wiki/Boy_or_Girl_paradox#Information_about_the_child) \n \n >Suppose we were told not only that Mr. Smith has two children, and one of them is a boy, but also that the boy was born on a Tuesday: does this change the previous analyses? Again, the answer depends on how this information was presented – what kind of selection process produced this knowledge. Following the tradition of the problem, suppose that in the population of two-child families, the sex of the two children is independent of one another, equally likely boy or girl, and that the birth date of each child is independent of the other child. The chance of being born on any given day of the week is 1/7.\n \n^([ )[^(F.A.Q)](https://www.reddit.com/r/WikiSummarizer/wiki/index#wiki_f.a.q)^( | )[^(Opt Out)](https://reddit.com/message/compose?to=WikiSummarizerBot&message=OptOut&subject=OptOut)^( | )[^(Opt Out Of Subreddit)](https://np.reddit.com/r/statistics/about/banned)^( | )[^(GitHub)](https://github.com/Sujal-7/WikiSummarizerBot)^( ] Downvote to remove | v1.5)']",0,12,https://www.reddit.com/r/statistics/comments/122ku2g/discussion_on_probability_paradoxes_in_the_real/
624,2023-03-26 16:38:41,[Question] why logistic regression shows all variables are statistically NOT significant when dataset is unbalanced?,"When I balanced the dataset, I could see which variables are statistically significant. However, when I choose to work on the original dataset (where Y binary value is in ratio of 65:35~ish), all of the variables suddenly has P values > 0.995, which means I can’t have a logistic regression?","['You [shouldn\'t need](https://stats.stackexchange.com/questions/6067/does-an-unbalanced-sample-matter-when-doing-logistic-regression) to ""balance"" a logistic regression. Unbalanced data is more of an issue in non-parametric ML models (e.g., random forests, etc.) where the models typically won\'t have an intercept term to account/control for the overall marginal class probabilities.\n\n(Although small samples CAN certainly lead to issues with logistic regression - for example, it increases the likelihood that one or more combinations of your predictors will perfectly align with your class labels, leading to what\'s called ""[separation](https://academic.oup.com/aje/article/187/4/864/4084405)"", which leads to issues like non-existent maximum likelihood estimators, etc.)\n\nBut in any case, a ratio of 65:35 is frankly not that unbalanced, and I would suspect any attempt to ""rebalance"" is only doing more harm than good. \n\nAnd as another person said, having ""significant"" variables does not make a model ""good.""', 'You have 200 Datapoints for single independent variable male/ female equally split\nIn the dataset,\nMen have a 1/100 chance of label 1\nWomen have a 2/100 chance of label 1\n\nIt should be clear that this sex differences  is likely chance.\n\n\nNow you rebalance the dataset keeping 200 points\nMen now have a 33/100 chance\nAnd women have a 66/100 chance\nNow this sex difference no longer seems random', ""Who told you you can't have a logistic regression with insignificant variables? In any multiple regression scenario, individual p-values are close to worthless."", 'Could you expand on this topic more please?', 'Hello thanks for the elaborative answer! So in the event I need to perform logistic and random forest for a particular dataset, I’ve to use unbalanced and balanced data respectively instead?']",26,15,https://www.reddit.com/r/statistics/comments/122g31k/question_why_logistic_regression_shows_all/
625,2023-03-26 13:57:02,[Q] Literature review - how to filter out redundant search results from similar search iterations?,"Hey all, I've got sort of an unusual research question. Basically, I'd like to perform a comprehensive review of all the literature of a particular topic. To do this, I'd like to use combinations of search terms. For example, I'd conduct a search using terms ""A"" and ""B"", then I'd conduct another search using terms ""A"" and ""C"", then again using ""A"" and ""D"", etc. The problem with this is that there's a decent amount of overlap of search results among these different combinations and there are thousands of search results for each combination so I want to minimize redundancy as much as possible in order to save time. Is there a way for me to conduct an initial search (e.g., A + B) and then conduct each subsequent search (A + C, A + D, etc.) that will only show search results that are NOT included in the initial A + B search?

I'm using OVID Medline as the search database, but I'd be open to any general workaround solutions as well. From my limited knowledge on a possible solution, I was wondering if it's possible to export all the search results, copy them as a list into a column within Excel, and then use the Excel function that can highlight duplicate values. This method would allow me to avoid redundant search results from each search iteration. This isn't an elegant solution imo, but I imagined a possible solution like this. The most ideal solution would be for the database to filter out redundant search results for me automatically.

I can explain or clarify the problem further if that's helpful. Thank you for any help or suggestions with this problem!!","['Your query isn’t one of statistics.\n\nThis said, in Scopus and Web of Knowledge you can exclude items. Perhaps the same can be done in OVID.\n\nDo A + B. Then do A + C - B, which should retrieve all combinations of A and C that do not contain B. Perhaps then do C + B - A.\n\nI suspect all the various combinations will require post-selection filtering in some fashion.', 'Thanks for the suggestion and you are correct that this question isn\'t about statistics specifically. I wasn\'t sure which subreddits have a large enough community that might be able to help with this problem, so I posted it here.\n\nI like your suggestion and will do some tests to see how it goes. One thing I will mention is that ""B"" and ""C"" are subcategories of ""A"", so I don\'t think a C + B - A search will be necessary, but I think this is a great thought. Thanks again for your help!!']",1,2,https://www.reddit.com/r/statistics/comments/122cq6o/q_literature_review_how_to_filter_out_redundant/
626,2023-03-26 09:33:13,[Q] total possible tic tac toe combination,"I’m bored and don’t know how to solve, and google said it is 3^9 meaning that it is possible for a board to be full of Os

Things to note
1. X goes first meaning that it’s possible for a single X to be on a board, but not a single O
2.game doesn’t have to be complete, meaning that the board being completely empty counts and as possibility
3.games can’t go on after someone wins, so X can’t win twice unless one X completes 2 lines, and that both X and I can’t win

If there’s something I forgot I’ll add it","['Going through all options with a computer is probably easier than trying to do this with pen and paper.\n\nIf we treat rotations and reflections as different games then there are:\n\n* 1 option with no filled field\n* 9 options with 1 filled field\n* 9\\*8 options with 2 fields\n* 9\\*8\\*7/2 options with 3\n* 9\\*8\\*7\\*6/(2\\*2) options with 4\n* 9\\*8\\*7\\*6\\*5/(3\\*2\\*2) options with 5\n\nFor 6 fields you now have to subtract all cases where X already won. For 7-9 fields you also need to consider O winning.', 'On the first move, there are only 3 real choices for X. Middle, Corner, or Edge, since the 4 Corner and Edge positions are just rotations of each other..', ""I'm not a statspert but I believe it's 3^9 /2-1, as like you said there must be an even amount of x and o.""]",2,3,https://www.reddit.com/r/statistics/comments/1226wh2/q_total_possible_tic_tac_toe_combination/
627,2023-03-26 02:33:18,[Research] Research,"Can you guys suggest me some statistical tools to identify correlation between 10-50 variables? I'm only aware of ANOVA tests. 
Thank you!","['Do you want to compare them against *each other*? If you have 10 variables, you\'re looking at 45 comparisons, if 50 variables you have 1000+ comparisons. \n\nOr do you have one specific variable that you want to compare with the other 49?\n\nDifferent methods are available depending on what you\'re trying to do. Some could be seen as more ""sophisticated"" than others.', 'I have 50 variables and I want to compare them with each other.', ""What type of data are the 50 variables? (continuous, categorical, etc.)\n\nIf they are all continuous, you could do a correlation matrix to get an idea where the correlations are. If some continuous some categorical you might have to parse the different data types and use the appropriate method for each comparison:\n\n* continuous v. continuous\n* continuous v. categorical\n* categorical v. categorical\n\nAlternatively you could look into Auto ML approaches as they seem to be getting popular these days. Though I am not familiar with them, so wouldn't be much help."", ""It is for a project in which I will be assigning marks to my subjects (50 subjects) and check whether  correlation exists between them or not. \nAuto ML won't work for me because I don't have enough time to learn ML.""]",1,4,https://www.reddit.com/r/statistics/comments/121us1n/research_research/
628,2023-03-26 00:12:12,[Question] Basic knowledge with stationary process,"In a stationary process how do we read interpret the q-stat and prob columns on eviews with the correlogram and also, why don’t we seek in the normal law table to find the critical value ?",[],0,0,https://www.reddit.com/r/statistics/comments/121ql00/question_basic_knowledge_with_stationary_process/
629,2023-03-25 16:40:57,[E] Can I get into a top MS/PhD Statistics program with a pure math undergrad?,"Background: Graduated top of my program (University Medal) in pure maths from a top Australian University (GO8), and have been working in industry as a quantitative researcher for \~2-3 years now. I'm considering going back to do a a MS or PhD in Statistics; however, I'm concerned that I don't have the background to get into a competitive stats program (i.e. Harvard or Stanford) as I only ever took two stats related classes: a first year econometrics unit and a second year mathematical statistics classes (with rigorous proofs). Back in my undergrad, I did a bit of research in my and co-authored a few papers as at that point, I was considering in pursuing a PhD in pure maths in the US.

&#x200B;

EDIT: My Honours thesis was to do with metric spaces, and I pretty much took ALL math courses (advanced, reading classes, some grad level courses) during Uni as back then I was pretty set on pursuing a pure math PhD.

Hope the information that I provided wasn't too vague as I'd like to avoid doxxing myself too hard, but feel free to let me know if I need to clarify or add on anything.","['Short answer, yes. A strong math background is much more important than a stats background for those programs.', 'Yes, a lot of people have done so. In fact, they would prefer a strong math student to a strong statistics student.', ""PhD programs want people who can handle the math-intensive coursework. Many stats majors don't take enough the necessary math to do well in those courses. On the other hand, you'll be competitive with any major if you have good grades in the full calculus sequence, linear algebra, and real analysis."", 'They are looking for strong math skills; they can teach you statistics.\n\nAlso, many, many graduate programs in many fields will snap you up, if math skills are the limiting factor among potential students.', 'This is just my opinion, I am not an expert on this, but I am a Stats PhD student in the US so thought I would chime in. Applying and getting into an MS is completely different than applying and getting into a PhD with funding at the types of schools you described. I think given your gpa is 3.9 or higher, you could probably get into some good MS programs fairly easily. In terms of the PhD it’s going to be extremely difficult, even if you had a perfect resume and academic history. The fact is that the Top 3 stats programs in the US or even Top 5 each only take at most 10 people a year. You are competing with thousands of people, some of whom have worked at FAANG, published tons of Stats papers as researchers at these companies and have US based education, which will be viewed more favorable than an Australian education. It also doesn’t help that you will be put it the international application bucket, which means you are likely going to be compared directly to students from all over the globe. \n\nThis is also when you have to mention there is tons of luck and random chance involved in the PhD application cycle even ignoring everything else involved.\n\nPersonally, if I were you, I would try and get into a good MS program, do really really well in that and do a program that is thesis based, then apply to a PhD program if you still really want to do it. Applying with an MS and a good GPA will give you a much higher chance of getting into some excellent PhD programs. \n\nBut make no mistake, there is a chance you will never get into a Top 5 PhD program given how competitive they are and how small the departments are. There also just aren’t as many Stats programs as there as say history or psychology or biology, so every spot inside the T50 in the US is pretty damn competitive, talking under 10% acceptance rate at the highest and as low as 0.5% at the top end.\n\nIt might not be what you want to hear, but I think as someone who went through the process this is very realistic and honest advice.\n\nAgain just my opinion, but I hope this helps!\n\nGood luck!']",32,26,https://www.reddit.com/r/statistics/comments/121fphf/e_can_i_get_into_a_top_msphd_statistics_program/
630,2023-03-25 12:02:26,Is it worth still trying to pursue a PhD in stats? [Q],"I was essentially 0/13 on phd stats programs this cycle. I got into the one school I applied to which was a funded MS program in stats at a small school. While I should feel excited, I just don’t anymore. Because I have to do this intermediate step as a MS, I feel like I’ve been “held back” so to speak and can’t start my PhD at the same time everyone else is. Now a process which normally takes 5 years is gonna take me 7, because I have to do this MS, then reapply and then none of my courses transfer so I have to retake the classes. Not to mention, my school I go to is so small that there is no PhD program (R2 school) so I can’t transfer in. Thus, with this MS I really am doing everything again for the first two years.

It sucks cause a semester ago I was passionate about research and going down that path but now that it’s gonna take more time I’ve decided it’s probably best I try and get into industry after my MS and give up on the phd. 

Can anyone here speak to how they felt towards doing their PhD after completing an MS in stats at another school first? I’m trying to find good in this situation rather than bad. I figured maybe it’s a time to figure out if I want to do a phd still. Did an MS in stat first help you focus your research interests more? What was the benefit of doing an MS first.","['[deleted]', 'Yeah I’m thinking of getting the ms and then just working, I can’t even get into a top program for my PhD so a phd isn’t worth it for me at that point', 'Are you positive the classes won’t transfer anywhere? I have heard of some PhD programs requiring people to take a certain class again if their version is specialized or particularly rigorous, but not ignoring all classes completely. If nothing else, you may be able to test out. I would certainly double check on that aspect. \n\nI think a funded MS sounds great! You can build up your professional network and still have the option for professional R&D or a PhD (ETA: at programs that do not offer them; the faculty members I know all have PhDs themselves but are just starting out) at the end. \n\nI might make the decision based on some of the faculty. Not stats-specific, but I know a couple faculty members at some programs without PhDs that are doing really cool research with their students. If this is that kind of place or if there is a faculty member there publishing on topics you find compelling, this may be a great option.', ""I gona tell you my experience (I stopped at MSc) and some friends (they got their PhD). I started to work as soon as I ended my bachelor's and after a year I manage to get permission to do the MSc while working. I have now +10 years of experience and a well pay work. By the other hand, some of those friends that got the PhD have returned to the country and have been struggling to find a good job. Some of them have jobs which have a salary inferior to mine and I find it disturbing. Some others are doing pretty well, they are working as professors abroad. Some of them would like to return home but they just can't because it would imply getting an inferior payment.\n\nThat being said, money isn't all, If you truly want something with passion and conviction you should totally pursuit. If you are really good at whatever you do, you will succeed in the end.\n\nThose are my 2 cents, best of luck ."", '""Life is not a race, and comparison is the thief of joy"" \n\nI needed to hear those words thanks!']",6,10,https://www.reddit.com/r/statistics/comments/1219y0q/is_it_worth_still_trying_to_pursue_a_phd_in_stats/
631,2023-03-25 10:48:54,[Q] How advanced is this statistics question?,"I took an introductory statistics course at college. I just thought of this question and realized I don't know how to solve it:

""Take a random man and random woman from the population. Given the mean height and variance for both groups, what is the likelihood the man will be taller?""

Is it an intermediate or advanced question? In what class are we typically taught how to solve questions like this?","['This question is impossible to answer without making an additional assumption about the distribution of these heights. If we assume that both groups are normally distributed, then you can do it as follows:\n\nX is the height of a random man. It has mean m and variance v.\n\nY is the height of a random woman. It has mean M and variance V.\n\nX-Y is normally distributed with mean m-M and variance v+V. The man is taller if X-Y>0, so we need the probability that a normal distribution with mean m-M and variance v+V is greater than 0, which you can calculate in any statistical software.\n\nHow advanced is it? Uh . . . you need to know the distribution of the sum of two normal variables. This is not very advanced.\n\nNOTE: The assumption that both groups are normally distributed is probably quite wrong in this situation. Heights of humans are often approximately log-normal.', '>Is it an intermediate or advanced question?\n\nIt uses elementary concepts, but it\'s a question students will struggle with. The difficult part of this is to translate it into the more abstract ""sum of two normals"" problem people have been describing itt.\n\n You don\'t learn that by memorising (or even understanding) the rules of how means or variances add up', 'and just to add on to this to provide some more theoretical justification, suppose X = male height follows normal with those parameters and Y = female height follows normal with those parameters.\n\nWe are curious about the P(X > Y) or other words, P(X-Y>0). You can derive the joint distribution but also iirc, X-Y follows a normal as well (under independence) with mean (\\\\mu\\_X - \\\\mu\\_y) and the variance is the sum of the variance (recall some properties of variance) and if you plug it into R to find the probability that a women is taller than a man:\n\n`pnorm(0, mean = 177 - 163, sd = sqrt(7.1^2+6.6^2))`\n\nyou get 0.07433852 which is close to the simulations! I believe I did everything correctly but feel free to correct me.', 'It is a typical first-semester statistics question.\n\nYou\'ve probably been told to assume both groups are normally distributed. You\'ve probably been given a formula to compute the mean and variance of the sum or difference of two random variables (or more generally, the mean and variance of aX + bY where X and Y are two known distributions and a and b are two constants.)\n\nIt is a bit unusual if your book used the word ""likelihood"" rather than ""probability"" in the question.', 'This comes before the 2-sample z and t tests do: you build the test statistics for those tests using this formula, applying it to the sample means of the two subgroups you are comparing.\n\nPut another way, your question conducts a 2-sample z test with sample sizes 1 and 1.']",11,17,https://www.reddit.com/r/statistics/comments/12185gy/q_how_advanced_is_this_statistics_question/
632,2023-03-25 05:47:26,[Q] Can I produce Quantiles as predictions,"Hello everyone, I am working on a demand forecasting model. It predicts the demand in the upcoming month. How can I produce quantiles as forecast instead of a single estimate. I need qunatiles to account for the capital availability. If the available capital is low for example take the 50th percentile as the forecast. If it's high on the other hand take the 75th.","['Quantile regression', 'You can also look into „Conformal Prediction“.', 'What does this mean? It\'s difficult to imagine there could be a model agnostic way of producing claims like ""Conditional on information X, the probability that demand is q or lower next month is 0.75""', 'Exactly what am looking for, thanks.', 'Quantile regression trees (or forests) perhaps?']",1,7,https://www.reddit.com/r/statistics/comments/120zvv5/q_can_i_produce_quantiles_as_predictions/
633,2023-03-25 05:15:31,[Q] How could I perform a mediation analysis with mixed binary logistic and linear regressions?,"Hello r/statistics denizens!

I'm an undergrad med student in the UK, currently doing a thesis on the role of social cohesion as a mediator of the relationship between family affluence and mental health in adolescents. I'm using SPSS statistics version [28.0.1.1](https://28.0.1.1) for data handling and analysis.

The variables I'm using are:  


\-pfas\_iii (predictor)  = proportional rank on a family affluence scale (ridit transformation of a family affluence scale to between 0 & 1)  


\-mhc1 (outcome) = binary variable denoting whether or not a respondent experiences multiple psychosomatic health complaints in an average week  


\-soc\_cohesion (mediator) = composite sum scale of 3 questions asking respondents to rate from 0-4 how strongly they agree with statements related to the cohesiveness of their communities

I've validated my social cohesion scale with a cronbach's alpha, and the other variables have already been validated in previous published work on the same dataset.

I've carried out binary logistic regressions assessing pfas\_iii->mhc1 and soc\_cohesion->mhc1. I've also assessed the relationship between pfas\_iii and soc\_cohesion using a linear regression (validated by plotting standardised Pearson residuals against pfas\_iii, and assessing residual mean and stdev). All these are statistically significant at p<0.001

How do I incorporate these different regressions into a full mediation analysis? My supervisor (a senior statistician at my medical school) has promised to send me the necessary equations, but they haven't got back to me yet, and the deadline is fast approaching.

Any help would be much appreciated  


Edit: I have also run a binary logistic regression with mhc1 as the outcome and both soc\_cohesion and pfas\_iii as predictors, confirming that the beta coefficient for pfas\_iii changes by -25,4%.  


I have betas for all the regressions I have performed  


I've also adjusted for some confounders, but this probably isn't too relevant","['I always built my mediations with binary variables in an SEM (structural equation modeling) environment and not SPSS.', 'Have you googled ""SPSS mediation analysis""? The top result yields [this](https://uedufy.com/how-to-run-mediation-analysis-in-spss/). It seems like you have enough of a grasp of regression to interpret the results. Let us know what roadblocks you come up against. \n\nAs an aside, I urge you not to use phrases like ""I\'ve validated my social cohesion scale with a cronbach\'s alpha."" Validation is an ongoing process that is not a property of scales, but of the *use or interpretation* of scales. Cronbach\'s alpha is a measure of consistency within items, but on its own is far from sufficient for making validity claims.', 'Thank you!!']",8,4,https://www.reddit.com/r/statistics/comments/120yy1a/q_how_could_i_perform_a_mediation_analysis_with/
634,2023-03-25 04:53:17,[E] Which class for Stats PhD application?,"I want to go specifically into statistical ml research and am wondering if I had to take one, which would be stronger in admissions (or equal): Intro to ML or Intro to Analysis.","['Analysis', 'Analysis - they are looking for good math skills.  They can teach you the rest.', 'Analysis', 'Analysis ALL DAY. (Really, both.)', 'If you are into math, you will definitely enjoy ""Intro to Analysis"". \n\nBesides having analysis fundamentals under your belt can make your stats-ML journal reading experience much more smooth.']",1,5,https://www.reddit.com/r/statistics/comments/120ya9n/e_which_class_for_stats_phd_application/
635,2023-03-25 03:14:44,[Q] could you help me with a Reliability Analysis issue?,"Hi everyone! 
So I’ve edited a scale and I’m checking the reliability of the subs scales. I’m using JASP. 
As I put the questions in, the note came up that Q6,7,9,10 are negatively correlated so I knew I had to reverse them.
I then put those questions as reversed items but still under item-rest correlation I see negative correlation in Q9 and 10. 

So now I’m confused! Should those 2 items be reversed or not in the next steps of my analysis? I know it’s best to drop them as it shows it will improve my cronbach alpha but I was gonna put that as limitation. 

Please help and sorry for my poor english.
Thanks x",['[removed]'],1,1,https://www.reddit.com/r/statistics/comments/120uqht/q_could_you_help_me_with_a_reliability_analysis/
636,2023-03-25 02:05:24,"[Q] Hoping to go to Grad school for Biostats, should I major in Math or Stats?","I'm currently an undeclared second-year at University of Washington. My original goal was to apply to Statistics (w/ a concentration in DS) this coming Spring Quarter.

This past Winter Quarter, however, I took Adv. Linear Algebra and Intro. to proofs and I have to say, minus the stress, I thoroughly enjoyed both these classes. Beyond these two and Real Analysis I (which I'm taking Spring), there's very little overlap between the Stats and Math major degree requirements, so I need to make a decision soon.

Pros of majoring in Math are that it'll give me a strong foundation and ability to think rigorously for biostats grad school.

Con is that I will potentially suffer alot. I don't want it to consume my life because I have hobbies and obligations outside of school, and I'd have to be extremely careful not to fall behind.

For those who've pursued grad school, what path would you have taken?","[""&#x200B;\n\n>Con is that I will potentially suffer alot. I don't want it to consume my life because I have hobbies and obligations outside of school, and I'd have to be extremely careful not to fall behind.\n\nI don't think this is a con. If you want to do well in grad school, you're going to need to learn how to balance competing obligations, take care of your mental well being, and not fall behind. building a strong foundation now will reduce the burden down the line and avoid falling behind (which is how grad school ends up consuming people's lives)."", 'Major in math and minor in stats? Or double major? I imagine that the overlap between course requirements and electives is really significant between math and statistics. \n\nIf you have to pick, I\'d still say go for math, as it will be pretty easy to pick up statistics later if you\'ve got a really solid general math foundation. And who knows, it might expose  you to interesting approaches to data analysis that you wouldn\'t get from a ""classic"" statistics education. Things like topological data analysis are very hot now.', 'Also, I would caution you not to fall into the trap of viewing graduating on time as being ""on track"". If you need to drop a class and take it later on to avoid falling behind, so be it.', ""statistics isn't like economics or something where it's really dumbed down at the undergraduate level. besides the introductory classes (which are like that because they're a requirement for like every other major) undergraduate and graduate statistics classes aren't that different"", ""Major in stats, take math classes relevant to statistics (real analysis, probability, measure theory). If you try to minor in math, you'd likely be required to take courses with minimal applicability to biostats, like abstract algebra or complex analysis. If you want to take a more challenging course load, it would be better to take relevant grad classes than irrelevant undergrad ones.""]",25,32,https://www.reddit.com/r/statistics/comments/120sl2d/q_hoping_to_go_to_grad_school_for_biostats_should/
637,2023-03-24 22:32:34,[Q] Walsh Test,What do you think is the parametric counterpart of the Walsh Test?,"['""Counterpart"" in what sense?', ""Can you clarify which Walsh test you mean? I've seen at least three things get called that, that I can think of at the moment.\n\nCan you clarify what you mean by counterpart? \n\nAre you referring to a confidence interval based on the test? Or an estimator based on the test? Or something else?"", 'Can you clarify which test, exactly, you mean?  In nonparametric statistics, I know of two tests that might get called the Walsh test, but maybe it\'s not even either of those.\n\nDo you mean the book by Siegel and Castellan, *Nonparametric Statistics for the Behavioral Sciences*? If so, I haven\'t seen that book since the 80s (likely 1st edition, and I only read bits of it even then), so my memory of it is not perfect. A Google books search of the second edition turned up no mentions of ""Walsh test""\n\nI did find a mention of a Walsh test based on pair-differences in *Introduction to Nonparametric Statistics for the Biological Sciences Using R* by MacFarland & Yates, and that specific test corresponds to a paired t-test.', 'Thanks, that helps, but given I don\'t have that book, can you give the exact test statistic, and explain how they\'re computing the p-value?\n\n**edit**: managed to borrow a copy of second edition of S&C briefly. .... there\'s no ""Walsh test"" in it. Under the chapter on paired and one-sample tests, there\'s the Wilcoxon signed rank (which does have as its estimator for the location difference the Walsh average), and the permutation test (which *also* relates to Walsh averages). I am wondering if they start calling the second thing Walsh\'s test at some later edition.\n\n\n> since I have series of questions that are unanswered\n\nWe could probably help as long as exactly what you\'re looking at is made sufficiently clear', ""The test is called The Walsh Test in which it assumes the difference score between two related samples are drawn from symmetrical population. It uses atleast interval scale. And yes i am using Siegel and Castellan's. It's just so hard to find books that discusses Walsh Test 😅. I wanna learn more about it since I have series of questions that are unanswered 😅.""]",0,9,https://www.reddit.com/r/statistics/comments/120m4sw/q_walsh_test/
638,2023-03-24 17:43:46,[Question] Non-conformances for QC Charts based on Warning Limits and Control Limits?,"Hi all,

I work in a lab and we have a number of QC charts for the methods we use in the lab (e.g. pH 7 is one of our QC buffers).

The mean for our QC charts is calculated based on the previous 60 QC checks, with the UWL/LWL being the mean +- SD\*2, and our UCL/LCL being the mean +-SD\*2.

Recently, we have been getting a lot of non-conformances. We have to raise one if we get two consecutive data points above/below the WL, if there is one data point above/below the CL or if we have 10 consecutive data points above or below the mean.

Does anyone know if the above is standard practice for raising non-conformances with QC charts? I've just been curious because I feel like our QC charts is penalizing us for getting daily QC results that are close to the mean, as this in turn affects every new QC chart we make (after 60 days) by making the WL and CL more narrow, thus increasing the likelihood of us having a non-conformance. Like, if we're using pH7 as a QC chart, in an ideal world wouldn't it be better to get a result as close to 7, rather than the mean established by the QC chart?",[],3,0,https://www.reddit.com/r/statistics/comments/120fbw8/question_nonconformances_for_qc_charts_based_on/
639,2023-03-24 17:07:22,[Q] How to compare two distributions of crash data?,"I am trying to determine if the distribution of crash injury severity is different between two populations. A crash can be coded as one of 4 injury severity levels...fatal, major injury, minor injury, or no injury. Each population...in this case, I'm looking at different census tracts... Can include several hundred crashes.

I thought about just doing a bunch of pairwise tests of proportions... For example, is the proportion of fatal crashes significantly different, and repeat this for each level. But is this really the best approach?","['What you have is an ordered 2 x K table. If you just want to compare the distributions, you could use  chi-squared test, but this would ignore the ordering of accident severity. To take this into account, you might consider a trend test such as the Gamma trend test using concordant/discordant pairs. \n\nThere are probably others you could find online, but you definitely do not want to be doing multiple pairwise tests.', 'Ordinal regression might give you more power to reject than chi-squared and uses all levels of severity at the same time.  Chi-squared is more general, but could also work.  Ordinal regression is also called “proportional odds” and you’d be doing something like a t-test (but not a t-test) for ordinal data.  You get one number — an odds ratio — to quantify group differences so interpretation is easy.  Testing individual proportions could also work, but you might lose power to see differences and you’ll be making 4 times as many comparisons.', '""Different"" can mean various things. You can check each category separately, but what does it mean if one dataset has fewer minor injuries while the other one has more major injuries and no injuries instead? There is also the look-elsewhere effect to consider if you compare 4 groups (and them presumably focus on the one with the largest deviation in the discussion).\n\nYou could assign a score to each category (3, 2, 1, 0 or whatever) and see if the average score differs.', '[deleted]', 'Yes, either test would give you the first sentence above.  For ordinal regression your outcome would be severity: 1, 2, 3, or 4 and the independent variable would just be group.  As a bonus OR gives an overall odds ratio, e.g. “odds of worse severity is 2.3 times greater for group A vs. B”.']",6,4,https://www.reddit.com/r/statistics/comments/120enfi/q_how_to_compare_two_distributions_of_crash_data/
640,2023-03-24 06:11:55,[Q] Understanding variance calculations in scipy.optimise.curve_fit (Python),"Howdy,

I'm trying to use Scipy's `scipy.optimise.curve_fit` to calculate the parameters for a non-linear least squares fit of my data to a function. I have 2-3 response values for each value of my explanatory variable. Each of these response values is assumed to be errorless.

I need help understanding what I should be specifying for the `absolute_sigma` and `sigma` arguments and what effect these have on the output variance-covariance matrix, as I need the matrix for calculating confidence intervals for my fitted parameters and confidence bands for my function via the Delta method.

Any advice would be very much appreciated!",[],2,0,https://www.reddit.com/r/statistics/comments/11zzbzu/q_understanding_variance_calculations_in/
641,2023-03-24 04:45:05,[Q] Admitted to UChicago and UIUC for MS Statistics - Difficulty Comparing and Choosing,"Hello Statisticians,

I am going into graduate school with the intention of exploring the field as much as possible and then deciding on a PhD or industry job. I have a background in CS & have a couple of projects in ML/Statistical Modeling so I'm naturally inclined towards research position in industry, I'm also looking to explore financial statistics more. If possible, I would defn prefer to work for 1-2 years before proceeding to my Ph.D, if I choose to do it. I'm not very interested in data science job profiles.  


I'm not a fan of UChicago's course structure(9 courses in 9 months, makes me feel like it might be difficult to pick up anything else) but at the same time it has a stellar reputaion in the field. 

Most UIUC graduates look like they end up in data science. I would also have to take a loan for UChicago while UIUC has the appeal of graduating debt free because of RA positions.

Does anybody here have experience with graduates from either of the uniersities? 

I would love to hear about your thoughts and any advice is greatly appreciated!","['Yeah I was looking into UIUC MS too as I have a BS in Stats. The financial assistance for MS students looked attractive to me, do you know if its easy to get a TA a research position with a prof to qualify there? \n\nBut id go with UIUC for the potential to be financially free basically.', ""> while UIUC has the appeal of graduating debt free because of RA positions\n\nThen this is a no brainer, go to UIUC. Grad school is supposed to be free. If it's not free, you're getting scammed."", 'If ""research position in industry,"" especially CS-related, is in your sights, another plus for UIUC is being right next door to the National Center for Supercomputing Applications, the dozens of tech companies at the UIUC Research Park, and Wolfram Research.\n\nAlso would add that, from your second job onward, the name of the school you attended means relatively little in industry and at lower tier academic institutions. Absent a very specific goal of getting a PhD or an eventual professorship at a super-high-prestige school, I would never tell someone to pay extra just to go to a school with a ""stellar"" reputation rather than merely an excellent one.', 'A close colleague of mine(read fiancé) graduated from uiuc in stats. If you need a stress free life, go uiuc. If you want to end up in tech, go uiuc. If you want to graduate with minimal debt, go uiuc. If you want to end up in finance, choose uchicago or uiuc based on the type of finance you\'d like to end up in. I work in risk and can validate that the students are equally good in either place.\n\nIf you end up in finance; as a financial advisor, the first thing you will end up telling anyone is: ""go for a debt free life"" and I would suggest the same!', '2010 UC grad here. The 9 month program was intense. I was able to finish it in 4 quarters (i was able to finish 3 classes the first quarter but had to slow to 2 classes for the next three quarters). I don’t regret going. It was a big accomplishment for me. Class sizes were small (10-15 students). Teachers were good to great but the TAs where excellent. They also have a strong statistical consulting  group where you can some good experience. Like others said it wasn’t cheap. I was able to get some minimal financial support from the department but had to fund the majority via a grad plus loan.\n\nDid I learn the fundamentals - yes. Did it help me get a job quickly - yes (this if were UC’s reputation helped me but some employees care about school names and others don’t). Would I do it again - yes. But honestly here is the secret regardless which school you go to -  the majority of your stats knowledge will come through work experiences and self research lol. The school is there to get you through the door']",2,11,https://www.reddit.com/r/statistics/comments/11zwrt0/q_admitted_to_uchicago_and_uiuc_for_ms_statistics/
642,2023-03-24 03:43:36,[Q] Repeat measures ANOVA,"Hello,

As the title says I am struggling very much with performing a repeat measures ANOVA. The data is laid out as follows:

[https://imgur.com/a/sPy75rx](https://imgur.com/a/sPy75rx)

Tried it in JASP, can't figure out how to do the post-hoc tests. Then I used this website [https://www.socscistatistics.com/tests/anovarepeated/default.aspx](https://www.socscistatistics.com/tests/anovarepeated/default.aspx)

but had to do exercise systolic, exercise diastolic, control systolic and control diastolic all separately. None of the results were significant for p < .05 which is what I want but it is ok to present the 4 groups separately or is there a way to do it together? I know pretty much no stats but got stuck with data analysis in this group project because I went to the bathroom at the wrong time. Any advice helps, thanks.","[""> None of the results were significant for p < .05\n\nFYI, it doesn't change the conclusion here, but in general you should do some correction on p-values when you interpret many in order to account for the 5% random chance of seeing a rare event accumulating over multiple tests. A common way of doing it is through Bonferroni's correction, where each p-value is multiplied by the number of p-values you want to interpret. Then, you can interpret them.\n\n> ok to present the 4 groups separately or is there a way to do it together\n\nI believe it is fine to present them separately. Do the p-value correction before you interpret though.\n\n------\n\nIf you want to do something more fun in data analysis for your project, read below.\n\nIf you have not been told that you need to do ANOVA, then my advice is (if you have time!) to instead fit a Linear Mixed-Effects Model, and don't customize the covariance/correlation matrix, leave it as it is.\n\nI am not aware of many situations where it is not simpler to use than ANOVA. I have not used repeated measures ANOVAs since I learned about it, and neither has my professor (except to teach in class).\n\nLMMs work like most linear regressions but can take into account random variations per e.g. person or group (e.g. student_score ~ intercept + A * time + B * student_age + C * student_height + random_base_value_per_student).\n\nIn your case, I assume `SYS 1` is the SYS score at time 1, same for the rest, and you want to analyze how SYS and DIA are related in the control and test group.\\\nUsing R, you would need to organize your data into columns `ID`, `Group`, `time` (taking values 1 2 3), `SYS`, and `DIA`, then execute (with the right variables) e.g.\n```\nsummary(lme4::lmer(formula = SYS ~ time + group + DIA + (time | ID) + (time | group),\n                   data = your_data_frame))\n```\nor (same model, same results, but different package)\n```\nsummary(nlme::lme(fixed = SYS ~ time + group + DIA,\n                  random = ~ time | ID/group,\n                  data = your_data_frame)\n```\nFeel free to ask your teacher for help to do this and interpret the model. Fitting a model like this is much more versatile than repeated measures ANOVA and allows for more interpretations."", ""If you're doing repeated I take it your groups column isn't important? As you currently have it in JASP, just run repeated ANOVA and create two factors each with 3 levels (Sys 1/2/3, Dia 1/2/3). Place your items into that and run it. There's a posthoc option if you scroll down... Simply place your two factors into the right hand side box in that section to get those posthoc statistics. Note that if there isn't a significant effect of a factor, it doesn't need a posthoc test for it.""]",0,2,https://www.reddit.com/r/statistics/comments/11zuzsr/q_repeat_measures_anova/
643,2023-03-24 03:29:54,[E] Why We Divide by N-1 in the Sample Variance Formula,"Hi guys,

I have made a video [here](https://youtu.be/E3_408q1mjo) where I explain why and when we divide by n-1 instead of n in the sample variance.

I hope it may be of use to some of you out there. Feedback is more than welcomed! :)","['Dividing by (n-1) gives unbiased estimation of variance. That means that expected value of sample variance is actual variance. If dividing is done by (n), then you get Maximum Likelihood Estimation which is biased.', 'Two degrees in stats here, this is proved mathematically by solving that the expectation of an estimator has to equal the population estimator in order to be unbiased; E[hat(sigma)] = sigma\n\nWhen this equation is solved for sigma, you get that it divides by n-1 instead of n. To start this proof use standard expectation theory with the normal distribution: E[hat(sigma)] = integral from -infinity to infinity(sigma* phi(x) ), where phi(x) is the normal distribution function\n\nAlso fun way to think of it is if you have only 1 data point, there’s zero degrees of freedom in the variance, since it is always zero, so starting here you can also prove this by induction that degree of freedom = n-1', 'We divide by n-1 to correct for the degrees of freedom. So when we are estimating the variance we are estimating one parameter and estimating that parameter has the consequence of our degrees of freedom dropping by 1. To correct for this and get an unbiased estimate we take n-1.', '[Citing wikipedia](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics\\)): ""In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.""\n\n[The wikipedia page on Bessel\'s corretion](https://en.wikipedia.org/wiki/Bessel%27s_correction) has a pretty good explanation of how it comes into play here: The ""inputs"" to the sample variance is the collection of n numbers (x₁ - x̄, ..., xₙ - x̄), and these numbers sum to 0. So if you for example increase x₁, the mean x̄ will change accordingly, and the total sum still sums to 0.\n\nIf set y₁ = x₁ - x̄, ..., yₙ = xₙ - x̄, this means that y₁ + ... + yₙ = 0. We can rearrange this like so:\n\ny₁ = -y₂ - ... - yₙ\n\nThat is, y₁ is completely determined by the values of y₂ , ...,  yₙ. Thus there are only n - 1 ""free parameters"" in the formula for the sample variance. Even though we have n ""inputs"" (our samples), the fact that we subtract the sample mean has the effect of projecting the input onto an (n-1)-dimensional hyperplane, that is, there are only (n-1) ""degrees of freedom"" in the input parameters.', 'Ideally we would subtract each value from mu, the true population mean. But because that is unknown, we use the sample mean xbar. The problem is that xbar fits the data a little too well and on average (x_i - xbar) is just a little bit too small. The appropriate fix is to divide by n-1 instead of n.']",100,28,https://www.reddit.com/r/statistics/comments/11zumf5/e_why_we_divide_by_n1_in_the_sample_variance/
644,2023-03-24 01:13:57,[Question] Swedish Transgender Study,"I recently heard from a republican representative about a trans follow up study based on Sweden, I looked online for this study and cannot actually find the study just mentions of it and information about it. Does anyone know where I can find the original study?","[""Since this is a statistics forum, note that the study is comparing individuals with ***both*** gender dysphoria ***and*** gender change on the government records ***to*** matched individuals in the general population.  It doesn't isolate either of these potential effects.\n\nIt's less a critique of the study, and more of a critique what I imagine to be a likely misinterpretation of the study, but practically speaking, identifying individuals with both a diagnosis and a treatment doesn't lead to conclusions about the treatment. \n\nI don't have any problems with the conclusions of the study:\n\n*This study found substantially higher rates of overall mortality, death from cardiovascular disease and suicide, suicide attempts, and psychiatric hospitalisations in sex-reassigned transsexual individuals compared to a healthy control population. This highlights that post surgical transsexuals are a risk group that need long-term psychiatric and somatic follow-up. Even though surgery and hormonal therapy alleviates gender dysphoria, it is apparently not sufficient to remedy the high rates of morbidity and mortality found among transsexual persons. Improved care for the transsexual group after the sex reassignment should therefore be considered.*"", 'You mean the study that concluded *""Persons with transsexualism, after sex reassignment, have considerably higher risks for mortality, suicidal behaviour, and psychiatric morbidity than the general population....""*, as well as finding that *""...Female-to-males, but not male-to-females, had a higher risk for criminal convictions than their respective birth sex controls....""*?\n\nhttps://pubmed.ncbi.nlm.nih.gov/21364939/', 'Yes thank you']",0,3,https://www.reddit.com/r/statistics/comments/11zqtm2/question_swedish_transgender_study/
645,2023-03-23 23:41:27,[D] Hypothesis Testing For Time Series?," Suppose there is a company and we have their weekly sales over a period of 20 years (from 2000 to 2020).

Suppose in 2010, the company adopted a new policy (e.g. bought more powerful computers) and is interested in seeing **if their weekly sales improved after adopting this policy**.

Normally, I would have approached this problem using ""Standard Hypothesis Testing"" . That is, I would have taken the average of weekly sales before the policy and after the policy - and then used a hypothesis test like the T-Test or the Mann-Whitney Test to test whether these averages are statistically significant or not.

However, one thing that is making me reconsider if this approach is suitable or not, is that I am dealing with data which is not IID. As such, I am not sure if standard hypothesis tests are suitable for such a problem.

While trying to learn more about different approaches that can be used for this problem, I identified the following approaches:

* **Regression Discontinuity** : Regression Discontinuity ([https://en.wikipedia.org/wiki/Regression\_discontinuity\_design](https://en.wikipedia.org/wiki/Regression_discontinuity_design))) might be applicable to test whether the implementation of a policy statistically changed a time series by exploiting a natural cutoff point or threshold in the policy, and then estimating the difference in the outcome variable before and after the cutoff, controlling for other factors that may affect the outcome.
* **Difference-in-Difference** : Difference-in-Differences ([https://en.wikipedia.org/wiki/Difference\_in\_differences](https://en.wikipedia.org/wiki/Difference_in_differences)) might be applicable to test whether the implementation of a policy statistically changed a time series by comparing the changes in the outcome variable over time between a treatment group that received the policy intervention and a control group that did not, while controlling for other factors that may affect the outcome.
* **Change Point Analysis** : Change Point Analysis Detection ([https://lindeloev.github.io/mcp/articles/packages.html](https://lindeloev.github.io/mcp/articles/packages.html) , [https://www.marinedatascience.co/blog/2019/09/28/comparison-of-change-point-detection-methods/](https://www.marinedatascience.co/blog/2019/09/28/comparison-of-change-point-detection-methods/)):
* **Causal Impact Analysis** : Causal Impact ([https://cran.r-project.org/web/packages/CausalImpact/vignettes/CausalImpact.html](https://cran.r-project.org/web/packages/CausalImpact/vignettes/CausalImpact.html)) might be applicable to test whether the implementation of a policy statistically changed a time series by modeling the counterfactual outcome that would have occurred in the absence of the policy intervention, and then estimating the causal effect of the policy on the outcome variable based on the difference between the observed and counterfactual outcomes, while controlling for other factors that may influence the outcome.
* **Model based F-test/ Granger Causality**: Such methods might be applicable to test whether the implementation of a policy statistically changed a time series by comparing the coefficients of a time series model fit before the policy vs after
* **Bootstrap**: Randomly sample (with replacement) weeks before the policy and after the policy - take the average of these randomly resampled sales before/after and statistically compare using T-Test ... repeat this process many times and count the ratio of times they were statistically different vs not.

**My Question:** However, I am not sure which of these methods (or possibly other methods) are well suited for this problem when the data is non IID. Can someone please provide some comments on this?

Thanks!","[""I have used the CausalImpact package. As you wrote, you need to have data to generate the counterfactual. That's one thing I'm not sure you have in your case.\n\nI also used Granger causality as part of the model specification for vector autoregressive model (VAR). I'm not sure, but I think this makes more sense when you have one time series affecting another time series (e.g., advertising affecting sales, and sales affecting advertising over time), not something binary like before/after."", ""Get 'Forecasting : Principles and Practice'.  I would suggest fitting a model to the pre-intervention data, and then make forecasts through the post-intervention period.  Graph the forecasted and actual values.  The set up one of more dummy variables, and get estimates."", 'I think at least the first 3 might be the wrong set of methods.  I recommend the chapter in Kutner et al. ""Applied Linear Statistical Models"" (you can find a pdf online at a reputable .edu address with the right search) on Autocorrelation in Time Series Data.  There\'s a statistical test called Durbin-Watson for detecting the autocorrelation that violates i.i.d. assumption and a few methods for managing it.  First differencing, Cochrane-Orcutt, and one more that I don\'t remember.  There is also a way to account for autocorrelation by fitting an ARIMA model to the regression errors with maximum likelihood.  \n\nYou\'d perform this procedure (first differencing, etc.) on the data to remove the severe autocorrelation, then fit a model on the transformed data, and translate the coefficients on the transformed data back to the original according to the theory.  \n\nI only am familiar with these from a textbook perspective, so I would also be interested if a real expert in econometrics could weigh in on classical techniques like this (that don\'t go overboard with DAGs and IVs!).', 'There is virtually no difference in hypothesis testing for cross-sectional data and time series data other than that you have to model time in some way.\n\nIn regression, for example, this is literally including time as a variable.']",11,4,https://www.reddit.com/r/statistics/comments/11zo6o3/d_hypothesis_testing_for_time_series/
646,2023-03-23 22:10:11,[E] Does econometrics provide a sufficient background for biostatistics/ statistical genetics,"Hi I’m currently doing a double degree in science and economics in Australia. My science majors are genetics and microbiology. 


I’m interested in going into biostatistics in postgrad but I’m worried that my science courses haven’t really given me a good background in statistics. However my economics has given me a decent statistics background but it’s all directed towards econometrics. 


Can my knowledge be applied towards biostatistics or are they completely separate fields","[""Different fields use statistics to solve different problems, often very different problems, but the fundamental logic and underlying methods are the same. You shouldn't have any problems getting onto a postgraduate stats course with that degree."", 'No problems at all', 'I was in what sounds like a similar boat as you. Science grad, clinical research oriented with heavy focus on stats in medicine. \n\nI did a few stats courses in my undergrad (mixed biology, psych, econ) and become an intermediate in R and applied stats. \n\nI now do most of my own stats and do fine in my research areas.', 'I wouldnt worry- there should be a lot of overlap.', 'you’ll gain an understanding of statistical logic so you’d be pretty prepared and many concepts are similar, but econometrics focuses more on making sense of messier social science data that you learn special methods and interpretation for. you might focus on more lab/experiment methods in science. you might not have to worry about things like endogeneity issues and using funky causal inference methods to work around them as much']",27,8,https://www.reddit.com/r/statistics/comments/11zlkyl/e_does_econometrics_provide_a_sufficient/
647,2023-03-23 21:07:53,"[E] Going for a Biostats MSc, am I in over my head?"," I am a doctor in training looking to do a masters in biostats. I want to also take courses in ML, decision analysis, and epidemiology.

I am worried I don’t have strong enough math background for for the stats and especially ML sections. It’s still a 15 months away and I do have time to prepare

What I have: Biology major in undergrad. Calc 101 (not multivariable), stats up to 200 level (probability, regression, bayes), and very comfortable in python/R (including stats packages, OOP)

I am lacking: multivariable calc, linear algebra, ???

Programs I’m considering: UPenn and Harvard health data science, basically pure biostats streams (I know people there I can do thesis projects with)

Question: Am I in over my head trying to go for a biostats masters? Will I be overwhelmed? I am honestly very passionate for and committed to learning data science, and I want to learn it properly rather than superficially in an epidemiology course. I just don’t want to not be able to keep up with classes.

In the same vein, any online courses you’d recommend I take to prepare? I have 15 months

Thanks for any advice you can offer","['From what you\'ve said, you might be over your head. Go look at the entry requirements for the programs you\'re interested in, I think that most competitive biostats programs expect you to have already completed coursework in analysis and linear algebra.\n\nWith that said, there are a lot of programs called things like ""health informatics"" and similar that might be more up your alley. They\'re more data science focused and less hard-core statistics focused. Maybe even look into bioinformatics programs too', ""I'm confused, are you going to stay in the residency (is that what you are doing?) while studying?\n\nI think that, first, already a residency is 24/7 and you have to focus on your career.\n\nSecond,  in a comment,\n\n>I really am trying to learn biostats for an academic research career.\n\nIf you want an academic career in the medical field, you will be writing NIH grants and will use the grant money to pay for statisticians to work for you and also, collaborate with professors in other departments who can do the research design/analysis or that have PhD students who can run the analysis for you. You are not going to have time to do anything with the data.\n\nMy personal opinion is that you need a reality check as to how academic research works and you will be a PI. You will have people that do everything, you will spend time writing grants, you will need to find collaborators, and you will also have to work as a doctor in the medical hospital."", 'I’ll be honest, not really interested in bioinformatics. I really am trying to learn biostats for an academic research career. If I’m not ready for that now, I’ll just have to gain the math background to do it later at some point. This is the thing I’m interested in, and my primary job will be physician but I want to ground my research career with strong data science knowledge (of course I’ll never touch what a biostatistician can do, but at least a strong working knowledge)', 'just do the courses in linear algebra and multivariable calc , is totally worth it and if you can learn that you  will be totally fine in the rest of the program.', '(1) If the clinical fellowship is at a university, you probably have tuition remission (at least a %) as an employee and you can attend in person. You might be able to enroll as non-degree student as well and just take a couple of courses, instead of a whole degree. It\'s not necessary to collect degrees at top universities.\n\n(2) Even if your fellowship is ""relatively chill"", you are supposed to study medicine when you have your chill moments; not biostats which will not help you when you are with a patient. There\'s so much to learn, that you should be focusing on that. Would you rather be a mediocre doctor that has yet another degree at an Ivy that cost a fortune? Or a good solid doctor?\n\n(3) To communicate studies or results of clinical studies, or whatever you want to do, you do not need a lot of knowledge of biostatistics. You need to be a good communicator. Most of the time you won\'t be in a room full of biostatisticians, because for that you have the PhD student in stats or the postdoc do the presentation. You will be presenting to an audience that\'s full of different people with different backgrounds and for that, your presentation is not going to be 20 minutes of math.']",17,26,https://www.reddit.com/r/statistics/comments/11zjvn2/e_going_for_a_biostats_msc_am_i_in_over_my_head/
648,2023-03-23 16:11:37,[Q] Which ML algo. is best for sentiment analysis....SVM or Logistic regression ?,,"['To be honest, the input features are most important. The final classifier can be quite arbitrairy. (logreg over BERT features vs SVM over tfidf)', 'Extremely based reply. For any NLP project, creating a worthwile dataset is like 99.9% of the work.', ""Completely agreed. To add to that, I wouldn't suggest a priori limiting yourself to only those options (logreg vs SVM).""]",0,3,https://www.reddit.com/r/statistics/comments/11zdekb/q_which_ml_algo_is_best_for_sentiment_analysissvm/
649,2023-03-23 09:49:12,[Q] Transition from Pure Math to Statistics/Machine Learning grad school?,"So I'm a rising senior undergrad in Pure Math who also have taken quite a bit of statistics courses including time series, statistical inference, stochastic processes etc. (with really good grades) and did a lot of personal reading in machine learning but I only have done undergraduate research in pure math topics like number theory or stochastic processes. How would grad schools in statistics/machine learning view these types of candidates? Did anyone make a similar transition as well? I'm afraid I would be at a disadvantage when applying to statistics/machine learning grad schools as a Pure Math student only having done research in pure math before...

Also, I have interned quite a few times as a data scientist and a quantitative analyst before so I guess I have some exposure to working with data, modelling, and coding up machine learning models if that's any relevant to the discussion.","['A lot of grad schools will be happy with any pure-math major who gotten good grades and has taken probability/mathematical statistics. Extra classes like stochastic processes are a bonus. Doing quality undergrad research in any mathematical or scientific area is a bonus. Number theory probably gets more respect that some random applied lab experiment does.\n\nYou will likely find that the machine learning path and the classical-statistics path are about to diverge for you. The former may even live in the CS department or a separate data-science program from the statistics program. You can certainly learn about both, but you may have to choose one or the other when it comes to choosing a thesis topic / major advisor / department.', 'Pure math with good grades should have the door open in a lot of places.  \n\n> Also, I have interned quite a few times as a data scientist and a quantitative analyst before    \n\nThen you\'re not even coming ""just"" from pure maths, you also had exposure to the topics. You\'ll be fine.', 'Stats looks for strong math skills so in my experience that was not a problem. (Pure math BS with no stats courses) machine learning may look more for computer science background (I had a CS minor and a strong rec from a CS professor which I’m guessing helped me get into a joint stats and ML PhD)', ""I made a similar leap.  My undergrad degree was in general mathematics and physics, with a concentration in astronomy and astrophysics.  You actually have more experience with statistics than I did.  I feel like I've done pretty well.  It didn't feel like I was hindered by it.\n\nIn fact, some of the pure mathematics is pretty useful. Some people experience difficulties in calculus and linear algebra, but those are pretty common for a pure math student to know. So, that will be helpful in your mathematical statistics classes.  Also, your previous experience will help you in programming. I'm sure at some point, you have also done numerical analysis, so you have experience with programming and will find it useful too.\n\nI think they would look very favorably on your skills."", 'I think you will be welcomed with open arms, at least at theoretically inclined departments. You might also consider econometrics or theoretical economics. Many interesting applications of probability theory there.']",18,8,https://www.reddit.com/r/statistics/comments/11z4qxz/q_transition_from_pure_math_to_statisticsmachine/
650,2023-03-23 09:19:07,[Question] Don't know when to choose upper vs lower bound of a one-sided confidence interval,"Hey all, 
This may be a dumb question, but I'm in a biostats course right now and we just covered confidence intervals. The one thing that is tripping me up about it is, when performing a one-sided confidence interval, how do I know whether to use the upper or lower bound for that CI? My textbook just assumes we know how to make that distinction and my teacher hasn't explained making that choice very well.

Any help is appreciated.","['> or a trend in the other direction is negligible\n\n... or a trend in the other direction is literally irrelevant, equivalent to no difference for your purposes.', 'Look for clues in the context such as “at most,” “at least,” or the like.', 'And to the purposes of any of your possible imaginable readers, which you cannot foresee, so virtually never', ""It depends on the alternative hypothesis for the study. The textbook assumes you can make the distinction because, as the researcher, you're the one who has to make the distinction.\n\nIt's the researcher who formulates the specific objectives of the study."", '[deleted]']",1,6,https://www.reddit.com/r/statistics/comments/11z3yj7/question_dont_know_when_to_choose_upper_vs_lower/
651,2023-03-23 08:34:51,[Q] What analysis should I run for examining school differences in discipline event counts?,"Hi everyone - I really appreciate your help in solving this question. I'm kind of stuck and wanted other opinions.  Keep in mind I'm using SPSS.

My boss asked me to see if there are statistically significant differences across schools in our district in their disciplinary event counts (e.g., aggression, language, disrespect, etc).

She had suggested running t-tests or ANOVAs to answer these questions.  My concern with running an ANOVA is a violation of normality for using a dependent variable that's a count.  What analysis would you run in this scenario (categorical IV, continuous DV that's a count)?  I assumed Kruskal-Wallis would be more appropriate due to the violation of normality, but then there's another issue....

I also have concerns related to differences in group size because some schools are much smaller than others, so I've considered making a disciplinary event to enrolled student ratio to help solve this problem and then use this ratio as the DV instead of the disciplinary event count.  I still have normality concerns using this ratio as the DV, though, but it does solve the issue of group size across the schools.  I'm also unsure as to whether I can run an ANOVA when each group is made up of only 1 member (because the ratio I calculate would be representative of the entire school and would no longer be student level data).

I've also considered running chi-square between the schools (IV) and disciplinary event type (binary code - 0,1 for whether the event was aggressive, language, etc) rather than pursuing the previously mentioned paths.  I'm not sure if group size would be an issue if I pursue this path.

Sorry if it seems like I'm rambling at times, but I'm trying to put my thoughts out there.  I've been really debating about what path to take with this. Thanks again for any and all advice.  Open to ideas and suggestions.","[""Correct me if I'm wrong, but it sounds like you are more or less using census data here, and not trying to generalize to a broader population, right? If so, I don't think statistical tests are appropriate at all. \n\nThe techniques you are mentioning are for if you are trying to estimate something about the population from a sample. So if your district has 50 urban schools and 50 rural schools, and you want to use data from 5 of each to see if there is a difference between urban and rural schools in terms of disciplinary events per student, then your question would be reasonable. \n\nBut it sounds like you have complete data from all the schools of interest. In this case, you are not using a sample to estimate the frequency of disciplinary events in the population. You *know* the frequency of disciplinary events in the population. I would just report things descriptively, and I agree with using the frequency per kid as your outcome. You could make plots of frequency vs size of school (do larger schools have more disciplinary events per kid?), plots of frequency by disciplinary action type, report the average events per year per school, etc. But I can't think of appropriate statistical tests when you have complete data on your population of interest. Someone else might though."", ""Thanks so much for the response. It's great getting some other opinions.  I don't really have someone to bounce ideas off of so it's really helpful.""]",1,2,https://www.reddit.com/r/statistics/comments/11z2rkp/q_what_analysis_should_i_run_for_examining_school/
652,2023-03-23 04:54:55,[Q] How to determine a baseline to compare against future data,"I have data that has average runtimes of ***n*** process over software builds. I am trying to determine how to best determine a baseline given, let's say the runtimes of 20 software builds, and then compare this baseline to future builds' runtimes - but I'm not having the best of time figuring this part out.

My initial idea was to compare the latest build average versus the average of runtimes of ***x*** builds to date (essentially average of the averages), and then see if it is within ***y***% (y = 5 is what I was thinking). If it is, then use the latest builds' average in the calculation of the 'average of averages' number for future builds. The issue with this method is that the 'average of averages' could just keep climbing if the latest build is within 0% to +5% and would hence not be great long term and can hide performance issues.

Any ideas would be greatly appreciated!","['What if instead of using the average runtimes of each build, you determined the actual distribution of runtimes for each build?\nThen you can test (possibly with ANOVA or KS) if the distributions are different (specifically greater or less than).', '[deleted]', ""Data comes from normal distribution. \n\nCorrect me if I'm wrong - you're suggesting that I perform an ANOVA on the latest (lets say x) build and compare x ANOVA to x-1 ANOVA and see if there is a significant difference? \n\nThis is possible (if I've understood you correct, of course), but is there a way to determine a baseline given previous build's data? Or is there a way in ANOVA that allows me to do that that I'm not seeing haha"", ""In my case, a build is a version of existing software with fixes, new features, etc. \n\nI want to reject new builds when comparing the runtimes of n processes from this new build to the performance benchmark from past build information, and hence where I thought of the 'average of averages' idea if they are slower than this benchmark."", 'Whoops, I lost sight of your question. I was instructing how to tell if each successive build was quicker or not.']",11,4,https://www.reddit.com/r/statistics/comments/11ywbst/q_how_to_determine_a_baseline_to_compare_against/
653,2023-03-23 02:05:20,[Q] Calculating probability of investment strategy.,"I am an amateur with no background in statistics and wondered simply how to calculate the probability that an investment strategy ""works""

For something simple like a 200-day Simple Moving Average on the SPY... would this be correct?

1. Pick a value (e.g. CAGR, Sharpe, etc.)
2. Create a randomized data set (e.g. 1,000 simulations on randomly sampled SPY data)
3. See how many simulations have a CAGR (or sharpe, etc.) higher than our strategy.
4. Get the p-value by dividing the count by # of samples.

Is that it? Seems too simple, so assuming I'm missing something significant.","[""Not my field, but I think number 2 is much easier said than done. I have no idea how'd you generate a 'null distribution' of SPY time series from simulations. It's a time series, so you can't just randomly sample SPY data and squash it together. Similarly, if you take 1000 'slices' of the time series, certainly there'd be significant overlap between samples (so they're not independent) which have events that would bias your results.\n\nI think the best you can do is just back test your method and see if it works well."", 'Thank you for the reply.\n\nThe basis for the ""randomized"" data is from two articles I read where they used that method to simulate ""randomized"" data for the S&P500:\n\n[https://www.datacamp.com/tutorial/stocks-significance-testing-p-hacking](https://www.datacamp.com/tutorial/stocks-significance-testing-p-hacking)\n\n[https://www.winton.com/research/seasonal-volatility-and-the-multiplicity-effect](https://www.winton.com/research/seasonal-volatility-and-the-multiplicity-effect)\n\n\\--\n\nThe premise is that historically October has been an especially high volatility month and December low. The conclusion is that, after correcting for multiplicity bias, the volatility effect is not statistically valid.\n\nI had trouble following the argument though, as the first test did seem to show statistical significance, but then they only took the highest volatility values from each backtest to show that it was in fact not statistically valid.\n\nIntuitively the results seem wrong though. December being lower volatility makes logical sense. There tends to be a positive market effect in December, arguably from increased revenue/sales and people\'s generally optimistic mood around the holiday season, which would also show up in lower volatility.\n\nOctober is tougher from a common sense standpoint though. It may be some effect from Q4 beginning and an increase in activity to reach year-end goals, or it may just be a historical anomaly from effects like Black Friday.\n\nI don\'t have enough of a background to make sense of the argument, but was rather interested in validating other models.\n\nYour approach is what most people do though I believe (test something, see if it works, stop using it if it no longer works).\n\n&#x200B;\n\nThank you for the reply.']",2,2,https://www.reddit.com/r/statistics/comments/11yr7sx/q_calculating_probability_of_investment_strategy/
654,2023-03-23 01:34:00,[Q] Which would be the better choice? Majoring in Mathematics and Computer Science or Statistical Science and Data Analytics?,"these are not four majors, they are just two majors available at my school. I am struggling to decide which will be the better option in terms of employment/ being financially stable. Also I don’t plan on going to graduate school so which degree do you guys think will hold more weight?","[""What kind of employment would you rather have? It's probably easier to find a well-paying job with a computer science degree, but if you want to work in the intersection of those fields it's easier for a statistician to teach themselves basic coding than it is for a software engineer to teach themselves statistics."", 'Which do you enjoy more?\nYou’ve got half a lifetime of work ahead of you- better enjoy it!\nPlus, you’ll likely produce better results and show more initiative (resulting in more money) if you enjoy your work. \nOut of the gate, CS will likely pay more.', 'Statistical science and data analytics will be more applied and will set you up for a work. Math and CS can also be applied from a software engineering perspective, which will always be useful.']",0,3,https://www.reddit.com/r/statistics/comments/11yq9j0/q_which_would_be_the_better_choice_majoring_in/
655,2023-03-22 23:55:52,[Q] How do you think AI will impact the role of staticians in the near future?,"I ask as an undergraduate in statistics, close to finish it

How do you think AI will impact our role as staticians? Do you think it will lead to new statistical methods that will improve our work? Will AI make it easier? Will it take our jobs (i hope not lol)?","[""It could automate some simple tasks (e.g., entry level data analyst positions), but I don't see it having much of an impact otherwise. Current AI methods are good at tasks which require reasoning by analogy/superficial similarity, but struggle with deductive reasoning and more sophisticated tasks. This is an inherent limitation of the neural net approach, so it's probably not going to disappear soon. Most of statistics falls in the latter category, so it's unlikely to be affected in the near future."", ""More companies will need to have human statisticians to come in and actually explain the statistics they've tried to automate because AI is suddenly a buzzword that can attract irrational and stupid investors."", 'A demonstration of poorly programmed AI I guess XD', ""I'm not a professional statistician, but my work involves a lot of stats and ML. I've tried different LLMs and so far non can even begin to help me with my job. I tried getting them to write STAN code, but debugging the result is way harder than if I had just written it myself. If LLMs eventually can translate models in math language into super-optimized STAN (or GRETA or whatever), I don't see them as being much use.\n\nEdit: Moreover, they'd need to be easier and better than things like brms, which is already *very* close to something like this."", 'I think for code monkeys its kind of a danger but theres more than just being a code monkey and also knowing how to ask it the right questions and checking/debugging the output (it can be wrong totally).']",27,41,https://www.reddit.com/r/statistics/comments/11ynaey/q_how_do_you_think_ai_will_impact_the_role_of/
656,2023-03-22 23:48:30,[E] Recommandations for learning Probability/Statistics,"Hi everyone,

I am in the process of trying to get job interview for quantitative finance. Problem is, I severely lack background in probability and statistics. What is the best resource to learn them? I am looking for books/lecture notes, but I really don't like slides/videos. I don't think I need extensive proofs, but examples would be very nice, as well as exercises.

My background is a PhD in mathematical physics, so you can be as mathsy as you want.

Cheers!","[""With a background in mathematical physics, you likely will be able to jump right into graduate level material; that is, I'm assuming you have a decent background in linear algebra, calculus, and real analysis.\n\nIf you really don't have solid basics, you may want to breeze through an upper undergrad level treatment of the fundamentals, e.g. something like Hoel, Port, and Stone's Introduction to Probability Theory and Wackerly's Mathematical Statistics with Applications. The math will be simple for you but you'll build a familiarity with the basics.\n\nAt a slightly higher level, Casella and Berger's Statistical Inference is a fairly standard intro graduate non-measure theoretic treatment of statistics that would build on those other texts more.\n\nBeyond that, you get into more serious texts like Durrett's Probability: Theory and Examples, which has a measure theoretic treatment of probability and builds to important concepts that come up in financial math like martingales and stochastic processes. Another alternative would be Çinlar's Probability and Stochastics, which is another treatment of similar material. A deeper dive into measure theory that still builds to probability applications can be found in Billingsley.\n\nThere are a number of texts to consider if you wanted to explore more traditional statistics, such as laying the foundations of hypothesis testing, constructing confidence regions, etc. Bickel and Doksum is all right and is reasonably rigorous to build up solid foundations on very classical statistical methods. van der Vaart's Asymptotic Statistics does a decent job developing asymptotic theories that underpin many common practices. However, these sorts of classical statistical approaches are perhaps less common in the world of finance.\n\nThe ultimate goal for quant. finance would be to develop a solid background in stochastic calculus/processes. J. Michael Steele's Stochastic Calculus and Financial Applications has a focused approach to these topics, which would probably be a solid primer.\n\nObviously, all the above would require a good deal of time and effort to master. A more streamlined approach would be: build basics of prob/stat -> introduce more rigorous treatments of probability necessary to understand the foundations of stochastic processes -> stochastic calculus and other financial math topics."", 'Sure, but the only relevant things I did there were ensembles and partition functions', 'You could try Elements of Statistical Learning.', ""> My background is a PhD in mathematical physics, so you can be as mathsy as you want.\n\nThe main mathematical tools for general statistics will be calculus and linear algebra (not enough that you won't already have those well in hand) but for quantitative finance you'll definitely need some some work on stochastic processes. If you don't have any measure theory, you'll want to pick it up for that -- and if you get into the statistical theory, again you'll need it for the probability it's based on. You may also want some exposure to time series.\n\nProbability wise I'd start with something simple to get going; the Harvard 110 course is decent (NB no measure theory in this). Go to https://projects.iq.harvard.edu/stat110/home -- the book is downloadable there; there's exercises and such to be found. You don't *have* to watch the lectures, the book is pretty easy to read. You will want something more technical later, especially when it comes to stochastic processes, which tends to come up a lot in finance work\n\nPerhaps Wasserman's *All of Statistics* (the title's a bit overblown) might be a good overview book on some of the statistical ideas? If there's too much of a gap to get to that, you might want to add a more basic mathematical statistics book in between the probability course and this book, but you might be okay.\n\nYou'll probably want some practical regression text. I don't have a specific suggestion for that right now. \n\nIt's possible you may want some exposure to something like *Elements of Statistical Learning* (downloadable for free from the website of the lead author), though you probably only need parts of the book to begin. Some of the earlier ideas would be particularly useful in any case.\n\nI don't have a good suggestion on stochastic processes, nor on stuff like ARCH/GARCH models.\n\nHopefully someone with more finance background that me has some good ideas."", 'Out of curiosity, did you ever take a course on statistical mechanics?']",3,5,https://www.reddit.com/r/statistics/comments/11yn2p5/e_recommandations_for_learning/
657,2023-03-22 23:42:03,"[Q] Multi-input, multi-output regression modeling","Let's say I run a recycling business. Each day, a truck goes out to a variable number of local businesses and picks up aluminum cans and brings them in to our processing facility. In our recycling facility, we melt down each truck load of cans and then output a variable number of batches of aluminum sheets. 

Each batch of aluminum sheets we produce must pass a purity threshold before we can sell it. We would like to use knowledge of the inputs to predict the probability that a batch of aluminum sheets will be pure enough to sell (90% pure or more).

Some complicating details:

* We have knowledge of each supplier's ""purity"" as well as some other characteristics
* We know how much of each supplier's cans go into each truckload, and how much of each truckload goes into each batch
* Most days, we do not utilize a whole truckload, and so we store it and use it later. Thus a produced batch may contain a mix of different truckloads (we know the mix)
* The number of suppliers and number of batches produced each day is variable, between 1 and 30

In other words, the dataset looks like this: [structure](https://imgur.com/a/KjpvLwz)


If it were single input single output, we could use standard regression. If it was single input multi-output, we could perhaps use standard regression with some multi-level structure. But this is multi-input, multi-output and I would like to utilize as much input information as possible. Weighted averages of inputs works OK, but assumes linearity and I would like to see if I could utilize more of the input information for prediction.

It seems like this is a situation where latent variable modeling would work: Combine all of the inputs into a single set of features and use those to predict the output purity. The only example I can find is [this one on stackexchange](https://stats.stackexchange.com/questions/429655/regression-model-with-aggregated-targets), but I feel like I'm completely missing something as this doesn't seem like a novel problem. I would like to use an existing library if at all possible, as it will be more difficult to productionize and/or handoff some bespoke R/py/stan code to someone else.

Does anyone have references, sources, or suggestions?  Thanks!","['If you know the purity, amount and source of input aluminum and also control the mix for batch processing, can’t you directly calculate the output purity? Why do you need statistics at all?', '>I think the issue with this is that the number of ""suppliers"" in my actual problem is very large (30k+) and not repeating often, so we might have 30k columns with all entries 0 except for a few which isn\'t practical IMO.\n\nI guess the question is what you\'d like to use to predict purity. Are there characteristics of suppliers that you could use instead of individual suppliers? E.g. if some of the suppliers are supermarkets sending you returnables, while others come from home pickup services,\\* would you have this information, and would it be useful if you constructed a model that predicted purity based on supermarket v. home pickup? \n\n&#x200B;\n\n\\*I\'m making things up here - I clearly know absolutely nothing about your business.', 'This looks like multi-input, single-output to me. The outcome variable is \'purity\', right?\n\nTry the following data structure: one line per batch, and for each batch, you have the following variables: S1 . . . S11, which are the indicator variables for each supplier. Example to illustrate ""indicator"" variables: If suppliers 3, 7, and 8 contributed to the batch on this row, then S3 = 1, S7 = 1, and S8 = 1, while all other S# variables = 0.\n\nOnce you put this together, run a regression model where purity is the outcome and S1...S11 are the predictors. The coefficient of S# is the estimated difference in purity for batches that include S# compared to batches that do not contain S#. In other words, if S3 = 0.1 and S4 = -0.2, then we would expect that adding cans from both S3 and S4 would have a net effect of 0.1-0.2 = -0.1 on the purity.', 'I think the big question here is ""can I treat the outputted batches as independent?"" \n\nIf you can, just use a standard multivariable regression to predict each batch\'s purity given the X inputs (if you want to predict its over 90% purity or whatever, convert your dependent variable to be binary and use a logistic regression).\n\nIf you can\'t, look into multivariate (note that this word is different than multivariable) regression which allows you to regress on multiple dependent variables (your outputs).\n\nI think how I\'d start with this is see how good your model is assuming batches are independent. If it\'s good enough, great. If it\'s not, compare it to the multivariate model (this model should theoretically be better because it includes more information, but it will be messier to work with and interpret).', 'I would love it if that were true. Unfortunately:\n\n* Both the input and output purity measurements have some noise\n* Parts of the processing can degrade the purity or improve the purity, in ways that are noisy. This is sort of what we are trying to quantify.\n* We do not control the mix of batches, it is arbitrary and we only know after the fact. Although a model like this could help change that process.']",9,11,https://www.reddit.com/r/statistics/comments/11ymvv0/q_multiinput_multioutput_regression_modeling/
658,2023-03-22 21:19:44,[E] Online Master's with TA (or something similar) available?," Sorry if this is not the right place to ask for this advice. Please direct me elsewhere if so!

I'm looking to get my Master's in Statistics (or something Statistics-related) and would ideally like to do so online. I'd also like to minimize the cost as much as possible by being a Teacher Assistant or Research Assistant or something along those lines. With my initial search, I've found plenty of online programs without funding and plenty of in-person programs with funding. However, I haven't quite found anything that ticks both boxes. Any advice?

If it's relevant: I have a Bachelor's in Mathematics and Secondary Education, and have 4 years of high school teaching experience (including 2 years online). Would really love to be able to further my learning in Statistics, a true passion of mine, while also doing some form of teaching.

Thanks in advance!","['Penn state has a masters online of applied stats. Not sure the cost. Stats adjacent check out Georgia tech online masters in comp sci specializing in ML, it’s only $10k', 'I’m not sure I would reach out to the program coordinators.', 'Thanks for the info! Do you happen to know if either of them offer funding opportunities? I suppose if I just find a relatively cheap program it’s not as much of an issue, but I kinda like the idea of being a TA or RA to help pay for it.', 'Thanks, I will!']",3,4,https://www.reddit.com/r/statistics/comments/11yixdd/e_online_masters_with_ta_or_something_similar/
659,2023-03-22 20:35:57,[Q] Help with p-values!,"Hello all. I am currently writing up my dissertation and have used SPSS to analyse my data by carrying out a one-sample T Test based on gender groups.

My research is orginal research which aims to explore thoughts and feelings toward AI from a student radiographers perspective. This research topic has never been investigated, so it's hard to have any hypothesis to work from! 

So a long story short, I have some statistically significant results (mostly for how females perceive AI) and wondered two things:

1). As original research, do I need a null hypothesis when using p-values? 

2). If yes, which hypothesis would you work from, bearing in mind there are some concurrent statistically significant results for males too. 

Just worried that right now my statistically significant results have zero context to someone reading my paper!","['> As original research, do I need a null hypothesis when using p-values? \n\nYou can\'t even begin to compute p-values without one. How do you calculate the distribution of the test statistic under the null if you don\'t know what null to compute it under? For that matter, how do you choose a test statistic if you don\'t know your hypotheses?\n\n> This research topic has never been investigated,\n\nAttitudes to AI more generally has certainly been studied, and I expect attitudes to some form of *technological change* among radiographers has probably been looked at before, and likely differences between attitudes of students and working radiographers, if that is needed to bridge a gap to some such studies. \n\nFurther, it\'s often the case that a question of interest can be formulated *even with no prior relevant studies*. I don\'t know that this is the case here. Why do you care about feelings toward AI from a student radiographers perspective? What value would knowing about that have? What sort of differences in attitude would be important to know (between what groups would be important to the goals of the study?)\n\nThe problem is you\'re considering such questions at entirely the wrong end of the process.\n\n> so it\'s hard to have any hypothesis to work from! \n\nIf you were truly not in a position to have any hypothesis, *you should not be testing at all*. You would run an exploratory study designed to help formulate one.\n\n> which hypothesis would you work from,\n\nI would not be in the position of even collecting data without knowing what the purpose was in a very much more precise sense than you have articulated, and what my analysis was going to be to answer that question (or those questions, if there was more than one central issue). How can you design a study properly if you don\'t know what the precise research question is? \n\n> my statistically significant results\n\nIf you have p-values, you had a null hypothesis that the test was based on. The question is, *why would anyone care about that hypothesis*? It appears you didn\'t even know what you were testing, so the question is was testing was clearly not that important to you.\n\nFirst question to address is ""What am I trying to achieve/find out?""\n\nNext question is ""What\'s a good approach to doing that?"" (that\'s something we might be able to help with)\n\nStart with Q1.\n\nDon\'t just throw data at hypothesis tests if you haven\'t thought about what you\'re testing or why. If you\'re looking at p-values of tests you don\'t actually need, you\'ll greatly increase the chances of false discovery.', ""Nothing is really 100% original, we build upon the existing literature. \n\nI won't say you need a null hypothesis. But if you don't have hypotheses, you should have a good reason for that. \n\nIf you are testing differences between females and males, why do you believe there is, or there isn't a difference, based on the existing literature? That will help you to justify your hypotheses. \n\nFor example, is it because females are more risk-averse than males, and, therefore more careful about innovations like AI? In this example, there is a lot of prior research about gender differences in risk aversion. \n\nBut that's just one possible explanation. Exploring possible explanations is usually part of the job. Identifying what explanation is really true can be a huge part of the job. \n\nBy the way, you should have the hypotheses before running the tests and knowing the p-value. Otherwise, you are HARKing (HARK = Hypothesizing After the Results are Known)."", ""I think simply reporting means and SDs on something that's crucial to your dissertation isn't nearly rigorous enough, but this is ultimately a conversation you should be having with your advisor."", 'I\'ve had to use literature which explores the thoughts and feelings of post-registration radiographers in practice as there is no existing literature focused solely on students. This has allowed for comparisons to be made and some interesting parallels to be drawn.\n\nI believe that the male/female divide is down to cognitive theory which forms preconceptions, i.e., AI is technology, technology is seen as \'uncool\' for girls and a guys domain, so those form barriers. Males spend more time with technology as a whole, so are more confident (which reflects in my results).\n\nI\'m limited to 1,700 words for my discussion but have uncovered so much I could easily do 3,000!\n\nWould having a hypothesis which says ""understanding and perception of AI is gender-identity dependent"" be too generic or non-specific?\n\nFirst time doing this and it shows 😂😅', '> ""Female understanding and perception toward AI is less than males"",\n\n> by carrying out a one-sample T Test based on gender groups.\n\nIf you administered the same measure to both men and women, and you want to determine if the mean values of your measure are different between those two groups, then a one-sample t-test is not appropriate. That would be at least an independent samples t-test, but I would hope you\'d be considering a more robust test that can control for potential confounding factors (like prior experience with computers in general and AI in particular).']",0,13,https://www.reddit.com/r/statistics/comments/11yhs5i/q_help_with_pvalues/
660,2023-03-22 15:50:48,[Q] In logistic regression when we calculate probabilities from log(odds) why do we consider maximizing they probability/ likelihood of respective classes...,"Let's say we have to predict whether something will belong to calss 1 or class 2....why while maximizing the likelihood we maximize the probability/likelihood of each data point belonging to class 1 and class 2 and not maximizing the probability/likelihood of every data point belonging to either one of the class...

LEST SAY WE HAVE 4 DATA PONTS.... a,b,c,d.

a and b belong to class 1 while c and d belong to class 2.

Why do we maximize this....

(Here pr means probability )

pr(a belonging to class 1) * pr ( b belonging to class 1) * pr ( c belonging to class 2) * pr( d belonging to class 2) 

and not this.....

pr(a belonging to class 1) * pr ( b belonging to class 1) * pr ( c belonging to class 1) * pr( d belonging to class 1) 

Because probabilities of each data points with respect to a single class makes more sense to me.

Please Help 😅🙏","['>pr(a belonging to class 1) * pr ( b belonging to class 1) * pr ( c belonging to class 1) * pr( d belonging to class 1) \n\nIf you maximise this, then you may as well ignore all the variables and just set p(in class 1) = 1 and p(Y in class2) = 0']",2,1,https://www.reddit.com/r/statistics/comments/11yb9xn/q_in_logistic_regression_when_we_calculate/
661,2023-03-22 12:41:13,[Q]Unbiased Estimator and Recall and Accuracy,"I have a doubt about something of the unbiased estimators  


I know I can have an unbiased estimator and it can still have a high variance, but I am not so sure how this would like to practice, and I remember that there is a difference betweeen being accurate and the level of recall you get, are those ideas related? Also, I do not know how values being calculated with a unbiased estimator but with high variance would look? Does anyone have any example of this? Because I know the definitions are not related but i cannot see how something unbiased can high dispersion levels",['See the image [here](https://www.researchgate.net/figure/The-dart-example-for-a-high-bias-and-low-variance-b-low-bias-and-high-variance-c_fig1_333505702).'],1,1,https://www.reddit.com/r/statistics/comments/11y76ez/qunbiased_estimator_and_recall_and_accuracy/
662,2023-03-22 11:43:55,"[Q] Is economics a good major if I plan on working in Statistics or related fields? (risk analysis/management, financial analyst, insurance)",I’m a high school senior planning on doing a transfer program at my state university (since it’s really my only option) and I just figured out it only allows admission into the liberal arts college. So I was wondering if Economics is a good major for me to choose if I plan on working in the field of statistics.,"['It can be.  You just need to take as much econometrics as possible.  It would also be good to take some linear algebra, SQL, and python programming, if at all possible.\n\nDepending on the program, an undergrad econ degree can be a lot more theory and a lot less math.', ""Economics is a math-heavy discipline, but many undergrad programs aren't that rigorous. Just to give a sense of the lower bound, you can get a bachelor's degree in econ from my *alma mater* without taking a single semester of calculus."", 'I mean technically economic theory is (or can quickly get) pretty math heavy', 'Yes, it is a very good major for that purpose. I work as a data scientist at an AI department of a multinational company, and the vast majority of my colleagues have Economics background (and an MSc in Statistics or Econometrics afterward).', 'If you can take econometrics modules yes.']",40,29,https://www.reddit.com/r/statistics/comments/11y5sc4/q_is_economics_a_good_major_if_i_plan_on_working/
663,2023-03-22 11:01:55,"[R] Given that there are 676 computer animated films and the average movie runtime is 130.9 minutes, there is approximately 61.45 days worth of computer animated films. That's only 2 months.","Sources:

[https://en.wikipedia.org/wiki/List\_of\_computer-animated\_films](https://en.wikipedia.org/wiki/List_of_computer-animated_films)

[https://www.statista.com/statistics/1292523/lenght-top-movies-us/#:\~:text=In%202021%2C%20the%20average%20length,one%20hour%20and%2051%20minutes](https://www.statista.com/statistics/1292523/lenght-top-movies-us/#:~:text=In%202021%2C%20the%20average%20length,one%20hour%20and%2051%20minutes)).","[""Guideline 4 (at least in old.reddit):\n\n> Just because it has a statistic in it doesn't make it statistics."", 'It’s more of a curiosity than an application of statistics.', "">Not even shower thoughts \n\nSo we're lower than shower thoughts?"", 'Statistics is probably not for you then.', ""What? I didn't say that.""]",0,9,https://www.reddit.com/r/statistics/comments/11y4q4w/r_given_that_there_are_676_computer_animated/
664,2023-03-22 10:28:59,[Q] Would this be considered a nested design?,"
Here is the scenario:

""An animal scientist conducted an experiment to study the effect of water quality on feedlot performance of steer calves. Four water quality treatments were used for the experiment. The water sources were designated as normal (N) and saline (S). The saline water was formulated to approximate the mineral concentrations in some underground water sources utilized in practice for watering livestock. Four combinations of water used in two consecutive 56-day periods of the experiment were N-N, N-S, S-N, and S-S. The feeding trial consisted of the four water treatments with two replicate pens of animals for each treatment in a completely randomized design. The trial was conducted on two separate occasions (two consecutive summers). ""

For the first part of my assignment question, I conducted a 2 factor analysis of variance with the water quality treatments as fixed effects and the summers as random effects. However, the last part of the question is pasted below, and I feel lost as it doesn't really seem different than what I did in the first part?

e. The water treatments have a 2 × 2 factorial arrangement. The first factor (*A*) is normal or saline water in the first 56-day period, and the second factor (*B*) is normal or saline water in the second 56-day period. Write the linear model for the experiment with this arrangement, considering summers as random effects and factors *A* and *B* as fixed effects. Repeat parts (a) through (d) of this exercise with the new model.",['Read rule 1.'],1,1,https://www.reddit.com/r/statistics/comments/11y3v4l/q_would_this_be_considered_a_nested_design/
665,2023-03-22 09:35:03,[Q] Predicting future price from sales data," I want to be able to predict what a product's price would be based on its current sales listings and its sales history.

The historical sales data would be a list of (date, price, quantity)

The current listings data would be a list of (price, quantity)

A very simple idea would be just to calculate the average quantity sold per day from the sales data(say, for the past week) and assume the same amount will be sold tomorrow. Then, we can remove that quantity from the listings data sorted by price ascending, and predict the new price of this project to be the new lowest listing price.

Obviously this does not account for stuff such as price elasticity, demand trends, etc.

Just wondering if anybody had any ideas/leads on how I can figure out better ways to solve this problem","['I’d recommend looking into the stochastic optimization Newsvendor problem. You can find materials for it on gurobi’s website as well, and while it’s different in nature than what you’re proposing, I think it’s quite relevant. Just a note, I hope you aren’t trying to make any assumptions or prove anything as this type of question is highly speculative and not practical without some hard optimization.', 'Cool thanks! I will take a look - and yeah I am not looking to prove anything']",1,2,https://www.reddit.com/r/statistics/comments/11y2f1x/q_predicting_future_price_from_sales_data/
666,2023-03-22 08:54:01,[E] Online Resources Like stats110 or cs229,"I'm feeling disappointed about the quality of the corresponding courses offered in my school for stats110 and cs229. I regret not having known about these earlier, and I'm looking for some amazing online resources to learn more.

Specifically, I'm interested in finding resources that cover the following topics:

1. Linear regression
2. Statistical inference
3. Stochastic processes
4. Optimization
5. Data science technical skills

If anyone has any recommendations for online courses, books, or any other resources that cover these topics in depth, I would really appreciate it.",[],12,0,https://www.reddit.com/r/statistics/comments/11y1amb/e_online_resources_like_stats110_or_cs229/
667,2023-03-22 08:46:38,[S] Stata help?,I have to learn time-series data analysis on Stata in one (and maybe a half) month. I have the software installed in my laptop today. Now zero idea what to do next. Where do I start? Any suggestion would be very welcome.,"[""Do you know any stats programming? UCLA stats has some great step by step programming guides for Stata, though I don't know if they have time series specifically.\n\nIf you can figure out the function names the pdf help files for stata are pretty helpful - the seem too technical at first but there's some decent theory and examples at the bottom"", 'There are walkthrough videos of every type of analysis on every software there is on YouTube. Your best bet is just go there and follow it.', ""Udemy has a few helpful courses on stata, sometimes it's accessible for free through your institution"", "" STATA is a powerful statistical software program frequently used in research, policy analysis, and business. It is also regarded as a key tool for statistics students due to its vast utility. As a result, STATA is used in most reputable institutions' statistics courses, and businesses depend on it to tackle challenging statistical problems. Nearly all STATA-related topics are covered by **STATA help** specialists, guaranteeing that students always get the best assistance possible with their assignments.""]",3,4,https://www.reddit.com/r/statistics/comments/11y13cp/s_stata_help/
668,2023-03-22 05:37:44,[Q] fx= (1/sigma√2π)*e^(-(x-u)^2/2sigma^2),[Q] How is this formula to calculate likelihood used in logistic regression to find the maximum likelihood ? Because I have seen videos where people calculated the likelihood by converting log(odds) to probabilities using the sigmoid function. Is this formula used in anyway to find the likelihood in logistic regression.,"[""> How is this formula to calculate likelihood used in logistic regression to find the maximum likelihood ?\n\nIt isnt\n\n> I have seen videos where people calculated the likelihood by converting log(odds) to probabilities using the sigmoid function.\n\nSure, but what does any of that have to do with the normal density? The sigmoid you're referring to is the cdf of a logistic distribution not a normal. \n\n>  Is this formula used in anyway to find the likelihood in logistic regression.\n\nin *any* way? That might cover a lot of possibilities. \n\nGenerally, a big no (the likelihood itself doesn't involve the normal at all) -- but note that under fairly broad conditions the log of the likelihood function will be *asymptotically* quadratic near the peak, so the normal distribution *could* enter into some computations involving likelihood."", 'This is the pdf for the normal distribution.  Outcomes for use in logistic regression follow a binomial distribution.  This has nothing to do with the likelihood function for logistic regression.', ""> how is this particular formula used in logistic regression\n\nIt isn't. Where have you seen this used in the context of logistic regression?"", 'This looks like homework? \n\nFrom what I recall with MLE and the normal pdf (this “formula”), it is rather convenient to work with it as a log.', ""I am just stuck here, i know how MLE works but how is this particular formula used in logistic regression. The thing is that we convert the log odds using the sigmoid function to calculate the likelihood/probability and then we maximize the product to find the maximum likelihood.   \nI don't see how mean ( mu ) and standard deviation ( Sigma ) is used in any way to find maximum likelihood particularly in logistic regression because we r finding it through some other means i.e sigmoid function.""]",0,5,https://www.reddit.com/r/statistics/comments/11xvjof/q_fx_1sigma2πexu22sigma2/
669,2023-03-22 03:40:45,[Q] How to choose the correct number of items in a batch for testing to deem the batch passable.,"Idk if this is the right place to post. I have a data set of timestamps with data for each timestamp. I need to cross reference each timestamp with another program to see if it matches or not. Problem is there's are 1000s. My task is to test enough to deem the batch passable or not.

I know there is a math problem similar to this where there is a batch and someone can't test each product in the batch but test every nth or a random x% of it to say that it is most likely passable.

I don't need to this, but it got me thinking what would the optimal solution look like.","[""Optimality requires a more stringent definition unfortunately. Essentially you'll need to determine an acceptable probability that a batch is deemed passable when it actually contains x% of non-passing components. \n\nWithin the world of industrial quality control this concept is generally called Acceptance Quality Limit and there are [reference standards](https://asq.org/quality-resources/z14-z19) available to make sure suppliers and customers can have a default sampling procedure to use when defining contracts. Its been a bit since I worked with these but essentially you'll plug in a few parameters to the model and it will give you back a sampling procedure."", 'What specific condition on the batch as a whole makes a batch passable? What probability do you want to meet the condition with?', 'The hypergeometric series works, you vheck the probability 1-p of zero defects in the sample. That is the probability your sample size will catch at least one defect. but as other comments says for efficiency use the tables. They for whatever reason tend to allow for smaller sample sizes (made by smarter people than me lol). Double sampling plans can get you down significantly which is important for labor costs.\n\nAlso you prob already know there isnt really such thing as a 0 aql. There is no way to use qa stats techniques to guaruntee zero defects so you have to negotiate with customers what they define as an acceptable level of defects.\n\nProcess steps that 100 percent inspect without human error is the only way to get a very low/near zero defect rate. Examples would be ai label verification, metal detection with a size limit, continuous parameter monitoring such as oven temperature, automated dispensing scales or check weighers.\n\nThough calibration is essential to these types of control steps.', 'If the item in the batch, essentially a timestamp with the data match with another program, it meets the criteria. If not it fails.\n\nFor example:\n\nBatch X:\n\n2023-03-21 06:00-08:00 => 5 degC +/- 1 degC\n\nReference Program :\n2023-03-21 06:00-08:00 => 5.4 degC +/- 1.1 degC\n\nThis would be acceptable as the error predicted in Batch X of that timestamp is within 10% of the reference program with the same timestamp.\n\nIdeally, to make sure the batch is passable, >80% must match.']",5,4,https://www.reddit.com/r/statistics/comments/11xrvrz/q_how_to_choose_the_correct_number_of_items_in_a/
670,2023-03-22 00:49:32,[Q] Error of A/B,"Greetings

There are several websites listing the error propagation formula to calculate the error of Z, where Z=A/B. I just want to make sure that I understood them correctly...

1. Divide the standard deviation of A by the value of A and square the result.
2. Divide the standard deviation of B by the value of B and square the result.
3. Sum the results from step 1 and 2.
4. Take the square root of step 3 to get the error of Z.

Does this sound right? Do I need to take anything else into account?

Apologies for any incorrect use of terms.

Thank you

EDIT: I am asking because I want to divide regression coefficients by one another.

EDIT 2: corrected ""of step 4"" to ""of step 3""","['If by ""the error of Z"" you mean the *relative* error of Z, yes, that\'s the standard formula you\'ll be taught in a physics lab class.\n\nIf you want to estimate the standard deviation of Z, you need to multiply by Z.\n\nThe formula assumes that A and B are independent, and the standard deviations of A and B are small relative to their magnitudes. In particular, B must never be near zero with any significant probability.', ""https://en.wikipedia.org/wiki/Taylor_expansions_for_the_moments_of_functions_of_random_variables\n\nNote the E[X/Y] and Var[X/Y] formulas\n\n> I am asking because I want to divide regression coefficients by one another.\n\nBeware; you have to make some assumptions (which mean, among other things, that the the estimates themselves are *not* normally distributed; quite distinct from what you would assume for inference on A or B alone) for the Taylor approximation to work. If the density is not 0 in the neighborhood of 0 or at least if it doesn't decrease to zero sufficiently quickly as you approach 0, the variance of the ratio will not even be finite. (edit: indeed even the expectation will  non-finite; typically it will be undefined)\n\nAdditionally, as a practical matter, the denominator needs to be multiple standard deviations from 0, or the approximation will be poor in any case as the higher order (remainder) terms can't be neglected.""]",1,2,https://www.reddit.com/r/statistics/comments/11xmoqc/q_error_of_ab/
671,2023-03-21 22:55:47,[Q] Multiple comparisons correction when multiple groups and multiple features are being tested,"Hi all,

I think I understand the concept of multiple comparison correction when it is applied to a basic design. If I have 2 groups and I am testing 1000 features (or genes, etc.) I have to correct for multiple comparisons. I generally use Benjamini-Hochberg. However, how do I approach this if I have multiple groups?

These are two scenarios where I don't know how to correct the p-values properly:

1) T-test of 1 control (A) versus 3 treated (B, C, and D) for 1000 features: AB, AC, and AD

2) ANOVA for all possible group comparisons (AB, AC, AD, BC, BD, CD) for 1000 features

&#x200B;

I use R for my analysis, so, for example, should I do a Benjamini-Hochberg using an n = 3000 in case 1) just because I have 1000 features being compared 3 times?

In case 2, I don't even have an idea of what I should do. If I perform an ANOVA followed by Tukey HSD, the p-values should already be corrected for the multiple group comparisons, but how do I further correct for the multiple features comparison?","['When comparing control to three groups you can use Dunnett’s test which corrects for the multiple comparisons with control but does not over-adjust by correcting for comparisons among experimental groups. Tukey corrects for all pairwise comparisons. You can use the Benjamin-Hochberg to further adjust for multiple features.', 'Thank you for your reply!\n\nDunnett\'s test already correct for both the group and feature comparisons?\n\nAs for the ANOVA, do I use ""n"" as the number of features since the group-wise comparison was already corrected by Tukey? I am asking about an ""n"" because it is one of the parameters I can adjust using the ""p.adjust"" function in R.', 'I’m not sure what p.adjust does but Dunnet and Tukey control for the number of comparisons among means but not the number of features. I think you are right to make n the number of features.']",1,3,https://www.reddit.com/r/statistics/comments/11xj9fw/q_multiple_comparisons_correction_when_multiple/
672,2023-03-21 22:40:44,[Q] Estimating p in a binomial situation such that the p-value is .05,"\[Q\] I was watching a YouTube video the other day where someone said they had 500 good encounters in a video game without a single bad one. The probability of a good encounter is unknown, but bad encounters are possible. 

I was wondering if there was a way to set this up as a Binomial problem with an unknown p and n=x=500 such that doing a cumulative sum from 1 to 500 gives a p-value of .05. Could this be considered the ""highest plausible value of p"" assuming an alpha of .05? 

With brute force, I got that p = .000102579999 is the value of p such that the probability of 500 good encounters and 0 bad ones is .05. 

Can this be considered the maximized p for this problem? We are 95% confident that p is at most this value?

I am curious if this is something that is regularly done when p cannot be estimated. Is there a formulaic way to solve this rather than trial-and-error?","['There\'s a simple approximation known as the [rule of three](https://en.wikipedia.org/wiki/Rule_of_three_(statistics\\)) which gives the approximate 95% CI for p as (0, 3/N) if no events have been observed in N tries (and N is at least 20-30, because it relies on the CLT).\n\nAssuming that they were setting out to measure the probability of having a bad encounter, in 500 tries you can be reasonably sure that bad encounters (for this particular individual, given the way they play and interact) are no more common than 1 in 167.\n\nIf they weren\'t setting out to measure it, just going *""hey, I\'ve had no bad encounters in aaaages, this game must be really cool""*, they\'re likely to be underestimating p. They\'ve generated a hypothesis and need new data to test it.\n\nE2A: plain text link for anyone the original link isn\'t working for: https://en.wikipedia.org/wiki/Rule_of_three_(statistics)', 'I *think* OP is trying to ask about the largest possible proportion which could not be rejected at alpha = 0.05.', 'Are you using _p_ to mean the probability of a bad encounter in the (unknown) population ?  Or the p-value for some statical test ?', "">With brute force, I got that p = .000102579999 is the value of p such that the probability of 500 good encounters and 0 bad ones is .05.\n\nJust to be clear, isn't it the case that for this P, it's 499 and 1 that lead to a *p*\\-value of 0.05 ? \n\n*binom.test(1, 500, 0.000102579999)*\n\n\\### *number of successes = 1, number of trials = 500, p-value = 0.05*  \n\\###\r  \n*### alternative hypothesis: true probability of success is not equal to 0.00010258*"", '> I was wondering if there was a way to set this up as a Binomial problem with an unknown p and n=x=500 such that doing a cumulative sum from 1 to 500 gives a p-value of .05. Could this be considered the ""highest plausible value of p"" assuming an alpha of .05?\n\n> With brute force, I got that p = .000102579999 is the value of p such that the probability of 500 good encounters and 0 bad ones is .05.\n\n> Can this be considered the maximized p for this problem? We are 95% confident that p is at most this value?\n\nYou are using the same letter p for many different purposes. clarifying/using different letters will make your question much clearer.']",7,17,https://www.reddit.com/r/statistics/comments/11xiu3f/q_estimating_p_in_a_binomial_situation_such_that/
673,2023-03-21 22:05:58,[Q] what is the best way to go about this scenario?,"context: I am playing basketball. 
if i had a 57% chance of making a free throw, and the goal is to make 5 free throws in a row. would the probability of me hitting 5 in a row increase with more shots being taken?  or would it be worse since with more shots would lead to me getting closer and closer to my 57% average. i guess what i'm saying is what is the best opportunity to get variance that'd be on my side, shooting a ton of shots and hoping eventually i can get 5 in a row, or shooting a smaller amount? keep in mind, in said competition , the higher amount of shots taken the 5 in a rows is expected at a higher amount. i.e someone who shoots the ball 12 times, they only need to hit 5 in a row once. vs someone who shoots the ball 50 times, they're expected to hit 5 in a row at least 2-3 times now.","['If 57% is the true probability, then it doesn’t increase or decrease as you take more shots. \nIf the probability of making it is conditional on the number of shots taken, then it is not safe to assume that the probability will stay 57% the whole time - thus, it is not guaranteed that you’ll have exactly 57% over time. \nTo simplify calculations, assume each free throw is independent of each other. But in reality, fatigue and pressure can have a big effect. \nIf the probability changes depending on situation/number of previous shots, this problem is much harder', 'The usual model for basket ball throws is that each throw is *independent* of the past. That means you chances do not improve regardless of how many throws you have taken before.\n\nIn reality, the probability is not a constant number. Probably it is lower when you are tired/low morale. Probably it is higher when you are succeeding/fresh.', ""Given an assumption of independence (or indeed some roughly plausible assumptions about mild dependence), the probability of *failing* to make 5 in a row decreases monotonically in number of throws (assuming it's already at least 5)""]",1,3,https://www.reddit.com/r/statistics/comments/11xhw3k/q_what_is_the_best_way_to_go_about_this_scenario/
674,2023-03-21 19:50:51,[Q]What is best books for statistics from beginner to advance,"Hey everyone, 
I just start my data science journey and i wanna know about somw good book for statistics and if you know any good youtube channel so its help me lot","[""For the real statistic basics I'd recommend Discovering Statistics Using R by Andy Field. There is also the same textbook but for SPSS, which has more up to date editions.\n\nStatQuest on YouTube is really great in terms of understanding basic concepts. In this regard, it might be the best source. \n\nFor a bayesian perspective (in R) you should definitely have a look at Statistical Rethinking by McElreath. There is a YouTube lecture working through the book chapter by chapter."", 'Once you are advanced you’re going to want to crack open casella & burger, but I have an advanced degree and that book still scares the daylights out of me. I would recommend Wackerly Mendelhamm mathematical statistics for now. Really fucking boring and poorly organized imo but it gets the job done for undergrads and some grads.', ""Both books are surprisingly fun reads. Field's book is a great suggestion, super approachable yet still takes the time to go into the nitty-gritty; I think it might be my top pick for anyone just getting into statistics. Also, FYI, he's supposed to be releasing a revised edition (of the R book) some time this year. \n\nStatistical Rethinking is also a stellar book. He leads with Jewish folklore about golems, which is the coolest opening a stats book could have imo. The caveat being that Bayesian stats are still considered unconventional (at least in my field, psych science) so it can be a hard sell to get people on board with them."", 'The [American Institute of Mathematics](https://aimath.org/textbooks/approved-textbooks/) has a list of free recommended math books including statistics books.', '> casella & burger\n\nCasella & Berger\n\n> Wackerly & Mendelhamm\n\nWackerly, *Mendenhall* and Scheaffer ... I think since the 4th edition in 1990(ish?).']",67,25,https://www.reddit.com/r/statistics/comments/11xeig9/qwhat_is_best_books_for_statistics_from_beginner/
675,2023-03-21 18:17:52,[E] Stats major with minor in Business Administration/Economics/Computer Science,"The stats major I'm looking at is actually ""statistics and data science"" at a reputable universtity, so I guess it already has a fair bit of programming, albeit with a quantative focus (primarily using Python and R). Which minor would you pick to maximise income later in your career? I'm thinking business administration could give a fairly broad scope. I'd kind of like to go into finance, but if there is more money (or a more pleasent career, as I would rather earn 20k less than work for 100+ hours a week) elsewhere then I'm also willing to go in that direciton.  


I could also do the major in math but I'm not sure if I'm up to for the rigorous program. Also it would include less programming.","['If you plan on grad school, get some math under your belt.\n\nFormal, professional-level programming classes.\n\nProfessional speaking and writing classes.', 'For maximizing income I guess you will just need to find great jobs in whichever realm you participate in. If you want to be a high tier business support / development “Data Scientist” then do economics. If you want to be a high tier product developer involving data, computer science. I’m not sure which has a higher income ceiling or is more / less stressful but that’s my general thought on the matter.', ""I would steer away from business administration. Such programs exist mostly to give grasping, ambitious people the opportunity to network -- if you're not already that kind of person, it will be wasted, and you will be bored.\n\nEconomics could be interesting. However, the number of jobs actually requiring economics is probably pretty small compared to statistics in general. My advice is to look for announcements for jobs you might want, and see what they're stated requirements are.\n\nOverall my advice is stay on your statistics & data science path, and maximize your later employability by actively trying to connect with other people, students, faculty, people inside and outside the school.""]",4,3,https://www.reddit.com/r/statistics/comments/11xcrtw/e_stats_major_with_minor_in_business/
676,2023-03-21 11:42:21,[E] Masters program with a below average GPA,"Hi All,

I’m currently in my last year as a Statistics Undergraduate at a pretty high ranking public school in the US, and I’ve been heavily interested in going to graduate school for a while now. I now have come to the realization that I will most likely end up with a GPA around a 3.2 to a 3.3, which is below the average GPA in my major (3.4). I have internship and research experience under my belt, and would love to continue my education for my masters. Is this possible, or is my GPA too low to go to graduate school?

Thank you","['I know someone who got in a similarily ranked masters programs to UC Davis with around 3.1 gpa. It was unfunded but he was international. \n\nI think US citizens have a much much greater chance for acceptance and for funding. You should be fine, but try to get around 167 in GRE Quant. GRE just needs a lot of practice, so start early.', 'Have you looked into masters programs at your own institution? You mentioned research; if you have a respected professor in your desired department vouching for you and you meet the base masters entry requirements them I see no reason you wouldn’t be accepted.', 'I had a lower GPA and was able to get in to a masters program. Sometimes work experience itself overcomes GPA, so worst case you might just need to take a gap.', 'That’s very good to hear. I’ve already been studying for the vocab, so I’ll just need to start for the Quant section. Thank you!', 'I have looked into my institution, however it’s heavily competitive and ranked very highly for a masters program. I’m going to apply regardless, but the chances are quite low.']",2,6,https://www.reddit.com/r/statistics/comments/11x5t5w/e_masters_program_with_a_below_average_gpa/
677,2023-03-21 09:46:31,[Q] Mixed ANOVA question," 

Greetings,

I am conducting an analysis using several mixed 2x2 ANOVAs where the nature of my variables requires that I perform post hoc testing if I have interactions. I know that people will argue that you do not need post hoc testing for less than 3 levels however for these particular variables I do. Does anyone have a source where this rationale is explained or documented? Most sources are just black and white and dont seem to give any leeway.","["">for these particular variables I do.\n\nIf you only have two levels, a significant interaction effect defines a significant difference between these levels. Why would you need to conduct a post-hoc test for this? It's already clear and you have your model adjusted coefficient and associated p-value. A post-hoc test is effectively repeating a t-test using the model adjusted terms."", ""Depending what OP means by posthoc, it's a bit worse than doing a t-test, because of adjustments for multiple comparisons that don't exist in this case.\n\nThe main point is with 2x2, the main effects and interaction completely partition the space. If you want to partition it differently, that's fine, too. So instead of 2 main effects, follow up with simple effects of A at each level of B, which I'm guessing is what's desired in this case. I would not expect an adjustment for familywise alpha.""]",3,2,https://www.reddit.com/r/statistics/comments/11x38hd/q_mixed_anova_question/
678,2023-03-21 07:39:15,[E] Anyone here with an Operations Research background?,"Operations Research is not a background I see often mentioned, yet it seems so relevant. Curious to hear about anyone with that background: How do you brand yourself? What do you do? What’s your title? What’s the pros and cons of OR?","['I did one year Masters in OR before switching to Statistics. The field of Operations Research is a Decision Science, while the well-known field of Data Science (clustering, classification, regression) is a Predictive & Prescriptive Science. OR involves simultaneously making a lot of decisions to maximize an objective (like profit) or minimize a loss, while adhering to constraints.\n\nExamples:\n\n* Location Planning: How far apart should franchise stores like McDonalds or Subway so they can maximize revenue, while not cannibalizing each others sales?\n\n* Location Planning: How should a city bus system plan its routes, and bus stop locations. The objective could be to serve the most people, or to maximize profit\n\n* Routing & Scheduling: How should a delivery company like Fedex, Amazon or UPS assign their vans?\n\n* Scheduling: How should an airline schedule their crew so that they are best used, with the constraint that they have to be in their hometowns for days off\n\n* Pricing: How much should a grocery store charge manufacturers to display their products by shelf level and aisle?\n* Dynamic Pricing: How many seats should an airline sell at each price level?\n\n* Demand & Supply: How should an Energy company allocate bids (price & quantity) between its suppliers and consumers of energy?\n\n* Portfolio Management: What stocks should I have in a portfolio to maximize expected profit, with constraints on maximum drawdown permitted, volatility allowed etc\n\n* Sensitivity Analysis: What will be the effect of changing decision variables, like adding a new stock to a portfolio, adding a new train route etc.\n\nThere are many more examples on [Wikipedia](https://en.wikipedia.org/wiki/Operations_research)\n\nThe theory involves advanced math, but applying OR is easier because ""solvers"" are used to do the heavy optimization, and they are available for Python, R, Julia, C++. Heuristics can be used when the optimization is too gnarly even for a solver.', ""I'm a data scientist with a master's in OR. No one knows what OR is in any of the industries I've worked in, so I just brand myself as a very good data scientist. I've done some queuing theory and some linear programming for some of my industry projects, but I mostly do machine learning, stats, data engineering, etc."", 'I am from OR/ Industrial Engineering background. And it is often tough to convey my background to non-Tech or non DS people.\n\nMostly, I term myself as a Business Analyst nowadays and that is my title too. Or if I had to tell someone what I do generally say: I build mathematical models to solve business problems.\n\n\nThe thing is, what I do lies somewhere between a Business Analyst, A Data Scientist and a Data Analyst. And by my skill level, I have width but lack depth. Jack of all trades and master of none. It is tricky sometimes to find a suitable job.', 'Well thought out and written answer. OR is extremely useful OP, but nobody knows what it is outside of people who have been a part of it.', 'I did IE undergrad with some OR classes, but never quite used it in my analytics career. It is quite relevant in that it is heavily laden with statistics and optimization modeling. I feel confident that if I had the time and need to revisit an OR style problem that I could do so using the things I am learning in my stats MS. \n\nIt has been a long time since then, truly fascinating stuff from what I recall.']",33,31,https://www.reddit.com/r/statistics/comments/11x0542/e_anyone_here_with_an_operations_research/
679,2023-03-21 07:17:39,[Q] Odds for Magic the Gathering,"In MTG Arena, there's an [event](https://i.imgur.com/pUn1Srp.jpg) when you can play up to 7 games of Magic, and get various rewards. Once you've lost 3 times, or once you've won 7 games, the event ends and you receive rewards based on how many games you've won. So for example, if you win 3 games and lose 3, then you get the rewards for having 3 wins. Alternatively, if you win 7 and lose 2, then you get the rewards for having won 7.

&#x200B;

What I want to know is this: If I have a 50% chance of winning each game, how do I figure out the odds of entering the event and winning 1 game? Or winning 3 games? Or 7 games? Or 0 games? etc

Also, sorry if this is the wrong place to ask, I wasn't really sure where I should go.

&#x200B;

EDIT: I explained wrong, you play until you get 3 losses or 7 wins, so you can play more than 7 games of magic (for example getting 5 wins and 3 losses is 8 games)","[""You could either write out the formulas for the probabilities directly by enumerating the various outcomes, or you could work with the [negative binomial distribution](https://stattrek.com/probability-distributions/negative-binomial), adjusting for impossible combinations of outcomes. Either way I don't think there's a standard distribution that exactly models what you're after."", ""To win exactly 1 game and end with 3 losses, you have to win exactly one of the first three games and then lose the following one. That's a chance of 3/8 * 1/2 = 3/16 because there are (3 choose 1) = 3 options where your win can be. Every other specific outcome can be treated in the same way. To win 7 games (with any number of losses) you can sum the chance to lose 0, 1 or 2 games, or use [this trick](https://www.reddit.com/r/probabilitytheory/comments/10m9s1w/odds_of_striking_out_in_baseball/j6220k8/) to convert it to standard formulas that can sum it directly."", '>  Alternatively, if you win 7 and lose 2, then you get the rewards for having won 7.\n\nHow can you win 7 and lose 2 if you can only play ""up to 7 games""?\n\nYou want the probability of number of wins before the third loss? That\'s negative binomial. If you can\'t play more than 7 games, it gets truncated at 7 games (which makes things a bit more complicated, but still doable).\n\n\n\nhttps://en.wikipedia.org/wiki/Negative_binomial_distribution\n\nhttps://en.wikipedia.org/wiki/Truncated_distribution\n\nIf it wasn\'t for the truncation, I\'d do it straight up as the number of successes version, but I believe the truncation would be easier if you do \nit in terms of the number of trials version and then translate back after.\n\nAlso, if you seek probabilities rather than [odds](https://en.wikipedia.org/wiki/Odds), don\'t call it odds, which is related, but different.', '>How can you win 7 and lose 2 if you can only play ""up to 7 games""?\n\nmy bad, what I meant to say is that you can only get up to 7 wins', 'That ... changes things. Please edit to clarify that part of the original question']",6,6,https://www.reddit.com/r/statistics/comments/11wzl9f/q_odds_for_magic_the_gathering/
680,2023-03-21 04:33:12,"[Q] What's does "" y axis "" mean in a probability distribution or normal distribution ?",,"['For a continuous density, it represents density. For a discrete pmf it represents probability. For a cdf it represents P(X≤x)', ""Context matters. Look what the axis labels. Usally you'd either get the probability (or relative appearance) or absolute appearance on y against the charactersitics on x.\n\nExample the common gaußian bell for a continous characteristic shows the relative appearance (probability) against the characteristic, e.g. the IQ chart."", 'Are you asking what the values of a probability density mean? [Here](https://stats.stackexchange.com/questions/4220/can-a-probability-distribution-value-exceeding-1-be-ok) is a good introduction with a few examples.']",3,3,https://www.reddit.com/r/statistics/comments/11wuwos/q_whats_does_y_axis_mean_in_a_probability/
681,2023-03-21 02:17:24,[E] Data Science vs PhD in Stats,"I'm a junior now but planning out potential paths of my education and career after my ug in stats. Right now I'm interested in two particular paths. One is becoming a data scientist which is interesting since I enjoy coding and statistical decision making, may or may not need a MS I guess. The other is in research something like a PhD doing research in statistics or machine learning, which I'm not sure on whether or not I'd like it since I haven't had a strong undergrad in upper level stats and ml courses after a late major switch from business. How do I go about deciding which of these paths would be better for me after graduating from undergrad? Any advice would be great because I'm not sure if I should be studying leetcode or similar for data science roles or studying analysis to catch up to phd programs and I can't find enough time to do both.","['There’s a lot of advice here which I think is frankly bunk. A couple of thoughts:\n\n-\tDoing a PhD doesn’t preclude your becoming a data scientist, it just delays it by ~4 years. This might seem like a long time when you’re young, but it’s not much in the grand scheme of things.\n-\tIf you go into industry, the likelihood of leaving to do a PhD if you decide it’d be fun decreases every year.\n-\tEven on a FIRE path with great success, you’re still looking at ~20 years of work, which is an appreciable amount of time.\n-\tBeing a PhD is an instant credibility boost, especially with the C-suite\n\nI went straight into industry after my undergrad, and the all the really clever people appeared to have PhDs. I realised that I enjoyed maths and drinking in the afternoons so much that I left after a year and spent 4 years doing a (pure) maths PhD. I then returned to industry at the tender age of 27 in 2012, and it turned out that the independent learning and problem solving that I developed in my PhD were _incredibly_ useful for proper work too.\n\nPhDs are hard hard work; you’ve gotta love what you’re doing or it’ll be miserable. But if you do *love* it, it’s great.', 'Yes, but you spent that time making peanuts compared to the MS who was making a decent living and investing that entire time, who now also has a similar level of experience. You really shouldn’t get a PhD unless you want to do research', 'How much do you like money? \n\nHow much do you like getting to follow your interests and intellectual curiosity? \n\nIf a > b, choose data science.', 'When i got hired into an industry job, my PhD was able to count as 5 years of work experience', 'I’m not in any of those fields…is your goal to work in industry? If so, better to get a MA and start working in the field. Phd is mainly for academia. I’ll let other industry folks let you know if you even need a PhD for industry']",49,36,https://www.reddit.com/r/statistics/comments/11wqmna/e_data_science_vs_phd_in_stats/
682,2023-03-21 00:42:12,[Q] Is there any inherent issue with plotting concentration against the factor used to calculate it?,"I'm working on my Master's thesis and I have some data about microplastics in mussels which I standardized in particles/gram of tissue. My results are indicating that weight plays an important role in both abundance (particle/organism) and concentration (particles/gram of tissue). I've plotted both abundance and concentration against weight to show the patterns of distribution between different species/size class, but my PI seemed to think there was maybe an issue with plotting concentration against weight, but couldn't pinpoint why or say for sure. I know this is not strictly a statistical question, but was hoping someone here could answer it. Thanks!","['To state the obvious, there\'s nothing wrong with just plotting something. The issue is what you say based on the plot, which might be complete nonsense or an actually relevant insights, but you did not indicate that.\n\nI have no expertise on the topic, but my intuition is that plotting abundance vs weight is actually not an interesting relation, since it is kind of expected that bigger organisms have more stuff in them. On the other hand, concentration vs weight would seem to me to be the more interesting relation, as the concentration of a substance is generally what determines its potential impact, so in a null hypothesis-like scenario, if two mussels had different size, you could expect the larger one two have more particles when both have the same concentration but to be equally ""affected"" by the contamination since the ""load"" of particles is equal between both.\n\nOf course, the above argument assumes that all of the mussels tissues and processes scale in proportion to its weight, but the fact that the concentration is not the same could mean there are areas, e.g. nervous system, which may not be readily contaminated but which do not need to grow linearly with weight, therefore representing a larger part of the small mussel and explaining the lower overall concentration.\n\nOn the other hand, the difference in concentration vs weight could be completely unrelated to any specific internal characteristic of the mussel and simply be an association caused by the passage of time: older mussels tend to be larger, therefore they have been accumulating particles for a longer time.\n\nSo as you can see, there are many possible angles to investigate based on that initial finding and you would need a more detailed model of the organism and its interaction with the environment to formulate some meaningful statement from those plots.\n\nBut to reiterate the actual answer to your question, I don\'t see why there is something inherently wrong with plotting concentration vs weight. Perhaps your PI feels that because they are making an intuitive parallel with chemical solutions, where by design the concentration is designed to be constant and so a non-informative flat line against weight would result. But this is different because the weight doesn\'t represent some standardized mixture of mussel mass and microplastics being progressively added, but rather the cumulative result of a series of biological processes in interaction with the environment.\n\n(and just a last minute analogy that occurred to me: if you noticed that the last half of a cup of coffee from a vending machine feels sweeter than first, you would certainly be right to plot the relation of sample weight vs sugar concentration and upon finding a non-flat relationship, would be justified in thinking there is some underlying process of interest that explains the difference e.g. insufficient mixing of the sweetener causing it to stay at the bottom, even if in this case it is not exactly the weight that is causal but rather an incidental association between product weight and its vertical distance from the bottom of the cup where the sweetener lies)', 'Concentration is a function of weight.\n\nIf you plot weight vs. concentration of course there will be a clear correlation. Like the other guy said, nothing with plotting it and visualizing it. But you should interpret it with caution to not mislead', ""I'm not sure what they might be thinking, sorry.\n\nThere's definitely a potential issue with estimating the relationship between two different concentrations when dividing by the same weight value (so a plot of concentration of substance 1 vs concentration of substance 2 could be potentially misleading in such a situation, potentially inducing an 'illusory' relationship that's not representing a real association between the substances), but you're not doing that.  \n\nWhatever else they're worried about, I'm not seeing it right now.""]",1,3,https://www.reddit.com/r/statistics/comments/11wnw2n/q_is_there_any_inherent_issue_with_plotting/
683,2023-03-21 00:26:19,[Q] Monte Carlo Error,I'm struggling to understand how to compute the number of simulations for a Monte Carlo error of 0.1% with 95% confidence interval for a probability estimate (e.g. 0.5811). How does this work?,"[""This provides a nice overview of what you're looking for: [Monte Carlo simulation ](https://openturns.github.io/openturns/latest/theory/reliability_sensitivity/monte_carlo_simulation.html)"", '> How does this work?\n\nhttps://en.wikipedia.org/wiki/Binomial_distribution\n\nhttps://en.wikipedia.org/wiki/Margin_of_error\n\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval']",5,2,https://www.reddit.com/r/statistics/comments/11wnfm9/q_monte_carlo_error/
684,2023-03-21 00:24:17,"[Q] How do I estimate a transition probability matrix for each time point (i.e., a time-dependent transition probability matrix)","Hi everyone, I have a Markov chain with twelve states. I want to estimate a transition probability matrix for each time point (except for the last time point).   I found a function in R to do this. It is called seqtrate and it's in the TraMineR package. However, it isn't clear to me how they estimate a time-dependent, 3D  transition probability matrix.

Does anyone know how this is done, or can anyone point me toward any resources where I can learn more about this?

Thank you!","[""For continuous time, discreet state Markov processes, the target of estimation is the transition rate matrix. If you understand survival analysis, it is a generalization of the hazard function for time to a single type of event. That is, \n\n    P( X_{t+h} = j| X_t = i) ~ q_{i,j}(t) h \n\nIt's estimated using the tools of time to event data. For  example, a fully nonparametric estimate is given by the Nelson Aalen estimator, which can be considered a piece of the more familiar Kaplan Meier estimator for survival curve. At each time, t, at which there are transitions from i to j, we count the number of individuals at risk for an i to j transition, e.g., number of individuals, R_i(t),  in state i at time t, and number of transitions, dN_{i,j}(t), from i to j. Then, an increment to the cumulative transition rate estimate is \n\n    dQ_{i,j}(t)  =  dN_{i,j}(t)/R_i(t)\n\n\nOnce we've put these together for all states, i and j, at all transition times, t_k, then we form the cumulant:\n\n    Q_{i,j}(t_k) = sum_{t_l<=t_k} dQ_{i,j}(t_l)\n\nOne more thing. This defines the entries of Q off the diagonal, e.g., for all distinct i and j. The diagonal entries of Q are the negative of the corresponding row-sum\n\n     Q_{i,i}(t_k) = - sum_{j != i } Q_{i,j}(t_k)\n\nNow, the part you've been waiting for ;)   The transition probabilities are expressed in terms of the cumulative transition rates via the matrix exponential:\n\n\n    P( X_t = j  | X_s = i ) =   EXP(Q(t) -Q(s))_{i,j}\n\n\nWhat, you ask,  is the matrix exponential?  It's defined by the power series with terms vbeing matrix powers of an exponent:\n\n\n     EXP(Q)  =  sum_{n=0}^{infty} Q^n / n!\n\n\nUsually, five terms is enough. Hope that helps."", 'You could treat it as multinomial regression with eg previous state and time ( amongst others) as inputs', ':0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', 'THANKYOU!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!', 'This made my day']",4,13,https://www.reddit.com/r/statistics/comments/11wndhx/q_how_do_i_estimate_a_transition_probability/
685,2023-03-21 00:16:57,[Question] Pareto scaling: Can anyone explain why group means are opposite and equal?,"Hi! I'm a bioinformatics generalist (so close enough to good programmer/bad statistician) and am working with a dataset for which I am told the typical normalization steps are:

1. sum-normalization > log10 transformation > Pareto scaling\*.

For each of my response variables (>10,000), I have ten observations, 5/= per group in two groups. I have not come across Pareto scaling before.

After running those three steps on the full dataset, for every single response variable, abs(mean(group1)) = abs(mean(group2)).

I have read briefly about Pareto scaling but still can't get myself to understand why this is the result. Can anyone explain why this is? My values are grouped in my data such that the first five columns are in sequence and then the second five (though I can't see any reason this could possible matter).

I did it by hand for a few response variables and find the same result.

Example:

Input: 7019731.63, 6101363.44, 6639039.79, 6433115.66, 6763978.24, 7267775.37, 6968062.19 7027620.04, 7440666.63, 6161609.30

Sum-normalized (1000\*(x/sum)): 103.501, 89.960, 97.888, 94.852, 99.730, 107.158, 102.739, 103.617

Log10: 2.015, 1.954, 1.991, 1.977, 1.999, 2.030, 2.012, 2.015

Pareto-scaled ((x - mean)/sqrt(sd)): 0.093, -0.265, -0.049, -0.130, -0.002, 0.181, 0.074, 0.096

mean(group1)= -0.0705

mean(group2) = 0.0705

If you try it yourself you'll see that if you randomly group the final ten observations different ways, sometimes abs(mean(group1))==abs(mean(group2)), but not always. However, across my >10K response variables, this is \*always\* the result. What am I missing??

Thanks for any help","[""I don't use this technique in my work, so I'm just going off the math. The scaling is accomplished through the division of the sqrt(std).\n\nBut you're also subtracting the mean from each observation in the numerator, thereby centering the results. The mean of ALL the pareto values calculated this way will be zero, and by extension, your subset means will total zero as well.\n\nIn practice, are you applying the pareto to each variable/column?  Hopefully someone more familiar with your field will come along with additional thoughts and we can all learn from them. Good luck."", 'Thanks for taking a look! I think that\'s the whole answer, no field-specific input needed. It was a failure of like sixth grade math that I didn\'t make the step from centering the variables (mean = 0) to the means of any two subsets totaling zero - And I suspect it was because in my ""testing"" I made an error that resulted in the means of two random subsets not always totaling zero - tried it more carefully and of course they do.\n\nThanks again!']",1,2,https://www.reddit.com/r/statistics/comments/11wn6ab/question_pareto_scaling_can_anyone_explain_why/
686,2023-03-20 21:24:45,[Q] significance of cluster,"I have data that is clustered in 2D. I want to use a clustering algorithm like k-means, but I want to determine the significance of the clusters. Is there a way to do that?","['What does ""significance of clusters"" mean? You might want to take a look at model-based clustering for this (mclust in R). There is a whole world of cluster validation techniques, from profile plots to permutation tests to actual model comparisons.', ""What's your null model? That should give you a direction to go when considering how to build a NHST."", 'Thanks that gives me some key words to try. Hypothesis is the data is clustered more strongly in one direction than in the other, so actually only need 1D clustering.', ""Might also recommend the silhouette score. It's what I've used in the past."", 'Thanks!']",12,11,https://www.reddit.com/r/statistics/comments/11wisx1/q_significance_of_cluster/
687,2023-03-20 20:49:54,[Q] Probability and likelihood are different but then there are scenarios where they assume probability and likelihood to be same/ equal. So when are these two terms considered equal ?,,"[""You use them for completely different purposes. Don't worry about when they might happen to be equal. Unlike the probability of an event, a likelihood is almost never useful by itself. It's (nearly) always used in comparison with another likelihood. \n\nThink of likelihood as an optimization metric that somewhat resembles probability, but it really doesn't matter if it's the same. An 'optimization metric' is something that you use to calculate the 'best' estimate. A sum-of-squared error terms is an optimization metric, albeit one that does not resemble probability at all. You can calculate the parameter estimates of a linear regression (slope and intercept of the line) by a procedure that minimizes the sum of the squared error terms.  \n\nIt turns out that these linear regression parameter estimates are the [exact same answer as the parameter estimates you get when you maximize the likelihood](https://en.wikipedia.org/wiki/Least_squares#Least_squares.2C_regression_analysis_and_statistics). In other words, it does not matter which metric you use in your optimization when you calculate the estimates for a linear regression. The regression that has higher likelihood (or lower sum of squared errors) is 'better', so when you've reached the 'peak' likelihood (or nadir of error), you've got the 'best' model. \n\nFor other regression models, the sum of the squared error terms is less useful, so we use the likelihood. The answers based on either approach aren't equal, but that's okay; we know which answer to use."", 'A probability is a general idea that says if you have population, A, then P(A) = 1 (roughly speaking). There are one or two other additivity rules etc.\n\nA likelihood function is often expressed via probability theory as well, but it\'s a specific usage which generally involves evaluating something like P(X = x | theta) = L(theta | X = x). That is, we are interested in a particular function that changes wrt parameter theta, when X=x (X being our random variable) is observed. Now, in order to evaluated the conditional we need information like:\n\nP( X | theta) = P(X, theta) / P(theta), we typically don\'t know the numerator P(X, theta), and we guess a form for P(theta) for the denominator normally. But therefore that means we don\'t really know the true form for P(X | theta), so we consider it as something ""non-normalized"" in practical settings, hence why we tend to work with it as ""the likelihood function"". \n\nAlso it\'s because in conditional probability with something like P(A | B=b), we typically work with B fixed, and A being allowed to vary typically. But we work with P( X | theta) in reverse, X is fixed, and we optimize over theta in some way therefore the form of P(X=x | theta) keeps varying in its functional ""form"" as theta varies also, which is weird in practice (but if we had information about everything, it would be still OK to calculate, and I suppose consider it strongly as a pdf). Based on this I suppose we try to emphasise that we work with P(X=x | theta) in a more ""functional"" way, hence why we like to write it as ""the likelihood function"": P(X=x | theta) = L(theta | X=x), i.e. X is fixed, theta varies.', 'A probability is a function normalized so that the integral over the entire event space is 1. \n\nA likelihood is not normalized, because it is a function used to compare the ""likelihood"" of different outcomes, usually taking their ratio (which would wipe out the normalization anyway).', 'The functional forms are exactly the same but probability is a function of values of one or more random variables, conditioned on values of the parameters, while likelihood is a function of the parameters, conditioned on the observed values of the random variables (the data).', 'What helped me understand likelihood is learning that it is very similar to conditional probability. \n\nPr(X|theta) is a probability when theta is fixed (and X is variable). Imagine a simple coin flip / bernouli distribution scenario. If theta = .5, the probability that X = 0 is .5, and the probability that X = 1 is .5. Note that the distribution adds up to 1, as probabilities should. \n\nPr(X|theta) is a likelihood when X is fixed (and theta is variable). Again imagine a coin flip. If X (i.e. the data) is 0, then the likelihood of theta being 0 is 1 (from p1 * (1-p)0), the likelihood of theta being .1 is .9, the likelihood of theta being .2 is .8, etc. Note that the ""distribution"" of theta is not a probability distribution since it doesn\'t add up to 1. Also note that it\'s highest (i.e. maximim) at 0; this makes sense, right? If you flip a coin once and it lands heads, your best guess would be that the coin always lands heads. (Well, according the logic of ML. ML can be a little dumb sometimes, because it is constrained by the data and the model.)']",25,12,https://www.reddit.com/r/statistics/comments/11wi0xd/q_probability_and_likelihood_are_different_but/
688,2023-03-20 18:18:21,"[question] Boxes B and C are equally likely (20% each), but B ""looks like"" Box A (60%). Is there a measure under which B is less surprising than C?","You are playing a guessing game where you have to choose one of three boxes, each containing three different marbles. Box A contains red, green, and blue marbles, Box B contains red, green, and yellow marbles, and Box C contains black, white, and purple marbles. You are given the probabilities (60%, 20%, 20%) for each box. You make your choice and open the box. Intuitively, the Box B outcome should be less ""surprising"" than the Box C outcome in ""marble color space"" because it contains the red and green marbles, which are 80% likely to be encountered. Is there a probabilistic measure that captures this intuition?","['Your intuition is in some sense wrong, because the *combination* B is just as surprising as the *combination* C. However, if you look at how surprising each individual marble is without considering combinations, then it is true that some of the marbles in B are less surprising.', 'I am aware that P(Box = B) = P(Box = C). But suppose you are a machine that represents beliefs about which Box it is. Then you observe the marbles, as a machine. There\'s something about your belief update that is smaller for Box B than Box C. \n\nFor example, if you open Box B, your probabilities for those before and after opening the box are (red: 0.8 -> 1.0; blue: 0.8 -> 1.0; green: 0.8 -> 0.0; yellow: 0.2 -> 1.0; black 0.2 -> 0.0, white 0.2 -> 0.0; purple: 0.2 -> 0.0). The absolute value of the change in probability is 2.6 for Box B.\n\nOn the other hand, if you open Box C, the probability changes are (red: 0.8 -> 0.0; green: 0.8 -> 0.0; blue: 0.8 -> 0.0; yellow: 0.2 -> 0.0; black: 0.2 -> 1.0; white: 0.2 -> 1.0; purple: 0.2 -> 1.0). The absolute value of the change in probability over all marble colors is 5.0 for Box C.\nI am trying to find a meaningful quantity provided by probability/information theory that captures the ""absolute value of the change in probability"", which is larger for observing Box C than Box B.', ""If you reason this way, processing the marbles individually, you need to look at conditional probabilities. For instance, once you've seen black, the probabilities for white and purple become 1. Or in other words, the probabilities for the different marbles are not independent.""]",0,3,https://www.reddit.com/r/statistics/comments/11wetok/question_boxes_b_and_c_are_equally_likely_20_each/
689,2023-03-20 13:34:23,[Question] [Research] How do I find the line of best fit (using python) when I'm completely unsure of what sort of equation it is?,"I've been working on some physics research and I'm trying to figure out what the line of best fit for it is. I have tried many types of equations (e.g. logarithmic, exponential, etc.), but none of them seem to fit very well. I was wondering if there is any way I could put the points into some python library and get a line of best fit that looks reasonably plausible. I know after finding this I would need to use some statistics to compare models and see which one is most likely with the Ockam factor included. If anyone could please help me in just finding a type of equation that would work for a line of best fit of my data that would be great. Thank you!","[""The best fit given what assumptions? You could draw a perfectly fitting, likely very squiggly, line, but this isn't likely to be what you want. Understanding of your data, the underlying phenomenon, and the needs of your inference would all be helpful (necessary) to answering your question."", 'What are you trying to do with the model?', 'Is your data a vector of positive values?', '“Line of best fit” as it’s taught in middle school would likely refer to linear regression: in essence, “y = mx + b” (although statistics would write this as Y = Beta * X + epsilon). \n\nIt depends on what your goal is — are you predicting the “probability” of some event (aka is your y a [0,1])? Then logistic regression is what your after.\n\nIn Python… Scipy might have something? Tbh I’d just use R for this.']",0,4,https://www.reddit.com/r/statistics/comments/11w9s76/question_research_how_do_i_find_the_line_of_best/
690,2023-03-20 12:37:53,[Research] [Question] data analysis of self collected responses for results section of research paper,"I’ve never been so confused

Im just a student researcher and I have a bunch of data from my results. 120 responses to be exact. I’ve been trying to teach myself how to analyze data but nothing is helpful. I have no idea what code is or java or python or projects. I have a very small background in statistics but it’s basic stuff—think high school stats class. Will someone direct me to a website I can use that makes sense? Or even explaining where I should start, what procedures to use for analysis? I’d be happy to attach a link to my sheet if anyone wants to look at it themselves or explain my research","[""Hello,maybe the first thing to consider is stating some of the claims/hypotheses you thought of testing before collecting the data. If you had none which will be rare then think of some relevant ones in your research and also familiarise yourself with how the sampling and data collection was done.I would then suggest starting with exploratory data analysis (EDA) using plots, crosstabs and other numerical summaries of your data by groups. I will be assuming you have access to popular software for data analysis like Python or R. If not, a quick google search on installing any would guide you.\n\nHere is a link to a simple example of EDA in R : https://www.statology.org/exploratory-data-analysis-in-r/\n\nHere is a link to a simple example of EDA in Python: https://www.geeksforgeeks.org/exploratory-data-analysis-in-python/\n\n\nAfter EDA you can proceed to test your hypotheses as seen fit..Be careful to check and see if your assumptions necessary before using these tests hold at least approximately or be sure to try robust alternatives or non parametric equivalents of these tests. \n\nHere is a sample guide in R:\nhttps://data-flair.training/blogs/hypothesis-testing-in-r/amp/\n\nHere is one in Python: https://www.visual-design.net/amp/an-interactive-guide-to-hypothesis-testing-in-python.\n\nI would also recommend that in your free time you try to read Sharon Lohr's accessible book on survey techniques titled Sampling and Design and also Jim Albert's/Rizzo Maria's book on  simple data analysis methods titled R by Example -Concepts to Code. Good luck."", ""It looks like you shared some AMP links. These should load faster, but AMP is controversial because of [concerns over privacy and the Open Web](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot).\n\nMaybe check out **the canonical pages** instead: \n\n- **[https://data-flair.training/blogs/hypothesis-testing-in-r/](https://data-flair.training/blogs/hypothesis-testing-in-r/)**\n\n- **[https://www.visual-design.net/post/an-interactive-guide-to-hypothesis-testing-in-python](https://www.visual-design.net/post/an-interactive-guide-to-hypothesis-testing-in-python)**\n\n*****\n\n ^(I'm a bot | )[^(Why & About)](https://www.reddit.com/r/AmputatorBot/comments/ehrq3z/why_did_i_build_amputatorbot)^( | )[^(Summon: u/AmputatorBot)](https://www.reddit.com/r/AmputatorBot/comments/cchly3/you_can_now_summon_amputatorbot/)""]",4,2,https://www.reddit.com/r/statistics/comments/11w8nyg/research_question_data_analysis_of_self_collected/
691,2023-03-20 10:47:06,[Q] How do you interpret a negative interaction coefficient in a logistic regression model?,"Say for example, you include an interaction term ageXsmoking in a model predicting heart disease as an outcome, and it turns out to be statistically significant, how would you interpret the coefficient if it is negative? I understand that it would mean that the effect of one of the variables within the interaction term  would decrease as the other variable increases…but how do you know which one? Does it go both ways? 
Hope my question makes sense.

Edit: thanks so much for your responses everyone, I will take a look through them in more depth soon and see if I understand what everyone is saying","[""Trying to interpret coefficients is often hard and confusing. It's best to make conditional effect plots and see what's going on."", ""This is the answer. Plot out the conditional (marginal) probabilities and see how they change for one predictor at different points along the other predictor. I say probabilities specifically because I recommend not even bothering with interpreting terms in odds ratios or log odds. Probabilities are much more directly understandable but you need to plot them since they aren't linear."", 'Don’t forget to consider that non interaction term’s effect on your variable. But considering the interaction term in isolation, you are correct: if one goes up, the other goes down and vice versa', 'I think you’re onto something and I would add that the younger folks that smoke could also exhibit lifestyle choices or be from families that have much higher risks associated with heart disease.', 'Interaction plots are the best way to interpret interactions in my experience. They might look a bit wonky with logistic regression, but will hopefully give you some insight on what’s actually happening.']",13,14,https://www.reddit.com/r/statistics/comments/11w68xk/q_how_do_you_interpret_a_negative_interaction/
692,2023-03-20 07:11:04,[Q] differences in mean or median between two groups that have a positively skewed distribution,"I have two ‘treatment’ groups (not paired), A and B, and I initially wanted to know if there is a statistically significant difference between the means of the group. However both groups have a positively skewed non normal distribution (A skewness 0.85, B skewness 1.50) (A kurtosis -0.13, B kurtosis 1.63) 

Google tells me comparing means is usually better for symmetric distributions, which neither A or B show. Would a Mann-Whitney U test be more appropriate than a two sample t test? However there is a lot online that says the t test is still valid if sample size (N) is above 30, (N=50 for A and N=53 for B). 

But now I’m confused if I should be comparing means or medians! Let’s say I am looking at the distance traveled by flies, A in the dark and B in the light, I have removed all 0s and I’m only looking at distance travelled if the flies moved at all. I would expect a positivity skewed distribution because not many flies would  be encouraged to move due to the experimental design. So the number of flies that actually moved would be an important part of the analysis, which the median wouldn’t consider… 

I have no idea what I’m doing, does anyone have anything I can read so I can understand things better? Is my reasoning and understanding so far completely wrong and unjustified? The more I read the less I understand statistics.","[""> Google tells me comparing means is usually better for symmetric distributions,\n\nGoogle can lead you to poorly informed resources. This is, broadly speaking, wrong. *Population* means are very often perfectly sensible things to estimate, and sample means often perfectly reasonable ways to estimate them, even in the presence of skewness. For example, consider a gamma-distributed variable; it's perfectly reasonable to estimate the population mean, and to estimate it using a sample mean, even though the distribution may be quite skewed.  Similarly consider a Poisson-distributed (i.e. discrete) variable. Again,  it's perfectly reasonable to estimate the population mean, and to estimate it using a sample mean, even though the distribution may be quite skewed.\n\nOn the other hand, if you were dealing with a symmetric variable, there's no guarantee that the sample mean is a good estimator of its center; much better choices may be both indicated and easy to use.\n\n---\n\n> Would a Mann-Whitney U test be more appropriate than a two sample t test?\n\nYou would be changing the hypothesis. With a t-test you were looking at a hypothesis about means. With a Mann Whitney, you *don't* (and neither is it a hypothesis about medians). It's perfectly possible for the direction of of a difference in either means or medians to be in the opposite direction to what the Mann-Whitney looks at.\n\nIf you were interested in a hypothesis about means, you should not change that (though you might change how you test it). If you *weren't* interested in a test of means, why consider a t-test at all?\n\nYou can't just swap hypotheses like this willy-nilly, they're asking very different questions -- and you should definitely not be changing your hypothesis *in response to [what you discover in the data](https://en.wikipedia.org/wiki/Testing_hypotheses_suggested_by_the_data)*.\n\n---\n\n> However there is a lot online that says the t test is still valid if sample size (N) is above 30, (N=50 for A and N=53 for B). \n\na lot of what you find online is abject nonsense. It's very easy to find distributions which are simultaneously good descriptions of real data, and for which this advice is clearly wrong.\n\n---\n\n> Let’s say I am looking at the distance traveled by flies, A in the dark and B in the light,\n\nWhy not talk specifically and directly about what variables you actually have, rather than what you seem to be casting as a hypothetical (if it were the real situation, it would not be necessary to open with 'let's say'), so that the particulars of the problem you invent don't accidentally lead us down a blind alley that turns out to result in poor advice for the actual circumstances.\n\n---\n\n> But now I’m confused if I should be comparing means or medians!\n\nWhat was your original hypothesis? If you were talking about distance travelled would you have wanted to look at changing means if you could? Then that's what you look at (not necessarily differences, though; ratios and percentages changes might make more sense for distance travelled than differences, since distances are necessarily positive, and are ratio variables)\n\nYou can't just treat means and medians as direct substitutes; it is quite possible for the differences to point in opposite directions. If the mean increased but the median did not, what would you want to conclude? What if the median increases but the mean did not - what would you want to conclude? What if the chance that a random value from the second group was larger than a random value from the first group was clearly over 1/2, but  medians were equal and (yet) the mean decreased? What would you want to conclude? All of these are possible, and demonstrate, I hope, that *it matters what you are talking about*, they're not just able to be freely swapped one for the other as if they're all telling you a consistent story.\n\nIf you are interested in means, the mean amount moved for the ones that do move will be well right skew; I'd used a continuous generalized linear model for that, such as a gamma GLM. That will address several problems at once.\n\n>  So the number of flies that actually moved would be an important part of the analysis, which the median wouldn’t consider… \n\nIn your earlier description you conditioned on them having moved. Now you aren't. Puzzling.\n\nIt sounds (perhaps) that what you really should be doing is looking at first *whether* they move (0/1 response) and then at how far they move when they do move, which seems to give a more complete picture. If you are interested in the mean overall (average distance moved irrespective of whether they moved), then you'd have a zero-inflated continuous distribution (for which fairly typical advice would be to fit a zero-inflated model, which corresponds to 'first model whether they move and then model how far they move for the ones that did move')\n\n---\n\n> I have no idea what I’m doing\n\nBefore you start to gather data, indeed, before you even design your study -- step 1 is to very clearly identify your specific research question. Among many things, this includes figuring out what population quantity your hypothesis is about.\n\nIt seems like you have not done this, or at least not with anything like the requisite care and rigor. This won't be your fault, it will be what you were no doubt taught. Whole disciplines are very carefully (and seemingly deliberately) teaching their students *very bad* practice under the guise of 'teaching' research/statistics -- including the practice of making hypotheses as vague as possible so you can pivot on a pin after you collect data, including looking at your data to choose your test and hence your hypothesis,  including not thinking about what you can know about your variable before you collect  data (and hence how to formulate assumptions in any reasonable way), and much else besides. In short, teaching you to replace science with some sort of cargo-cult approach to your research question, complete with a whole slew of largely useless activities and concerns that really don't relate to the problem at hand, in order to properly sanctify the study.\n\n(Just in case it is not absolutely clear, my frustration is not aimed at you. You're trying to do what you can with what you've been taught, or been able to figure out, or find. The internet - indeed even published textbooks in many application areas - can be a pretty dangerous resource unless you can already tell truth from half-truth from outright confabulation.)"", 'Anova is generally conservative with skewed distributions so a significant difference is meaningful.  However, you may increase your power with a transformation such as the log transformation to reduce skew.', 'Would an ANOVA still be valid if a levenes test for homogeneity of variance showed a P value of 0.02?', 'Generally speaking heterogeneous variance is not a serious problem when you have equal n. However, to be on the safe side you could use the Welch test that does not assume homogeneous variances.']",1,4,https://www.reddit.com/r/statistics/comments/11w0xg4/q_differences_in_mean_or_median_between_two/
693,2023-03-20 02:08:01,[Q] Random sampling and EDA,"Hello,  I have some questions about random sampling from dataset and sample size from practical point of view. I learn by myself and would like to find an  answer for a few questions. I read many articles but need some practical  point of view from experts.

Let's say I have a dataset consisting of 800 000 rows. This is my entire population (dataset). Questions:

1. I  want to create histograms showing distribution of all features. I  suppose creating histograms for  each column based on 800 k rows  doesn't make sense. So, I will most likely need to take a sample. What  number of (randomly selected) rows would be enough to create reliable  plots? It's more the question about: should plots be created on the  whole datasets (say I would have smaller dataset e.g. 3000 rows) or just  a sample?
2. As  for the sample itself - I read some articles about determining the  sample size. They usually use a formula - example here. It says, that,  given confidence level, sample size is strictly determined (e.g. 385 for  5% CL). Do you use such formulas to determine sample size?
3. On  the other hand I found a source, where there's an info about taking 10%  of the population as a sample (but not more than 1000 elements and not  less than 100). Should I follow this principle? What if I chose 2000 or  3000 elements instead of 1000?
4. What are the goals, other than hypothesis testing, of using random sampling? Is it also appliacable for simple EDA analysis?
5. As  for point 4 - I conduct EDA on abovementioned dataset. So I create  plots, summaries, data cleaning, feature engineering etc. The big  question is: should I either do my EDA on the whole 800 k dataset  (including making plots, feature engineering etc.) or, at the very  beginning, determine a random sample (say 1000 elements) and proceed  with EDA on this sample?

Thank you for your answers","[""> I want to create histograms showing distribution of all features. I suppose creating histograms for each column based on 800 k rows doesn't make sense\n\nWhy? That seems like the most reasonable thing to do.\n\nYou haven't explained anything about what these data are, or what you're actually trying to learn from them. What are you trying to do?"", 'The main reason to take a sample is because the cost (in terms of money, time, manpower, etc) of collecting or processing the full data is prohibitive. If you already have the data and the tools to process it, what do you gain from only analysing a fraction of it that offsets the introduction of sampling variance?', 'I don’t understand your concerns? Histogram bins your numbers so you are looking at far lesser than 800k rows. Let’s say you have 30 bins. Then that’s just 30 rows x 2 cols worth of data?', 'I just wanted to do EDA and was wondering if it makes sense to grab the whole dataset. For example - I need to identify data distribution fo reach feature. Taking 800 k will probably give the same result as if I to the same with 50 k rows. If I had 10 mln rows then I should take all of them to indentify distribution of given feature?']",2,4,https://www.reddit.com/r/statistics/comments/11vslzx/q_random_sampling_and_eda/
694,2023-03-20 01:52:51,[Q] Does these data look normally distributed (histogram).,"I want to analyze some data with statistical method that assumes normallity of distribution.

For each analysis (2W-Anova) i have 4 datasets of the meassurment  (4 different timepoints).

Would this data be ""acceptable"" for using in such an analysis that assumes normal distribution?The 4 datasets ""pass"" the Shapiro-Wilks test but to me they dont look so normally distributed.

[https://imgur.com/a/gV6vbaV](https://imgur.com/a/gV6vbaV)","[""No, the data don't look normally distributed.  Are you sure that Shapiro-Wilk, with a sample size of 2500, doesn't suggest they aren't ?"", '(Assuming this is a repeated-measures design) Anova is quite robust to non-normality but not at all robust to violations of the sphericity assumption. I would pay more attention to sphericity than to normality.', 'Are you absolutely sure the method assumes the data must be normally distributed?', 'Check the residual distribution', ""> I want to analyze some data with statistical method that assumes normallity of distribution.\n\nTwo way ANOVA does not assume that *any* of the variables are marginally normal, or even conditionally on only some of the conditions; it's important not to be vague about your variables nor about how the assumptions relate to them. It's the within-cell conditionals that are assumed normal, and even then only really under H0, and *even then*, you don't really need normality (it's not especially sensitive to it -- fortunately because you never actually have it in practice), and ***even then*** you shouldn't be relying on the sample you want to use in the test to tell you about the assumption (even if it were relevant to making a decision about what to assume under H0, which it probably isn't).\n\nOther assumptions will typically matter more -- including independence (which is unlikely to hold - even conditionally - with repeated measures).""]",2,16,https://www.reddit.com/r/statistics/comments/11vs26v/q_does_these_data_look_normally_distributed/
695,2023-03-19 23:42:19,[Q] What courses are a must in order to be a competitive candidate for statistics masters program?,"Right now I've taken the calc sequence, linear algebra, differential equations, couple of applied stats courses, and a intro to proofs course. Currently enrolled in intro to real analysis and probability theory. Are there any other courses I am missing?","['If you’re able to, I’d probably take at least one programming course.', 'Stats in practice is pretty much all done though code now, so that’s something that will either help you get into the program or help you after you get in', 'From someone who has a few acceptances and is in the process of deciding, that + programming experience would be great. It’s likely the applied stats gave you that, but having places where you’ve shown (outside of a course) that you can produce something by way of code is a huge plus.']",6,3,https://www.reddit.com/r/statistics/comments/11vok8u/q_what_courses_are_a_must_in_order_to_be_a/
696,2023-03-19 23:09:34,[D] on randomness and normal distribution.,"I was reading this [post](https://www.reddit.com/r/Economics/comments/11vcwia/eli5_why_does_wealth_distribution_follow_pareto/) and some users are stating thinks like :

"" you wouldn't expect a *normal* distribution unless you believed that wealth was distributed *randomly*""

""Normal distributions tend to arise from randomness""

""random distribution of wealth would give you a flat distribution though...

the normal distribution is non-random, with the central parts being favored...""

""If you take each dollar and randomly assign it to a person until you run out of dollars, it'll be normal ""

So here is this idea that randomness equals normal distributions, if not always, most of the time. This led us naturally to the question of what is randomness, and as far as I remember in probability we studied in depth the random variable concept which in the end led us to the density and distribution function. 

I remember as if it were yesterday a one shocking example that my teacher gave us back in the day:

""a constant is a random variable, it has density function of mass one in the value of the constant."" It has expected value (the constant) and variance as well (0).

Taking all this into account, behind randomness is a density/distribution function always, so things like a uniform distribution, a normal distribution or a pareto distributions, they are all describing different kinds of ""randomness"". I remember that I was talking about this with a partner in the university and in Spanish we have two words that seems to be translated the same way in English: ""azar"" and ""aleatorio"". Some one said that ""azar"" is describing a phenomenon with a uniform distribution whereas ""aleatorio"" is more close the the random variable concept.

by the other hand, this discussion leads to an other interesting question: how normal distributions arise in nature? I was surprised to find this [answer](https://www.quora.com/How-do-Gaussian-distributions-a-k-a-normal-distributions-arise) in quora, in particular:

""The Gaussian distribution is singled out by the [Principle of maximum entropy](http://en.wikipedia.org/wiki/Principle_of_maximum_entropy) as the unique maximum entropy distribution with a fixed mean and variance""

I think this could be a very important fact in explaining why some natural phenomena tends to behave as normal distributions.

Anyway. I appreciate your feedback and thoughts on this.","['You have a lot of quotes that don\'t make sense. They\'re misusing the terms ... but  also not properly defining their intended meaning in its place, leading to a lot of miscommunication.\n\nYou have different people using the same word to literally mean different things. \n\n> "" you wouldn\'t expect a normal distribution unless you believed that wealth was distributed randomly""\n\nThis doesn\'t make sense to me; unless they mean something very unusual by the word *random* that doesn\'t work. \n\nI did wonder briefly if they were thinking about a broken stick model of allocation, but that produces a fairly skewed distribution, not a normal distribution.\n\n> ""Normal distributions tend to arise from randomness""\n\nNo, at best only very specific kinds of randomness. You have to define a random process (and a resulting variable to look at) that would lead to a normal distribution. \n\n> ""random distribution of wealth would give you a flat distribution though...\n\nThey mean *uniform*, which is not what *random* means. I don\'t think that\'s what was meant by the first mention of *random* above -- though it\'s not clear what the exact intent was then either.\n\nAgain, they would have to define what scheme this \'random distribution\' should follow, a method for actually distributing this wealth so that we could see. I can think of ways to get an approximately uniform distribution across people.\n\n> the normal distribution is non-random, with the central parts being favored...""\n\nAgain, misusing the term \'random\'\n\n> ""If you take each dollar and randomly assign it to a person until you run out of dollars, it\'ll be normal ""\n\nIt depends on what they mean by \'it\'. If you look at the distribution across people, that won\'t be normal. If you pick a *person* and look at how many dollars that individual got, that would be binomial, which with enough dollars in the original pool (many more dollars than people) would at least be approximately normal.\n\n---\n\n> So here is this idea that randomness equals normal distributions, if not always, most of the time.\n\nSome people expressed that idea, others expressed the idea that it meant uniformity. Either way, they\'re all wrong, that\'s not what *random* is. You could assume some particular scheme of random allocation that yielded different distributions (and even within one scheme, what distribution you\'re talking about depends on what you\'re looking at)\n\nLet me give you an example of that last thing. \n\nA *Poisson process* is a mathematical description of a particular sort of random event that occurs in time or space; depending on what you look at, you may get Poisson (how many events in this amount of time), exponential (how long until the next event), Erlang (how long until the kth event from now), uniform (what\'s the distribution of events across time  during this fixed period of observation), etc etc.\n\nSimilarly with a random Bernoulli process, you could observe the Bernoulli distribution, or the binomial, geometric, negative binomial, etc. depending on what, exactly you\'re looking at.\n\nThey\'re all *random*.\n\nUntil the people conversing can agree to use the terms in the same way (and for that, I very strongly suggest sticking to using the statistical jargon in its conventional way), there\'s no point in them conversing at all, they\'re simply talking past each other, wasting perfectly good oxygen.\n\n\n>  that seems to be translated the same way in English\n\nOnly because you are reading things written by people that *don\'t* understand the distinction. \n\n*random* and *uniform* are very distinct concepts in statistics. English statistics class is not different from Spanish statistics class in that.\n\n It\'s just that the people you\'re quoting are simply misusing ""random"" in a variety of ways.', ""(Approximately) normal distributions tend to arise in nature when you're looking at a quantity that gets contributions from many independent random sources, and none of those sources dominates the others.  A very good description of how this arises is in this playlist:\n\n[https://www.youtube.com/playlist?list=PLzk87GkUC3kbm-MktEVTzZDoPovmKIlpT](https://www.youtube.com/playlist?list=PLzk87GkUC3kbm-MktEVTzZDoPovmKIlpT)\n\n(The first 3 videos are especially relevant to your question.)\n\nThe videos in this playlist are describing why measurement errors tend to be Gaussian.  If you have many independent sources of measurement error, some of those sources of error will push your measured value above the true value of the quantity, and some will push it below.  When you add them all up, the sum tends to look like a Gaussian.  (Although this video series also shows you how that can fail.)\n\nIt helps a lot if you understand a basic random walk first, which is the topic of the first video in the playlist.  The first three videos take you from that to Gaussian errors pretty quickly.""]",1,2,https://www.reddit.com/r/statistics/comments/11vnsss/d_on_randomness_and_normal_distribution/
697,2023-03-19 20:06:15,[Question] Data with python. Where do you start if you have experience only with SAS and R?," 

Hello statisticians and python-enjoyers,

I am a statistician in the medical\\pharma field and in my everyday job i only use SAS. In my university's experience when i was studying biostatistics, i also used R and i liked it, although it was profoundly different from SAS.

Days ago i have received a job offer as data analyst that requires python programming. They know that i have never written a single line of code in python, but they are really interested in my experience and they made an offer. Since i don't want to accept with 0 experience with python and python's environments, i write this post. Where do i start? What do i install? Is there some fancy environment for programming like RStudio for R?

Just write me what do you think i need to know to start practicing with python.

Thank you very much for your time and patience.","['I was in a similar position a few years back, but had even less modeling experience. I would download Anaconda, open the Spyder IDE, import pandas and  statsmodels, and try and replicate the output and figures that you made in R/SAS for a linear regression. Just use some simple dataset in csv that’s commonly used for linear regression examples. \n\nIt helps me to compare my own code side-by-side for simple cleaning tasks and then a simple model fit. And I’ll start figuring out how to extract coefficients, plot the coefficients, etc.', ""IDE wise someone already mentioned Spyder Spyder is more or less an Rstudio clone for python data science. \n\nPycharm is also a good IDE more focussed on software development than on data science.\n\nBut if you already use Rstudio, just use Rstudio, it has added lots of Python support.\n\nLibrary/Environment wise: start with anaconda, it the easiest to start and has the most important datascience packages packaged. \n\nStart with some basic data handling and plotting with pandas and matplotlib or seaborn (seaborn is a nice plotting library with a similar approach as ggplot). \n\nAlso try numpy, statmodels and skikit-learn for numerical math, statistics and machine learning.\n\n(That was more or less the syllabus of the introductory python course in my statistics bachelors program. We also started from a similar background, everyone familiar with R and a little bit of stata knowledge. And I think it's a quite useful foundation for further learning.)"", ""I started with *Learning Python the Hard Way* and would highly recommend it. I love Anaconda, but I disagree with it being a good place to start given that it abstracts away a ton of things you're going to need to learn how to do independently."", 'If you know R then learning python should be relatively easy. The languages are very similair imo.', 'you write your code in r then ask chatgpt to translate it to python 5head']",55,32,https://www.reddit.com/r/statistics/comments/11vjnas/question_data_with_python_where_do_you_start_if/
698,2023-03-19 20:02:34,[Question] What's the Best Approach for Modelling Movie Reviews?,"Basically, it's a y is in [0,1] problem.

I've looked this up on stack and I'm currently reading [this vignette on Beta Regression as result](https://cran.r-project.org/web/packages/betareg/vignettes/betareg-ext.pdf) but I'm wondering what my other options are. This is particularly important because the vignette assumes a greater competence with the mathematical aspects of statistics than I personally possess. If indeed beta regression is my best option, does anyone know a good free resource pitched more towards doing rather than understanding?

Beta regression does seem reasonable-ish. The first sentence notes that we're stuck in (0,1) but it's obviously remarkably unlikely that data which is the mean of discrete votes (typically assumed to be in {0,1,2,...,10} but arguably more likely to be in {1,2,3,...,10}) is ever going to be either 0 or 1. What I believe to be a random sample of movies from IMDB sort of has a IMDB ratings that follow a Beta, though it uses IMDB's weighted average procedure rather than the straight arithmetic mean, so that dataset is much more discrete than the tiny dataset I'm interested in (just 23 values).","[""Much more important than the functional form or distributional assumptions of the regression model are variables you're considering to predict with. The time spend trying to decipher a note on beta regression is probably better spent on thinking about why movies get better or worse ratings and gathering data this  - unless this is a homework type problem and the real goal is to understand regression models for fractional outcomes"", ""I am interested in trying to imagine what a defunct survey's results would look like if it was continued past its last instance (in late 2019). The predictors are also movie reviews (i.e. IMDB, Letterboxd, Metacritic, Rotten Tomatoes) that don't have this time restriction.""]",1,2,https://www.reddit.com/r/statistics/comments/11vjklj/question_whats_the_best_approach_for_modelling/
699,2023-03-19 20:00:00,[Q] How to gauge performance for data sets with varying ranges,"So i play this fantasy game called Dream11 with a group of friends and they don't really tally our points gained properly for the leaderboard. So we decided to rank ourselves by tallying the points ourselves.

How this works is everybody makes new team every time there's a new game, over the course of 70 games, and we get points based on the performance after every game. Now the issue with simply tallying up all the points is that some of us forget to make our team before the game, so we miss out on the points for that day. To account for this, we decided to use Average points instead as the metric. The issue now is that the range of points that can be scored in a game always varies. For example, the highest scorer in our group for the 1st game won with 412 points, however the highest scorer for the 2nd game won with 665 points while the lowest scorer for that game got 425 points (which is higher than the best performer in the previous game). What I want to do now is standardize/normalize the performances each game and have a performance index that's better than average so we can properly gauge who's performing best.

a few things I tried are:

1. Used the highest scorer's points as the base/denominator, and divide everyone's points by that base. After doing that for every game, I took an average of the calculated values over each game they played and considered that as the performance determinant.
2. Used the X-Xmin/Xmax-Xmin formula, where Xmin is the lowest in the group, Xmax is the highest, and calculated each persons normalized points, and then averaged all the points they gained for the games they played.
3. Used X-Mean/Standard Deviation, and followed the steps above of averaging all the points for the games they played.

All 3 methods give slightly different results, for some people the ranks remain same across the board but not all. I'm very unsure what the best method is or what the best way is of going about this. I'd really appreciate some expert help on this. Please let me know if i didn't explain the situation well so i can clarify things if required!",[],1,0,https://www.reddit.com/r/statistics/comments/11vjic0/q_how_to_gauge_performance_for_data_sets_with/
700,2023-03-19 17:45:42,[Q] Normal Approximation for Log Normal distribution?,"Let's say, I am trying to extract some inference ( an estimate for a quantile) from a lognormal distribution and for reasons, i am more familiar with normal distribution. Is it possible to transform a log normally distributed random variable 'x' to normally distributed (taking a Log of  'x')? .And would the quantile estimate from the  transformed normal distribution be a good approximation the original log normal distribution if scaled/transformed back?","['Quantiles are equivariant to monotone transformations. That means, for example, that the exponentiated 10th percentile of log(X) is the 10th percentile of X. So you can estimate the quantiles of the log(X) normal distribution and exponentiate them to get estimates of the quantiles of the corresponding log-normal distribution.\n\nQuick illustration in `R`:\n\n    exp(qnorm(0.1, 1, 2))\n    [1] 0.209485\n    qlnorm(0.1, 1, 2)\n    [1] 0.209485\n    qlnorm(0.1, 1, 2)', ""> Is it possible to transform a log normally distributed random variable 'x' to normally distributed (taking a Log of 'x')?\n\nYes. That's exactly how most people tend to do many calculations involving the lognormal\n\n> And would the quantile estimate from the transformed normal distribution be a good approximation the original log normal distribution if scaled/transformed back?\n\nQuantiles transform exactly, not approximately.\n\nHowever, moments don't; If X=log(Y) is normal, with mean μ then the mean of Y is not exp(μ) ... it is the median though, because quantiles transform. See the wikipedia page on the lognormal for details on mean, variance etc."", 'Not so, the reverse *IS* true. Monotonic transformation works both directions.\n\nAs COOLSerdash was already explaining (but perhaps here in a little more detail), the math.SE  post you linked to has nothing to do with transformation from a lognormal to a normal by taking logs,  but instead with the fact that if σ is very small the lognormal itself may be treated as approximately normal *without transformation*. Aside some specific purposes, there\'s no need to approximate though, since you can take logs. (The ""only 0.08"" in the post there is amusing. A 0.08 error in a tail proportion of... oh, say 0.05, for example, would be disastrous. Assuming relative error matters more than absolute error, it\'s a good approximation for much smaller σ than that case in the post, as long as you don\'t go too far into the tails)', ""Thank you for the reply. i wish to ask, is it so, with exact accuracy? As upon further investigation, it seems like an approximation which ,to be viable, needs to satisfy certain conditions, unless I'm interpreting it wrongly. Please look here : https://math.stackexchange.com/questions/525179/normal-approximation-to-the-log-normal-distribution"", 'Thank you so much, i understand now.']",2,8,https://www.reddit.com/r/statistics/comments/11vgzam/q_normal_approximation_for_log_normal_distribution/
701,2023-03-19 15:36:45,[Q] What do you ask interns to do do?,,[],0,0,https://www.reddit.com/r/statistics/comments/11veqy7/q_what_do_you_ask_interns_to_do_do/
702,2023-03-19 15:15:12,[Q]Weak Law of Large Numbers and Estimators,"I do not know if this is a dumb question but I am going to it anyway, If i understand it correctly the weak law of the large number says, that for big samples the sample mean value converges in probability to miu (population mean value) but then we learned how to get estimators for different things on a distribution, one of them being the mean value, so why do we need to do this if the mean value of the sample is already an estimator that converges in probability to the mean value of a population?

I have two theories:

1. The mean value is not necessarily 1/n of the sum of the x\_i for every distribution and that is why we need to know which one really is the estimator and that one converges in probability
2. There is not always an epsilon for every distribution that fullfils the weak law of great numbers so we need to find one that does it","[""> that for big samples the sample mean value converges in probability to miu (population mean value)\n\nClose, but not quite. Under certain conditions, in the limit as n goes to infinity, the sample mean converges to μ (mu).\n\n>  then we learned how to get estimators for different things on a distribution, one of them being the mean value, so why do we need to do this if the mean value of the sample is already an estimator that converges in probability to the mean value of a population?\n\nVery often there's a more efficient way to estimate the population mean than the sample mean. Knowing that you'll get there if only you had an infinitely large sample size is no help when you can only afford to take a sample of 8 observations; in that case you want to get as much information from your data as you can.\n\nSo *if* you have a good distributional model, you can often\\* get a more efficient estimator of the population mean than the sample mean. e.g. if you're looking at a uniformly distributed variable between two unknown limits, the average of the largest and smallest observation will be much more efficient at estimating the population mean than the sample mean would be.\n\n\\* Not always, though; if your model is a member of the exponential dispersion family (Gaussian, Poisson, exponential etc etc), the most efficient estimate of the population mean should indeed be the sample mean.\n\n> The mean value is not necessarily 1/n of the sum of the x_i for every distribution\n\nYou seem to be confusing the sample mean with the population mean there.\n\n> There is not always an epsilon for every distribution that fullfils the weak law of great numbers so we need to find one that does it\n\nIf the other conditions hold, all you need is a finite mean.\n\nhttps://en.wikipedia.org/wiki/Law_of_large_numbers\n\nhttps://en.wikipedia.org/wiki/Expected_value\n\nhttps://en.wikipedia.org/wiki/Estimator"", ""1. It's a very simple proof that regardless of the distribution, if a population mean exists the sample mean is an unbiased estimator for it.\n\nE (sum x/n) = sum (E(x)/n) = n mu/n = mu\n\nI do agree when you go through all the laws of large numbers, method of moments, etc it can feel like it really goes on and on."", ""Regardless of sample size, sample mean is an unbiased estimator for any distribution's population mean provided a population mean exists."", 'That really makes sense, thank you, I only did not understand one part, why did you say that I am confusing the sample mean with the population mean there? And to check if i understand it correctly, the mean value of a sample always converges to the population mean for big samples(a limit to infinite), but we might not have that size of samples so we need to find an estimator that with smaller samples aproximates to miu, is that right?', 'Thank you, but it is only unbiased under the limit right? for smaller samples that does not necesarily fullfil?']",1,7,https://www.reddit.com/r/statistics/comments/11vedlk/qweak_law_of_large_numbers_and_estimators/
703,2023-03-19 08:46:53,[E] Science Forum: Ten common statistical mistakes to watch out for when writing or reviewing a manuscript,[https://elifesciences.org/articles/48175](https://elifesciences.org/articles/48175),"['Taken from the author response:\n\n“This commentary is written by neuroscientists to their neuroscientist peers and their trainees”\n\nMany biologists took 1 stats course 15 years ago, and don’t have a solid stats background.  With “running stats” becoming easier and easier for the layman, a lot of these problems are easy to overlook, I have personally seen it myself in genomics/genetics.\n\nThe goal here was just to talk to other neuroscientists about what they found to be important problems with how stats are being ran.', 'Why did they write a paper that is essentially just snippets from an introduction to statistics course textbook? All they did was paraphrase all the ""ethical statistical practice"" sidebars.  Some of these are probably not mistakes, if someone is doing them too--such as p-hacking lol  it started with mistakes and ended up just being unethical', ""Why would literally anyone complain about another discipline working hard to make sure their members are usually statistics correctly? They're not allowed to work to gain knowledge about statistics? Only statisticians are allowed to do stats (the math that is literally the foundation for all other science)?"", 'I’d say because most of the time the experiment doesn’t require robost complex methods that a biologist couldn’t learn to, and because there aren’t biostatisticians hired that have the time to answer these questions for everyone.  Especially in small companies there might not even be a biostatistician.\n\nYes it’s a problem, and the authors of this paper are trying to address it in the way that they are able, by sharing their knowledge on an issue that is widespread enough that causes them concern.\n\nI’m a little confused why you think this paper is a bad thing.', ""And even if you had dedicated experts, you'd still want to know those basic things yourself so that you can ask them relevant questions instead of handing the whole ownership to them and ending up like this\n\nhttps://www.youtube.com/watch?v=BKorP55Aqvg""]",50,15,https://www.reddit.com/r/statistics/comments/11v6czs/e_science_forum_ten_common_statistical_mistakes/
704,2023-03-19 06:19:13,[S] Feedback On First Coding Project (Regression),"&#x200B;

I'm an undergrad working in an analytical chemistry lab and I'm trying to teach myself python for statistical analysis for my projects.  Recently I made my first real coding project. I've only been coding for the better part of a week so a lot of it is patched together from a bunch of random sources. I finally got my code to work and output a decent graph, but I feel like I could improve the code a lot. It would be great if someone could look over my code and let me know what improvements I can make!

An area I would like to work on is making a textbox. I can't figure out how to use bbox with strings, I keep getting errors.

Another area would be streamling this code because I feel like there's a lot of clunky junk that just bloats everything up for no reason.

Thanks :)

[Code (pastebin)](https://pastebin.com/kXJzbwi8)

[Graph](https://imgur.com/FAa22QD)",[],1,0,https://www.reddit.com/r/statistics/comments/11v2opk/s_feedback_on_first_coding_project_regression/
705,2023-03-19 06:06:14,[Q] How do I set up my linear mixed-effect model correctly in R?,"I have data on days in which the greening of trees happen across America. This includes meteorological and topography data, etc. I want to predict when the day of greening happens through a linear mixed-effect model by meteorological data and topography data being fixed effects while the states of America is the random effect. I have looked into how to conduct the linear mixed-effect model in R, and I tried to perform this, but the output looks strange. I have looked at many examples and [this](https://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet) 'cheat sheet' on how to perform linear mixed-effect models in R.

Right now, when I for example plot the relationship between relative humidity and day of greening I get strong positive relationships in many states which makes a lot of sense in this study but in my overall linear mixed-effect model I get a negative variable for relative humidity, while a simple linear regression generates a positive variable for relative humidity as predictor.

This is how my models look (in R):

`lmm.reg <- lme(doy ~ postDC15+postDMC15+postFFMC15+postCAPE15+postPR15+postRH15+postTEMP15+postWS15, data = postSM, random = ~1+postDC15+postDMC15+postFFMC15+postCAPE15+postPR15+postRH15+postTEMP15+postWS15|NA_L2NAME, method = 'ML', control = lmeControl(opt = ""optim"", msMaxIter=1000, maxIter = 1000, msMaxEval = 1000))`

Why does this happen? And, do I need to set-up my linear mixed-effect model differently?","[""I'm sorry, my phone might show it a but funky, but did you put all variables in as fixed AND as random effects?"", 'No worries. I assumed that I, by setting the random effect as: `random = 1+fixed-effects|random-effects` in my `nlme` code, I would basically find a unique slope for each fixed effect variable per state (the random effect). So I would not say I have set the fixed effect as both that and random. Or am I wrong?']",0,2,https://www.reddit.com/r/statistics/comments/11v2d1f/q_how_do_i_set_up_my_linear_mixedeffect_model/
706,2023-03-19 05:09:02,[Q] What are some must-have books for a data analyst/statistician in industry?," 

I work as a data analyst in a role where I'm routinely tasked with using regression analysis/statistics to assess a variety of business problems (I work in international development, so one example is comparing answers on surveys from people who receive services from our NGO to their broader community members to see whether their are differences in material outcomes for the two groups). Beyond this, I'm also involved in casual inference work with experiments the company is running, and I also will be doing more machine learning oriented work with potentially developing a fraud detection system. As you can tell, that's a pretty wide gamut. What books/resources would you recommend to someone in my position, particularly focusing on the practice of statistics in industry for business problems? I have an undergrad in statistics and I already own/am aware of the books on the list below:

\- Applied Linear Statistical Models (Kutner et al.)

\- Categorical Data Analysis (Agresti)

\- Statistical Inference (C&B)

\- Mostly Harmless Econometrics (Angrist)

\- Introduction to Statistical Learning and Elements of Statistical Learning

\- Time Series Analysis and its Applications (Shumway and Stoffer)

\- Statistical Rethinking

\- Bayesian Data Analysis

\- Regression and Other Stories

Thank you!","[""That's a decent set of books. Where do you feel you need some knowledge or reference material you don't have?\n\nI feel like I could suggest all manner of things you might not need.\n\nI'd probably at least add the book on bootstrapping by Davison and Hinkley though."", ""Great list, only must-haves that I would add are if you're working in their specific areas/applications, which are pretty self-explanatory from their titles:\n\n-\tTrustworthy Online Controlled Experiments by Kohavi, Tang, and Xu\n-\tForecasting: Principles and Practice by Hyndman and Athanasopoulos (free online at https://otexts.com/fpp3/)\n-\tInterpretable Machine Learning by Molnar\n\nThe above I think are useful for anyone in the field if they're working on any of those three problems/applications. \n\n For R users specifically I would add the following three:\n-\tR for Data Science by Wickham and Grolemund\n-\tTidy Modeling with R by Kuhn and Silge\n-\tText Mining with R by Silge and Robinson"", ""This book contains a lot of the kinds of insights that most analysts have to learn through experience and trial and error. It should really be way better known, it's a great resource. \n\n[Edna Ridge - Guerrilla Analytics: A Practical Approach to Working with Data](https://www.amazon.com/gp/product/0128002182/)"", 'If you work with time series data: I’m a big fan of Hamilton’s book (bible, really) on time series.', ""Thanks for the suggestion! I think the thing that I could most benefit from is a book that dives into how to best use statistics for glean valuable insights for business applications. I imagine something on the practice of statistical consulting could be up this alley. Like for instance, on a recent project (one mentioned in the post already), we had a survey dataset containing both people whom our company has served and broader community members. I conducted analysis to see whether there were differences in responses on the various questions asked from the survey participants we serve vs. the broader community members by using logistic regression (the dummy variable indicating whether or not a survey participant received services from us being the primary predictor of interest- I also included age group, gender, marital status, and education dummy variables in the regression too). Thinking through whether that was the best approach for the problem is something that I feel the more theoretic books don't provide a lot of help for.""]",90,14,https://www.reddit.com/r/statistics/comments/11v0up5/q_what_are_some_musthave_books_for_a_data/
707,2023-03-19 03:26:19,[Q] Looking for statistics/probability books that follow the scientific literature/divulgation genre.,Hey! I'm just a grad student looking for ways to fall even more in love with my bachelor's and I would really like to know some book recommendations that deviate a bit from technical books.,"['I have read and enjoyed Nate Silver\'s ""The Signal and the Noise"", Judea Pearl\'s ""The Book of Why"" and Persi Diaconis\'s ""Ten Great Ideas about Chance"". The last book is about probability and the first two are more statistics focused.\n\nBooks I want to read but haven\'t yet are Nassim Nicholas Taleb\'s ""Black Swan"" and Stephen Stigler\'s ""The Seven Pillars of Statistical Wisdom""', 'Check Jordan Ellenbergs ""How not to be wrong"", David Spiegelhalters ""The art of statistics"". Both were super interesting and easy to read, with little technicalities. The art of stats has a dedicated part with more maths at the end. Love both of these books! You can also check other books by Spiegelhalter']",5,2,https://www.reddit.com/r/statistics/comments/11uxkh2/q_looking_for_statisticsprobability_books_that/
708,2023-03-19 01:03:47,[Q] Stats test for dissertation help,I need to do a stats test for my dissertation. I have done questionnaire data. What stats test shall I use to compare gender (nominal) with multiple dependent variable in the form of ranking data (1 bad - 5 good). Any help would be much appreciated.,"['Is your ranking variable one item (question) ?  Or multiple items combined into a *scale* per se ?\n\nAre you designating one variable as the independent variable and one as the dependent variable ?', 'are you trying to do something like see if the mean answer value (y var) by each gender (x var) is statistically significant? or are you trying to run some sort of regression?']",0,2,https://www.reddit.com/r/statistics/comments/11utrmr/q_stats_test_for_dissertation_help/
709,2023-03-19 00:28:03,"[Q] Struggling with my dissertation, can anyone help?","Hi everyone, I'm currently very behind on my diss and am in desperate need of help. I want to do a Mann Whitney stats test to see if there is a difference between the amount of socialisation before lockdown and during lockdown. My data is oridinal from 'A lot (5) - Never (1)' and my sample size is 96 people. And I can't find a critical values table that goes up to 96. Any help would be greatly appreciated.","[""Well, the data aren't really independent, because the answer someone gave on the before survey is likely to influence the answer they gave on the after survey.\n\nThe dispositive question now is, Did you record the identity of the participants such that you can match an individual's before response to their after response ?"", 'Are the data paired, so that the same person answered before and after ?\n\nIn general, these tests are trivially easy to conduct with online calculators, R, or other free software like Jamovi.  This might be a option for you to pursue, especially if you have other analyses to conduct.\n\nConover, Practical Nonparametric Statistics, has an approximation for the Mann-Whitney critical values for sample sizes greater than 20 (Table A7),  Can you just confirm that you are using Mann-Whitney, and not Wilcoxon signed-rank for a paired test ?', ""One thing I realized might be unclear: Mann Whitney and Wilcoxon tests are very closely related, so R uses the same function to do both. The wilcox.test function uses the argument 'paired' set to either TRUE or FALSE to distinguish between them."", 'You should simply use software for like ‘wilcox.test’ in R for running your test. Common practice is to use a normal approximation of sample sizes are greater than 50, but the software can also compute an exact p-value on the fly. \n\nCritical value tables are mostly an artifact of pre-computer age statistics and really shouldn’t be used outside of early statistics education.', ""If you can match  peoples during to their before, it's paired so not Mann Whitney\n\nIf you go off the end of tables, either use a program or use the normal approximation (adjusted for ties)""]",2,13,https://www.reddit.com/r/statistics/comments/11usurf/q_struggling_with_my_dissertation_can_anyone_help/
710,2023-03-18 23:44:40,[Q] For those familiar with the Rethinking R package: Looking for help with specifying a Bayesian logistic regression model,"Hi all,

I'm attempting to specify a Bayesian logistic regression model using the rethinking package in R. This is for a research project I am working on. The posterior estimates from the model I fit seem very off from anything that would make sense and I'm having trouble understanding what I am doing wrong. I didn't post the results or my code here because I'm not sure if this would violate the rules. If this would be okay to post please let me know. If not, are there any forums focused on the rethinking package where I can get help? From someone new to Bayesian statistics, thank you!","[""BRMS is really easy to use if you're looking to try an alternative package."", 'Hi, I have some familiarity with that package. What are you using to fit the model, `quap()` or `ulam()` ? Feel free to message me', '[removed]', ""Thanks for letting me know! I'll give it a try with BRMS too."", ""Thanks. This was using quap(). There weren't any errors. I think I probably just did something in the model definition that I'm misunderstanding how to use properly. I'll message you with more details in a moment.""]",8,6,https://www.reddit.com/r/statistics/comments/11ursc6/q_for_those_familiar_with_the_rethinking_r/
711,2023-03-18 19:53:23,[Q] Finding raw data for data analysis course,"Hey, 
So i got asked by my prof to prepare some example data analysis (from like whiskers plots to anova) for one of his courses. 

The challenge im facing rn is finding suitable raw data that is free to use and in the domain of the automotive industry. Can be anything from questionnaires about autonomous driving to idk, just has to be vehicle related

Anyone an idea on how to find something like that? I looked at statista but they are alle paywalled. acea only publishes the results of their analysis and i couldnt find anything suitable on data.world

I would need 3-4 data sets","['There are a couple of small datasets in the base R software that you can access.\n\nIf you go to [https://rdrr.io/snippets/](https://rdrr.io/snippets/) , you can run each of these lines separately.\n\nE.g *cars* will give you the data.  E.g. *?cars* will give you the description.  The output is a little funky. You can ignore the *Examples*.\n\n*cars*\n\n*?cars*\n\n*mtcars*\n\n*?mtcars*', ""Try to look for datasets published in journals, be sure to check the SI. You can also browse journals like Data in Brief. It's open access."", '[This dataset](https://dasl.datadescription.com/datafile/cars/) may be suitable.', 'Openml.org', ""P.S. There's another one you can access with:\n\n*library(ISLR)*  \n*Auto*\n\nlibrary(ISLR)  \n*?Auto*""]",4,6,https://www.reddit.com/r/statistics/comments/11ume24/q_finding_raw_data_for_data_analysis_course/
712,2023-03-18 17:08:19,"[Q] i feel like a failure, I really need some help.","Im writing this as i have gotten rejected from all the MS stats programs that i applied to, and I suddenly feel like i don’t know what to do. I’m really passionate about Math and it has been my dream for the past couple of years to pursue a formal education in this field.

I am a computer science major with a 3.25 gpa, and have 3 years of experience working as a Quant Researcher in a hedge fund. I have some academic projects in deep learning as well. 

I applied to schools ranging from ~30th to ~170 ranked universities on QS. And have gotten rejected from all of them(around 6). 

I feel directionless suddenly and im not able to focus on anything at all.

I don’t want to give up on my dream but i feel like I already gave my best shot and even if i try next year I’m not sure what I’m going to change about my application.

I am desperate for some help, i am not even sure if this is a right sub for this but i feel like crying and hoping to find something of value. If this is not the right sub, please direct me there. Thank you in advance.","['While I don’t have advice for your next step, I still want to send good vibes. Remember, this, too, shall pass.', 'That’s most likely why you weren’t accepted. You need to retake those courses somehow (like a community college) and show you can master the material.', 'The GPA is too low to be competitive. Retake the math courses and get A grades in them. That will likely improve your chances. Source: I’m a math/stats professor.', ""Odds are the programs that you applied to require a bachelor's degree in mathematics/statistics. \n\nI understand that you are passionate about mathematics, but if these programs require formal qualification in it, it is for good reason. \n\nFor example, I have a difficult time imagining someone doing a measure theoretic probability course in a master's degree without the appropriate background knowledge (hopefully) obtained in a bachelor's degree."", 'You’re not a failure, but if you intend to re-apply next year you can ask the graduate admissions people from those courses for feedback. I think your work experience is impressive. Also worth mentioning that Masters programmes will differ in countries and institutions about their aims / goals - some are intended as pathways to academia and further learning as a continuation of your bachelors, while others are more explicitly designed for upskilling people with work experience, and a way to transition into a new field / career. You’re looking for the latter. Either way your best bet is to find people to speak with at the places you want to apply to, be upfront about your qualifications and why you want to do a master’s, and they’ll be able to tell you whether the program is right for you or not. They won’t tell you directly “don’t bother applying, you won’t get in”, but if your explain your background and ask something like “are there current students and recent graduates on the course who had a background similar to mine?” you’ll be able to read between the lines. It’s not about being a failure or success, it’s about being resilient and having the ability to deal with setbacks. All the best to you.']",65,37,https://www.reddit.com/r/statistics/comments/11uj1j9/q_i_feel_like_a_failure_i_really_need_some_help/
713,2023-03-18 14:28:13,[S] Need help with figuring this out,"I am trying to create a data simulator for a entity ( stock trades ) where each entity has a attribute called valueDate , I am expecting two input parameters

Total trades : example - 1 million
Date range : example - 02/Jan/2023 to 09/Jan/2023

I want to know how to calculate the number of trades that belong to a particular valueDate such that it roughly follows a normal distribution. 


Example : 

Total trades for 02/Jan/2023 : 10k
Total trades for 03/Jan/2023 : 20k
Total trades for 04/Jan/2023 : 30k
.
.
.
Total trades for 09/Jan/2023 : 10k

These numbers should add up to the input :
 1 million",['Very unclear to me what should follow a normal end even less clear why.'],4,1,https://www.reddit.com/r/statistics/comments/11uggxm/s_need_help_with_figuring_this_out/
714,2023-03-18 11:25:18,Can you get jobs with an MS in statistics nowadays? [Q],"I’m in an MS program in statistics, and have the choice of going into a phd program. My goals are to be within the quantitative research space in finance post phd (if I go for phd). Many of these roles in banks also are allowing MS candidates (they say at minimum MS). However, outside of quant finance, and in general for statisticians going to industry, are the MS level statisticians still able to get high paying jobs? Should I consider a PhD if I want to get into industry in finance? Truthfully I haven’t really thought about doing research, but I’m wondering how much the market is saturated with MS and if a phd is needed in statistics to secure a good role these days in the industry.","['You can easily get a Data Science role if you know some programming. The DS on my team in a large tech company don’t even do machine learning, they just run regression.', 'I just hired a BS with 2 YoE for a $110k position.\n\nI’d imagine MS holders do okay.', '\\> entry level  \n\\> 5 years of experience  \n\n\nExcuse me?', 'At mine, we just do plots', 'Yes, at insurance companies, in the actuarial sciences group.']",14,23,https://www.reddit.com/r/statistics/comments/11ud3ar/can_you_get_jobs_with_an_ms_in_statistics/
715,2023-03-18 10:00:32,[Q] How to compare two means from non-normal distributions?,"First time posting here, so please let me know if I break any rules / question is dumb lol

As a part of my research, I'm analyzing two sets of ambulance response times in a state: one rural, one urban. I want to test if the urban times are longer (that is, higher). I have a list of mean rural response times (from about 63 towns) and a list of urban response times (from about 97 areas). I have found the average of both lists.

Now, I can't compare the two using the students T-test, because I can't assume both distributions are normal. Indeed, when I did a Shapiro-Wilk test on both, the rural dataset was normally distributed, but the urban one was not (it was skewed right).

How do I proceed from here? I thought Whitney test could only be used if both distributions were not normal and also skewed the same way?

Much thanks!

Edit: Thank you everyone for the responses, it helps a lot!","[""> How to compare two means from non-normal distributions?\n\nFinally! Someone who doesn't change their hypothesis to no longer being about means the second they hit a non-normal distribution. Congratulations. There is quite a lot that can be done (assuming it's needed at all).\n\nOne question though -- are there any times where the waiting time grew so long that the attempt was essentially abandoned, so that we don't know how long the waiting time *would* have been, only how long it was before the waiting ceased? (or indeed if the actual waiting time was only known to be longer than some specific time for any other reason). This situation -- where a known waiting time is less than the actual (but unknown) time it would have taken -- is called [*censoring*](https://en.wikipedia.org/wiki/Censoring_%28statistics%29#Types), specifically, such data are *right censored*. You can neither omit the censored times, nor include them as if they were the actual waiting time - such a situation requires special techniques (which are nonetheless easily dealt with in a good statistics package).\n\nIssues resulting from that question aside, there's two cases to consider, and a third resulting from those:\n\n(i) you have a plausible model, whether from theory, previous studies, pilot data, expert knowledge, understanding of similar variables, or whatever else. Response times are a decent candidate for this sort of treatment, because many similar response time data sets exist and we know things about response times -- they're necessarily positive for example, they're almost certainly going to be right skew, heteroskedastic, and so on.\n\nIn this case you can do a suitable parametric test, such as via a likelihood ratio test, Wald test, score test for example. In some cases the model might be in the exponential family (e.g. exponential or gamma wait times), in which case a generalized linear model will be straightforward. Another possibility is survival regression models which include many right-skewed distributions explicitly designed for duration data, including the Weibull, for example.\n\n But also see (iii)\n\n(ii) you don't want to assume any particular distributional model\n\n In this case you can do a form of permutation test. I'd tend to suggest basing it off the ordinary two-sample t-statistic.\n\n You could instead do a bootstrap test; this has an advantage and a disadvantage. The disadvantage is that it no longer gives you the significance level guarantee (the test is not exact, though it gets better as the sample size increases); the advantage is that you can make one that relies on slightly weaker assumptions than the permutation test.\n\n They are both similar in that they rely on a form of resampling of the data, but under different basis.\n\n(iii) the test statistic you get from an approach like (i) might also be used as the basis for a permutation test like in (ii), if you want a little extra exactness guarantee for the case where  the sample size is too small for the asymptotic distribution to kick in, or in case you want a little coverage for the possibility that the model might just be a tad off.\n\n----\n\n> I can't compare the two using the students T-test, because I can't assume both distributions are normal.\n\nThis is not necessarily so; your sample sizes are reasonably large, so a certain amount of skewness is not necessarily all that much of an issue; your significance level in particular should tend to be reasonably close (and if it is off, it's likely slightly conservative rather than anti-conservative).\n\n> the rural dataset was normally distributed\n\nNo it certainly was not. It is literally impossible for it to be normally distributed, because all normal distributions take values less than 0 with non-zero probability but waiting times can never be negative! Testing a hypothesis that is *certain* to be false is pointless, you already know H0 is false, but it's also useless, because reject or not, it doesn't tell you how much the test would be affected by the non-normality you have (probably not too badly level wise, perhaps a little more power wise).\n\nIt would be better to spend the time to investigate the behavior of the t-test under similar conditions than waste time doing a pointless test of normality.\n\n> I thought Whitney test could only be used if both distributions were not normal and also skewed the same way?\n\nNeither of those things are true. Having a normal distribution is not a problem (albeit you don't), and the *data* don't have to have the same exact shape. What you need is that the distributions would be the same *when H0 is true*. The shape could be different when H0 is false, without any necessary harm to the test whatever.\n\nThe *problem* with the Mann-Whitney test is that *it does not test your hypothesis* which was about means. You should test the stated hypothesis, not a different one. (You might be disturbed by the fact that it's quite possible for the Mann-Whitney to reject in the opposite direction to the direction the means differ in; I've seen more than one researcher in confusion over this, when they could have just tested the original hypothesis and remained unconfused.)\n\n(You mean Mann-Whitney by the way, since it comes form Mann & Whitney 1947, though Wilcoxon 1945 - and arguably some other people as well - should get precedence.)\n\n---\n\nIf it were me, I'd be inclined to use a gamma glm (barring that censoring possibility), but a permutation test should also work just fine."", 'T-tests are still quite well behaved even when data are not normally distributed. If you really are specifically interested in means, I would suggest just going with the t-test.', 'The most practical options are to stick with the T-test, which is robust to normality, or use a bootstrap approach to estimate the sampling distribution. Non-parametric tests are an option as well but they don’t test precisely for the mean.', 'There are a lot of good alternatives already describes by other people. The simplest one, in my opinion, is to use a permutation t-test. That way, you can obtain p-values robust to non-normality, while being able to measure the effect size.\n\nIn R, you can use the `perm.t.test()` function from RVAideMemoire package.', ""Thank you for all the information! I'm not a statistics person by any means (as is evident I assume) but you actually explained everything quite neatly. Your assumptions about response time is certainly true (positive, right-skewed), but let's say I were to want to go with a permutation test. As someone who's not super experienced with R, would u/civisromanvs approach below work?\n\nEdit: Realized that I didn't answer your question. The real answer is that I can't be certain, but I'm fairly sure that censoring isn't an issue here. Even if it was, the problem is that each town's mean response time is in itself a mean from a dataset, and I don't have access to those base datasets.""]",3,8,https://www.reddit.com/r/statistics/comments/11ubbyj/q_how_to_compare_two_means_from_nonnormal/
716,2023-03-18 09:02:28,[Q] Can I treat my ordinal dependent variables as continuous for the sake of a regression analysis?,"Hi all! I've consulted a research methods book by McNeil and checked on stats stackoverflow but I'm still struggling with this concept. Apologies for the rookie question. If it's too basic, please just direct me to a great resource!

I have medical data with multiple independent variables (all continuous–percentages or ratios) that I am trying to map to multiple ordinal dependent variables (questionnaire responses). 

1. Is it wrong to just run a multiple regression (unsure of linearity) and the dependent could have a number that's not possible (like 1.2 but it has to be 1 or 2)? 
2. If I suspect that some of the independent variables may overlap or impact each other, is that confounding and how do I check this between pairs of independent variables?

Thanks!","['You need ordinal regression, likely with item-varying cut points and respondent-varying propensity. See [here](https://betanalpha.github.io/assets/case_studies/ordinal_regression.html).', ""Doesn't seem unusual to me."", '1. Yes.  Just use ordinal regression, like MASS::polr.\n2. Calculate VIF on the independent variables.  VIF >10 indicates significant multicollinearity.', ""I'd be inclined to use an ordinal regression model like an ordinal logit or an ordinal probit.\n\n(It does seem ... unusual to try to predict questionnaire responses, though)"", ""Probably not, no, but there is a concern there I'd like to raise (which relates to my earlier concern)\n\nSome issues aside\\* if you're just looking for correlation it doesn't matter what direction you look in anyway. For the mediation you might need to think carefully about the distinction between what you're measuring and what you want to model. Those IVs you're thinking about might make perfect sense as drivers of well-being, but that's not what you're actually doing here.\n\n\n\\*  The problem is you're not measuring the same thing as you are interested in talking about (this seems t be a very common problem). You don't know what someone's well being is from that instrument, you oniy know what they *reported* to be their well being (and that rather different variable measured by a fairly crude instrument). If they're anything like me, that report of well-being might be very different to their actual well being. I notice a lot of people seem to be pretty poor at figuring out what condition they're in; a few years down the track they might finally start to recognize they had a problem.\n\n If you don't conflate *reported well being* with *well being*, there's no problem, but if you're going to claim they're the same thing you better have a good argument handy for that claim.\n\nDepending on your instrument, it might *perhaps* make sense instead to take your mediation arrow-diagram\n\n [IV] ---> [mediator] ----> [Well being]    (leaving out the direct arrow that you hope is null or weak for now)\n\nand add a box that allows for some distinction:   \n\n [IV] ---> [mediator] ----> [Well being] ----> [self-report of well being]\n\nThe problem is, of course, that Well-being is unobserved in that diagram, which may require some additional techniques (or additional argument, such as why the other arrow might be drawn to that last box instead)""]",2,9,https://www.reddit.com/r/statistics/comments/11ua1uo/q_can_i_treat_my_ordinal_dependent_variables_as/
717,2023-03-18 07:02:40,[Q] GLMM and correcting for type 1 error?,"Hi all! Would you recommend doing Bonferroni correction, or holm-Bonferroni (sequential Bonferroni) correction when multiple GLMM were run for the same experiment?

I have two experiments, A and B.  
*A:* Ran **two** GLMMs, one for each response variable. Both results show significance in my interactions, so I did a-priori custom bonnferonni post-hoc to determine where my differences were.

*B:* Ran **three** GLMMs, one for each response variable. All three results show significance in my interactions, so I did a-priori custom bonnferonni post-hoc to determine where my differences were.

the question is, how should I determine significance? alpha = 0.05 across the board for both A and B or for A: alpha =0.05/2 = 0.025 and for B: alpha = 0.05/3=0.016",[],5,0,https://www.reddit.com/r/statistics/comments/11u6zml/q_glmm_and_correcting_for_type_1_error/
718,2023-03-18 05:28:51,[Q] How should I address these outliers?,"I recently finished collecting data for an experience sampling study about emotions and social interactions. In the study, 70 participants answered 4 surveys per day for 10 days. During each survey they were asked questions about specific social interactions they had in the time since the last assessment One question, *which I included purely for descriptive purposes and is not a part of any of my analyses,* asked participants to report how long the interaction lasted in minutes. Several of my datapoints include absolutely wild values that make no sense (interactions lasting several hours or even days). Despite this, there is no indication that my participants gave false or invalid responses. I also strongly prefer to alter data to the next highest non-outlying value rather than deleting things, but I'm not sure what to do in this case.

I tried to examine histograms and boxplots, but since there are so many observations and the spread of data is so wide I can't distinguish where I should draw the line on data that is acceptable vs. needing to be altered (see here: [https://imgur.com/a/b81On8j](https://imgur.com/a/b81On8j)). And given the uniqueness of the study design there isn't literature to clarify that for me. I did calculate z scores but there are only 5 cases out of over 2000 that are >3.29, and based on that the next non-outlying value should be 600 minutes (aka 10 hours) which still does not make sense. Is there anything I can do to get a general idea of the average length of my participants interactions? I think if I could figure out how to get SPSS to list the points it identified on the boxplot in a table that would be useful, but I can't seem to find a way to do that.","['Outlier management depends on the purpose you are trying to achieve. Are you simply exploring this data or are you trying to test something?', ""Is there any way you could reach out to the participants and ask them to clarify? Maybe they accidentally entered the wrong value or they misinterpreted the question. \n\nBeyond that, the obvious thing to do would be to look at the median since it should be more resistant to the outliers.\n\nBut more importantly, if you aren't using it for any of your analyses, it's possible that it's not that important. It might not be worth your time to solve this problem."", 'This particular variable is just for descriptive purposes', 'Take a CDF, and say 90% of people take less than 5 min to complete the survey', 'Some ideas:\n\n\\- Bin the data into intervals, where the last interval I\\_c contains all observations above some threshold c. \n\nThen report the average as weighted average of  each bin weighted by what proportion of the data falls in that bin.  \n\n\\- You could report a weighted average. Where the weights come from a distribution that you consider to be reasonable. For example, let us say the median of your data is 1 hour. So take the weight of observation xi as f((xi - 60)/60) where f is the standard normal distribution.  You could vary the parameters to pick a robust weight. \n\n\\- Assume that the interactions follow some distribution. Then vary the parameters of the distribution and report the Bayesian expectation \n\n\\\\int E(x|P) f(P) dP \n\nwhere P is the parameters.']",1,5,https://www.reddit.com/r/statistics/comments/11u4e8h/q_how_should_i_address_these_outliers/
719,2023-03-18 03:10:49,[E] [C] Has anyone had any luck at the Joint Statistical Meetings?,"I am a master's student who will be graduating soon with my degree and would like to know if there were any statisticians of any field out there that had any luck getting a job after going to JSM. I've been applying to jobs and internships and they reject me left and right. I have been out of work for close to a year now with no good luck. I need some advice on what I should be doing to improve my profile as a statistician or data analyst.

I have a github with all my projects, I have writing samples, I have samples of statement of qualifications, cover letter samples, resumes I've constantly received feedback on and modify for every jobs, every few months I'm changing the by-line in my LinkedIn and improving the descriptions on the jobs I've done. The few times I haven't been automatically rejected by ATS was because there was an actual human reading my resume, but when I get to the zoom conference call stage I usually strike out. I'm told that I'm highly qualified but at this point I feel like I'm being lied to.","['What kinds of jobs have you been applying for? Which industries? Post an anonymized version of your resume or PM me and we/I can help you out.', ""The industry job I'm currently having is what I got from the JSM Job Fair back in 2019. I was a postdoc at that time. Got screening by phone then face-to-face interview during the conference. Another colleague of mine was also hired at the same occasion."", ""Have you looked at Handshake? That's a great site for people looking for internships and jobs who are about to graduate or have recently graduated.\n\nAlso, check with your college at your university.  The program coordinators and advisors in your department often have leads on jobs."", 'I will dm you. Thank you', 'Thanks for responding. This gives me hope 😭']",28,5,https://www.reddit.com/r/statistics/comments/11u0m8t/e_c_has_anyone_had_any_luck_at_the_joint/
720,2023-03-17 18:46:14,2 period dynamic panel correct standard errors [Q],"Hi there, I have 2 periods of panel data. I want to estimate an AR(1) regression on some of the variables. I know that in general, this is dodgy in panel data since errors won't be exogenous anymore. However, all the solutions I know require more periods of data. How would I correctly estimate the coefficients + SEs in this case?",[],3,0,https://www.reddit.com/r/statistics/comments/11tnkr6/2_period_dynamic_panel_correct_standard_errors_q/
721,2023-03-17 18:25:38,"[Q] Can I use weighted regression to analyze the relationship between log-transformed GDP per capita and life expectancy, accounting for country population size?","Hi everyone!

I am working with country-level data on average life expectancy, population sizes, and log-transformed GDP per capita. I am interested in using weighted regression to analyze the relationship between GDP per capita and life expectancy, while accounting for the potential impact of country population size.

Specifically, I plan to weight the data by population size to account for the fact that larger countries may have more impact on the overall relationship. I am wondering if it is appropriate to use weighted regression in this case, and if there are any particular considerations or caveats that I should be aware of when doing so.

Any advice, suggestions, or resources on this topic would be greatly appreciated. Thank you!

Edit: deleted logistic, I meant log regression, yet log regression is basically linear regression with log transformed variables, which is why I deleted that. Please excuse the confusion.","[""Are you simply just interested in controlling for population size?\nEvery observation is a country right?\n\nMaybe I'm missing something but why could you not simply include the pop size variable in your model?\n\nI've heard mention of random effects but my worry is that if observations and countries map 1-1, then I don't believe that you'll be able to use them as a random effect, because each group would simply be comprised of a single observation. Maybe I'm not grasping something that others are.\n\nCheers"", 'Ok. The simple answer here is ""oh yeah it sounds like you are just trying to control for population size. Include it as a term in your model and then your life expectancy estimate will be appropriatey conditioned.""\n\nThe tricky part is that the causal flow between life expectancy, pop size and GDP may play a really important role in how you approach this.\n\nIf both GDP per capita and life expectancy have a justifiable causal impact on population size, then conditioning on pop size may actually introduce some perceived association between the other two variables that shouldn\'t be there otherwise. It really comes down to how it has been established that these things affect one another, and that knowledge needs to come from outside your model.\n\nSo maybe you should control for it, and maybe you actually shouldn\'t, depending on that causal knowledge.', 'Not a statistician and I may be misinterpreting your study, but here are my thoughts. \n\nIt seems like you would want to run a linear regression with life expectancy as your response variable and population size, log-GDP, and pop*log-GDP (the interaction between the two) as your predictor variables. \n\nAre you interested in whether life expectancy differs across countries? It’s possible the model may be overfit if you include county as a fixed effect. It seems like country is only relevant for understanding how your data were collected. You may want to consider a mixed effects model and include country as a random effect. This should hopefully incorporate any variation unexplained by your predictors.', 'Hey! I want to weight the countries according to its population size. A large country should have more influence on the model. I regress Life expectancy on log gdp per capita. But hence china has 1 billion inhabitants and Germany 80 Million, I want that China has more say in the model. I thought weighted least squares regression could be a tool, but I only found that this is to be used for heteroskedacity problems.', 'Hi, thanks for your response! do you have some good resources for learning mixed effect models?\n\n\nEdit: \n\nMy idea is just to see how much life expectancy increase if gdp per capita increases. You are right, that data was collected country wise.']",1,9,https://www.reddit.com/r/statistics/comments/11tn6sb/q_can_i_use_weighted_regression_to_analyze_the/
722,2023-03-17 18:25:04,[Q] Small sample size,"Can I run a linear regression with only 16 cases and 4 predictors? If not, what is the best way to establish correlations in such a small sample?","[""Yes, you can. For small sample sizes an alternative is to have strong priors. This will depend on your domain knowledge and problem you're trying to solve though."", ""Is it possible to observe more than the 16 data points currently available? \n\nThere isn't enough information and context to provide a suitable answer to your question. A number of scenarios and use cases exist where statistical inference is made on with minimal specific data. But in each of those domains, appropriate methodologies are used in light of the constraints, and it's never a one size fits all exercise.\n\nWith that being said, 16 observations and 4 covariates for an OLS regression? Don't"", 'Of course you can. People do this all the time with designed experiments. Just keep in mind the assumptions of the linear model and be sure to check whether they reasonably hold.', 'There is no make up for under power, as nothing really changes the fundamental issue — large standard errors. Meaning whatever resampling method OP uses doesn’t improve the validity of established correlations. Even Bayesian priors are technically from previous data.\n\nUnless those effects are very strong, e.g. temperature of oven ~ time to cook pizza. What OP can do here is to perform descriptive analyses, and possibly based on domain knowledge as well as correlation assessment between variables, to reduce the number of variables in the model.', 'Others have mentioned Bayesian modeling which is a great option.\n\nI’d also bring up bootstrapping. Both are computer intensive and both are more than just an lm command in R, but they will be your best options in making up for the low n-size.']",23,19,https://www.reddit.com/r/statistics/comments/11tn6de/q_small_sample_size/
723,2023-03-17 13:38:10,[E] Minoring in statistics,"I’m thinking of getting a minor in statistics to build a better profile for when I apply to graduate school (undecided on which career path). 

Is there anyone who is currently a math/stats major/minor that can tell me about your experience?","['I did math and stats. It is always beneficial to have some statistical fundamentals under your belt. In retrospect, a lot of math & stats majors suffered from theoretical probability & statistical inference courses. If your math maturity is not yet there,  take more applied methodology courses without harming your GPA.\n\nGreat reference: [https://www.eecs70.org/](https://www.eecs70.org/) (Fundamentals)\n\nCheck out the lecture notes and ask yourself whether you are comfortable with them or not. \n\nCheers!', 'Public health with a focus in epidemiology or biostatistics are both great fields for applied stats and interdisciplinary thinking.', 'Much appreciated. \n\nMy biggest fear is that I might not do so well in upper division stats classes. Despite that, I took a look at the site you provided. For the most part, I’m sure with continuous practice and actual study time on my end, it would not be so bad. It seems like reasonable material one can retain. \n\nI just always had this assumption that one either has to like numbers or be naturally gifted in math to even consider majoring in mathematics or statistics.', ""I did a study it for a while the exact combination and i found it to be much too theoretical to my liking. The whole thing seemed to be a funnel into academia later on which was not my end goal. I wanted to solve problems in industry and the major focus of the math/stats programme was to turn stats interested students into great problem solvers for specifically the esoteric flavor of problems that occur most often to PhD / postdoc students. YMMW but i'm enrolling in an applied math course instead and hopefully that will be a better fit."", "">For the most part, I’m sure with continuous practice and actual study time on my end, it would not be so bad. It seems like reasonable material one can retain.\n\nHaha, I am glad you found it useful, but don't post this in the Cal sub, each year students are screaming about how cruel this course is :). I think the best shot would be to talk with the program coordinator. The requirement for minoring would not be as strict as double majoring in stats. \n\nFrom my experience, one must have the passion to develop intuition over time in statistics. Honesty, statistics is much harder than one might think of it. The deeper you dig, the more theoretically and philosophically challenging it will get, and you need to be persistent enough to power through them. I wouldn't say everyone can do well / have fun in statistics, but I would like to encourage you to explore the possibilities. Cheers!""]",12,11,https://www.reddit.com/r/statistics/comments/11tiav1/e_minoring_in_statistics/
724,2023-03-17 08:25:12,[S] Why does alpha_results$std.alpha not work in R programming?,"Hello r/statistics community, posting here for the first time!

I just need some help, I've already successfully performed cronbach's alpha, and ran a bunch of them. In an effort to see only std.alpha values, I decided to use the operator ""$"" pulling just that in the output. However, all it returns with is NULL.

*Call: alpha(x = alpha\_results)*

&#x200B;

*raw\_alpha std.alpha G6(smc) average\_r S/N   ase mean   sd median\_r*

*0.87      0.87    0.87      0.46 6.8 0.018 0.66 0.33     0.48*

&#x200B;

*95% confidence boundaries*

*lower alpha upper*

*Feldt     0.83  0.87   0.9*

*Duhachek  0.83  0.87   0.9*

*> alpha\_results$std.alpha*

*NULL*

Does anyone have any idea how to do this?? Thank you!","['The $ operator is how you pull things out of named lists (or derivatives of them like data frames). Check the object type of alpha_results using class() and then try to figure out how to index that. (You might be able to coerce it to a data frame but I have no clue)', 'The psych package `alpha` function (is that what you used?) stores those results in the total component. You can often see learn the output structure quickly if you call `str` on the output. \n\n    results <- psych::alpha(dataset)\n    results$total$std.alpha', ""Thank you! That makes total sense, I knew something wasn't right. With a little bit of help from ChatGPT I was able to coerce it into a data frame and then was able to pull from it! \n\n`alpha_results_df <- as.data.frame.matrix(alpha_results$total)`\n\nIn case anyone else is interested in how to problem solve this!""]",0,3,https://www.reddit.com/r/statistics/comments/11tbj70/s_why_does_alpha_resultsstdalpha_not_work_in_r/
725,2023-03-17 06:31:29,[Q] What test of statistical significance and post hoc test are best for determining if one UX design is preferred over another design. Design A vs Design B vs Design C.,"I’m used to working with more complicated statistical methods, ratings, etc….and simple categorical data is breaking my brain. Please help.

Edit: the order in which the various designs are presented have been counterbalanced to minimize order effects, but the final question asks which design do you prefer A, B or C (while all 3 are visible on screen)? They choose one option. 

What is the best way to show customers 3 potential versions of a UX, and then gauge preference quantitatively for statistical significance?","['What data do you have?', 'If you have numerical ratings of each design by participants, you can do ANOVA with pairwise t tests for means. If the same participants rated multiple designs, you can use a linear mixed model with fixed effects for design and random effects for participants.\n\nIf each participant viewed all three designs and picked a favorite, you can use a chi squared goodness of fit test against the null hypothesis that all three designs are equally preferred (or whatever null hypothesis you want).', 'There are a few things you can do depending on what you are trying to accomplish.\n\nIf all you want to do is ask ""is some design preferred more or less than the others"", you can do a chi square goodness of fit test with a null hypothesis that all three are preferred equally.\n\nIf you want to estimate the proportion of people in the population who prefer each design. You can make confidence intervals for each proportion, ""prefers A"", ""prefers B"", and ""prefers C"".', ' Is it randomised and controlled?', 'I only captured their favorite choice for each person. So in excel, if they selected “C” as their favorite, I coded it as:\nUsername (Tom653), A (0), B (0), C (1)\n\nAnd I have that for 200 participants.']",1,8,https://www.reddit.com/r/statistics/comments/11t8hgd/q_what_test_of_statistical_significance_and_post/
726,2023-03-17 06:18:11,"[Q] Curve fitting at the ""edges""","I am fitting a Gaussian curve to a series of values (using Excel).

Let's say this is my data:  { {1,0}, {2, 20}, {3, 74}, {4,96}, {5,65}, {6,14} }

A value of X=0 can never happen, and a value of X>=7 can never happen.  That's the root of my question.

I calculate the Gaussian parameters \`A\`, \`mu\`, and \`sigma\` using Excel trying to minimize the sum of squares of the difference between the value the formula yields and the value in my data.

For example, Excel figures out that the A=99.78, mu=3.92, and sigma=1.10 for the data above.

&#x200B;

The issue is that that curve will have a non-zero value for X=0 (0.18), for X=7 (2.0), X=8 (0.11), etc...

&#x200B;

In general, how do you handle data at the ""edges"" of what you measure?  How is this sort of thing done properly?

&#x200B;

I'm thinking about the Chernobyl thing where they didn't think the radiation was too bad because the meters they were using didn't go very high...","['Constrained and penalized optimization is the general set of keywords relevant for this kind of thing, **but** what you are asking for is inherently impossible. The gaussian curve is strictly positive across the entire real line. It will never exactly reach zero at any point.', ""I am saying they are simply not well defined. Dirac functions are not actual functions that exist. When we use the Dirac function in calculations we're using it somewhat as an abuse of notation. We use it as a syntactic construction and algebraically manipulate it as if it's a function, and the calculations usually work out. However, the actual meaning of what we're doing has to be kept in mind to make sure everything is actually correct. E.g. we might be using dirac functions as a means of compactly working with mixtures of different measures and referencing the densities of these measures. The Dirac density is a fictitious density associated with a Dirac measure."", ""> A value of X=0 can never happen, and a value of X>=7 can never happen\n\n> The issue is that that curve will have a non-zero value for X=0 (0.18), for X=7 (2.0), X=8 (0.11), etc...\n\nOf course. If you want a smooth curve that goes to zero outside a finite interval fitting a Gaussian is definitely wrong, because that never goes to 0\n\n> How is this sort of thing done properly?\n\nBy not fitting a curve that doesn't match your constraints in the first place?\n\nOn what basis did you pick a Gaussian? Presumably not theory if you know things that rule it out altogether.\n\nbesides the curve going to zero outside that interval, what are the actual requirements on your function?\n\nWhy does ordinary least squares make sense here? I doubt it makes sense for the noise to be the same for y=0 as it does for y near 100.\n\nWhat are the response values? Are they counts, for example?"", ""The values are, indeed, counts.  It is impossible to count something for bin zero or bin 7 or more, since there's no way to get a count there.\n\nI picked a Gaussian because my initial data looks like a bell curve.  As you and the other person have noted, this is not correct.\n\nMy question comes from just wondering how, for something that matters (not my application), this sort of thing would be handled."", ""If you are insistent on using a gaussian fit, you can just clip the predictions to zero beyond those points. If they're impossible you shouldn't ever be generating predictions outside of those points in the first place though. If you are doing this you should go back and re-evaluate if you are doing something incorrectly, framed the entire question incorrectly, or otherwise need to adjust something else. \n\n&#x200B;\n\nThe other better alternative is to abandon the gaussian curve, since it inherently violates the basic constraints of your problem.""]",0,12,https://www.reddit.com/r/statistics/comments/11t84mb/q_curve_fitting_at_the_edges/
727,2023-03-17 05:26:23,Seeking Advice for data analysis with my quality project [R],I am a medical laboratory science student doing a quality project. In my project my question is: Is it valuable to monitor patient data over time for critical care patients? I have data from a patient that is now deceased and have it organized in an excel data sheet with established normal ranges. I have also logged the dates that all testing took place. My question is what would be the best way to prove or disprove the significance of fluctuations of patient data over time? I am familiar with basic statistics and am pretty decent at excel. Would just like to know how other people would approach this.,"['You only have one patient, and you have no patients who did not die.', ""It's hard to give a blanket answer to this other than to say that there are what is referred to as medical decision levels (MDLs) where, once you pass that, some intervention is recommended. (To be clear, I'm not a medical professional, my only experience in this is from the R&D side having developed diagnostic assays.) Other than that, it's really hard to generalize. Some things vary normally. Think about sodium levels in the blood before and after a very salty meal. Obviously they are going to change, but probably not so much that it warrants concern. On the other hand, something like cardiac troponin (an indicator of MI) is something that (last I knew) you would monitor if, for example, someone showed up at the ED with symptoms consistent with an MI. What used to be the practice is in a situation like that (again, to the best of my knowledge) you would check the cardiac troponin levels, if they're elevated above the MDL, send the patient to the cath lab. Otherwise they would be monitored over some number of hours to see if troponin levels were increasing. If the levels are increasing over time the patient probably gets admitted, if not, sent home or they look for other causes for the patients symptoms.""]",1,2,https://www.reddit.com/r/statistics/comments/11t6stj/seeking_advice_for_data_analysis_with_my_quality/
728,2023-03-16 23:42:09,[question] Expressing intuition probabilistically: comparing surprise factor between three boxes in a guessing game,"Suppose you are playing a guessing game where there is a box with three marbles. There are three kinds of boxes. Box A contains red, green and blue marbles; Box B contains red, green, and yellow marbles. Box C contains black, white and purple marbles. You are told that there is a 60% chance that this is Box A, 20% that it is Box B, and 20% that it is Box C. 

Suppose you make a choice, and open the box. Although Box B and C are both equally unlikely, there is an intuitive sense in which Box B would be a less ""surprising"" choice than Box C, because it contains the red and green marbles (which are 80% likely to be encountered). How do I express this intuition probabilistically? Is the KL divergence between my marble color beliefs before and after opening the box smaller for Box B than Box C? How do I express this?

I know that P(red AND green AND yellow) = P(black AND white AND purple) = 20%.",[],3,0,https://www.reddit.com/r/statistics/comments/11sxmue/question_expressing_intuition_probabilistically/
729,2023-03-16 23:19:51,"[Q] Linear Algebra, What is it for?","For some background, I started taking ap stats my junior year in high school about 4/5 years ago and now i’m almost a junior in college with it being my major. I took linear algebra last semester and went through it with flying colors due to an amazing professor. 

All that being said, why exactly is linear algebra used for? Right now I’m in a stat class where we’re using R for ANOVA, simple, and multiple regressions. What exactly am I able to do with linear algebra in statistics to draw parallels in my learning?","['Well, almost everything related to regression and machine learning is basically some flavor of linear algebra under the hood, and that’s two reasonably broad categories. One example is the Moore-Penrose pseudo-inverse, what is it and why is it useful? What even is an inverse? Linear algebra is quite useful to answer questions like these.', 'How do you express your least squares estimates for multi-variable regressions? \n\nHow do you express the joint distribution of two or more random variables? \n\nHow would you minimize the likelihood function of any data that has more than one data point and more than one parameter?', ""The common theme here is that linear algebra provides a systematic language that allows you to express questions in multivariable statistics. Irrespective of your statistical prowess, without linear algebra you wouldn't be able to express yourself and you will find it very difficult to understand others."", '[removed]', 'me not knowing how to exactly answer these questions lets me know I have a lot of reviewing to do...']",6,18,https://www.reddit.com/r/statistics/comments/11sx1w5/q_linear_algebra_what_is_it_for/
730,2023-03-16 21:58:12,[Q] I need help! Possibility of an draw in a game of 2 players Yatzy?,"We had a 264p draw with my friend, it has been bothering me to know the possibilities of it happening. Thanks Reddit!",[],1,0,https://www.reddit.com/r/statistics/comments/11suxlv/q_i_need_help_possibility_of_an_draw_in_a_game_of/
731,2023-03-16 21:02:17,[Q] Standardization vs normalization and other techniques,"Standardization vs normalization

Good evening, I would like to have some insights or resources about which is better and in which context. Also I would like to know about more techniques and compare them, like logarithmic transformation, for instance.

Thank you in advance.","['Please edit this clarification into the question', 'My understanding of standardization is with location-scale family distributions, you can find probabilities of any combination of location scale using the standardized combination of location and scale. The same logic for a Z~normal(0,1) distribution being equal to (Y-Theta1)/Theta2 where Y~normal(theta1, theta2^2 )\n\nThere are other location scale families besides normal. Not all distributions are location scale distributions. \n\nI bet there are others that can comment more confidently on this topic. \n\nFor normalization, I’ll let someone a bit deeper into their education give a response.', '""normalization"" is an overloaded term (it has at least three distinct meanings in common use). Which sense of the word to you mean? \n\nAre you talking about scaling values to be on the unit interval?', 'Thanks :)', ""Yes I'm referring to that: (X-min)/(max-min)""]",2,6,https://www.reddit.com/r/statistics/comments/11stjoe/q_standardization_vs_normalization_and_other/
732,2023-03-16 20:36:58,Accuracy of 3Blue1Brown’s video on Central Limit Theorem? [Q],"For those who haven’t seen it yet: [Link](https://youtu.be/zeJD6dqJ5lo)

Since the CLT comes up often in this sub—particularly regarding how it’s taught—I’m wondering your impressions on the video’s accuracy.","[""There wasn't anything wrong with what was said.  But none of the questions it asked were answered.  Apparently there is another video (or perhaps more) that will try to answer.\n\nI don't think he actually can answer those questions at the youtube level, but we'll see.  Somehow he has to prove the characteristic function uniqueness theorem, and I don't see how he is going to youtube that."", 'I watched this. I have zero bones to pick. Some of it is slightly imprecise, but he explicitly _tells you_ that it is imprecise, and to a significant degree, improves the precision later on.\n\nHe did a good job.', 'Well, to each their own I guess. You clearly seem to be in the minority though, the vast majority of people who are exposed to his content clearly find it to be very helpful. I have a masters degree in math and stats and find myself learning new stuff from his videos periodically.', ""Watched it. I've watched all his videos and interviews and he seems to be very passionate about statistical integrity. Because of how deep the topic was he seemed to be focusing a lot on describing the behavior of CLT, saying he'd dive into the explanation in a follow up vid. I have no clue how he'll communicate it to his wider YouTube audience"", 'Theres also a nice proof by Steins method but that too will be too much for a YT video.']",102,33,https://www.reddit.com/r/statistics/comments/11ssxzz/accuracy_of_3blue1browns_video_on_central_limit/
733,2023-03-16 16:36:22,[Q] Could you interpret these two graphs in terms of distribution?,"Hi,

There are two boxplot graphs in the google document i attached below. Could you interpret these graphs in terms of distribution? Is their distribution normal, right skewed, or left skewed?

[https://docs.google.com/document/d/1OGU2WYj8QggNE2-bTu7ZBWVZcaNWB\_DlAPrfnHWP2vA/edit?usp=sharing](https://docs.google.com/document/d/1OGU2WYj8QggNE2-bTu7ZBWVZcaNWB_DlAPrfnHWP2vA/edit?usp=sharing)

Thanks in advance.","["">  Is their distribution normal, right skewed, or left skewed?\n\nThis is homework?\n\na) there's an infinite number of distributions that are not any of these three options (distributions may be asymmetric but not so obviously skewed one way or another and typically symmetric distributions aren't actually normal)\n\nb) there's not really any good basis to assert that a population distribution *is* normal from just looking at a boxplot (very non-normal distributions can have exactly the same boxplot as a normal distribution), and little basis to talk about skewness (inconsistent or even quite misleading impressions of skewness can happen with boxplots). \n\nThe main value with boxplots is comparing location and spread across many groups, not in assessing distributional shape."", 'Do you have the data, so that you can plot histograms and compare to the boxplots ?', 'I asked because it would affect the answer I would give (if it was homework there would likely be an ""expected"" answer that would not match reasonable statistical advice, which is what I was attempting to give; I could easily lead someone to give an answer that would be marked wrong).\n\n> i have never use boxplot to interpret a data set\n\nAs I said above, it\'s very difficult to infer shape from a boxplot.\n\nYou can tell some things -- there appear to be a couple of relatively extreme observations (many IQRs above the upper quartile) in the second sample that mean that it\'s not consistent with having come from a normal distribution. The first one *could* arise from a (not very large) sample from a normal population but it could easily arise from a sample from a non-normal distribution.', 'I see. This is not a homework, i have never use boxplot to interpret a data set and i wanted to understand it. I understood the basic shapes but i couldnt understand the ones i attached to google docs link.\n\nThank you.']",0,4,https://www.reddit.com/r/statistics/comments/11so9s4/q_could_you_interpret_these_two_graphs_in_terms/
734,2023-03-16 15:06:41,[D] How much data is needed to successfully train a Machine Learning model?,"From a statistician’s point of view, how much data would be needed to (successfully, meaning higher testing accuracy than training accuracy) train an ML model? Is there a fixed yardstick? Is there a formula for calculating this number. I being a beginner in stats myself do not know of such formulae, but I have done a lot of Machine Learning and from my perspective, to “just” train the model the input size would have to be greater than 1, but to successfully train it, the input would have to be great in size and diverse in variety, but is there a fixed number and a formula for calcifying such number. Suppose, for example, a neural network to recognise emotions from faces, and a total of 7 emotions.","[""> successfully\n\ndepends what you mean by 'successfully'\n\n> an ML model\n\ndifferent models require different amounts of data to achieve good performance\n\n> Is there a fixed yardstick?\n\nNo"", 'then it will depend on the model and problem, there is no fixed answer.', 'That almost never happens unless the test data is much “easier” than the training data and that would obviously be problematic.', 'If you get a higher test score than train score your test dataset distribution is not representative.', ""I think you're missing the point. It really depends. As a general rule of thumb, you need at least 10 observations for each estimate for linear regression. If it's a simple linear regression, you're estimating the slope and intercept. Thats 20. \n\nDifferent methods are more sensitive to sample sizes. Different datasets can represent the relationships with less error. Asking general questions like this is not helpful, because there's not a general answer for all datasets and all methods.""]",0,15,https://www.reddit.com/r/statistics/comments/11smv1t/d_how_much_data_is_needed_to_successfully_train_a/
735,2023-03-16 11:26:51,[Q] significant between subject factor in mixed ANOVA,"Hi! Sorry for potentially rookie question, but I just wanted to do a little sanity check.

Let’s say I am running mixed ANOVA with within subjects factor (repeated measures, intervention 1 and intervention 2) and a between subject factor (Gender). My results show no main effect of within subject factor (Intervention ), nor an interaction with gender. However, gender by itself is significant. My question is, how should I interpret this significant effect of gender? My understanding is that averaging for each subject across levels of within subjects variable (I.e., (Intervention 1 + intervention 2)/2), there are differences between levels of between subjects variable, between men and women in our case. Is my understanding correct? Thank you!","[""That's up to OP to determine, not us. We don't know any context here. It might very well be interesting to their work."", ""Of course. But it's not relevant here as it's not significant."", 'Exactly, the lack of interaction could be extremely important in some contexts.', 'Yes - the model indicates there are significant differences between genders. Since there is no significant main or interaction effect for intervention this difference can be attributed to gender alone (with respect to the model).', ""That's right, though the main effect in a RM design is often not very interesting. The interaction is usually what we care about. Do the interventions affect change over time? For example, do those who consume protein powder after workouts gain strength faster than those who don't?""]",2,6,https://www.reddit.com/r/statistics/comments/11siuc2/q_significant_between_subject_factor_in_mixed/
736,2023-03-16 09:32:08,[Q] Should I take Linear Algebra or Sets and Logic (or both?),"Hi! I have to sign up for my Spring courses in a couple of weeks and I was wondering which Math elective I should sign up for? My stats majour requires at least one math elective, but the way I've stacked my courses (making room for volunteering + double majouring) I think I can only take one along with the already required courses. I heard that Linear Algebra is pretty useful for advanced stats, but i generally suck at Algebra and I'm a bit hesitant of taking it since I want to maintain a good GPA for grad school. Thank you!","['I would take linear algebra. Frankly, not knowing the subject is a bigger impediment to pursuing graduate work than a lower undergrad GPA.', 'Linear algebra. It’s very different from high school algebra.', '“I generally suck at Algebra”\n\nBetter fix that before grad school son!', 'Linear algebra and it’s not close.', ""My advice would be linear algebra, and you don't need to get to advanced stats for it to matter -- it's fairly important for  a good understanding of regression, but also in many other things -- e.g. glms, time series analysis, survival analysis, etc. \n\nDepending on what they cover the other course might be useful, but linear algebra will be helpful in stats.\n\nIf you're hoping to go to grad school in statistics and you don't have linear algebra, they're probably going to wonder why.""]",35,44,https://www.reddit.com/r/statistics/comments/11sg533/q_should_i_take_linear_algebra_or_sets_and_logic/
737,2023-03-16 08:44:33,[S] I'm not able to install packages in R/RStudio.,"I am currently using macos Catalina. It's abundantly clear that there are issues with the the installation. For example, I had ran with:

`install.packages(""tidyverse"", dependencies=TRUE, type=""source"")`

After I attempted to install the package, I got errors such as:

**ERROR:** **configuration failed for package ‘ragg’ \* removing** ‘/Library/Frameworks/R.framework/Versions/4.0/Resources/library/ragg’ Warning in install.packages : **installation of package ‘ragg’ had non-zero exit status** \* installing \*source\* package ‘rlang’ ... \*\* package ‘rlang’ successfully unpacked and MD5 sums checked \*\* using staged installation \*\* libs xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun ERROR: compilation failed for package ‘rlang’ \* removing ‘/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rlang’ Warning in install.packages : **installation of package ‘rlang’ had non-zero exit status ERROR: dependencies ‘rlang’, ‘fastmap’ are not available for package ‘cachem’ \* removing** ‘/Library/Frameworks/R.framework/Versions/4.0/Resources/library/cachem’ Warning in **install.packages : installation of package ‘cachem’ had non-zero exi**t **status ERROR: dependencies ‘cli’, ‘rlang’ are not available for package ‘lifecycle’ \* removing** ‘/Library/Frameworks/R.framework/Versions/4.0/Resources/library/lifecycle’ Warning in install.packages : **installation of package ‘lifecycle’ had non-zero exit status ERROR: dependency ‘lazyeval’ is not available for package ‘rex’ \* removing** ‘/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rex’

Afterwards,  I tried to library the package but the error message like the one in the photo above:

**Error in library(tidyverse) : there is no package called ‘tidyverse’**

I  tried the same process with other packages like olsrr but I got the same outcome.

I would like to know how to rectify this problem.","['Blow it away and reinstall?', 'With r install issues ... you might find one of the r-specific groups (e.g. /r/rstats) helpful', 'You might want to install additional packages on your system in order to make tidyverse intallation work.\n\nFor example on my fedora machine, i need to \nsudo dnf install python3-devel openssl-devel cmake curl-devel etc...\n\nThe error message from R is quite long but it tells you what to install. You have to go up the error message. It should solve your problem', 'tidyverse has dependencies which are not being installed correctly. Start with lazyeval and see what error message you get.', 'Do you have the command line tools installed on your Mac?']",1,22,https://www.reddit.com/r/statistics/comments/11sey6t/s_im_not_able_to_install_packages_in_rrstudio/
738,2023-03-16 05:00:15,"[Q] Does a random intercept with 0 variance ""do"" anything?","Imagine an analysis that collects reaction time from participants, ten times each. The predictor is something like colour of the shape being responded to: red or blue. If there is 0 variance in the random subject intercept, is there any difference between a linear model with a random effect, and a linear model without one?","['A random variable with zero variance is constant with probability one. So a random effect model where the random effects have zero variance is the same as a linear model.', 'I think you basically have it right. Look here for a more in depth perspective.  Mostly focused on fitting your models in R but also full of good general information: \n\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#singular-models-random-effect-variances-estimated-as-zero-or-correlations-estimated-as---1', "" A random intercept model under the assumption that the variance is zero corresponds to each random intercept being the same constant almost surely.\n\nIn practice, an estimate of a random intercept model's variance being zero can arise if the conditional sample means of each individual's data points are identical. In the trivial random intercept model, where the only covariate is each individual's random intercept, then the maximum likelihood estimate of the random intercept is the sample mean of each individual's corresponding data points. If these are all identical, then the variance of the random intercepts is zero. Making inferences on such a model is identical to a model without random effects."", 'I think others have answered your question well, but just wanted to say that if I got that result the first thing I would assume is that something went wrong somewhere, because it seems very unlikely to get a variance of 0 with this type of data.', 'True in the predictions, however, couldn’t the within subject variance be greater than 0 while the between subject variance equals 0, which would at least provide some information regarding the subjects, and ultimately result in differing predictions if the error structure, and not just the mean point estimate, is used?']",7,7,https://www.reddit.com/r/statistics/comments/11s93vt/q_does_a_random_intercept_with_0_variance_do/
739,2023-03-16 03:54:13,[Q] low ppk when I transform due to non normality.,"I have a data set n=50 and need to meet a ppk above 1.33. I get a ppk over 1.59 but was told I need to do normality for Ppk. I ran AD, RJ, checked skewness/ kurtosis, and transformed to distributions with p value above .05 also was told try chevrons theory, but that doesn't work either. I can't get the normality to pass or a transformation that will give me a ppk above 1.33  with a p value above 0.05. Best I achieve is 1.15 on transformed data(largest extreme value)

Do I have any other options to get the data normal in order to accept the Ppk thats above what i want or do I need to measure more samples, and would this even work if I did or would I be wasting time.","['Note to readers of this post. If, like me, you went ""huh? what\'s this even asking about?"" I can save you some time.\n\nPₚₖ a term from Statistical process (/quality) control (SPC/SQC).\n\nspecifically see:\n\nhttps://en.wikipedia.org/wiki/Process_performance_index', ""Its really hard to make judgments without seeing graphs of the data or understanding the process. What industry are you in?\n\nEdit: It sounds like the data are skewed towards the nearest spec limit. If there is a technical reason why the data is skewed, you might be able to make process/measurement adjustments. Just re-running the process more times is unlikely to help, since the acceptance criteria is not based on a confidence interval (its based on a point estimate). The only confidence intervals I know for ppk assume a normal distribution, so a confidence interval wouldn't help to create a rationale either."", ""Thanks. Plastic injection moulding. Data appears slightly skewed, but this is due to one or two samples out on their own. Of course, the samples were scrapped after measurement, so can't check for anomolies.Take them and lovely bell-shape.\n\nYes, confidence interval same issue. It's a five up tool and no issues with the other cavities. \n\nThanks anyway.""]",0,3,https://www.reddit.com/r/statistics/comments/11s7agu/q_low_ppk_when_i_transform_due_to_non_normality/
740,2023-03-16 03:30:20,[Q] Can you recommend me a good introductory course for various distribution/regression?,"First, I have mechanical engineering background but I did not take any statistic courses during college. Thus, I vaguely understand what is standard deviation but I was fine for 99% of my career. However, recently I am responsible for working on scripting Weibull Distribution and regression.

It has been very difficult to follow the concept. With lots of mentoring from my lead, I was able to somewhat understand the concept. However, I feel very lost when talking about Hessian matrix, covariance, gamma function, location, scale factor etc. There is also talk about maximum likelihood for optimization, mean versus basis, chi squared, 90% confidence etc… It is all overwhelming. These terms are all foreign to me and I am chugging along but wondering if there is a some awesome introductory class that can light the bulb in my head.

Right now, I am okay with math so I just think as the equation to solve for and know the I need to use maximum likelihood optimization because there are multiple variable to solve with not enough equations, so I have to numerically solve and MLE is the way. But I don’t know what all this means.

On top of it, I am trying to practice using Python and it seems like there is scipy package and people who built custom libraries on top of scipy that already can do what I am trying to do with Weibull distribution/regression. However, since I don’t understand all the things, I struggle to use these functions for my specific case.

Plus, is this type of work related to data science?","[""You are ranging over a fairly wide variety of topics here. You basically want a probability intro course, a statistics intro course (think Master level for both) and then Generalized Linear Models. \n\n>because there are multiple variable to solve with not enough equations, so I have to numerically solve and MLE is the way\n\nThis is not generally why you use MLE or why you solve numerically. \n\nYou solve numerically because you don't have a closed form solution.\n\nYou use MLE \n\n1) because it makes some sense, find the model that maximizes the likelihood that the model would generate the data you observed\n\n2) because it is efficient and has nice asymptotic properties\n\n3) because someone already wrote a library to solve for the parameters using MLE and not something else obscure you could dream up as a criteria"", ""You need some material in probability and mathematical statistics, plus some applied material related to what you're doing, by the look.\n\nIt may require more than one course or book to get what you need for all of those things - maybe 3 books or so.\n\nFor a mostly reasonable basic reference on distributions specifically, Wikipedia is often adequate.\n\nWhat sort of thing will you need to do with the Weibull distribution and regression? Do you mean you want to specifically fit Weibull regression? (Or something else?). \n\nWhat sort of phenomena are you modelling with the Weibull (e.g. failure times, wind speeds, rainfall etc)?\n\nAre any of your data [censored](https://en.wikipedia.org/wiki/Censoring_%28statistics%29)?\n\n> there are multiple variable to solve with not enough equation\n\nI'm not sure MLE will help there; it's useful for the opposite situation (more data than variables). If you have more variables than data to estimate them all you will either need some kind of *model* (to reduce the number of individual parameters by relating observations via the model) or perhaps  some form of smoothing or  *regularization* (to constrain the parameters somewhat)."", 'Given your background, you can probably draw a lot on linear regression and introductory statistics from  \n\n\nGreene, W. H. (2011). Econometric Analysis (7th ed). Upper Saddle River, NJ: Prentice Hall.   \n\n\nFor more extended [statistics study program](http://stanfordphd.com/StatisticsTutor.html), you would need to take a serious step inside probability theory and stochastic processes. Many modern problems in modeling and estimation involve [stochastic processes](http://stanfordphd.com/Stochastic_Process.html) / time series one way or another.', ""Thank you for detailed response. This also humbles me that I don't know what I am talking about regarding MLE haha. Looks like I gotta start slow and look into those intro classes."", ""Well, the gist is I have fatigue failed coupons and need to define a curve that fits the trend. So I can use the curve to predict when things fail when applying some certain fatigue load. So  industry standard is using Weibull distribution for a group of population. Then since I have multiple groups, there are different load levels, so I have multiple characteristic life. In order to fit the curve for those multiple lives, I use weibull regression. I think this isn't anything new, it was already researched and established like 50 years ago. But I am basically writing a script to automate the process and also be able to easily tweak things to explore different scenarios. In order do so, I would like to understand the concept under the hood.""]",1,5,https://www.reddit.com/r/statistics/comments/11s6mdg/q_can_you_recommend_me_a_good_introductory_course/
741,2023-03-16 00:53:55,[Question] Expectation of Sample Standard Deviation,"Is there a clever way to find the expected value of the sample standard deviation (S)? In particular for Xi distributed iid as N(mu, sigma^(2)), where S^(2) = \[(X1 - Xbar)^(2) \+ ... + (Xn - Xbar)^(2)\]/(n-1).

I thought there was but I'm having a hard time finding it online.

edit: Changed mu in S^(2) formula to Xbar, where Xbar is the sample mean.

**Follow Up:**

I think we have confirmed that the best way to do this with the minimum amount of knowing things beyond what most graduate level mathematical statistics courses require is to directly compute this:

Define a random variable *T = (n-1)S**^(2)**/sigma**^(2)*. It is known that this follows a *Chi**^(2)* distribution with *n-1* degrees of freedom. Then define a function *g(t) = (sigma**^(2)**/(n-1))**^(1/2)**t**^(1/2)*. Then *g(T) = S*.

Use the law of the unconscious statistician to find the expectation. From here, you can pull out the constants and rescale whats left in the integrand to match the pdf of a *Chi**^(2)* distribution with *n* degrees of freedom. The value of the integral is then 1. 

Multiplied by the constants you pulled out and the constants multiplied in order to rescale, the expectation turns out to be *sigma* x *\[ ( 2/(n-1))**^(1/2)* *G(n/2) ) / G((n-1)/2) \]*, where *G* is the gamma function. 

thanks everyone for the help and suggestions. ","['I commented snarkily too soon. The distribution of the sample variance I think might be the most reasonable way to find the expectation of the sample standard deviation. \n\nSorry for being an ass.', 'The sample *variance* is well-studied and well-documented, [Wikipedia in particular has explanations of the entire sampling distribution thereof](https://en.m.wikipedia.org/wiki/Variance) (look at the section Population and Sample Variance). From there you can get whatever you want with the usual rules for functions of random variables.', 'Not sure what you mean by ""clever"". But one way is to approximate it via simulation:\n\n1. generate Xi ~ N(mu, sigma^(2)) for i = 1, ..., N\n1. compute SD, \n1. repeat step 1-2, for large number of times, say 1000 times for example\n1. compute the mean of the 1000 SDs you obtained from step 3.', '[This simulation of mine](https://onlinestatbook.com/stat_sim/sampling_dist/index.html) could be helpful. It estimates  the sampling distribution of several statistics including the sd.', 'E(Xi-mu)\\^2 = sigma\\^2\n\nBecause of iid,  E(S\\^2) = n sigma\\^2/n-1\n\n&#x200B;\n\nUsually sample standard deviation is calculated by\n\n\\\\sum (X\\_i - X\\_bar)\\^2 / (n-1)  we use the sample mean.']",0,30,https://www.reddit.com/r/statistics/comments/11s2543/question_expectation_of_sample_standard_deviation/
742,2023-03-15 21:39:12,[Q] Mplus - Can we use multiple imputation with latent class growth analysis?,,"['I have done a SEM course with curran and he seems to use FIML.', 'Spoke to Linda Muthen who recommended FIML to avoid class switching.', 'Nothing pops up on the mplus forums?', 'Most of it is super old and showing preference for FIML, but I have a longitudinal dataset with biased attrition rates, so I think multiple imputation would be a better approach.', 'Have you looked up other studies? Are they using FIML instead of MI? Like Patrick Curran or Greg Hancock, what are they doing with their longitudinal data? I don’t have an answer for you, though. Sorry!']",4,7,/r/AskStatistics/comments/11rwoh7/mplus_can_we_use_multiple_imputation_with_latent/
743,2023-03-15 20:30:05,G*Power [Q],"So I am doing a g power analysis for sample size. I have 2 groups that I am comparing for 2 dependent variables. Do I do a separate analysis for both groups and each dependent variable, or somehow do it all in one calculation?

Thanks so much in advance","['You can perform a single analysis that takes into account both dependent variables and both groups simultaneously.\n\nTo do this, you would need to use a multivariate approach. In G*Power, you can use the ""Multivariate: Repeated measures, within factors"" option under the ""Test family"" drop-down menu. This option allows you to input the effect size, alpha level, power, number of groups, number of dependent variables, and the correlation between the two dependent variables.\n\nOnce you have entered these parameters, you can click on the ""Calculate"" button to calculate the required sample size for your study. It is important to note that the required sample size obtained from the power analysis assumes that the assumptions of the statistical test are met, and the effect size and correlation estimates used in the analysis are based on prior research or theoretical considerations.', 'f you do not see the ""Multivariate: Repeated measures, within factors"" option under the ""Test family"" drop-down menu in G*Power, it is likely because you have selected an inappropriate test family.\n\nFor a power analysis involving two groups and two dependent variables, you should select the ""t test"" option under the ""Test family"" drop-down menu. Once you have selected this option, you can choose the ""Multivariate tests"" option under the ""Type of power analysis"" drop-down menu. This will enable you to perform a multivariate power analysis that takes into account both dependent variables.\n\nIn the input fields, you can specify the effect size, alpha level, power, number of groups, number of measurements, and correlation between the dependent variables. Once you have entered these parameters, you can click on the ""Calculate"" button to obtain the required sample size for your study.', 'Under the test family drop down I only have exact, f, t, x2 and z tests options', 'Okay thank you so much', 'For one variable the g power said sample size should be about 180 for one variable and the other was like 11000 so I think I did something wrong']",5,10,https://www.reddit.com/r/statistics/comments/11ruykb/gpower_q/
744,2023-03-15 11:03:14,[Q] What does it mean to say that a distribution is skewed to the left?,"I have a poll running on Twitter ([https://twitter.com/AllenDowney/status/1635796990842875904](https://twitter.com/AllenDowney/status/1635796990842875904)) where I asked:

>When people say a distribution is ""skewed to the left"", some mean  
>  
>A: the central tendency is in the left side of the range  
>  
>and some mean  
>  
>B: the tail extends farther to the left side  
>  
>Which do you consider right?

The results so far are

1. A is right and B is wrong -- 46%
2. B is right and A is wrong -- 30%
3. It's ambiguous, so you have to specify what you mean -- 24%

What do the good people of /r/statistics think?

That is, what do you think the answer is, and what do you think of these results?","['Technical definitions are not a matter of general public vote\n\nRead stats books written by statisticians (which won\'t be very likely to contain the phrase ""central tendency"" at all)', ""If positive = right and negative = left then the answer is B. But I'd probably describe it in more detail or even show a picture if I really wanted to make someone else knew what I meant."", '2 and 3. And I’m not surprised by the results. Prior to learning it in school I would have said 1.']",0,3,https://www.reddit.com/r/statistics/comments/11rk81a/q_what_does_it_mean_to_say_that_a_distribution_is/
745,2023-03-15 08:53:00,Test for categorical data to determine which level is different [Q],Suppose I have the count of males and females in a dataset over 4 years. I know I can use chi squared to tell me if overall any years had a different proportion of males than other years. I also know that I can use a regression to tell me if any particular year is different from a reference year.  Is there any test that will tell me if a given year is different from all the other years?,"['Look at the Distributions, and compare if the mean/media are statically different from each other... t-test or ANOVAs should do the trick.', ""I asked chat gpt and it’s response suggested using a post-hoc test. Tukey's Honestly Significant Difference (HSD) and Scheffe's test both seemed appropriate based on the description. It’s been a long time since I took a basic statistics course, but I vaguely recall learning these, at least Tukey’s."", ""ChatGPT lied to you. Tuskey's post-hoc procedure is used after ANOVA, which, in turn, assumes the response variable is continuous. That's not the case with your data - you don't have a continuous variable, you have counts, i.e., the number of males or females by year.  \n\n\nThe easiest option is to run a chi-square test for each pair of years, and then adjust the p-values for multiple comparison. The pairs for which p < 0.05 are considered significantly different"", 'Hmm. It did say these are post hoc tests. But either I missed the part about being used for continuous dependent variables, or it didn’t mention it. Thanks for the explanation.']",4,4,https://www.reddit.com/r/statistics/comments/11rgta7/test_for_categorical_data_to_determine_which/
746,2023-03-15 08:43:22,[Q] Stats knowledge,"How can one get a deep understanding of multilevel models work? I keep being shocked by the amount of information I don't know about them and I feel like having more mathematical knowledge maybe would help?!? Like today I just realised I didn't know what the fixed effects correlations represent and have spent a bit on it and whilst it is not crucial for my work, I feel like I am always playing catch up. Does doing an undergrad or master's in stats help? I am a PhD in STEM, but not in stats. Or should I just keep adding and adding until at some point I'll know enough? I like stats btw, just have other obligations alongside learning about it.","['This is the curse of knowledge. The more you know, the more you realize what you know pales in comparison to what you don\'t know. That\'s why the old phrase ""ignorance is bliss"" is such a mainstay. If it\'s not crucial, don\'t put much thought into it. Play catch up when you need to. That\'s life - constantly learning and adapting. Nobody knows all, and that\'s the beauty of it.', '> I feel like I am always playing catch up.\n\nMe too, all the time and my PhD *is* in stats.', ""[http://www.stat.columbia.edu/\\~gelman/arm/](http://www.stat.columbia.edu/~gelman/arm/)\n\n&#x200B;\n\nI don't think knowing the detailed math helps if your goal is to work on real world models. In most cases you can look up the math and if you can understand the assumptions and the conclusion then you should be good. \n\nBut modelling isn't a science it is an art and in general you cannot find one true answer. However with practice you get better at it."", ""That's very reasonable but I still want to know everything xD"", ""An Introduction to Statistical Learning is a dumbed-down (in a good way) version of Elements of Statistical Learning. The *Intro* version offers an excellent overview of many ML algorithms, without getting bogged down in the math.\n\nIf you are *using* (as opposed to *writing*) ML functions in r/Python/etc., *Intro* is probably plenty.\n\nAll that being said, those books are about machine learning algorithms. They won't provide much in the way of elementary/applied statistics. I had stats professors at two different universities use *Statistics* by McClave & Sinich, so I'd assume it's a reasonably good text (for applied stats).""]",33,32,https://www.reddit.com/r/statistics/comments/11rgj1f/q_stats_knowledge/
747,2023-03-14 22:59:58,[Q] Is a LASSO an appropriate way to deal with predictors that sum to 1?,I have five predictors that are proportions and sum to 1. Would a LASSO be an appropriate way to approach the analysis?,"['Proportions, percentages, probabilities etc are called ""[compositional data](https://en.wikipedia.org/wiki/Compositional_data)"".  The two problems is they\'re inherently multicollinear, and that all points are constrained to [0,1]. \n\nPerhaps the simplest tool is to use some sort of logratio transformation (see link above) so you can typical linear models. There\'s a lot of literature on compositional data analysis and it\'s easy to get in pretty deep. IMO doing an additive logratio transform is a great first step and will give some familiarity with CDA.', 'Probably not. \nIf they all sum to 1 you more than likely have compositional covariates and should consider methods from compositional statistics.', ""It's the unit sum constraint that makes the variables perfectly colinear. `x_i = 1 -  (x_1 + ... + x_p)`"", ""Thank you! I was able to do this in R. But I don't find the results all that easy to interpret because I lose one predictor. \n\nIt's the reason I was drawn to the LASSO because I imagined I'd be able to include all predictors.\n\nThere is a reply below saying that I could run a simple regression and remove the intercept. Would that work just as well as a transform?"", "">the predictors being constrained to 0-1 doesn’t directly violate any assumptions of linear regression. It might lead to heteroskedastic residuals, but then I’d think a logit transform would be better than a log transform\n\nThe log-ratio transform isn't just a log. The logit transform is actually a special case of a log-ratio transformation when you have a simple proportion. \n\nFurther, while it's definitely not mathematically impossible that the linear regressions might be met on the untransformed data (other than dropping one possibly to remove collinearity), it does make the interpretation a mess. Because of the joint constraint across the covariates it's impossible to vary a single value of the covariate while holding the other covariates constant. This makes the coefficients basically totally un-intrepretable. \n\n>the logratio transform doesn’t help with the inherent multi-collinearity you reference, since it’s monotonic (right?)\n\nIt's not the constraint to 0-1, but the constraint that they sum to 1 across the covariates. This guarantees that the covariates are perfectly multi-collinear. The additive and isometric log-ratio transformations prevent this from being a problem by embedding the covariates in a p-1 dimensional space.""]",11,18,https://www.reddit.com/r/statistics/comments/11r8fdh/q_is_a_lasso_an_appropriate_way_to_deal_with/
748,2023-03-14 21:40:50,[Q] [R] - How to analyse event frequency?,"I urgently need help analysing event frequency. I recorded how many times a fly turned in two different conditions and am trying to figure out how to determine if there is a significant difference. Because I simply have two values, how many times they turned in the two conditions, I cannot produce any standard deviation. Am I missing something obvious ?","[""Spend a bit more time explaining your experiment and then statisticians can help you.( Add to main \nQuestion)\n\nAre you counting number of events in a fixed time?\nThen maybe a Poisson distribution test will help you\n\nhttps://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/hypothesis-testing/hypothesis-testing-with-the-poisson-distribution.html\n\nI guess you need a 2 sample Poisson test.\n\nThe standard deviation of a Poisson variable is function  of the mean\n( Similar to binomial) so you don't need to explicitly estimate it...(you have to convince yourself that your data is approximately Poisson distributed)\n\nSee a worked example with minitab\nhttps://support.minitab.com/en-us/minitab/20/help-and-how-to/statistics/basic-statistics/how-to/2-sample-poisson-rate/before-you-start/example/"", ""This is count data. You could be clearer but it sounds like you want to compare the two proportions; since they're perfectly dependent (there's only two options; the counts add to the number of trials), this is equivalent to testing whether the proportion of condition 1 differs from 0.5 (straight binomial proportions test), or it could be done as a chi-squared goodness of fit if the expected counts aren't too small.\n\nDo you have a basic stats textbook you can refer to? A decent one should cover both tests of proportions\\* and goodness of fit tests.\n\n\\* though a very basic book probably won't do the binomial test itself, but just the normal approximation"", 'It is not possible to draw meaningful statistical inference based on a sample size of one, at best you can report the frequencies themselves.', ""Umm... this isn't a sample size of 1. Every run can be treated as a Bernoulli trial.""]",3,4,https://www.reddit.com/r/statistics/comments/11r6enn/q_r_how_to_analyse_event_frequency/
749,2023-03-14 21:29:22,Mediation Analysis (Baron & Kenny) [Q],"Hi all,

I am currently doing my MSc dissertation and have just finished a series of mediation analyses on my data. I used the 4 step Baron and Kenny method (without step 1 as path c is arguably not relevant). I understand that there are alternatives to this method, such as bootstrapping. Is the Baron and Kenny method that bad? Is there argument to use it over newer methods? I kind of don't want to have to start all over again but will if I have to. [Q]",[],3,0,https://www.reddit.com/r/statistics/comments/11r64wz/mediation_analysis_baron_kenny_q/
750,2023-03-14 20:44:11,[Q] prediction with grouped data (scale data by group?),"This is a repost from r/askStatistics but I haven’t gotten traction there yet. Thanks for the help!


I have an assignment at work that is driving me a bit crazy that I need some help with. I can't divulge too much about the details of the project, given that the data and work is proprietary, so I will do my best to describe the data as best as I can given my limitations. 

I have a regression prediction problem that has data from the last 3 years, approximately 300 observations for each year. The distribution for the outcome for each year is clearly different, in that the range of values vary significantly, but all are roughly Gaussian and overlap. Furthermore, I believe that all of the features I've developed are likely to have a similar but not exactly the same relationship with the outcome across years (this pans out in the training data). I'm not satisfied with my out-of-sample MAE though, and I'm pretty sure that the differences in ranges are causing problems in my predictions.

What I""m wondering is if it is reasonable to standardize the values by group to get all of the values on the same scale and then reverse that transform by year when calculating MAE? 

For further context, this model will be deployed for future prediction. If i am to do this scaling I would wait for about 20 observations, retrain the model with the new year's scaled data and transform parameters (e.g. the new years mean and sd) and then deploy it. 

Under normal circumstances I would also try a multilevel model with a random effect for year, but this model is deployed in excel by my coworker, where he takes the model coefficients and plugs values in. It's truly not ideal but he ""has a system"" and spreadsheet made for this work and I'm relatively new so don't want to blow his work up. 

Thanks for the help!",[],1,0,https://www.reddit.com/r/statistics/comments/11r54gn/q_prediction_with_grouped_data_scale_data_by_group/
751,2023-03-14 19:54:07,[R] Calculating the distance of two variables to their correlation value.,"Hello everyone,

I  calculated for every age/year a correlation-matrix for different  variables. Then I vectorized the lower triangular matrix and ordered the  correlation values in a new matrix. This results in a matrix, where the  rows represent the age/year and columns represent the different unique  correlation values.

Now I have the  following problem, I would like to know distance of one subject to the  correlation values in their specific age group (to the vector of  correlation values). However, I only have the raw values of the  variables for one person and not the correlation values, and I can't  calculate a correlation with only one subject. Is there way to calculate  the distance from 2 variables to a correlation value?

I hope this brief explanation is clear.

Thanks in advance for any tricks!","[""I think you're describing the residual, y - yhat, in the regression that the correlation coefficient came from.""]",1,1,https://www.reddit.com/r/statistics/comments/11r43q8/r_calculating_the_distance_of_two_variables_to/
752,2023-03-14 19:45:02,[Q] Significance of categorical features,"Hello everyone! 

I’m wondering if I am doing this correctly. I have a set A of features (metadata) with only categorical data. I have another set B of features with only binary data. 

I would like to see which metadata features has a significant effect on the features in set B. Even if changes on a feature in A only has significant effect on one of the features in B, it’s enough. 

Currently doing chi-squared which is done for each combination between features in A and B. This is only a univariate analysis, but it should be sufficient. 

Is my approach correct or should I look at another method to determine significance? 
I feel like one single test on a feature from A to see if it significantly changes any value in B would be nicer, but not sure if chi-squared extend to that.

Thanks for any advice! 😄",[],3,0,https://www.reddit.com/r/statistics/comments/11r3xgf/q_significance_of_categorical_features/
753,2023-03-14 19:35:59,[Q] Partial Autocovariance,"**W**e **def**ine **part**ial **autocovariance o**f **l**ag **h** **t**o **""isol**ate"" **t**he **eff**ect **o**f **oth**er **lag**ged **varia**bles **wh**en **calcul**ating **autocovariance**. **Part**ial **autocovariance** **i**s **giv**en **b**y **t**he **coeffi**cient **o**f **t**he **vari**able **X** **s**ub **h** **fr**om **t**he **lin**ear **regre**ssion **o**f **t**he **vari**able **a**t **ti**me **t** **o**n **i**ts **lag**ged **val**ues (t-1, **.**.. **,** t-h) **.**

**M**y **ques**tion **i**s, **w**hy **isn**'t **part**ial **autocovariance o**f **la**gs h-1, **h-2.**... **equ**al **t**o **t**he **coeffi**cient **o**f **t**he **varia**bles **X\_t-1** **,** **X\_t-2** **.**.. **i**n **t**he **sa**me **lin**ear **regre**ssion **mod**el **?** (on **t**he **pa**st **lag**ged **val**ues **u**p **unt**il **X**\_t-h **)** **?**","['Why are some letters bolded in your question?', ""I use a chrome extension that bolden the first letter of every word on a page, it helps with attention while reading (helpful if you have sever attention issues and can't read a few sentences without skipping words/zoning out)\n\nI thought it only showed up in my own browser though"", 'Here is an explicit description of the regression coefficients in general.\n\n[https://stats.stackexchange.com/questions/196807/explicit-solution-for-linear-regression-with-two-predictors](https://stats.stackexchange.com/questions/196807/explicit-solution-for-linear-regression-with-two-predictors)\n\nConsider a model x_t ~ x_t-1 + x_t-2.  It shows that the regression coefficient of x_{t-1} would be related to lag one auto correlation coeff with some  additional stuff. \n\nIn fact the algebra gives you a recursive formula\nhttps://en.wikipedia.org/wiki/Partial_autocorrelation_function\n\n(See the recursion formula).', ""It's a technique called bionic reading that's meant to increase the speed at which you can read materials.\n\nFor materials written at a 6th-grade level, it works well in my personal experience. For materials written at the level of a master's degree holder, it's not so great."", ""Yeah and it doesn't work with latex""]",7,5,https://www.reddit.com/r/statistics/comments/11r3rkx/q_partial_autocovariance/
754,2023-03-14 19:20:56,[C] SAS Clinical Statistical Programmer vs Data Scientist vs Data Engineer: which of these jobs is more demanded and pays the most?,,"['YES. If you get a degree in data science you probably won’t even work in ds starting out. It’s data analysis/data engineering to start typically. Data engineering is arguably more important since you can’t draw any insights if you aren’t storing and moving data efficiently. \n\nMaybe DS is more in demand, but there’s just so much supply of fresh grads since a lot of schools are making programs for the field. Less people want to work on storing data in a cloud compared to fitting fancy models.', ""Titles mean very little, so you could tell me any of these paid the most a d I'd believe you.\n\nBut I still wanna play the game\n\n1. Data engineer\n\n2. Data scientist\n\n3. SAS Clinical..."", 'DE ≥>>>>>>>>>>>>>>>>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDS', 'Really?! DE more than DS?', ""Sure? It's all just made up, but in general I thought an engineer would make more than a scientist.""]",0,5,https://www.reddit.com/r/statistics/comments/11r3hhd/c_sas_clinical_statistical_programmer_vs_data/
755,2023-03-14 14:34:20,[Q] [D] MSc in Statistics or Computational Statistics & Machine Learning,"Hi all, I’m looking to do postgrad after my Bachelors in Data Science. Which of these MS would be more employable and able to get a higher salary?

Location in UK",['What do you want to do?'],0,1,https://www.reddit.com/r/statistics/comments/11qyt7w/q_d_msc_in_statistics_or_computational_statistics/
756,2023-03-14 14:30:06,[Q] Term for a statistical method similar to a weighted median,"Hi r/statistics,

There is a fairly simple statistical method I have come across with work, which appears similar to a weighted median. I am searching for the proper term for it. Is someone here perhaps be able to identify it?
The situation is that we have a large dataset containing the numbers of items per group. We’re interested in finding the balance point where the sum of items is equal above and below.
For example, with the following data, the sum is 30, the mean is 3.33 and the median is 3. The statistic we’re interested in is at what point does the sum of the bottom portion equals the sum of the top portion. In this case, it’s 4.

1
1
1
2
3
3
4
6
9

What is this called? It feels like it should be simple but it is eluding me.","[""I've no idea if this measure exists and has a name. If you had the set of \n\n1 2 3 4 10\n\nwhat would you expect your metric to be? 7?"", 'This is minimizing l1 loss subject to an exact constraint. Because of this it’s an alternative estimator of the median, but one that is not at all robust and also not guaranteed to exist.', ""Interesting. I would say my metric would be 10, as that's the point where the sum of the bottom portion equals the top. It really falls down with a small dataset like that, but with the large dataset I'm dealing with at work, it shows a valuable 'balance point'."", 'Yeah, okay. Thanks for the comment.']",1,4,https://www.reddit.com/r/statistics/comments/11qyqfa/q_term_for_a_statistical_method_similar_to_a/
757,2023-03-14 10:19:37,JS A/B testing [Question],"I'm writing a small analysis of data retrieved from testing variations of a website. The goal is to see which version cause the most events (clicking a sign up button, for example) to occur.

This is not a homework problem. I'm working on an open source project on github. I volunteered to attempt this portion since I have some knowledge of stats.

Each experiment has a control and up to 4 variants. I believe using a two sample binomial test to compare the number of distinct user clicks to the number of users for each group is appropriate. Since I have multiple variants, I also wanted to add a correction for that. I think the bonferroni correction is appropriate. For website designs, we prefer to error on the side of having false negatives instead of false positives. My understanding is that this is a conservative correction.

I appreciate any advice on the analysis design.

I'm also looking for advice on javascript libraries to implement this analysis. Some options I have found are: 
1. Stats.js << possibly discontinued? Erased?
2. jStat
3. Simple statistics
4. Math.js

Last time I did statistics I used python and excel so I have no experience with these libraries. Do you have one you prefer and why?

Thank you.","['Why does the JavaScript even need to perform the hypothesis test? Just do it yourself when you have enough data. Repeatedly testing the same data over and over again, as would be the case if you dynamically updated the test statistic for every new data point, is not sound statistical practice.', ""Javascript will be responsible for doing the statistical analysis in the backend of the website. This is an application that will be self hosted, so I won't have access to their data for more privacy. JavaScript will automatically perform the analysis once the experiment is complete, not an ongoing analysis.""]",3,2,https://www.reddit.com/r/statistics/comments/11qtu28/js_ab_testing_question/
758,2023-03-14 09:29:59,[Q] Is the effect sig higher than zero?," I want to check if the response time difference between the two types of trials (e.g., response trials of incongruent trials - response times of congruent trials) is significantly higher than zero in order to classify each individual as a learner/non-learner. Is there any good way of doing it? I have a decent number of trials so I was thinking about running a model per participant and checking whether there is a sig effect of trial type. Would this work or is there a better way?","[""How do you know that your classification rule is yielding *correct* labels? (That is, the ones you're calling learners are really learners and the ones you're calling non-learners are really non-learners?)\n\nPresumably (if you can be confident that you really can tell a learner from a non-learner) it would be better to try to minimize some sort of misclassification cost rather than to fix the rate of one and try to minimize the rate of the other.\n\n\nWill it work anyway? well, yes, in that you can define a classification rule based on it. Whether it's especially useful is not as clear, but you could perhaps make some assumptions and see whether it is satisfactorily dealing with simulated cases."", 'That\'s a great point especially when considering how difficult it is to determine what ""learning"" consists of. Do you have any suggestions for a paper which has looked at this issue?', 'Not my area, sorry.']",1,3,https://www.reddit.com/r/statistics/comments/11qsnwz/q_is_the_effect_sig_higher_than_zero/
759,2023-03-14 08:23:52,[Q] How would you approach this problem? Predicting sales for a month.,"So I have a dataset from 2019 Dec - 2020 Nov (plus 10 days of data in 2020 Dec)
So 12 months worth of data at a date level. Some dates are missing. And I have 10 dates for December. 

I have to predict the total monthly sales in 2020 Dec overall. 

- Is 12 months of data enough to predict December's sales, specially a month with a spike in shopping behaviour? 
- Would you group total sales by month, or create the forecast at a date level and sum the dates in December? 
- What model would you use?
- What would you do the 10 dates of data available in December? If calculating at a month level, do I just filter those out?","[""* There is a strong seasonal effect that you cannot ignore. With just a year of data, it means almost all months are useless for your analysis.\n* There is an obvious difference between 2019 and 2020 that you cannot ignore.\n\nIf you can, try to get data on seasonal variations (or even comparable Nov 2020 to Dec 2020 ratios) from other sources. If not, you could try to scale the 10 days of 2020 to the full month using the ratio observed in 2019, but it's likely the pandemic shifted the shopping behavior within December as well."", 'COVID broke the ""the future looks similar to the past"" assumption of most forecasting models, particularly those trained with supply-side data. COVID notwithstanding, you\'ll need at least two years of data to model your seasonality. Three years or more is better.', ""Let's say we don't want to factor in COVID, I just want to understand conceptually - would it be better to run the model at a day level dataset (ie. Model predicts the remainder of the days in December and I add them up), or run the model at an aggregated month level. \n\n\nI'm fantastically new to this, as you can imagine."", ""You have an annual cycle, so you'll need at least two years of historical data to train your model for seasonality. Generally speaking, at least one additional year is desirable for validation.\n\nIn terms of grain, there's the grain of your data. You usually have no control over this. There's the grain of your delivered forecast. This is a business decision, not a data science decision. Finally, there's the modeling grain. This is a data science decision.\n\nSuppose you have daily data, and the business desires a monthly forecast. You might get the best results by modeling days and aggregating to months, or maybe by modeling quarters and disaggregating to months. Or maybe modeling months to start with is actually best. You might even find that one method works better in the short term, while longer-range forecasting is improved with a different method. These are the sort of things you'll need to determine for each project."", ""Got it. Thank you very much, this is a really helpful response. I'll keep this in mind moving forward with the project. Probably try a bunch of different approaches. If nothing else, I'll at least learn something.""]",2,5,https://www.reddit.com/r/statistics/comments/11qr2hr/q_how_would_you_approach_this_problem_predicting/
760,2023-03-14 06:50:01,Findings jobs with undergraduate degree [Q],"I’m taking a year off after my undergrad which I’m finishing this spring. My concern is that I’m having a hard time finding stats-related jobs that I’m qualified for, and I’m not sure where to set the bar if I want my job to help me get into a good masters program.

Most of the stats-data related jobs I’m finding that don’t require a masters of PhD, either require data science experience or it’s an internship for current students. I’ve taken data science classes but my only hands-on experience is in a statistical genetics lab on my campus, which is a field that generally requires at least a masters (Im an unpaid undergrad researcher). That experience is more helpful for getting into a grad school than finding a job with an undergraduate degree. I found a few biostats internships for recent graduates but they’re extremely competitive with hundreds of applicants. I almost wish that i got a data science internship instead of joining the lab. I’ve only found a few data science jobs in all my searching that did not require extensive experience and wanted a recent undergraduate. 

I’ve also found some more general jobs in labs, like psychology or bacteriology. They don’t require any specific major so it seems like a general research role, and the descriptions mention that you would handle data but probably not in the way that would impress a stats ms program. 

Does anyone have advice for finding jobs? And what kind of job should I aim for if I want it to look good to a stats ms program?","[""Depends what you want\n\nYou can probably learn SQL online in under a month and apply for data analyst roles. They will likely be reporting-heavy and light on actual statistics but you can use one to then pivot into data science more easily\n\nOr you can go for an internship if you think it's the stepping stone towards better roles""]",1,1,https://www.reddit.com/r/statistics/comments/11qon85/findings_jobs_with_undergraduate_degree_q/
761,2023-03-14 05:32:27,[Question] Best book for dynamic linear models,"I am looking for a good resource on dynamic linear models. I generally prefer books because of the strong editorial voicing and educational framing. I have only been able to find two books on the topic: [West and Harrison's Bayesian Forecasting and Dynamic Models](https://link.springer.com/book/10.1007/b98971) and [Campagonli, et al Dynamic Linear Models with R](https://link.springer.com/book/10.1007/b135794). Can anyone recommend either book, or a third option?",[],8,0,https://www.reddit.com/r/statistics/comments/11qmhk0/question_best_book_for_dynamic_linear_models/
762,2023-03-14 04:34:23,[Q] What does it mean to choose a distribution vs have the qualities of a distribution?,"This might be a broad question but I'll help narrow down the scope as best I can.

I'm trying to reconcile ""distribution"" as the shape/nature of a sample/population versus the idea that you *choose* a ""distribution"" to calculate a probability.

Some statistical tests assume a population has a Normal distribution to work properly. This gives me the impression that a population has a type of distribution by its nature.

However, when reading about discrete probability distributions, the writers tend to use language like ""select the right distribution"" to calculate probabilities (e.g., Geometric vs Hypergeometric).

But in both continuous and discrete distributions, authors often use a more active language when describing distributions. Student's T has its own distribution but it can ***become*** more Normal as DF/n increases. 

So what does it really mean for a population to *have* a type of distribution versus telling the student to *choose* the correct distribution?","['> Some statistical tests assume a population has a Normal distribution to work properly. \n\n\'Work properly\' suggests that we\'re discussing  the assumptions under which the null distribution of a particular test statistic might be derived (which gives alpha to be the value youselected from the available significance levels if the assumptions are true)\n\nSomeone might make some assumptions under H1 in order to calculate power functions under some sequence of alternatives but that\'s usually not what people mean when they say \'work properly\'\n\nOf course (a) we don\'t have to assume normality, any suitable distributional assumption under H0 might be used in its place, or indeed in some cases just a few mild conditions might be imposed on the distribution rather than some fixed family; (b) distributional assumptions are almost never going to be true - though in some cases that won\'t make a substantive  difference to (say) significance level. Models are always going to be abstractions/ approximations; what\'s important is how much it matters to the inferential properties you care about. (Inexact probability models - like the proverbial spherical cow in physics - can still give very useful answers. What matters, then, is being able to tell *when* they\'re going to be useful and when they aren\'t.)\n\n> This gives me the impression that a population has a type of distribution by its nature.\n\nI\'d perhaps put it this way: \n\nDefine the population distribution function F(x) as the proportion of the population values less than or equal to x. \n\nSo far this is just description,  but here\'s how we get probability (and so tests, CIs etc) out of that. A random variable has a distribution. The random variable you obtain by randomly selecting a single member of some population  is a random draw from the population distribution.\n\n[When you\'re dealing with some data-generating process rather than some ennumerable population, replace the phrases referring to population with ""by observing some  random process"" and ""process distribution"" respectively.]\n\nThe problem is you simply don\'t know the population distribution (at least not exactly, though you may be aware of its general characteristics), and hence you don\'t know what the probability distribution of a randomly selected observation will be. It still comes down to choosing a model - an abstraction / approximation. It\'s really about models all the way down. \n\n> However, when reading about discrete probability distributions, the writers tend to use language like ""select the right distribution"" to calculate probabilities (e.g., Geometric vs Hypergeometric).\n\nI dont know that I\'ve seen this particular phrasing but presumably this is mostly about recognising characteristics of the situation/problem that would suggest a particular model as a suitable approximation (and other possible distributions as relatively unsuitable because they dont arise from those abstracted characteristics). \n\nE.g (a) recognising a situation might be modelled as a set of  Bernoulli trials and knowing which of the associated distributions fit the random variable of interest, (b) noticing whether you\'re dealing with sampling with replacement  in a finite population (or not),  and so forth.', '>I dont know that I\'ve seen this particular phrasing but presumably this is mostly about recognising characteristics of the situation/problem that would suggest a particular model as a suitable approximation (and other possible distributions as relatively unsuitable because they dont arise from those abstracted characteristics).\n\nAfter coming back to it today, this is exactly what it is.\n\nMy new understanding: Sometimes you don\'t know the distribution of the population you drew your sample from, so trying to make an inference or prediction about the population requires matching your sample as best as possible to a distribution (i.e., ""choosing"" the distribution). Matching the data to an incorrect distribution can cause you to select a poorly fit model (because the model follows certain assumptions that aren\'t being met).\n\nIf my understanding is off, feel free to let me know. Thank you for your help.']",1,2,https://www.reddit.com/r/statistics/comments/11qkv7e/q_what_does_it_mean_to_choose_a_distribution_vs/
763,2023-03-14 03:00:03,[Q] Multivariable regression: how to account for uncertainty in estimated variables,"Hey everybody!

I'm looking for guidance on how to account for the uncertainty in the point estimates that I'm using as variables to build a multivariable regression model. 

I have the 90% confidence intervals for each estimate, but I don't know how to ""plug in"" that uncertainty to be appropriately reflected in my analysis. 

In short, I'm trying estimate the correlation between two variables using publicly-available U.S. CDC/Census Bureau data, with several other estimates used as controls:

>Dependent variable = county-level Labor Force Participation rates

>Independent variable = county-level Diabetes prevalence


>Control variables: county-level Poverty Rates, Median Educational Attainment, Disability Rates, etc etc...

My dataset basically looks like:

>County 1: Diabetes prevalence = 10.0% (+/- 2.0%); Labor Force Participation = 60% (+/- 5%); Poverty rate = 20% (+/- 3%) ... ...

and so on for 100+ counties.

What method(s) could I use to account for the uncertainty in the ~1000 county-level estimates I'm using to estimate the correlation between my two main variables of interest? 

If anyone could point me in the right general direction, I would be very grateful. I have Stata but I'm willing to explore other tools if needed.

Thank you!","[""Apart from the methods suggested by one of the commenters here, I think you can try a simulation based method.\n\nLets say your final estimate of the quantity of interest (e.g. correlation) can be written as\n\nY = f(X1, X2, ...Xn)\n\nIn your case, (X1,...Xn) is not fixed and has uncertainty associated with them.\n\nEach Xi has a distribution with (mean =Mi, sd = Si).\n\nCurrently your final estimate is\n\nY\\_hat = f(M1, M2, ...,Mn)\n\nWhat you can try is to generate a realization from those distributions.\n\nInstead of (M1, M2, ...,Mn),  \\*generate\\* data (z1,z2,..zn) where say\n\nzi \\~ Normal(mean =Mi, sd = Si)\n\nThen you new estimate will beY\\_hat = f(z1, z2, ...,zn)\n\nDo it again and again.Each time values of (z1, z2, ...,zn) change and hence the value of Y\\_hat changes. Thus if you repeat these 1000 times, you have 1000 Y\\_hat values. That is the distribution of Y\\_hat.You can then report mean, median, other percentiles of that distribution.For example, you can report mean of these 1000 Y\\_hat numbers as your point estimate and sd of those numbers to report CI.\n\nThere are some of the catches though.\n\n* Here  we assume zi \\~ Normal(mean =Mi, sd = Si). that may not be true. Some Xi may not be symmetric, unimodal. Some Xi's distribution may be highly skewed with long right tail say. Some Xi may take only positive values, but you simulation occasioanlly generates negative values.\n* We are also assuming that the Xi's are independent. Hence we can simulate them independently. That may not be true.\n\nHence if you have some idea on how the individual distributions look like, you may want to simulate from them. The distributions are additional assumptions and your final point estimate and the CI is going to be affected by them."", 'Interesting question! You could use an errors-in-variables model like [Deming regression](https://en.m.wikipedia.org/wiki/Deming_regression) or you could go the Bayesian route where you include a measurement error parameter in your final model [like this](https://mc-stan.org/docs/2_21/stan-users-guide/bayesian-measurement-error-model.html).', 'Thank you so much for the helpful response! It feels good to at least be diving down the right rabbit hole, haha', 'If I understand what you\'re saying correctly, when you say ""account for the uncertainty in the ~1000 county-level estimates"" I assume you\'re talking about the (+/- n%) parts of your data. So concisely, you have a measure of uncertainty which is different for every variable of every observation which you want your model coefficient estimates to reflect. Two possible ways forward to include this in your regression: 1. Average the uncertainty over all observations of a variable, or 2. average the uncertainty over all variables of an observation. \n\n1: Bayesian regression allows you to specify prior uncertainty about an entire variable (the average (+/- n%) value of a variable across all counties). You plug this in as the prior distribution for that coefficient estimate. \n\n2: Weighted least squares allows you to specify the uncertainty of each specific observation (the average (+/- n%) value of a county across all its variables) and use that as a weight for how much each one affects the coefficient estimates\n\nIn both of these cases you are losing some information. The best choice depends on if there is a higher difference of variance between variables or observations. For example, if counties who have large CI for one variable typically have large CI for all of their variables, then approach 2 makes more sense. It\'s possible to combine these methods to do a ""Bayesian weighted least squares"" (google that and you will see some discussion), although you will likely have to write the code yourself and do some research because it\'s not very common. Your scenario of having uncertainty estimates for every variable of every observation is pretty uncommon, so that\'s why it might be hard to find information about how to model it. \n\nHope that helps', ""Thank you for the help!\n\nThis is kind of the direction I was trying to go down before I confused myself really badly haha\n\nI don't want to lose all the detail and nuance in your post, but quick question:\n\nI have the mean (point-estimate) and 90% CI for each estimated variable, which means I can calculate the Standard Error (I think).\n\nBut I do *not* know what the sample size was (i.e how many survey responses the original researchers used to create their estimates). I know how many people in each county are *estimated* to fall under each variable, but that's a population, not a sample, right?\n\nTherefore I don't think I'll be able to calculate Standard Deviations - is it possible to do this kind of simulation without knowing each variable's SD?""]",8,6,https://www.reddit.com/r/statistics/comments/11qi83g/q_multivariable_regression_how_to_account_for/
764,2023-03-14 01:20:05,[Q] studies quantifying amount of bias when log transforming dependent variable vs GLM,"I am interested in whether we know how much of an issue it is to log transforming the dependent variable when doing linear regression instead of doing log-linear regression. I keep seeing articles doing this, and I am aware of some [issues](https://stats.stackexchange.com/questions/47840/linear-model-with-log-transformed-response-vs-generalized-linear-model-with-log), but I haven't seen studies clearly showing how much of a problem this is irl (I'm not a statistician, and realize non-statisticians sometimes [reinvent the wheel](https://www.reddit.com/r/statistics/comments/11nwhbz/r_statistical_control_requires_causal/)). Has this been systematically studied?","[""It depends on the IVs in your model.\n\nThere's been a few papers on this in psychology because a lot of people liked log transforming reaction time data.\n\nhttps://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full\n\nTo transform or not to transform: using generalized linear mixed models to analyse reaction time data\nSteson Lo* and Sally Andrews\n\nSchool of Psychology, University of Sydney, Sydney, NSW, Australia\n\nLinear mixed-effect models (LMMs) are being increasingly widely used in psychology to analyse multi-level research designs. This feature allows LMMs to address some of the problems identified by Speelman and McGann (2013) about the use of mean data, because they do not average across individual responses. However, recent guidelines for using LMM to analyse skewed reaction time (RT) data collected in many cognitive psychological studies recommend the application of non-linear transformations to satisfy assumptions of normality. Uncritical adoption of this recommendation has important theoretical implications which can yield misleading conclusions. For example, Balota et al. (2013) showed that analyses of raw RT produced additive effects of word frequency and stimulus quality on word identification, which conflicted with the interactive effects observed in analyses of transformed RT. Generalized linear mixed-effect models (GLMM) provide a solution to this problem by satisfying normality assumptions without the need for transformation. This allows differences between individuals to be properly assessed, using the metric most appropriate to the researcher's theoretical context. We outline the major theoretical decisions involved in specifying a GLMM, and illustrate them by reanalysing Balota et al.'s datasets. We then consider the broader benefits of using GLMM to investigate individual differences."", 'Can use the delta method taylor expansions to determine it. The bias is greater when the conditional variance var Y|X in the data is higher.']",19,2,https://www.reddit.com/r/statistics/comments/11qfld7/q_studies_quantifying_amount_of_bias_when_log/
765,2023-03-14 00:30:12,[D] Help finding study with SPSS,Hello! Would anyone know about any studies that used the ANOVA test and have an spss data set? A link would be great! Thank you!,"['Sounds like homework. Read the sub rules.', 'I mean asking for places to look for resources isn’t exactly the same as doing someone’s homework.']",0,2,https://www.reddit.com/r/statistics/comments/11qec6w/d_help_finding_study_with_spss/
766,2023-03-14 00:24:41,[Q] Why my PCA looks like this?,"Hi all,

I am dealing with omics data of blood samples. Briefly, I have dataset A with 10 samples per group, 2 groups in total, and a quantification of 7500 features per sample. We ran a t-test for each one of the features and found no proteins statistically changing after correction for multiple comparisons using Benjamini-Hochberg. The collaborators still wanted to analyze the genes with unadjusted p-values < 0.01, despite the high chance of false positives, leading to 17 genes. Afterward, they ran a PCA on those genes which gave a perfect separation of the two groups. However, that sounded suspicious to me.

In R, I wrote a code to sample the same number of subjects and features, but everything from a single population of mean = 15 and sd = 2. I ran a t-test for the 7500 features and, to no surprise, I consistently got around 75 features with unadjusted p-value < 0.01. A PCA for the corresponding features resulted in the perfect separation of the mock data, even though all data points came from the same population. So far so good, I would be able to prove my point that the PCA the collaborators got at first was no better than a mock population.

However, I have another dataset, also blood samples but a different disease, let's call it dataset B. For this one, I have 50 samples per group and the same number of features. For this one, we ran a t-test, corrected multiple comparisons using Benjamini-Hochberg, and got 18 features with an adjusted p-value < 0.1. In theory, these are ""truly"" significant hits. If I plot a PCA of these features, the PCA has almost no separation!!! I was not expecting this! A mock data gives me perfect separation in the PCA while a dataset with features likely changing does not. What am I missing here???

I already tried to replicate the PCA for dataset B, where features are statistically changing but no separation is visible, but I failed every time. I am using 50 patients per group, the mean and sd of dataset B (11 and 0.25, respectively), and selecting unadjusted p-values < 0.01, which is around 75 features.

  
All PCAs can be found in this imgur link: [https://imgur.com/a/dpavXq6](https://imgur.com/a/dpavXq6)","['First off, it\'s very weird to subset only significant data to make the PCA. I don\'t see why you\'d ever do that. Usually PCA is done as an exploratory step to see what Metadata corresponds to different principle components of variation. \n\nTo your question, probably your blood data separate by treatment on some other PC than 1 it 2. If this is human data from multiple donors, then my hypothesis is that ""donor"" is associated with most of the variance.\n\nThe mock data you have ""forced"" into having the most variance by treatment, so it splits on PC 1.\n\nYour collaborators are polishing a turd, don\'t put your name on that without being very careful.', ""Separability of groups in PCA does not mean very much, since PCA does not consider the groups. It would just mean that the dimension of highest variance happens to correspond to the groups you've already identified. \n\nAs mentioned by forever-erratic, there is no reason to only use significant data to make the PCA. If you're interested in genes that show the greatest variability between your groups, you should use PLS regression. It is similar to PCA in that it can be used for feature selection, but it also considers the labels of the data. Therefore, the first dimension will be a combination of genes that show the greatest variance between your groups. \n\nAs with anything in statistics though, be careful how you use it. From your description it's unclear how you're using these methods for your research. Good luck !"", 'In general, principal components are used to collapse a high-dimensional problem, especially if you don’t care much about how a system works and just want to control for a bunch of things you don’t understand at once. Applying PCA to “significant” associations is sort of like double dipping, with the disadvantage that the second dip just obscures the relationships they’re probably most interested in.', ""Thank you for your help!\n\nThe idea of the project is to find features to be used in diagnostic settings that can distinguish both diseases. From my understanding, they used PCA to test if the supposedly changing features were able to distinguish healthy from diseased. Since the PCA did separate both conditions, they are planning to follow up on those features.\n\nIn my own dataset, dataset B, I plotted the PCA just to show that I would get the same appearance as the mock data, and the one they generated, but was surprised to see no separation. I plotted other PCs and still see no separation. The thing is, I don't necessarily want to see separation in my dataset, I just wanted to understand how this result is possible. In my mind, selecting features that are statistically changing would inevitably generate a PCA with good separation between the groups.\n\nOh, and all the mock data I generated came from the same distribution. I never simulated a healthy vs treatment. It is almost like I am comparing healthy vs healthy, where no difference should be seen."", ""Thank you!\n\nI am sorry, but I am having some trouble understanding the first part of your response:\n\n>Separability of groups in PCA does not mean very much, since PCA does not consider the groups. It would just mean that the dimension of highest variance happens to correspond to the groups you've already identified.\n\nIn PCA, if subjects are close to one another, is it wrong to assume they have similar quantification values for the features used when plotting the PCA?""]",10,12,https://www.reddit.com/r/statistics/comments/11qe75p/q_why_my_pca_looks_like_this/
767,2023-03-13 22:49:27,[Q] which sampling approach to take when measuring data of traffic speeds.,"I am recording traffic speeds on a road for 4 weeks in order to determine the best time of week for trucks to use the road to make deliveries. I have collected data at the same time each day for 4 weeks but I'm unsure what sampling approach to take. If I'm separating traffic speeds by day of the week, would this be stratified sampling or is this more like simple random sampling?","[""You have data for 28 days, four samples of each day of week.  Is this 28 data points? Do you have data for multiple samples at the same time of day (multiple vehicles)? For multiple times of day within day of week?\n\n\nIf it's one vehicle a day for 28 days, make a graph with day of week on the horizontal axis and speed on the vertical.  Each day will have a group of four data points. See if there is a clear separation of the groups.  If there is no overlap bewteen any of the groups, then the answer is clear: the day having the fastest speeds is the highest group.\n\n\nIf there are multiple samples within multiple times of day, then your sub-grouping will be nested.  If there is overlap and you need more analysis to establish confidence in a decisions, then the analysis becomes statistical.  Consider components of variation analysis, using control charts to look for instabilities within groups and differences between groups, and nested ANOVA to assess confidence in the results.""]",1,1,https://www.reddit.com/r/statistics/comments/11qbr92/q_which_sampling_approach_to_take_when_measuring/
768,2023-03-13 21:55:48,Regression with bounds on certain coefficients [Q],"Hi all ,

 My problem is of the form sales= intercept+ beta*var1 + beta2*var2+ betan*varn ... 

For var 1&2 i  have a prior understanding of what range they can lie in between max and min limits the other variables are unbounded.

How would I proceed to perform a constrained regression on the same ?

I tried Bayesian regression, but it doesn't really bound the coefficients to it's upper or lower bound, just provides me with a posterior range.

Can you folks point me in the right direction, any ideas will be appreciated.

Thanks","['> I tried Bayesian regression, but it doesn\'t really bound the coefficients to it\'s upper or lower bound, just provides me with a posterior range.\n\nWhat are you using? this should be easy to do in Stan. I think even [brms](https://paul-buerkner.github.io/brms/reference/set_prior.html) allows for this:\n\n>If desired, population-level effects can be restricted to fall only within a certain interval using the lb and ub arguments of set_prior. This is often required when defining priors that are not defined everywhere on the real line, such as uniform or gamma priors. When defining a uniform(2,4) prior, you should write set_prior(""uniform(2,4)"", lb = 2, ub = 4). When using a prior that is defined on the positive reals only (such as a gamma prior) set lb = 0. In most situations, it is not useful to restrict population-level parameters through bounded priors (non-linear models are an important exception), but if you really want to this is the way to go.', ""Sounds like you want least squares with box constraints on some variables\n\nhttps://en.wikipedia.org/wiki/Constrained_least_squares\n\nThere's a variety of ways of imposing such bounds; one approach is discussed at the link.\n\nThere's a number of approaches discussed here:\n\nhttps://stats.stackexchange.com/questions/61733/linear-regression-with-slope-constraint/\n\nThough if you're using the code there, I suggest carefully checking that the functions still work as they did.\n\nThere's some potentially useful discussion here as well (note, for example, one approach to introducing a positivity constraint as a nonlinear term in an equation in the question)\n\nhttps://stats.stackexchange.com/questions/295951/re-parameterization-of-a-nonlinear-model-as-a-linear-model/"", 'To stop reddit markdown from interpreting pairs of \\* characters as instruction to display the characters between them in italics, put a \\\\ in front of \\*, i.e. \\\\\\*. It will then appear as \\*, as you want it to in your regression equation', 'Well i was trying pymc with the nuts sampler, but i see what I did wrong , i was using a normal distribution with small std to limit the values, but essentially not having strict upper and lower bounds , will try with a bounded distribution function as you have suggested.\n\nThis was exactly what I needed thanks , sometimes the most simplest things slip your mind.\n\n Will post an progress update.', 'Thx will keep that in mind.']",3,5,https://www.reddit.com/r/statistics/comments/11qaelq/regression_with_bounds_on_certain_coefficients_q/
769,2023-03-13 20:58:47,[Q] How to use regression in the right way to check for a suppressor variable.,"Hello dear community, I am currently writing my bachelor thesis and am at a loss. I hope I can explain my problem in a comprehensible way. English is not my mother tongue.

&#x200B;

I would like to investigate the correlation between positive perfectionism and procrastination. These variables should correlate negatively. Positive perfectionism also correlates (positively) with negative perfectionism. Therefore, negative perfectionism 'contaminates' the positive perfectionism and I want to adjust this relationship for the influence of negative perfectionism. The relationship between positive perfectionism and procrastination should then **strengthen**. I wanted to calculate a simple linear regression (with positive perfectionism as IVs and procrastination as DV) and then a multiple regression (with positive perfectionism and negative perfectionism as IVs and procrastination as DV). 

&#x200B;

**My hypotheses (right now) are as follows:**

1a: There is a significant negative relationship between positive perfectionism and procrastination.  
1b: The strength of this relationship increases when negative perfectionism is taken into account.

&#x200B;

To answer 1a, I want to check the standardised regression weight of the simple linear regression  including the respective p-value. I can still keep up with that.

  
Can I then answer 1b by simply looking at the corresponding standardised multiple regression weight (the one of positive perfectionism) and comparing it with that from 1a? Is my hypothesis confirmed if this regression weight is larger than the one from 1a? As far as I know, standardised regression weights can be interpreted like a correlation according to Cohen's conventions. I really hope that I'm not completely wrong here.   


I'm thankful for every advice!","[""> Is my hypothesis confirmed if this regression weight is larger than the one from 1a?\n\nI don't think that this is sufficient, because the difference might just be by chance. I'd say you need to do a t-test that checks if the difference between the two coefficients could be zero. Maybe [this](https://andrewpwheeler.com/2016/10/19/testing-the-equality-of-two-regression-coefficients/) helps."", ""Thanks so much, I'll check that out!"", 'What does “taken into account” mean?', 'control for the shared variance of positive and negative perfectionism. My first idea was to calculate a (semi)partial correlation, but my supervisor insisted on solving it with regression.', 'Tbh I (almost) never see analyses of data that are like: var1 is positively correlated with my DV, but when we control for var2, the coefficient for var1 ceases to be significant. Therefore I conclude XYZ. There are so many reasons why that could happen, it’s just not a procedure that has any probative  value.']",6,12,https://www.reddit.com/r/statistics/comments/11q90br/q_how_to_use_regression_in_the_right_way_to_check/
770,2023-03-13 10:04:24,"[Q] Regression model where observations are nested into groups, and the value of your independent variable for each observation is only comparable to other observations within the group?","I am curious about whether you can make statistical inferences when you have nested data, and the way your independent variable is measured makes it only comparable within its own group.

For example, you want to know the relationship between a town's investment in recreation and the level of physical fitness of the people in the town. Your towns are nested within states, and each state has a different way of allocating public funds for things like recreation to towns and each state classifies recreation a bit differently. So the level of recreation funding in Town A in State 1 includes Town 1's spending on, say the local public track, but the level of funding in Town B in State 2 does not include spending on running tracks. If we simply regress fitness on investment in recreation, then we could just be capturing the effect of different expenditure classification schemes. Spending on recreation is only comparable within states.

So instead of measuring gross per capita recreation spending in each town, let's say we measure the amount of revenue that each town spends on recreation per capita as a percentage the entire state's spending on recreation. For simplicity's sake, let's say every town is the same size and has the same characteristics (although you could obviously use population weights if this weren't the case). Let's also say that you have reasonable evidence that cross-state variation in recreation spending [according to a common definition of recreation] is not very large; that is, most states spend a similar amount and that the variation is mostly at the sub-state level. So a town in State 1 may have higher gross spending but a lower percentage than a town in State 2, since State A has a more expansive definition of recreation.

Could you run a random effects, fixed effects, or mixed model estimating the relationship between recreation investment as a percentage of total state rec. investment and overall fitness? I understand that you lose some insights, since you're no longer looking at gross spending. But my question stands: Is it possible to make inferences about the relationship between fitness and recreation spending with independent variables that are only comparable within the group they're nested in, or is it kind of impossible to draw any conclusions about this relationship?","['It sounds to me like a hierarchical model would be the perfect tool to use. You model one general relationship but assume that the data in different groups can have underlying details of that relationship that can vary. See these examples\n\nhttps://num.pyro.ai/en/stable/tutorials/bayesian_hierarchical_linear_regression.html\n\nhttps://jramkiss.github.io/2021/01/29/hierarchical-models/', ""Thank you, i had suspected a hierarchical model would work, but I wanted to ask because of the specificity of my problem.\n\nFor the analysis I am doing, I am regressing a measure of fiscal performance (specifically, percentage of a fiscal transfer that is absorbed by a region) at the regional level on a number of predictors. I have 27 countries in the sample and 1500 observations in total. My observations are regions nested within countries. I have a measure of subnational public procurement autonomy, which is the percentage of total public contracts in a region that are awarded by a local government (as opposed to a central government). I have some concerns about whether this variable sufficiently captures how much autonomy a region actually has. So I want to include a new independent variable that represents a region's *share of total subnational procurement in the entire country*, adjusted for its fraction of the country's population. So for each observation, the variable would tell us how decentralized public procurement is in a given region, compared to other regions in the country. \n\nI know I can't compare this measure for regions in different countries (region 1 in country A may be overall more decentralized than region 2 in country B, but it could still have a lower value of this variable since it's less decentralized *compared to other regions in country A*). In this case, if I do a hierarchical model where I regress absorption on this decentralization measure, could I make valid inferences about the relationship between absorption and decentralization of public procurement?""]",9,2,https://www.reddit.com/r/statistics/comments/11px0gi/q_regression_model_where_observations_are_nested/
771,2023-03-13 10:01:48,"MDS plots interpretation, [Q]","Hello, Im just trying to implement a random forest model for my project, and I came across a multidimensional scaling concept and its 2D plots. What exactly is it telling me? For example, if I my model has some sort of mediocre predictive power with a OOB error rate being 0.4, would I be able to see that in an MDS plot? Can I use the plot to back up my findings, in my case poorly clustered samples on the plot would suggest more missclassifications? Thanks!",[],1,0,https://www.reddit.com/r/statistics/comments/11pwyfl/mds_plots_interpretation_q/
772,2023-03-13 07:52:08,Which School Would you Choose? [Q],"I applied to Statistics PhD programs. I am currently trying to decide between Rice and UT Austin. Which school would you choose and why? I do not yet know exactly what I want to research, but I know that I enjoyed classes in programming, statistical learning, and numerical analysis, so I am most interested in computational stats. The programs are similarly funded. Any help or advice would be much appreciated.","['UTA Stats is definitely known for computational statistics.', 'Both are good schools. Your choice should be guided by two questions-\n\n1. Which school is giving me the best package?\n2. Which school’s graduate students are going on to be what I want to be.', 'Related to #2.... alumni network. My grad schools alumni network and networking events were the non academic biggest pros of my ms program', 'Okay, thanks.', ""Thanks for the advice. It seems like #1 is pretty much the same, so it's going to likely come down to #2. I'll definitely look into that some more.""]",2,8,https://www.reddit.com/r/statistics/comments/11pu0sy/which_school_would_you_choose_q/
773,2023-03-13 05:10:37,[Q] Is there a statistical test/model that can tell whether there are long-range temporal dependencies in a Markov chain?," Hi all, I have a Markov chain. It consists of twelve states. I would like to determine whether there are temporal dependencies in this Markov chain, e.g., whether knowing a state at t may help you to predict a state at time t+10. I have tried fitting a higher-order Markov models to this Markov chain, but it occurred to me that to validate whether there are indeed temporal dependencies in this Markov chain, I would need some way to validate that higher-order Markov model.

So, is there an out-of-the-box method out there that could be used to determine long-range temporal dependencies in a Markov chain? Or, if there isn't, would a kind of cross-validation procedure be helpful to validate a higher-order Markov model (e.g., second order or third order), i.e., where that validation would validate whether there are long-range temporal dependencies in my Markov chain.","['> whether knowing a state at t may help you to predict a state at time t+10.\n\ncompute the tenth power of the transition matrix, to get the effect of moving ten steps in time.', 'You could look at the tenth power of transition matrix to get a rough idea.\n\nYou could also look at the autocorrelation of your Markov chain to check temporal dependencies.', 'Intuitively, i think the probabilities of transition matrix at tenth power would have to all be roughly equal in each row in order to say that knowing the state at t has no influence on the state at t+10', 'So I suppose you could look at the transition matrix at the tenth power. If in this matrix the probability of transition to any other state seems random then you can deduce that knowing the state at t has no affect on knowing the state of the system at t+10', 'So, in this case, a statistical test could evaluate the null hypothesis that the row transition probabilities are uniformity distributed. If they are uniformly distributed, then knowing the state at t is not informative for knowing the state at t+10']",1,5,https://www.reddit.com/r/statistics/comments/11ppyt9/q_is_there_a_statistical_testmodel_that_can_tell/
774,2023-03-13 04:41:18,Most useful chapters in Elements of Statistical Learning [Q],"I’m a senior stats major and after going through roughly 90% of introduction to statistical learning and then taking an elective course on it for my major, it’s safe to say that I would like to get a more theoretical understanding of some of the concepts in statistical learning. My goals are to get a better understanding of data mining tools in practice for analysis of financial data. I am mainly wanting to gain a theoretical understanding of some of these concepts for the purposes of model development in practice for applications, ie, be equipped with the theoretical knowledge to build empirical models. 

I wanted to know what the most useful chapters of this book are to give me a solid understanding of these methods. On a first pass, I figured the chapters 1,2,3, 4,7,13, 14, 18 are some of the most useful. 

1 - introduction 
2 - overview of supervised learning 
3 - linear methods for regression
4 - linear methods for classification 
7 - model assessment and selection
13 - prototype methods and nearest neighbor methods
14 - unsupervised learning 
18 - high dimensional problems 

I didn’t include chapters on  ensemble learning, random forests, undirected graphical models,neural networks, support vector machines, model inference and averaging, additive models, kernel smoothing methods, basis expansions and regularization.


my question to you all is if this is a   good list of topics to read for my goals, and if i should still read about any of the chapters not listed. My reasoning for excluding these chapters are because i feel the ISLR book has a fairly practical treatment of these topics, and reading about them in ESLR would be for if i was interested in research.","['""Additive Models, Trees, and Related Methods"" since I think this is the chapter that they they talk about GAMs which are awesome.', '>  I would like to get a more theoretical understanding of some of the concepts in statistical learning\n\nThen ESL is a better place to start than ISL, since the goal of ISL  is to skip most of the theory. The couple of decades of papers on which it relies may be better still, if theoretical understanding is the goal, but you need to start somewhere.', 'ISL is baby ESL. Just go read ESL and do the exercises if you want a deeper understanding of the material.', 'Just skim, and slow down when you get to parts you don’t know. Skimming the things you do know will be good reinforcement, and you have the context for the parts that are new.', ""Once you start reading it you will get a better idea of what you want to know. You're kind of like a chef asking which part of the cook book he should read. The answer is all of it until you know more""]",25,13,https://www.reddit.com/r/statistics/comments/11pp8d5/most_useful_chapters_in_elements_of_statistical/
775,2023-03-13 01:35:24,[Question] Is ARIMA good for forecasting time series forecasting with only 12 observations?,"How would you go about forecasting time series data with a small number of observations?

**Example:**  
Let's say we have 12 months of revenue data from an ecommerce business. How would you go about forecasting what the next 6 months look like?

My initial thoughts would be to use something like ARIMA but with only 12 observations, I feel like the accuracy wouldn't be great.

LSTMs, Prophet? Something else?","['Find a single covariate that correlates well with your variable over the  time span that you have data for, but has longer history. Fit a linear model with that covariate. Predict your covariate based on its longer history, then use the linear model to predict your time series.\n\nIf this is revenue data, the covariate could be some industry-wide index, maybe from the supply or demand side. If so, there are probably also industry-wide predictions made for that index already, which will likely be better than run-of-the-mill ETS or ARIMA.\n\nMore flexible models like the ones you mentioned will likely overfit.', '12 observations is not enough for a quantitative approach. I can almost guarantee you that simply asking a relevant expert to provide a best guess forecast would be better than a «model» in this case. You can’t even model any seasonality with 12 months.', 'I wish this was a youtube or article because this commenter has seen battles I want to learn from.', ""Usually with low data cardinality, expert knowledge comes into factor. \n\nFor example, I don't need 100 observations to know that pressure correlates with altitude."", 'With only 12 data points, how do you find out whether the covariate correlates ""well""? If it isn\'t redundant data?']",40,18,https://www.reddit.com/r/statistics/comments/11pkfop/question_is_arima_good_for_forecasting_time/
776,2023-03-12 20:20:25,[Q] One way Anova vs general linear model,"Hello everyone! I’m conducting my first research project so apologies if I’m being stupid.

I initially ran a general linear model of neuter status (in dogs), orthopaedic disease and age of onset (response factor) and got significant results (P<0.05) of both orthopaedic disease and neuter status. 

I then proceeded to run 2 separate one way Anovas of neuter status (entire vs neuter) and orthopaedic disease and got a P value >0.1 in both. Why all of a sudden are the results insignificant? Any help is much appreciated!","['Did you include an interaction term in both models? Which software did you use?', 'These questions pop up all the time. When you fit a one way ANOVA you are ignoring an important source of variation. That variation has to go somewhere! It goes into the error sum of squares which is what you divide your model sum of squares by, and so your test statistics become deflated. Boom, lower p-values. Basically, if the model with all effects has each effect statistically significant, you shouldn’t fit a model with fewer effects.', 'I would advise sticking to one planned approach to data analysis and not trying multiple alternatives. No method is valid when it is chosen over other methods that have already been tried.', 'ANOVA is not an alternative to GLM but rather a special case of it. Your difference apparently is because you had two factors in your GLM and one factor at a time in your one-way ANOVAs.', 'I don’ know why or how you did a Chi Square test with a quantitative dependent variable. No prior testing is required as a basis for an ANOVA. I would stick with the two-way ANOVA because neither the Chi Square nor the one-ways is justified.']",13,12,https://www.reddit.com/r/statistics/comments/11pd94t/q_one_way_anova_vs_general_linear_model/
777,2023-03-12 13:45:49,Random Forest paper recommendation. [Q],Can someone recommend an academic paper on theory/implementation of random forest?,"['Here is the first paper \n\nhttps://www.stat.berkeley.edu/~breiman/randomforest2001.pdf', 'Breiman paper is the best. Ground-up proof for why randomization can improve accuracy, and it applies to far more than just ensembles of trees.', 'Perhaps look at Susan Athey’s Generalized Random Forest paper', 'Strobl et al., various.', 'I wrote my own implementation of Random Forest and I was most satisfy with this book - [https://www.amazon.com/Machine-Learning-Random-Forests-Decision-ebook/dp/B01JBL8YVK](https://www.amazon.com/Machine-Learning-Random-Forests-Decision-ebook/dp/B01JBL8YVK)']",10,8,https://www.reddit.com/r/statistics/comments/11p6nyz/random_forest_paper_recommendation_q/
778,2023-03-12 12:02:20,[Q] Unsure of which statistical method to use for testing my research.," I  am wrapping up my dissertation proposal and have hit a snag on choosing  the correct analysis method for my research questions. Hoping this  group can give me some direction.

The first question/data set is from the following categories, and I will have five sets (five different companies)

· Utility generation capacity from all sources

· Excess generation produced from the utility source

· Forecasted growth of generation capacity from company assets

· Yearly electric demand on the utility from customer base

· Yearly received distributed generation from customer assets

· Forecasted growth of distributed generation capacity from customer assets

· Distribution system delivery efficiency

The  goal is to determine if any of these variables have an impact (and at  what level) on supporting electric vehicle growth. Linear Regression  seems to fit, but open to ideas.

In  the second data set to be analyzed, I have a fixed value and I want to  test different variables against it to determine saturation (0-100%+).  For example, if a generation source had a fixed capacity of 1MW, and I  wanted to test how many and what combination of level 1 chargers (5kW)  and level 2 (20kW) could be used. For this one, I am thinking a t-test,  but again, open to suggestions. Thanks!",[],11,0,https://www.reddit.com/r/statistics/comments/11p4o9u/q_unsure_of_which_statistical_method_to_use_for/
779,2023-03-12 10:59:46,Sports Models Education [E] [D],"Hey guys, 

I am new to the sub, I started looking into making sports model to predict games. I have been able to create a basic models, but would like to really get into making a great in-depth model. I am having a hard time finding education videos. Does anyone know where I should start looking? Or any good Youtubers that walk through a very in-depth model?

&#x200B;

This is not homework. This is something I have started to do as a hobby.","[""Not sure of any YouTube channels, but if you're looking for datasets you might try out Stathead or Sports-Reference. \n\nAlso R is pretty good for scraping data from online sources that host sports data. I think the name of the package is called rvest, but there are definitely others. \n\nCould also look into Bayesian Hierarchical modeling if you want to do more advanced sports modeling."", 'Thanks, I appreciate it!']",5,2,https://www.reddit.com/r/statistics/comments/11p3dnm/sports_models_education_e_d/
780,2023-03-12 09:47:00,What are these plots called? [Question],"I have seen more than one of these recently, I know there is an r package to make them, but I cant search for it because I don't know the name:

https://imgur.com/a/v9oPP3F","['you have 3/3 different answers already. clearly a good question lol', 'Dumbbell Plot', 'Maybe a [lollipop plot](https://stats.stackexchange.com/questions/423735/what-is-the-name-of-this-plot-that-has-rows-with-two-connected-dots)?', 'Feel free to throw a name in the hat', 'For me, a lollipop plot only has a dot at one end (double ended lollipops are not that common!). I’d go with dumbbell plot for this.']",4,5,https://www.reddit.com/r/statistics/comments/11p1tmq/what_are_these_plots_called_question/
781,2023-03-12 07:18:28,[Q] [R] Logistic Regression help,"I need advice on the logistic regression formula I'm working on.

&#x200B;

The dependent variable is a binary variable (1 for the good financial performance of the firm and 0 for bad performance). Because the purpose of my paper is to research the effect of managerial experience on firm performance, I gathered financial data on managers' current firm performance (the binary dependent variable) and on the same managers' previous firm's performance (the proposed independent variable - similarly a binary variable 1/0 for firm performance). The timeline of the measured performance in 4 years for both the new and the old firm. The only difference is that I'm using the average performance values for the previous firm and the maximum performance values at the end of year 4 for the new firm.

&#x200B;

So basically I have the data for managers' old and new firm performance and want to see if the experience in the previous firm affects the performance in the new firm.

&#x200B;

The proposed logistic regression formula is: Newperformance = a + b1 \* oldperformance + b2\* controls

&#x200B;

Now my supervisor has said that there is an issue with such an approach and I should choose a different method, while I cannot see any issue with it.

&#x200B;

Thanks","['Hello!  \nI believe it makes sense to use logistic regression in your case. I don\'t really understand why your supervisor doesn\'t find this method convenient.   \n\n\nAs far as I understand you are using previous financial performances to predict current financial performance. So the example would be like:   \n*previous performances = (0,0,0,1,1,1). Mean previous performance = 0.5 and you use this value to predict current financial performance which is either 1 or 0.*   \n\n\nEven though I don\'t see any problem with the methodology, I think by using this approach, you are losing time-order information. You are saying that (0,0,0,1,1,1) is equal to (1,1,1,0,0,0) because you are taking the mean. Maybe that\'s why your supervisor didn\'t like the method?  \n\n\nAlso, I don\'t quite get what you mean by ""*controls*"" in the formula.']",1,1,https://www.reddit.com/r/statistics/comments/11oyed3/q_r_logistic_regression_help/
782,2023-03-12 03:04:14,"[Q] ""Self learning"" statistics a viable career path.","Hello, long story short I am looking to change careers possible into statsics. I say self learn but I do have a bachelors in mathematics but no classes in statistics and had a mediocre GPA so I am doubtful I could get into graduate school . So I have a few questions

1. How viable is it to get a career in statistics without a proper degree in stats and/or a master?
2. How much is the minimum to learn to start applying for jobs? Looking through some online job applications the least usually say like ""24 degrees hours in mathematics including 6 in stats"". Would learning at the level of say *Statistical Inference* by Casella and Berger be adquate or are more advance topics needed (data mining, Bayesian stats, time series, etc)
3. What software/programming is worth learning R,SPSS,SAS, SQL, python?
4. If anyone was in a similar situation any advice to ""get my foot in the door"" so to say?","[""With this background, it would make more sense to go for data analyst/data scientist jobs. There you don't need such a precise understanding of the math behind the tools you use. \n\nI don't want to discourage you if you really prefer stats though, nothing is impossible with enough time/work/motivation, and I personally find stats a bit more interesting than a lot of data science work (or more precisely: I find data science interesting when it gets closer to actual stats). Just consider these options too, as they may also check what is drawing you to stats in the first place."", 'I honestly don\'t think even fully memorizing C&B would contribute towards employment in any meaningful way. That\'s all theory, employers care about what you can actually do.\n\nI would also  make a distinction between ""data"" jobs and statistics proper. Stats positions are almost always understood to be academic or research-adjacent e.g. clinical trials and the such. And all of those probably require at least a master\'s level education.\n\nMy personal path was self-learning R and Python, getting a job as a research assistant at my uni, then leveraging that into a job as a data analyst in the public sector and then using my salary to pay for my stats masters. I kept working in the public sector, doing mostly repetitive Excel/R based tasks but kept in touch with my professors and did some research projects on my free time. Then I found a PhD position that was related to my thesis topic and got in.\n\nBut whatever you pick, being comfortable with at least one programming language is really crucial as that\'s the way in which you actually implement the methods you learn about. SQL is a must but not too hard to achieve minimal competence; whether R or Python is the better choice depends on opportunities available to you, so you\'ll need to check with potential employers or job offers in your area. SPSS and SAS I would absolutely not pick due to their lack of versatility (although, again, you might actually find employers in you area in need of such knowledge, but I don\'t think it\'ll be too common and would probably mean it\'s a company with some fairly dated practices).', 'I was recommended DataCamp, and honestly it\'s not great. Most of the exercises are just ""fill in the blank"", and barely require more than knowledge of the syntax. Udemy is better.', 'I’m not in industry, so keep that in mind. I would learn SQL and python. There are sites like DataCamp that will teach you. Then lean on your math degree in applications and interviews.', ""People with a real MS in Stat or Biostat still don't get a phone screen with a recruiter without real stat or biostat experience.""]",4,6,https://www.reddit.com/r/statistics/comments/11os4ns/q_self_learning_statistics_a_viable_career_path/
783,2023-03-12 01:44:15,[Q] Mplus 8.9 PROCESSORS=,"I'm using Mplus 8.9 on an M1 MacBook Air, while I know it isn't built for M1 architecture, I  am wondering if there is a PROCESSORS= (or something else) parameter I can use to get Mplus to use the entire processor rather than a specific number of cores/threads?","[""You can tell mplus to use what processors you want using that command. \n\nYou'll see a corresponding numbers of Windows pop open as it starts the analysis (at least it does in Windows).\n\nCheck the mplus docs for the correct command per above and let me know if it uses all procs, as I'm interested in Mac's for mplus based on this very reason.""]",1,1,https://www.reddit.com/r/statistics/comments/11oq56d/q_mplus_89_processors/
784,2023-03-12 01:43:23,[Q] Linear regression or logistic regression?,"I'm looking for data analyst internships and I got a task from a company. They gave me a dataset with 240 different factors (& 9k+ observations). My response is either -1, 0, or 1. The point is to predict the -1s with accuracy and avoid predicting -1 when it is in fact 1. The 0s seem to be kind of irrelevant (i have no idea what this data is, it has no labels, except for Value1 Value2, etc.)

Note: I started learning R specifically for this task, I've only done stats in JMP and Excel in school

So I was thinking, should I do a regression in R and round the predictions (if negative =-1, if positive =1) or do some kind of logistical regression? Thank you!

Note2: This is the actual description of the task (quite short): *""Predict the ""Regulating"" value in the data set. The value can be 0, -1 and 1. If easier, ignore all 1, we are interested in finding -1 values and to avoid 1 values. If we predict -1 and it becomes a 0 value, it doesn't matter. We just don't like to predict -1 and get 1 in regulationg value.""*","[""I'd consider two options:\n\n1) Based on the encoding of the response variable, you could run an ordinal logistic regression.\n\n2) But it sounds like you have a specific loss function you want to minimize, so you might set up the problem as a custom regression that minimizes that loss.\n\nThe first is easy with off the shelf software; the second would require some programming and/or math."", 'I recommend lasso with ""binomial"" family.  Research the glmnet package in R.  Your function call will be something like `cv.glmnet(y ~ ., data = df, family = ""binomial"")`.  You can\'t do GLM with 240 predictor variables - the amount of multicollinearity will be staggering.', ""I dunno man, it is not a job, it's an unpaid internship. The whole point is to learn. And I'm confused about the Data. I thought someone  can give me clue. Sorry if I broke any rules."", 'He\'s got 9K+ observations so even a plain old logistic regression with glm should work. That said, it can\'t hurt to use  regularized regression even when the ""width"" of the data is only 240/9000 ~ 1/36.\n\nIt sounds like you want to model -1 (success) versus 0 or 1. \n\nLogistic regression in the glmnet package works with only binary variables or sums of them. The 0/0.5/1 comment below is leading you astray.  A response with more than two values requires the family=""multinomial"" option, but you should use family=""binomial""\n\n1st recode \n\n     Y1 <- 1*(YourData$Y==-1)\n\nAlso, as these fitters have no formula/data interface you\'ll need to create a predictor matrix\n\n\n     ## create a vector corresponding to all columns \n     ## containg the response variable or stratum \n     ## e.g. everything that\'s not the predictors\n\n     dr.cols <- which(names(YourData) ٪in٪ c(""Y"", ...))\n\n     X <- as.matrix(YourData[, -dr.cols])\n\n   \n\nThen proceed as follows\n\n     fit.glmnet <- cv.glmnet(y=Y1, x=X,  \n                 family=""binomial"",\n                  type.measure=""auc"")\n\n\nNext, keep in mind that when you use this fitted model downstream, you\'ll need to specify the regulation parameter e.g.\n\n     ## regression coefficents interpreted \n     ## as the log odds ratio for risk of 1 \n     ## vs 0. positive increases the\n     ## probability, negative decreases it\n     B <- coefficients(fit.glmnet, s=fit.glmnet$lambda.min)\n\n     ## AUC from 5 fold cross validation of best fit\n     with(fit.glmnet, cvm[index[1]])', 'Unpaid? Dude, have some respect for yourself and walk away. Companies get away with this type of shit because of people like you. I report every single unpaid internship I see on LinkedIn. I’m not supporting that kind of free labor.']",50,27,https://www.reddit.com/r/statistics/comments/11oq4gd/q_linear_regression_or_logistic_regression/
785,2023-03-11 23:54:01,[Q] How to deal with unequal distribution of target variable in dataset?,"I'm currently working on ANN that should predict the probabilty of the certain event. I have a dataset (~100k observations) with 1 binary, target variable (0 if event occured and 0 otherwise) and 20 continuous variables, describing the environment. But the problem is with the unequal distribution of the target variable (95% - 0 and only 5% - 1), which after training, leads to understimation of the probability - the max y_pred in the test set equals only 0.2.

What should I do to get better estimations? Am I suppose to limit the dataset to equal amount of 0 and 1?

Any advice would be appreciated. Have a nice day!","[""Why do you conclude that the probability is underestimated? In the dataset you have the baseline probability that an observation is positive is 5%. A 20% estimated conditional probability that an observation is positive is a factor of four change from that, which indicates some strongly informative features in your model.\n\nThis is generally just how models work, it's a manifestation of regression to the mean. You shouldn't expect models to provide you with anything close to certainty in most situations. This often is not an issue that needs correction.\n\nWhat are you using the model for? Is this a classification task where you are deciding to take some action or interventions in the positively classified observations? If so the correct procedure is to tune the classification thresholds with the costs of false and correct classifications in mind.\n\nTo make it explicit, I disagree with the other two answers here. Oversampling and undersampling, while popular in blogs and beginner literature, are often a solution in need of a problem. Quoting Murphy's *Machine Learning: A Probabilistic Perspective*:\n\n> It is worth remembering that all these difficulties, and the plethora of heuristics that have been proposed to fix them, fundamentally arise because SVM's do not model uncertainty using probabilities...\n\nThese techniques were developed for situations where models, like SVMs, combine the tasks of probability estimation and classification into one, and do not model the world in probabilities.\n\nSee [this CV thread](https://stats.stackexchange.com/a/405437/74500) for a more detailed summary of this area."", 'This is a classic problem.\n\nLook up the following:\n\nRandom oversampling\nRandom undersampling\nSMOTE (Synthetic Minority Oversampling Technique)\nROSE (Random Oversampling Examples)\n\nThese are different techniques that balance your dataset, either by oversampling the minority category or undersampling the majority. Try all, see what produces the smallest error in the validation set', 'It’s a classic problem with a classic solution … weighting or considering new methods. Generally the class imbalance is important and reflects the actual situation (conditional/marginal probability estimation) so your low probabilities aren’t anything to worry about statistically….\n\nPlease do not oversample, and definitely don’t under sample as that’s just throwing away information…\n\nWeighting or simply changing the probability threshold is generally seen as preferable to oversampling via SMOTE or another method…. A nice discussion in this sub from last year  can be found [here](https://www.reddit.com/r/datascience/comments/sxtppd/stop_resampling_data_in_classification_problems/)\n\nYou can think of over sampling as just making stuff up that seems “reasonable”, while this might make sense as the only thing to do about missing values other than dropping them, you really shouldn’t be making up entirely new observations based on your current, that’s a great way to overfit- put this way it should be fairly obvious to avoid these techniques', ""These methods don't work and may actually harm model performance. For example, check out this paper: [The harm of class imbalance corrections for risk prediction models](https://academic.oup.com/jamia/article/29/9/1525/6605096)\n\n> Imbalance correction led to models with strong miscalibration without better ability to distinguish between patients with and without the outcome event. The inaccurate probability estimates reduce the clinical utility of the model, because decisions about treatment are ill-informed."", 'Why do people double down when presented with a simpler, viable alternative? You should want to remove complication from your models as much as possible…\n\nThe point of the above paper and much of the discussion against sampling methods here is that simply adjusting the probability threshold yields similar payoffs\n\nWhy introduce fake data with oversampling or throw out data with under sampling? It’s kind of nonsensical imo to alter your data in this way by just making stuff up or throwing it out…. and the papers and discussions posted here corroborate that\n\nIf you really want to favor the minority, use weighting and/or adjust the probability threshold, don’t use sampling techniques']",14,13,https://www.reddit.com/r/statistics/comments/11onif1/q_how_to_deal_with_unequal_distribution_of_target/
786,2023-03-11 21:03:06,[Q] How to induce non-proportionality of the risks in a Cox model?,"For a thesis in survival analysis, I'm studying what happens when the assumption of proportional risks in a cox model is not respected and, consequently, which models to use in this case. The problem is that in my dataset out of twenty variables none violates this assumption.  I've been working for over a month on data cleansing and various exploratory and non-parametric analyses, so I don't want to have to start over on a new dataset.  I need at least one variable, even if it were only one, to violate the proportionality of risks assumption.  Is there a way to induce this?","[""If you don't understand the underlying truth of the data, how can you possibly know if your attempts to rectify non-proportionality are successful? In other words, simulated data makes the most sense here.\n\nFurthermore, you don't induce non-proportionality in the model (if you somehow did it would no longer be a Cox PROPORTIONAL hazards model!), that is a feature of the data itself. And while I would be quite surprised if your data always met the proportional hazards model for 20 different groupings (which is what I assume you mean by variable), then that is that. The model doesn't change the data. \n\nYou might be able to combine groups, I suppose. But at the end of the day you are far better off biting the bullet, simulating data under known and controlled assumptions, and then, if you really want, apply your results to this real dataset of yours."", 'Just transform the independent. Do something like sqrt(X), X\\^2, or log(X).\n\nIf X truly satisfy the proportional hazard assumption, then those non-linear transformation are sure to violate the assumption.']",6,2,https://www.reddit.com/r/statistics/comments/11ojon8/q_how_to_induce_nonproportionality_of_the_risks/
787,2023-03-11 20:15:30,"[Q] Flawed sample, should I still use Anova?","Hi,

I am new in statistics and I have trouble understanding the different tests to check for statistical significance. I plan on taking an actual class to compensate for this lack of knowledge.

I have 153 elements (tweets) gathered in 5 groups (topics). For each group then, I collected values (if a specific person was mentionned, number of likes, number of retweets and number of replies).

I tried use one-way Anova to check for statistical significance, although for each values the *p-*value was way above 0.05. While I do not absolutely seek statistically significance if that would mean twisting results, I have a few theories at what could have biaised my results.

&#x200B;

1) My 153 elements were made of all the tweets that used a certain hashtag and had a minimum of 100 likes OR 100 retweets. Therefore, given they are in a way only the ""top"" tweets, the distribution is flawed as tweets that received low likes and retweets are underrepresented.

2) One category was made of only 2 tweets so I excluded it completely from the test as it seemed too low.

So my question is: should I use a different test? Or is my sample too flawed to even be tested?

Thank you very much","['>\tFor whether the person was mentionned, I replaced “yes” by 1 and “no” by 0\n\nThat’s a binary response, not a continuous one. ANOVA is a linear model under the hood, and while you can choose to model probability linearly, there are some potential issues with doing so. For example, the model may predict probabilities less than 0 or greater than 1, which are meaningless. Instead of limiting yourself to the significance test, you can fit an appropriate model directly (e.g., via logistic regression) and look at estimates for the probability across groups.', 'Before we go any further, what test(s) did you actually run? It sounds like you intended to look for differences in the mean number of likes and retweets across topics, but that would require two one-way ANOVAs, and it’s not clear how that person being mentioned or not fits into the picture. If you’re interested in the likelihood of that person being mentioned across different topics, an ANOVA isn’t the most appropriate way to go about studying that.', 'Thank you for replying, I used a one-way Anova test per value (so I run a test for likes, one for retweets etc.). For whether the person was mentionned, I replaced ""yes"" by 1 and ""no"" by 0', ""Interesting, I didn't think about that, thank you, I'll give it a try""]",3,4,https://www.reddit.com/r/statistics/comments/11oiqo3/q_flawed_sample_should_i_still_use_anova/
788,2023-03-11 19:25:13,[Q] Guidance on probability and generalisability,"Apologies, as I know this is a bit basic, but I have read books, watched videos, asked colleagues, and paid people on upwork, yet still, I'm non the wiser... I would be very grateful for some help, please.

I carried out a survey with 88 Japanese university students studying English. I have one question from the survey that I'd like perform inferential statistical checks on, but I'm really not sure what tests to apply. Currently using JASP.

The survey question:

I asked 'What medium of goal setting do you prefer?' I offered them three options to choose from

a) online goal setting

b) paper-based goal setting

c) either is fine

The responses were

a) 58

b) 9

c) 21

What tests should I apply?

Thanks in advance for your assistance!","["">What tests should I apply?\n\nDepends on what you'd like to make an inference about. No testing necessary if you want to simply describe the responses of your sample. If you feel like your sample should tell you something about a larger group of people, you need to consider how your sample was selected from this group"", 'I was told by a statistician on upwork that ANOVA is the best way to get a p-value for this survey response. I\'m questioning this, as scaling categorical data like this seems wrong. Chat GPT suggests using a ""chi-square test to determine if the observed frequencies of each answer are significantly different from what you would expect by chance."" When I\'ve questioned the statistician on upwork he says:  \n""Anova is the best to check the significancy between three categories,,p-value tell us the results are not coming by a chance""  \nIt\'s fair to say I\'m still very much confused...', ""Thanks for your reply. I selected the sample based on convenience sampling.   \nRegarding population, I have a few options: EFL students at the institution (around 10,000); EFL university students in private universities within the prefecture (around 200,000); or within Japan (perhaps 5,000,000). Let's say within the institution. How would I test for significance in that circumstance?""]",4,3,https://www.reddit.com/r/statistics/comments/11ohsi7/q_guidance_on_probability_and_generalisability/
789,2023-03-11 19:10:44,[Q] Compare multiple groups proportions (for binary question),"Hi I was googling this but unsure if Chi2 or confidence intervals for the group are more suitable.

I have 4 groups with sizes between 100-1000. In ecevvery group I have numbers for outcomes A vs B (binary variable).

I can calculate the point estimate for A_share per grousroup
1: p_A = 0.8 (n=990)
2: p_A = ...
3: ...
4: pA = 0.3 (n=100)

How would you quantify if the groups are different?

Naively, I would have constructed CIs using binomial/normal approximation of it.
For groups with non overlapping CIs I would conclude that they do not habe the same p_A with a certain confidence.

However, online I mostly find that Chi2 tests are suggested.
To my understanding, a significant result would only mean tha not all groups come from the same population/same underlying p_A. This is a weaker claim than I would do with comparing CIs per group...

Can I do the CI approach? What to take care of when doing so?

(part of a private, non-academic and certainly non-representative survey I am doing)","[""Non-overlapping CIs will definitely tell you which groups are different. The trouble is that two groups can still be statistically different at some p-value level and have CIs that still overlap to some extent.\n\nAnd you are right, an omnibus test of a model with categorical (group-wise) predictors being better than a model without such predictors won't tell you \\*which\\* groups differ from each other - just that groups differ.\n\nI suspect a typical approach would be to treat it as a post-hoc comparisons problem. E.g., do a bunch of independent binomial tests, with some sort of p-value adjustment for multiple comparisons, like a Benjamini-Hochberg (aka FDR) correction. This is probably fastest.\n\nAnother method might be to take a Bayesian approach, treat the group effects as random effects, and directly compute the posterior distributions of any comparisons of interest."", 'Question of the form ""Do you prefer A or B"" with no other options. Posed to different subredits which I want to compare.', 'What’s the survey 🤔', ""Looking at non-overlapping confidence intervals is sometimes a controversial approach, but I think in this situation it's the approach I would recommend.  It's simple to calculate, interpret, and present graphically.  \n\nI would recommend using simultaneous multinomial confidence intervals.  (E.g. Sison-Glaz, and others).""]",7,4,https://www.reddit.com/r/statistics/comments/11ohj86/q_compare_multiple_groups_proportions_for_binary/
790,2023-03-11 16:32:51,[Q] How to determine what stands out in treatment levels after post anova?,"Hello an engineering student here. After using post anova whether it is confidence interval method, Fisher’s LSD, and/ or Tukey’s studentized range statistics, how do you determine what treatment level stand out the most ?","['Have you made and look at the boxplot of the data (response by treatment)? I think is a good start', 'What does ""stand out the most"" mean? Maximising the response variable?', 'I think the question was poorly worded because it appears it is asking for significance tests which are not a good way to determine what “stands out.” For significance testing, the LSD is considered too liberal (too high a Type I error rate) whereas the Tukey hsd controls the type I error rate. Some don’t realize it is fine to start with the hsd and skip the ANOVA entirely.', 'I suspect the question on the test is intentionally vague in order to get students to look at and think about the results.  Which group(s) have the statistically the highest mean(s) ?  The lowest ?  Are there two groups with a large difference ?  Do the differences have any practical importance ?  Is there one group that stands out compared to the others ?', 'Well in our test that was asked. There are 5 treatments and using anova, the alternative hypothesis was rejected thus post anova was used. Each treatment in post anova was tested (treatment 1&2, 1&3, 1&4 …. Etc). Then it was asked what stand out the most between each treatment']",1,5,https://www.reddit.com/r/statistics/comments/11oewjv/q_how_to_determine_what_stands_out_in_treatment/
791,2023-03-11 07:52:03,"[Q] Why is there ""total sum of squares"" and ""total degrees of freedom"", but no ""total mean square"" in ANOVA?","In SPSS's ANOVA output in the ""Total"" row, the ""Sum of Squared"" cell is populated, as well as the ""df"" cell.

However, the ""Mean Squares"" cell is empty. Why?","[""You can find it if you want; it'll be equal to the variance of your variable of interest. (Total sum of squares divided by n-1 should look like a very familiar formula to you.) \n\nBut it's not one of the numbers that is needed to calculate the F statistic; for that you need MSR, MSE, and their respective degrees of freedom."", ""Such a quantity exists (naturally) but it's not particularly useful/interesting typically""]",0,2,https://www.reddit.com/r/statistics/comments/11o4xeo/q_why_is_there_total_sum_of_squares_and_total/
792,2023-03-11 05:26:09,[D] Job more challenging than university,"Hi all! I work as a statistician in an factory. I would like to share my experience with you to know if it is common or not. For many reasons I find my current job more challenging than (or as challenging as) university. I had no difficulties during the first 3 years of university, while the fourth and the fifth year where tough but I finished with high final grades. Before getting a job, I did not expect to encounter so many difficulties at work. There are many things that troubles me:

* I realise I don't have much experience. I focused most of my time as a student to study statistics rather than to analyse many datasets. I still see myself as a beginner. I learn from every analysis. I always feel like I am not good enough and that data can be analysed in a better way.
* Datasets are more messy than university. It is very common to deal with outliers, short and/or intermittent time series, biases, etc.... Moreover data wrangling can take a considerable amount of time. I struggle a lot to get exactly the chart I want to report (maybe I need more time to get handy at using ggplot2)
* It is ridiculously easy to spend too much time doing a project
* I don't remember all the details of the methods I studied at university. Sometimes I feel the need to  revise some topics but there is not much time to do that. Sometimes I need to make decisions which I don't know fully how they would affect further analyses.
* At university it is obvious which methods are more appropriate to use for a specific dataset. Except for prediction problems, sometimes it is not easy to choose which method to use
* Sometimes it is not easy to think statistically
* I have poor social skills and talking is very important
* I tend to overthink about work a lot, even when I am not in the office. Having no teammates does not help either. I often feel the need to discuss with other statisticians but I don't have anyone to talk to except for online communities
* I often feel that the amount of effort I put in an analysis is not rewarded enough. I always compare my analyses with what I learnt at university. My analyses still look quite rough
* I feel a lot of pressure to solve tasks in a short time and get easily exhausted

Is it common ? Will it get better? Should I quit my job?

Thank you in advance.","['All of that sounds incredibly normal.', 'Wow. \nI thought I was the only one. \nYou just recited my exact experience!!!', 'It might get better. You’ll certainly get better and faster at the technical skills of analysis. \n\nCompany culture will have a big impact on whether the pressure and stress decrease or you just burn out.\n\nThe biggest thing you can do to up your game is to learn to translate your analyses into results that are meaningful to your customer. Talking to the production manager? They’ll probably be interested in takt time, defect rate, first time quality. Talking to business managers generally? It’s all financial: cost and profit. Engineers? Probably they care about performance against requirements. None of them understand statistics, so presenting outputs like p-values, odds ratios, or regression parameters is the least impact (good to have to show you’ve done your work, but they’re unlikely to understand).\n\nReal-world datasets are messy. In my experience, careful, by-the-book analyses was rarely possible. You do the best you can with what you’ve got; don’t let the perfect be the enemy of the good. That said, you can probably post small projects mid- to long-term to improve data quality. Look for opportunities to improve data collection methods, especially those opportunities that are easy and quick to implement and will allow you demonstrate a real improvement in the quality of your work or turnaround time. Think low-hanging fruit.', ""Agree that most of this is nearly at the level of cultural universals for statisticians. However, many statisticians at big companies work on teams with other statisticians and statistical programmers and don't feel alone in the stats work. Good teammates to chat/commiserate with can change the whole dynamic from misery to a great job. Bad teammates can make it even worse than being alone."", 'However I am enjoying learning how to make the skills translate. \nI spend an epic amount of time on google refreshing all the statistical methods.']",147,33,https://www.reddit.com/r/statistics/comments/11o1cmi/d_job_more_challenging_than_university/
793,2023-03-11 02:43:38,How to plot freq of zero in a scatterplot where taking the axes are log transformed [Question],"I want to make scatterplot comparing frequency of different diseases between two different populations

The issue I am having is where one population have a freq of zero but the other one does not.

How to deal with this? 

One thought is just to make it smaller than my smallest observed frequency, like .0001 or something. Thoughts?","['It is common to use Log(X+1) in this type of situation.', 'Don\'t just take logs of values that include 0? Why must it be logs, exactly?\n\nIf you must do it (take plain logs), insert a full scale break across the entire plot and mark it on that way (but make sure the axis-mark labels are in original-scale for that)\n\n(See the third plot [here](https://stats.stackexchange.com/a/63542/805) for an illustration of what a ""full scale break"" entails. That\'s for a boxplot, but it can be done for scatterplots just as readily. ... there\'s some other alternative options discussed there.)', 'that makes a ton of sense']",2,3,https://www.reddit.com/r/statistics/comments/11nx6qo/how_to_plot_freq_of_zero_in_a_scatterplot_where/
794,2023-03-11 02:16:31,[R] Statistical Control Requires Causal Justification,[https://journals.sagepub.com/doi/full/10.1177/25152459221095823](https://journals.sagepub.com/doi/full/10.1177/25152459221095823),"['I would guess they are complaining that this has been known for a while (although maybe not widely in certain fields).', ""They clearly are not passing it off as theoretically original. Many psychologists do not know about bad controls and colliders. Publishing what is basically a summary of prior theoretical results plus some applications to/implications for the field seems like a valuable addition. Psychologists probably won't seek out econometrics or causal inference literature from other fields, but might be more receptive to CI literature with some vague psych flavor."", 'I think it\'s funny how in a different context, the variables that are being ""controlled for"" are considered the ""features"" the model is extracting signal from. Who would\'ve guessed that the model would be biased towards the signals you use to fit it?', '> In this article, we illustrate that controlling for an inappropriate variable can result in biased causal estimates. \n\nHow does this stuff get published...\n\nOhh right, psychology', ""Ah, if that's the case then we're on same page. I agree, it is baffling and the reason I shared it is because I encounter reviewers who recommend throwing in a bunch of unjustified covariates or consult with students who have models with a million unjustified covariates and I'm really shocked by it sometimes.""]",12,15,https://www.reddit.com/r/statistics/comments/11nwhbz/r_statistical_control_requires_causal/
795,2023-03-11 02:13:23,[Q] Model prediction -- Can I report the model trained on full dataset first?,"Hi all, I am writing an essay on predicting a variable. I will be running multiple models like lasso, ridge, random forest, neural network etc. I would also like to explain on how to interpret the various model and their parameters. Can I first apply it on the full dataset and report their outputs for statistical inference and explanation, then perform prediction with cross-validation?","[""Usually it's done in the opposite order.  You select a model, using the performance with cross-validation or a test set, then you train the model on the full data for production.  You can do inference on the final model, however there will be a bias introduced from model selection that is difficult to quantify."", 'What bias do you mean? The generalization error estimated through cross-validation is an estimate whatever you do with that model afterwards, whether you use the full dataset or not.', 'It’s a noisy estimate, and by choosing a model you’re introducing a selective pressure for both high signal and high noise at the same time.  After you choose the model, the cv estimate is no longer valid because you’ve already used it to select a model (no peeking!).  In cases where there’s a lot of models to choose from, eg Kaggle competitions, the highest performing models likely have high variance.  For example, see https://laurenoakdenrayner.com/2019/09/19/ai-competitions-dont-produce-useful-models/ for an explanation of this idea.']",2,3,https://www.reddit.com/r/statistics/comments/11nwead/q_model_prediction_can_i_report_the_model_trained/
796,2023-03-11 00:44:22,[Q] Directional Statistics: Cramer-Rao and Rao-Blackwell,"Hi all,

I've been reading more into directional statistics and came across a question that I couldn't find an answer to. So I figured I would ask you all to see if there was a good reference for or a proof that the Cramer-Rao lower bound in directional statistics is truly the best variance and if there is a Rao-Blackwell theorem for the circle?

More specifically about the first part of my question. It's fairly easy to show that the unwrapped distribution (typically) fails the usual Cramer-Rao bound since it does not satisfy the regularity condition with interchanging differentiation and integration. In part, this is due to the fact that the estimator need not be cyclic with some frequency. It seems like, to me anyway, directional statistics only looks at the class of estimators that are cyclic and derives a bound on the variance. Does this mean that there could be an estimator that does not fall into this class of cyclic estimators that has a better bound on the variance? 

As a bonus question, suppose there are two parameters of interest: x and y. For illustrative purposes, I shall choose the distribution to be the generic cardioid (1+x cos(t-y))/(2pi). In this case, it's clear to see that y requires the directional version of the cramer-rao bound, but x does not and can be solved via the usual Fisher information. Can such a hybrid Fisher information matrix make sense and how would one define it? My best guess is that one could define the diagonal entries the same usual way, but then one has a weird off diagonal term. I'm almost certain that one could define it using some hybrid approach, but a proof illudes me at this time.

Best,
QoO","['There’s a YouTube channel called statisticsmatt (also a Reddit user with the same handle) with a video on Cramer-Rao that may answer your question. I was going through his playlists myself and the info seems high quality but I don’t have a sufficient background to guarantee the video answers your question as I’m not studying that part yet.\n\nLink: https://m.youtube.com/watch?v=KDsd1F6oy78', ""I suspect you've had issues looking for your first request because the proof you are looking for probably doesn't exist. The CRLB is a rather general lower bound, and it's not always the case for a given problem that there are any estimators that can in fact achieve the CRLB.\n\nThat is, the CRLB is not the best possible lower bound generally, there are cases where tighter bounds may exist. There are of course scenarios where estimators do in fact achieve the CRLB, which makes it a tight bound in those scenarios.\n\n&#x200B;\n\nFor the Rao-Blackwell theorem, I would suspect the standard proof of Rao-Blackwell can be pretty directly transferred, e.g:\n\n[https://gregorygundersen.com/blog/2019/11/15/proof-rao-blackwell/](https://gregorygundersen.com/blog/2019/11/15/proof-rao-blackwell/)\n\nThat said I wouldn't spend too much time focusing on the Rao-Blackwell theorem for directional data. Non-maximal sufficient statistics are actually pretty specialized to exponential families and often don't readily generalize. I don't have much intuition of how they will generalize to directional data. That said I haven't really worked with directional data and am just barely aware of the topic. That said, I probably will be deep-diving into it in about three to six months for a portion of own research I've been pushing off, so if you want to discuss with someone you're welcome to reach back out later.\n\n&#x200B;\n\nRegarding the bonus question, this can easily get messy. Your joint distribution is now a distribution on a new manifold that is a product manifold of Euclidean space and the circle. The covariance structure on this new manifold is going to be naturally defined in terms of the metric of this manifold. A lot of results you might expect to carry over into statistics on manifolds, e.g. Cramer-Rao: but you need to develop a lot of machinery to deal with it properly.\n\n[https://doi.org/10.1016/0047-259X(91)90044-3](https://doi.org/10.1016/0047-259X(91)90044-3)\n\n&#x200B;\n\nIf you need practical results soon, I do have an alternative suggestion that's not totally correct, but not totally incorrect either. Rather than treating the parameter as lying on a circular manifold, you can consider a rotation of the data so that you genuinely do not expect the parameter to lie anywhere near 0, and consider the parameter to be constrained to the interval (0, 2pi). You can then apply either constrained optimization techniques, or alternatively transform this interval to (-inf, inf), and use unconstrained estimation. Provided your parameter does not lie particularly near the boundary you should get roughly correct results doing this."", ""This is a good video. I've actually watched it before when I was learning about Cramer-Rao bounds. Unfortunately, this is for distributions described on the real line. It's quite interesting how the geometry of the underlying space changes how one should think of and utilize the statistics. This is effectively the heart of the questions I've posed. How is it different and how can you combine them (if that's even possible)?"", 'My apologies the question is a bit out my expertise and I’d rather not mislead but I hope you find someone more familiar', ""It's alright! I've found that this subject is quite new, there isn't a lot of literature on it, and niche. I mean, how often does one want to analyze a distribution which is defined on a circle? Anyway, it's a great video and thanks for reminding me about it. I'm currently writing a small tutorial about Cramer-Rao for others in my group and I'm sure that this will help write it. \n\nAnyway, there was no misleading here. If you are interested in learning more, I highly recommend Robby McKilliam's thesis and the book(s) by Mardia and Jupp. Unfortunately, they didn't quite get into the questions that I'm interested in, but they are incredibly readable and instructive.""]",5,7,https://www.reddit.com/r/statistics/comments/11nu3nk/q_directional_statistics_cramerrao_and/
797,2023-03-10 18:17:18,"[Q] Is this a ""Robust"" Mahalanobis distance (outlier detection)","Hello! 
I've found this webpage (https://github.com/friendly/SAS-macros/blob/master/outlier.sas ) where some SAS code is used to use Mahalanobis distance for outlier detection. Actually, since the first step consist in standardizing the data via PCA  ( proc princomp) the  distance used is the sum of squares.
So, can this be considered a robust version of the Mahalanobis distance to get outliers?","['Anything that uses a ""traditional"" variance computation (squared distances of all points) won\'t be ""outlier-robust"" in the usual meaning of that phrase.', 'Thank you for your answer! Then what do you think the author meant by using the adjective ""robust""? 🤔', ""I hadn't clicked through fully. The description says:  \n> The macro makes one or more passes through the data. Each pass assigns 0 weight to observations whose DSQ value has Prob ( chi² ) < PVALUE.\n  \nSo it's not just normalization by PCs, it's iterative reweighting and excluding data points. That's where the robustness comes in."", 'Ohhh got it, thank you very much!']",0,4,https://www.reddit.com/r/statistics/comments/11nl9ao/q_is_this_a_robust_mahalanobis_distance_outlier/
798,2023-03-10 15:45:45,[Q] What advantages does a bachelor's in statistics have over a bachelor's in computer science?,"Nowadays everyone and their grandmas are majoring in CS, and it seems like CS is a degree that can get you almost every job in tech or data analytics like software engineering, product management, data science, data analytics, data engineering, etc. That being said, are there any benefits to doing a bachelor's in statistics instead of CS for getting a tech or analyst job?","['Actually knowing statistics and making better analyses of data?', 'CS major here. ""Advantage"" depends on the kind of job you are looking for. For most jobs it doesn\'t really matter what you have studied. \n\nThere are however, a few jobs where you want a more mathematically minded person. For example I work in the medical device industry. And we definitely want people with a strong background in mathematics and especially statistics. \n\nIn the end we need to convince the regulatory agencies that our products are safe and have clinical value. And a proper statistical analysis goes a long way in doing that.', ""Don't get me wrong. Programming skills are important. But as an analyst you don't have to be a pro. A little bit of SQL, Python/R/Matlab or whatever your favorite poison is, is enough. Nobody is going to let you touch the production code base anyways. You are mostly building and evaluating models on your own. And nobody cares how bad the code you are running on your machine looks like.\n\nOnce you hit upon something that might be useful in a production environment, you team up with the engineering colleagues who have the expertise to properly implement it."", 'Are you talking about statistically significant advantages?', 'If you’re stopping with a BS, it’s hard to beat CS. If your career path necessitates grad school, things get murky.']",4,13,https://www.reddit.com/r/statistics/comments/11nikj4/q_what_advantages_does_a_bachelors_in_statistics/
799,2023-03-10 15:15:33,"[S] [E] Tool to ""play around with"" statistical analysis","Hi!

Is anyone aware of any tool where you get some set of data and can test different statistical methods for practice and fun (just to get a lot of repetition and practice an intuitive response to data analysis)? I have studied applied mathematics but that was many years ago so it would be nice to refresh my memory.","['R', 'Jamovi if you’re not ready for R just yet.', 'R has loads of built in datasets (possibly by installing a package, I can’t remember which datasets are in base R and which aren’t).\n\nFor the methods, yes, you need to know what you want to do. Just find a list of statistical tests online, I guess? This isn’t a problem a software can solve, they are tools, you need to tell them what to do.', ""Orange.\n\nR and Python have a steep learning curve if you are not into programming.\n\nOrange is all about drag'n drop & configure widget.\n\nThat being said you will need to learn Python or R if you want to get anything serious done."", 'You can try Swirl: [https://swirlstats.com](https://swirlstats.com/)\n\nIt has interactive lessons in R that teach basics of data science, stats and R itself with real datasets.\n\nAlternatively you could try generating your own data from probability distributions with certain parameters (population mean for example). See how much you can infer about the population parameters from the data using statistical methods - e.g. how close is the sample mean to the ""true"" population mean that was used to generate the data? How much closer does it get if you increase the number of datapoints? Etc.']",1,9,https://www.reddit.com/r/statistics/comments/11ni0c8/s_e_tool_to_play_around_with_statistical_analysis/
800,2023-03-10 11:24:33,[Q]Principal Component Analysis vs Multiple Regression,"I'm curious about PCA and what does it actually do to the data. And what does it explain compared to multiple regression? What are the differences between the two. Say I have 5factors for IV, and 1 DV. What gives me merit to use the PCA instead of Multiple Regression","[""PCA doesn't have a response. It's unsupervised and is aiming to explain the variance in the data itself.\n\nRegression is supervised and aims to explain the response."", 'It is perhaps worth mentioning that PCA can be used for regression. This is called «Principal Component Regression» (PCR). Although, since PCA attempts to describe the variance in the independent variables rather than the dependent variable, one will more often see a similar concept called «Partial Least Squares» (PLS). This regression approach also uses principal components but the PCs are estimated with covariance constraints.\n\nIn fact, PLS is a very popular tool in my field (chemometrics) as it can be used to predict outcomes from thousands of highy correlated independent variables. It is often applied to infrared spectroscopy.', 'It means there is a ""response variable"" or ""dependent variable"", and an algorithm is trying to make a map from a dataset to predict/recreate that variable.  \nThe specific terminology - guessing a bit - refers to the optimization, where the error/loss is ""supervised"" by comparing the ability to recreate the response variable, so there is an objective way of saying if a model is good. Unsupervised algorithms have no ""guide"" for what a good model is.', ""It does the regression in a new latent space that all predictors and response variables are projected into. It doesn't assign variation to any of the original independent variables"", 'PCA basically helps you find the combination / set of of variables that describe your data the best. You can then use these factors or a subset of them as IVs in your regression. For example, if you have a large number of variables (some of which may be correlated), PCA helps to reduce this down to those which capture the most variance in the data.']",45,18,https://www.reddit.com/r/statistics/comments/11nd922/qprincipal_component_analysis_vs_multiple/
801,2023-03-10 10:55:22,[Q] Cannot remember the name of this specific sampling error,"For example, if you want to collect data on job prospects of young adults in a certain town, but use a list of high school graduates as a way to collect your sample, you're missing the young adults who never graduated, thus not sampling the entire population. I swear there was a name for this specific kind of sampling error, but I cannot remember it. Any help would be appreciated, thanks!","['It’s called under coverage.\n\nHere’s the first Google result so you’ll believe me:\n\nhttps://ec.europa.eu/eurostat/cros/content/under-coverage_en', '[Selection bias?](https://en.m.wikipedia.org/wiki/Selection_bias)', 'Awesome, thank you very much']",1,3,https://www.reddit.com/r/statistics/comments/11nckyp/q_cannot_remember_the_name_of_this_specific/
802,2023-03-10 09:56:22,Standard deviation of a frequency [Question],"

500 of 10000 of my samples have a disease, which gives me 5% frequency.

I want to get a standard deviation for the 5%, because my thinking is that the error from 500 of 10000 samples would be smaller than say 5 of 100 samples and I want to communicate that.

Any help is greatly appreciated.","[""1. https://en.wikipedia.org/wiki/Binomial_distribution\n\n  Variance of the *count* (X) of people with the disease, out of n people is  np(1-p)  (where p is the population proportion)\n\n   The proportion with the disease is X/n. By [basic properties of variance](https://en.m.wikipedia.org/wiki/Variance#Addition_and_multiplication_by_a_constant), the variance of the proportion with the disease is \n1/n^(2) x np(1-p) = p(1-p)/n\n\n  The standard error will be the square root of that\n\n2. Also see https://en.wikipedia.org/wiki/Margin_of_error#Standard_deviation_and_standard_error\n\n Standard error of the sample proportion is √[p(1-p)/n]\n\n  Of course you don't know p but you can estimate it by X/n\n\n3. Some information here as well: https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval"", 'Your frequency is the sample average.  So your st. dev is the sample standard deviation. It is proportional to 1/sqrt(sample size). \n\nSo increasing sample size by a factor of 100 will decrease your st dev by a factor of 10.', 'So if I follow\n\np = .05\n\nn = 10000\n\nso stderr is\n\nsqrt((.05 * (1 - .05)) / 10000) = 0.002\n\nIs it right?', ""Well, p is the population proportion, not the sample proportion, but yes, if you estimate p by X/n then yes, that's the estimate of the standard error. \n\n[When p is small, a quick approximation is sqrt(X)/n  (about 224/10000 in this example; i.e. 0.00224); the 'correct' answer will be a little smaller than that approximation]""]",3,4,https://www.reddit.com/r/statistics/comments/11nb7aw/standard_deviation_of_a_frequency_question/
803,2023-03-10 09:53:01,[E]Triangle Test to see if two samples are the same (null hypothesis) at 95% confidence shows near half of respondents choosing correctly would result in failing to reject the null hypothesis. Am I missing something?,"I'm trying to wrap my mind around [triangle tests](https://www.sensorysociety.org/knowledge/sspwiki/Pages/Triangle%20Test.aspx). It might just be statistics in general as it's been a few years since I took a statistics class...

If the null hypothesis is the two samples taste the same, why can upwards of half the respondents (26/60) taste the difference and still be at a 95% confidence interval that the samples taste the same? It looks to me like half the population just said that they could taste the difference. I get that the chance of guessing it is 1/3 or 20 respondents guessing it correctly, but if comments are all in line with ""this was obviously different"" it doesn't sound like guessing. They could be lying, but still... close to half?

Unless triangle tests are meant to be taken by thousands (1000 people looks like roughly 358 correct responses) of people. Most of what I can find seems to suggest in the 30-50 range though. At 40 people that is 18 correct responses would yield a ""these effectively taste the same"" response. So even though half of the people chose the sample as being different it would be 95% confidence that their is no difference between the samples.

[Here](https://www.awri.com.au/industry_support/winemaking_resources/calculators/sensory-difference-test/triangle/) are a [couple](https://onbrewing.com/triangle-test/) of the online calculators I used after I doubted my math in google sheets. Seems to me if half of the people tasting something say it tastes different... saying they taste the same wouldn't be a great choice.

What would be a better confidence interval to choose? 99.9% shows me at 50 respondents would be 5 people getting it right (probably needs double checking by someone smarter than me).

Am I misunderstanding what a 95% confidence interval means? I read that as I have a 5% chance of being wrong. And when the null is the samples are the same... but half of people can taste the difference... that seems like more than 5% chance of being wrong.

Thanks for any help in getting this through my skull.","['Thinking about it in terms of what percentage of people get it right is misleading since the critical value (over what percentage you would reject the null) decreases when the sample size n increases.\n\nInstead, think about the probability of getting results at least as extreme under the null hypothesis (i.e. the p-value).  In this case with a probability of getting it right by chance of 1/3 the exact probability of getting at least 26 right answers out of 60 is 0.068 > 0.05. This means that your data are ""compatible"" with the samples tasting the same and that you did not gathered enough evidence to reject this hypothesis.\n\nIt\'s important not to change the significance level after observing the results. That\'s a definite no-no!', 'Thanks! That clears up what I thought the test was for.\n\nMy statistics comprehension is: I took the couple of classes I needed at university for my degree and then promptly forgot it for 5 years. So... I know enough to get in trouble, but not enough to be useful. 😂', ""Is a triangle test's purpose to show that in a larger population you would expect to see similar results as what you tested? For example, that we are 95% confident that if you tested 600 people, you would see roughly 260 respondents choosing the correct sample where we expect 200 of those to be by chance, and that the more people you test the lower those numbers would be? Or is this also incorrect?\n\nI am currently under the impression that a triangle test shows that samples are indistinguishable from each other. Failing to reject the null hypothesis should mean that the samples are effectively the same. If half the respondents can pick it out though, you should reject the null, as to half the respondents it was different. Do we subtract the amount we think would be randomly guessing? 6/60 seems more reasonable to say the samples are the same than 26/60."", ""You are testing whether the probability of getting it right is 1/3, which means that the participants are unable to detect a difference and are choosing the right option purely by chance. An event with a 1/3 probability occurs, on **average**, one-third of the time. However, in a single experiment, this proportion might change.  \nAn extreme example would be testing whether a coin is fair, with each side having a 1/2 probability. If you toss the coin four times and get heads all four times (i.e., 100% of the time), under the null hypothesis of a 50-50 probability of results, the probability of getting results at least as extreme as this (the p-value) would be 0.0625. Since the p-value is greater than 0.05, you fail to reject the null hypothesis, even though you observed a 100% frequency of heads. Observing the same outcome of all heads with a larger number of throws would allow you to reject the null, but with small sample sizes, you can obtain these kinds of results.  \nFinally, failing to reject the null hypothesis does not mean proving the null hypothesis. You may still have an effect, but if it's not that big your experiment may not be able to detect it since you don't have enough participants. \n\nI hope this is somewhat useful to you. I really cannot tell where you are at as far as statistics is concerned. I apologize for any confusion.""]",4,4,https://www.reddit.com/r/statistics/comments/11nb4hu/etriangle_test_to_see_if_two_samples_are_the_same/
804,2023-03-10 04:41:12,[Q] Forecast Accuracy,"Question:

My company is looking at a different way to calculate forecast accuracy, which is great, because it will start incorporating WAPE instead of MAPE. This will solve for our low selling, high volatility items.

As one of the main users of this data, I can say that I HATE seeing accuracy that is outside the range of 0 - 100%. HATE IT. As a fix, I'm thinking about suggesting that we can make an adjustment to how the WAPE is calculated in the form of an IF statement.

IF the SUM of all forecasts are less that the SUM of all the sales, then SUM(forecasts)/SUM(sales). This will ensure the accuracy is between 0 and 100. On the other hand, IF the SUM of all forecasts are grater than the SUM of all sales, then SUM(sales)/SUM(forecasts). This will also solve for weighted items and keep accuracy between 0 and 100%.

Can anyone offer why this might be a bad idea?

Thank you kindly","[""What's the problem with 102%? If you don't like the numbers over 100%, divide sMAPE by 2.\n\nWhat happens if you forecast 0 and observe 0?"", ""Thanks for the reply.\n\n102% would suggest we overforecasted by 2%. That's fine, and we can **BIAS** calculations for that (forecast of 100, but sold 102), but to be 102% forecast accuracy would suggest we were so exact on our forecast (100%) that an additional 2% needed to be added to convey how awesome our forecast was lol. With my equation, even though we overforecasted by 2, the accuracy would be 98%.\n\nAlso, if I recall correctly, we would have items with sMAPE over 200, which would leave us with the same issue.\n\nWe specifically want to use weighted accuracy for when we have multiple items like this:\n\nItem A forecast 1000, sales 1000. 100%\n\nItem B forecast 2, sold 1, 50%\n\nThe old way suggested out forecast accuracy was 75% overall, despite accurately forecasting for 1002 units and selling 1001.\n\nWe have filters and exception reporting for the 0 concerns."", ""sMAPE is bound between 0 and 200%. The worst case is when you forecast x but observed 0, sMAPE=|x-0|/(|x+0|/2)=2. If you divide sMAPE by 2, you won't get values above 100%."", 'man thats interesting. I am working on forecasting project at my work and i havent made it to evaluation part yet.\n\nCan u share some of your resources about what is the best way to evaluate multiple product forecasts? I will have the exact evaluation problem you are facing rn and i want to start searching', ""That's my understanding as well, but I've seen JDA calculate some items' sMAPE well above 200; sometimes over 1000. Never understood why. Perhaps if I come across one tomorrow I can take a heavily redacted photo.""]",3,6,https://www.reddit.com/r/statistics/comments/11n355r/q_forecast_accuracy/
805,2023-03-10 03:45:24,[Q] - Guidance for Causal Inference,"I am trying to learn causal inference as I find it a very fascinating and useful subject. I started reading Judea Pearl's book ""Causal Inference"" and I think it is very well written.   
I am still trying to grasp everything but I feel like a lot of methods are very methodical and can be programmed such as the do-calculus stuff. There is also the method mentioned in the book to verify whether a certain graph fits or not with the data. I haven't finished the book yet, but for now, from what I understand, causal inference is even harder with continuous variables.  


Do you have any specific framework, programming library, pro-tips and ways to approach problems that you would recommend?  
Is there a standard way to approach every causal inference problem?  
Any state of the art python library that is used in the field?  
Any exhaustive tutorial?  
Anything else I should be aware?  


Thanks for the help and guidance","['As a practitioner, this is one subject that I have strong opinions about. The one thing to be aware of is that there is no one method for causal inference that works for every problem. I will grant that Pearl\'s work on causality is an impressive intellectual edifice, but his methods just aren\'t very useful in applied, day-to-day work. Research, sure, but if you have an observational dataset you\'re not going to be breaking out the do-calculus juju or trying to fit a reasonable causal DAG from your data. (In fact, I would argue that there are vanishingly few instances where causal graph discovery is actually useful for what you want to do.)\n\nYou might want to focus on learning the bread and butter of methods for causal inference: treatment assignment mechanisms, matching, natural/quasi-experiments including DiD and RDD, and IV. Depending on the kind of work that you want to do, you can branch out from there. In terms of other starting points, I would argue that there are four ""schools"" of causal inference. Pearl\'s work falls squarely into the kind of algorithmic DAG/SCM approach that many CS researchers take to causal inference. Then there\'re the econometrics folks who take a regression-centric approach and tend to care a lot about identification arguments. Their work has more of a math stat flavor to it too. \n\nThe other two schools worth mentioning are the epi/social science, and observational study/design-based folks which overlap somewhat. Compared to the CS/econ camps, these folks do a lot more matching. A lot more. That\'s the one key difference. Beyond that, the similarities are harder to pin down. Particularly in the design-based approach, where you see a lot of fun stuff like sensitivity analyses and permutation tests based on sharp null hypotheses that you don\'t see anywhere else. Read the first two books on the below list to get a sense of both schools.\n\nIn terms of further reading, check out:\n\nHernan & Robins, *Causal Inference: What If*\n\nRosenbaum, *Design of Observational Studies*\n\nAngrist & Pischke, *Mostly Harmless Econometrics* (or *Mastering \'Metrics*. Scott Cunningham\'s Mixtape book is good as an intro as well.)\n\nImbens & Rubin, *Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction*', 'Hello, 3 books/resources come to mind, mostly in R:\n\n1) Richard McElreath: Statistical Rethinking a Bayesian Course in Stan and R (Has code translated to Python language too among others, plus online lectures)\n\n2) Nick Huntington- Klein : The Effect \n\n3) Babette A Brumback : Fundamentals of Causal Inference in R\n\nHope these help and cheers.', 'Absolutely. This and Bradley Neal’s lecture series on YouTube get you pretty far into grokking the basics of causal inference.', 'I recommend the crash course for causal inference on Coursera. https://coursera.org/learn/crash-course-in-causality', '>Is there a standard way to approach every causal inference problem?\n\nIt\'s all about being really familiar with the subject you\'re analysing, knowing all about the relevant science etc. Pearl and others provide tools for reasoning systematically about a set of assumptions that allow expressing causal parameters in terms of  estimands (at which point you can use statistical techniques to estimate them).\n\nSuppose you do all that and present your analysis and draw the DAG, and someone says ""so what about U. Doesn\'t that affect both Y and X""? or ""you only observe data if U, and U also affects Y"". If you can\'t justify the absence of these arrows in the DAG, no one will believe your analysis.']",36,20,https://www.reddit.com/r/statistics/comments/11n1p4c/q_guidance_for_causal_inference/
806,2023-03-10 03:15:34,[Q] Selecting the best experimental design for a project expansion,"I conducted n extensive research project where we designed an unbalanced factorial to evaluate the effect of 4 different factors (one continuous and 3 discrete). Now, I want to expand into a larger project, using some of the outcomes of the first phase but we are realizing that there might be more to the data we investigated. 

Traditionally we use full factorials as our experimental design but I think it is too inefficient. I'm thinking of proposing a Screening design, covering as many variables as possible, and after we have the results from that, create a new design to conduct all the experiments and generate a prediction model (we seem to be leaning towards a multivariate linear regression model for the prediction). 

With that in mind, can you help me think about potential experimental designs that might be more efficient (I'm thinking something like a Latin Hypercube but accounting for variability since it is for physical experiments).

&#x200B;

Happy to clarify more if needed. Thanks!",['How many potential factors and how many levels?\n\nDo you suspect any curvature/polynomial terms in the relationships?'],1,1,https://www.reddit.com/r/statistics/comments/11n0wpa/q_selecting_the_best_experimental_design_for_a/
807,2023-03-10 02:22:55,[Q] How do I get the value for an 85% C.I. of a given sample if the 95% C.I. is given?,"Hey folks, I am a beginner and am studying for an upcoming exam. I stumbled upon a task which says:

given is the 95% C.I. for a sample of monthly mean wage in country x \[3122, 3856\]

**What is the upper threshhold for the 85% C.I?**

The correct answer is 3759. I have tried drawing it, cutting percentages by counting S.E. but it didnt amount to that answer. Help would be much appreciated!","['You need to know how the 95% CI was calculated. Is it based on the normal or the t distribution? Then, consider these hints:\n\n* The formula for the CIs is x ± Q*SE. Q is the quantile of the normal or t distribution and you have all information you need to calculate it (or look it up in a table). SE is the standard error, which is the crucial missing information you need to calculate from the given information.\n* The CI is symmetric around the mean.', 'I got the S.E. by doing 3856-3122 = 734, then 734 : 4 = 184 (rounded)\n\nThen for x, it should be the median of the given C.I. (since its symmertrical) which is 3494 (rounded)\n\nThen I looked up the table for my quantile, which for the 85 % C.I. is 1,440\n\nThat amounts to: 3494 +- 1,44 x 184 = 3759 (rounded).\n\nThank you so much for trying to help me without giving everything away :D']",2,2,https://www.reddit.com/r/statistics/comments/11mzjaq/q_how_do_i_get_the_value_for_an_85_ci_of_a_given/
808,2023-03-10 02:17:44,"[Q] Linear Regression - Insignificant variables, what to do?","Hi all, I am running a Linear Regression for my analysis and I have some insignificant variables, and I am not sure which model should be my final model. I can think of 2 options:

1) Leave the model as is, but report the insignificant variables

2) Run the model model, but remove the insignificant variables.

What is the correct way?","[""There isn't one correct way and what you should do will depend on your purpose. \n\nFor instance, if you are looking to publish the model in an academic journal and the variables are important control variables, then you might want to leave them in your model for the edification of readers. \n\nAlternatively, if you are planning to implement the model for regular use, then removing the variables may provide a significant improvement on resource expenditure, e.g. computing time, memory."", 'Why would you remove variables because their coefficient has a higher p value than some arbitrary threshold?', 'Backward selection and other stepwise methods are biased, better to use regularization', 'I suggest that you keep the whole model and include all variables. If you remove insignificant variables your adjusted R^ will be seriously biased.', ""What is the purpose of your regression?\n\nAre you trying to make a predictive model or determine causality?\n\nIf you are making a predictive model, go with the one that fits the data the best. You could use a technique such as AIC which will tell you how well the model fits the data. You would compare models with combinations of your different predictors and find the one with the best fit.\n\nIf you are trying to determine causality it's a bit more complicated. You need to be much more careful about your parameter selection, ensuring you do not have conflicting parameters that will bias your estimations AND ensure you are using 'controls' properly (avoiding over-control bias). You need to draw out how you think the system works and generate your model based on that, it may require separate models for your various predictors of interest. [Here's a short-ish piece that might help.](https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecm.1554)""]",7,11,https://www.reddit.com/r/statistics/comments/11mzecu/q_linear_regression_insignificant_variables_what/
809,2023-03-10 00:44:25,[q] is there an R package where I can perform (group) lasso to fit a conditional logistic regression ?,"I am using package “clogitL1” but it doesn’t allow for group lasso. Is there a way around or accommodate this?

Is there a way to manipulate glmnet for conditional/group using family=cox (stretch)

Thanks.","[""Interesting question, there's a plethora of packages out there but it was surprisingly hard to find one for doing this. \n\n&#x200B;\n\nIt looks like the grpreg package should be usable to do what you want here. It directly supports group lasso for the cox model, so you should be able to use the cox equivalent of your conditional logistic model."", 'What do you mean by ""group""? As long as you\'re referring to regular old conditional logistic regression used when the number of events is fixed before the study, e.g. most common use is case control studies, then use glimnet with the ""cox"" family option, with common follow-up time (set to 1, it doesn\'t matter). For example, suppose the data frame, DAT, contains variables D (a 0/1 response) and a bunch of named covariate columns. Then\n\n    library(glmnet)\n    D <- DAT$D\n    X <- DAT[, names(DAT)!=""D""]\n    Ti <- rep(1, nrow(DAT))\n\n    fit <- glmnet(x=X, y=cbind(Ti, D), family=""cox"")\n    tune <- cv.glmnet(x=X, y=cbind(Ti, D), family=""cox"")\n\n    coefficients(fit, s=tune$lambda.min)', 'This will be a bit ugly but…Group lasso vs lasso refers to how coefficients are reduced to zero ie variables dropped. In lasso all variables are treated Individually where as in group lasso you can treat a series of variables as a group, either all variables are included or excluded. It’s important for certain types of responses/questions. \n\nhttps://www.r-bloggers.com/2021/10/exclusive-lasso-and-group-lasso-using-r-code/amp/', 'That would be totally fine!', 'Can u explain why logistic regression']",1,7,https://www.reddit.com/r/statistics/comments/11mwzbn/q_is_there_an_r_package_where_i_can_perform_group/
810,2023-03-09 23:53:20,[Q] Question about sample size calculation," I'm looking at a long term follow-up project and in the sample size consideration section it is described as such:

The total person-years of follow-up is approximately 9045 for the 1500 enrolled patients. This number of person-years of follow-up will provide 91% likelihood of seeing at least one event of interest, if the true rate per 15 years of exposure is at least 1:250.

I don't understand how the 91% likelihood is calculated. Can someone please explain?","[""Thank you, this is very helpful.  I didn't thought about using the survival and cumulative hazard function to solve this."", 'Most likely, assuming a constant event rate. Then the probability of observing 1 or more events is \n\n    1-exp(-h PY) =1-exp(-(1/(250*15))*9045)\n                 =0.9103642\n\n\nwhere h is the annual event rate and PY is the total person years.', 'I know what you mean. We usually use counting process data to estimate population level functions, like the hazard or survival function corresponding to a single person. But poison processes are infinitely divisible, which means the aggregate is a poison process with a cumulative rate equal to the individual annualized hazard rate the times person years on study. The probability of 1+ events is the complement of no events, which is the probability written above.']",2,3,https://www.reddit.com/r/statistics/comments/11mvolx/q_question_about_sample_size_calculation/
811,2023-03-09 22:45:01,[Q] Need some help with premium statista reports for a thesis!," 

Hello! I'm trying to get some data about digital marketing, specially in Spain, but the premium account is quite expensive for me.

I need these reports for now, wondering if someone can DM and send me the pdf's through email. Thank you!

These are some examples i need.

[https://www.statista.com/statistics/237974/online-advertising-spending-worldwide/](https://www.statista.com/statistics/237974/online-advertising-spending-worldwide/)

[https://www.statista.com/statistics/245585/distribution-of-the-global-digital-advertising-spending-by-region/](https://www.statista.com/statistics/245585/distribution-of-the-global-digital-advertising-spending-by-region/)

[https://www.statista.com/forecasts/1271893/spain-digital-advertising-spending-distribution-industry](https://www.statista.com/forecasts/1271893/spain-digital-advertising-spending-distribution-industry)

[https://www.statista.com/statistics/307005/europe-online-ad-spend/](https://www.statista.com/statistics/307005/europe-online-ad-spend/)

I'd be very greatful if someone could help me out.",[],0,0,https://www.reddit.com/r/statistics/comments/11mu01m/q_need_some_help_with_premium_statista_reports/
812,2023-03-09 19:51:15,[Q] How to report my linear regression model with 50 dummy variables?,"Hi all, I am doing a thesis on my dataset using Linear Regression. There are a total of 50 dummy variables of 3 variables. How do I report my final regression model? It looks funny if the equation is half a page long.","['Has anyone on Reddit already found out u are using chatgpt to answer? Nice social experiment btw', ""Sounds like you're trying to apply linear regression to categorical variables.\n\n[Try something like this.](https://i.imgur.com/VMACh1c.png)"", 'One way to report your final regression model with 50 dummy variables is to provide a summary table of the coefficients for each variable, as well as the overall model statistics (R-squared, F-statistic, etc.), rather than writing out the full equation. You could also consider using a regression tree or other visualization techniques to display the relationships between the variables in a more concise way.', ""As soon as I read that first sentence, I could tell it was a ChatGPT answer. It's such a distinct style of of writing."", 'I’m surprised that nobody has mentioned that, but you probably want mixed effects modeling (unless you are really specifically interested in the main effects for each individual town). Town name should be a random effect.']",40,35,https://www.reddit.com/r/statistics/comments/11mq7yw/q_how_to_report_my_linear_regression_model_with/
813,2023-03-09 19:24:53,Struggling with % [Q],"Losing my mind over this basic sum of percentages that I am using to calculate arbitrage bets: there are 3 odds of the same match (1.91 + 16.50 + 10.50), their % sum is ~68,2%, so the profit amounts to ~31,8%. However, when I calculate how much (total) money should I put on each odd  to get 131,8€ as a secure profit (should be 100€) using this calculation ( image under this post  ) but instead  it gives me ~89,5 and not 100 as it should. What am I doing wrong?",['[calculation used](https://ibb.co/XymGBDn)'],0,1,https://www.reddit.com/r/statistics/comments/11mpr5t/struggling_with_q/
814,2023-03-09 17:49:30,[Question] Why is correlation calculated using percentage change so different than using absolute values? What is the correct approach,"Suppose I have two variable (both in millions) and I want to see correlation between the two. I have two approaches and both give different results. What is the ""correct"" way to approach this problem and why do the two correlations differ?

Method 1: Calculate correlation between A and B using percentage change. (where percentage change shows in % how the value of A and B changed from the last quarter)

Method 2: Calculate correlation between A and B using absolute numbers.

The first method gives 0.25 (low) and the second method i.e method 2 gives 0.65 (much stronger). What is the correct way to go about calculating correlation between A and B in this case?

Sample Numbers below (Table in Method 1 is calculated from table in Method 2):

For Method 1:

&#x200B;

|a (percentage\_change)|b (percentage\_change)|
|:-|:-|
|59.10%|184%|
|53.7%|88.5%|
|\-10.8%|\-19.2%|
|\-1.1%|\-1.9%|
|0.2%|77.8%|
|150.4%|0%|
|\-35.6%|\-25%|

&#x200B;

For Method 2:

&#x200B;

|a (real value)|b (real value)|
|:-|:-|
|0.265133|301|
|0.407535|567|
|0.363445|459|
|0.359494|450|
|0.358887|800|
|0.898735|800|
|0.578721|600|","[""The denominator of Pearson's correlation includes standard deviation terms. When you converted your values to percentages the original scale of the data is lost, and hence the standard devs are different, because this is not a linear transformation."", "">Thank you! This makes sense to me. One question though, the deviations are in the numerator as well right? My intuitive understanding is that we don't do percentage change because the deviations of the data change overall (might increase or decrease) and in a way that doesnt capture in the original distribution. Would that be correct to say?"", ""Just go with good old Pearson's r. This uses the actual numbers from the data (and their means):\n\nr = [sum(xi - xmu) (yi - ymu)] / {sqrt[sum(xi - xmu)^2 sum(yi - ymu)^2]}\n\nSorry for the crappy formatting but just Google it. And sorry if this isn't at all what you're asking."", 'There isn\'t a single right answer; it depends on the circumstances, but ""neither of the above"" is a distinct third possibility\n\nI\'ll stick to a general comment: If your data are time series, it might be worth understanding the issues with Pearson correlation and time series; specifically you should be aware of \n\n1. spurious relationships between unrelated series if the series are non-stationary (indeed even stationarity is not of itself sufficient to avoid spurious correlation).  \n\n  For example, take two fair coins and toss them for many trials, and keep track of the number of heads for each after each trial. Clearly the coins are independent of each other, but as the number of tosses grows large the expected value of the absolute correlation between them grows high. This is spurious correlation; there\'s literally no connection but it looks like there is because the nonstationarity changes the behavior of the correlation coefficient from the behavior under the usual assumptions.\n\n2. in the case where the series are cointegrated, the considerations about suitable ways to model the relationship will change.', '> It’s correct to use the original data,\n\nNo, there’s no “correct” way to measure correlation, and there are some very likely scenarios where using the original data definitely isn’t the best choice.\n\nConsidering that OP is talking about percentage change from one quarter to the other, it’s reasonable to assume that there is autocorrelation,  and computing a correlation with raw values would be questionable.\n\nUsing percentage changes may not be the best solution, but the standard deviations are not “wrong” just because there is a nonlinear transformation. They’re just different.']",2,6,https://www.reddit.com/r/statistics/comments/11mo7mc/question_why_is_correlation_calculated_using/
815,2023-03-09 16:55:40,"[Q] Which statistical analysis would be appropriate for the project: reviewing different medications, their magnitudes and determining the link with disease outcome (not quite a chi-squared test)","Hi all, I'm trying to put together a retrospective research protocol at the moment and thinking ahead to the statistical analysis I will perform as I have to submit for ethics approval. i wont name actual medication and conditions but try to explain my dilemma as simply as I can.

&#x200B;

Data I will be collecting:

* Medications given to patients (A, B, C D... etc)
   * Patient 1 may be given A, B and C at different dosage magnitudes 10mg, 25mg and 50mg whereas patient 2 is given C, D and E at 30mg, 20mg and 10mg
   * Repeating this for all the patients in the cohort
* Each patient will then either develop disease X or not develop disease X - this is the outcome of interest.

Therefore, I am looking for a statistical analysis that can analyse the outcome of disease X, compared to being given the different medications and the magnitudes at which they are given. If I was just comparing Administration of Drug A/not given drug A and developing disease X and not developing disease X, I could use a chi-squared or t-test but when it comes to multiple drugs and involving a magnitude I am stuck.

If I could get up to that, that would be great but I also have one step further I would like to take it: grouping the medication administration into before hospital, at hospital before surgery and at hospital after surgery.

&#x200B;

Hopefully this is enough information, if I can clarify anything further please let me know and thanks in advance for any tips","['This is clearly a causal probelm and requires very careful considerations. The setting is not trivial at all, especially if the switches are not randomized. If they are you can ""more easily"" use Dynamical treatment models to estimate the outcome of interest.']",0,1,https://www.reddit.com/r/statistics/comments/11mndmr/q_which_statistical_analysis_would_be_appropriate/
816,2023-03-09 15:24:48,[Q] How to calculate regression inferences without sample population dataset?,"So all of the tools my stats I and II class have presented me allow me to calculate regression and inference with sample population data to input. However, for many problems I do not have a dataset, but rather just my regression equation, along with my significance level and standard error of the slope.

Short of doing the work long-hand, is there a tool that allows me to input this info to calculate other statistics?","['Your use of the terminology is confusing which is why you aren’t getting good answers. The phrase “..to calculate regression and inference..” doesn’t mean anything. We will have to extrapolate what you wrote to figure out your question. My guess is that your provided regression equation includes the estimates for the intercept and slope. If you also have the standard errors for these estimates, you can perform inference for the estimates related population parameters, often represented by beta_0 and beta_1.\n\nThat is to say, you don’t need the data to do these problems because the statistics have already been calculated from the data for you. As far as calculating other statistics without any of the data, you probably can’t in general. What other statistics did you have in mind?', 'What exactly are you trying to calculate?', ""P-values, confidence intervals, test statistics, basics 200 level stats stuff I think.\n\nWe're pretty much doing hypothesis testing and confidence intervals."", 'sounds like you could just make a set of ""fill in the blanks"" tables in excel where there\'s one tale for each possible unknown parameter.\n\nIf you know how the formulas are structured you could build the formulas such that you can populate each table with the known inputs and spit out the unknown input into the appropriate cell.', 'Hello. You could generate data under certain assumptions in R or Python if you have your parameters set. This data could then be used for other calculations.']",7,14,https://www.reddit.com/r/statistics/comments/11mlwz5/q_how_to_calculate_regression_inferences_without/
817,2023-03-09 08:20:25,"[Q] When do we say that the difference is significant when the computed p-value is less than or EQUAL to the critical p-value, but for the computed test statistic, it's JUST if it is greater than the critical test statistic?","I've searched all day and couldn't find a single source that says that if your computed test statistic is equal to the critical value, then the difference is significant. Is there a reason for this?

EDIT:

 I think I failed to make my point. When it comes to test statistics, there is usually just:

""If a < b, reject; if a > b, accept"", with no mention of the equal situation.

But when it comes to p values there would usually be:

""if a ≤ b, reject; if a > b, accept"".","['1. Beware, not all rejection rules are ""greater"". e.g. see [1]\n\n2. By the most common convention the critical value is *within* the critical region (and so equality is part of the rejection rule), though this is only consequential for discrete statistics. (also see [1]). For a conservative test you then choose that critical value so that the probability of rejecting under H0 is as large as possible without exceeding alpha. \n\n  You can define your critical values as just outside instead if you wish, but you must clearly indicate this since people will tend to expect otherwise; the required property for a conservative test is still that you must keep the rejection rule such that you don\'t exceed rejecting H0 at greater rate than alpha when it\'s true. Doing it this way has the slightly weird property that there\'s an interval of values of the statistic, just inside the boundary, that you say are in the rejection region but which never occur. \n\n3. The reason ;p-value exactly equal to alpha\' should be in the rejection region follows directly from the same convention; you want to reject as often as you can (for more power) while not exceeding a rejection rate of alpha when H0 is true. This implies rejecting p=alpha. If you stick to defining p in terms of the smallest significance level at which you\'d still reject, everything corresponds as it should between conventions.\n\n---\n\n[1]: https://math.usask.ca/~laverty/S245/Tables/wmw.pdf\n\nScroll to the bottom where it says: ""If Uobt ≤ Ucrit, reject H0""\n\nThis illustrates two of my points; first that some rejection rules are ""<"" not "">"", and that with discrete statistics, the common convention is that the quoted critical values are part of the rejection region.\n\nMany more examples may be found', ""Why are you asking?  It doesn't matter in practice.  The chance that the test statistic is exactly equal to critical value happens so rarely that you don't need to consider this boundary case."", ""Never happened and never will, sleep relaxed. Anyway put the = where you want it's exactly the same"", 'Thanks. But I think I failed to make my point. When it comes to test statistics, there is usually just:\n\n""If a < b, reject; if a > b, accept"", with no mention of the equal situation.\n\nBut when it comes to p values there would usually be:\n\n""if a ≤ b, reject; if a > b, accept"".', 'In my earlier comment I addressed where the ""="" case should go in detail. \n\nNaturally with actually continuous test statistics, exact equality should have probability 0, so it shouldn\'t matter very much. \n\nWhen it\'s not continuous, you can always work out what the rejection rate would be (under H0) when the equality is in the rejection region and when it\'s outside it, (at least by simulation, if its not feasible algebraically; just simulate the proportion of rejections under H0 for each scheme).']",1,5,https://www.reddit.com/r/statistics/comments/11md5vz/q_when_do_we_say_that_the_difference_is/
818,2023-03-09 05:19:08,[Question] Same inter-quartile ranges of two unpaired groups but significant p-value?,"Hello! I am comparing two unpaired groups and found them to have the same inter-quartile range but when I ran a Mann-Whitney U test the p-value was significant. Why is this? If they have similar distribution based on the IQR why would the p-value suggest otherwise? Sorry if I'm not being clear enough!

Thank you!","[""1. The Wilcoxon- Mann-Whitney doesn't compare interquartile ranges. They might be similar while the thing the statistic measures is fairly different\n\n2. With large sample sizes, even very similar looking distributions can nevertheless be  different enough that it can't be explained as random samples from identical population distributions"", ""Weird. It won't let me edit to fix whatever is causing the two 1s"", ""There are some tests that can compare medians, but in many situations the power is typically low. \n\n1. What was the original question of interest here (was it originally *specifically* about medians? Or did you decide to think about medians *after seeing the data*?) \n\n2. What is the variable you're comparing here?"", 'Thank you so much for the explanation! \n\nIs there a test that would be useful to compare medians? So even though these two groups may have a similar distribution in-between the 2nd and 3rd quartiles, it is possible the rest of the data distribution is what causes the significant p-value?']",8,4,https://www.reddit.com/r/statistics/comments/11m8i7m/question_same_interquartile_ranges_of_two/
819,2023-03-09 02:55:27,[question] Multiple partial correlation coefficient for linear regression model?,"I am working on my Master's thesis and currently trying to perform a post-hoc Power analysis for my (full) linear regression model in SPSS. I have to enter a multiple partial correlation coefficient as the effect size, but I'm not exactly sure what this means/ how to obtain that value (is it the same as r\^2, adjusted r\^2, f\^2, or…). Can someone please provide an explanation?

I know g\*power might be a better option, since it works with f\^2/r\^2, but I am unable to do so because it is not compatible with the screen reader software I am dependent on.",[],2,0,https://www.reddit.com/r/statistics/comments/11m4o5j/question_multiple_partial_correlation_coefficient/
820,2023-03-09 02:28:16,[Q] Power calculation for novel topic,"Hello everyone,

I need a power calculation for my research. I have already found an example from UCLA but there (partial) R squared was already known from previous research. My topic, however, is a quite novel one and doesn't offer so many numbers.

I have two continuous predictors and one dependent variable (at first we planned three IVs yet we decided to do an exploratory analysis).

I used g\*power with:

F tests - LMR; Fixed model, r squared increase

f square: .15

alpha: .05

Power: .95

Two tested predictors

Total number of predictors: 2

If I select determine, what do I have to fill in? How do I know my variance or partial r squared without previous numbers? Is there a default I have to use?

Sorry if my question may sound stupid, I am still new to this field.

Thanks in advance.","['If you don\'t have previous results on which to base your required effect size, you can justify some number based on what you think would be reasonable.\n\nSince you\'re doing ""exploratory"" analysis, this approach is fine. Looking for a ""medium"" f-squared of 0.15 should give you a ballpark number. Modify that with pragmatic concerns like how much time you have or how much data collection costs.', 'You make an educated guess now, e.g. based on the most similar type of study you can find, and then next time your current study is the literature you or others can based their power calculation on :)', 'You might start by considering what the smallest effects are that could be practically relevant across a variety of possible (but not impractical) situations and see what effect sizes they correspond to. That would give you a ballpark on a small effect. Then you might consider the other end - what would be the least effect that should still ""hit people between the eyes, a major impact"" (across a number of practical  scenarios), and that then would give a sense of a large effect size. That gives you a basis then to select what the smallest effects are that you want to be pretty sure to be able to pick up which presumably will lay somewhere along tahat scale between the small and large that you identify']",1,3,https://www.reddit.com/r/statistics/comments/11m3yhr/q_power_calculation_for_novel_topic/
821,2023-03-09 01:11:55,[E] What would be the repercussions of dropping my Theoretical Math Course?,"Hello, Masters' Biostats/Bioinformatics student here again. Brief Re-cap: I was a Biology Undergrad. Worked for some time, and came back for an Masters' in Biostats/Bioinformatics (joint program at my school).

1st semester of MS program was s\*\*\*, but I bounced back after that. I did well in in the 2nd semester, and last semester I got around a 3.6 GPA.

Current Situation:

I'm taking 3 Graduate Stats Courses and 1 Intro to Theoretical Math Course (Undergrad Level) and trying to complete my Graduate Project.

Long story short....the Theoretical Math Course is sucking up almost all of my time. Breakdown of weekly work:

1. 12-15 proofs/week (10 assigned for HW + 2-5 supplementary for quiz prep)
2. Quiz on same day that homework is due
3. Rinse & Repeat...

My university has 2 campuses. My Theoretical Math Course is later on one campus, and the Grad Stats Courses are all on the other one. I only have a 10 min. window between my Math Theory class and my Stats Class, and by design end up getting there 8-10 mins late (talked about this w/ my prof already).

I'm thinking of dropping my Theoretical Math Course, but at the same time PhD programs I've spoken to require some solid Theoretical Math background, and I feel I'd just be pushing things out further and further. I'm already in my mid 20s...

What would be the repercussions of dropping and having a 'Resign' on my transcript?","['What is ""theoretical math""? If it is real analysis then most statistics PhD programs require you to have analysis done. Biostats PhD at lower ranked schools are generally more applied and may not required real analysis. All (bio)stats programs will require multivariable calculus and linear algebra. The good programs want as much math background as possible. So much so that any statistics background seems largely optional if you have the math background.\n\nThough you said your program is biostats/bioinformatics... So if you are looking for bioinformatics PhD then real analysis is not needed. They would prefer more CS background as many departments list that as one of the biggest hurdles bioinformatics PhD students face.', ""I would say that is important if you are set on (bio)stats PhD. You'll need to show you have at least the ability to do proof based math (e.g. analysis). As I describe in previous comment this might not be as strict of a requirement for less methodologically driven biostats programs."", ""OP, I've been in your shoes, and if I could give my past self advice it is this: the course won't matter at all if you can't have the energy to finish it, and if it ruins your ability to learn in your other classes then it is a net drain on your education.\n\nIf you find that you can't really handle the proofs clas right before stats, then drop it.  Take it slower. Complete your 2 year degree in 2.5 years or 3 years.  Because if the consequence of taking that class right now is your burn out, then you won't be able to get anything done.  Put yourself first.\n\nOnce you decide to put yourself first, then decide if you should take it later, take a different course, or eliminate it completely."", ""To me, it doesn't sound crazy to drop it. You have a lot on your plate right now even without that course."", '> I’m already in my mid 20s…\n\nI graduated from college in my late 20s. You are still young and a year here or there will be easily made up. Your career won’t get off track. \n\nLike another poster said, if it’s taking too much time, it’s a drain on your whole education. A single withdrawal isn’t going to hurt you, especially if you can take the class or its equivalent after you graduate. Put off applying to a PhD for a year or two and go work while taking a couple theoretical math classes on the side. You’ll be better off for it']",15,10,https://www.reddit.com/r/statistics/comments/11m1xrj/e_what_would_be_the_repercussions_of_dropping_my/
822,2023-03-08 23:34:26,[Question] Calculating probability of NOT being selected in a small lottery (fantasy football),"I’m trying to adjust our lottery system for fantasy football. There are four participants in the draft lottery. 

8th place gets N8 balls in the hat
7th place gets N7 balls in the hat
6th place gets N6 balls in the hat
5th place gets N5 balls in the hat

When your ball is selected, all of your balls are removed. You can only receive one position.

Currently, we draw for the best position first, so you want your name selected. As such, 
N8 is the highest at 4 balls, N7=3, N6=2, and N5=1 (lowest).

What I’m proposing, to build suspense, is that we draw for the worst position first, and build up to the best. We would adjust how many balls in the hat each manager would get. As such, N8 is the lowest and N5 is the highest. You DON’T want your name selected. I’m calling this a “survival lottery” but it’d love to know the actual name. 

How can I calculate this? It’d like to continue to use variables to adjust the number of balls in the hat.

Edit1&2: Format",[],0,0,https://www.reddit.com/r/statistics/comments/11lzdha/question_calculating_probability_of_not_being/
823,2023-03-08 22:31:58,Z/Z' factor and high-throughput drug discovery [Question],"Howdy all, first time visitor to this community, looking for some stats help with a specific application. I work in high-throughput drug discovery where researchers will routinely test millions of compounds in some biological assay, trying to discover some beneficial activity.

There are statistical measures to determine the reliability of these assays - you're testing millions of drugs, you expect the VAST majority of them not to be active, and you can only screen once. So a reliable assay is a necessity. One of the most widely used parameters is something called a Z factor or Z' factor, which is not the same as Z score (publication describing this below).

My question is this - other than 'this is good enough' what else can a Z factor tell me about my assay?

In a sentence, how would you describe what the Z factor actually means?  
Can I use the Z factor to determine what my false positive rate is likely to be for a given sample size and a given number of replicates?  
Can I use Z factor to to determine how many replicates of each drug I aught to use? If not, any ideas on how I'd go about that?

Thanks!

[https://pubmed.ncbi.nlm.nih.gov/10838414/](https://pubmed.ncbi.nlm.nih.gov/10838414/)","['First time here too. At a glance, it seems that Z can only be interpreted indirectly, through a transformation W = (1 - Z) / 3. Assuming that signal values for positive and negative controls are independent and normally distributed, with ""positive signals"" coming from one normal distribution and ""negative signals"" from another, W is approximately normal\\*, characterizing the ability of the assay to discern positive from negative. If the positive and negative distributions had the same standard deviation (not necessarily assumed, but maybe reasonable), then W is half the number of standard deviations separating the means of the two distributions.\n\nAdmittedly this is pretty unsatisfactory, because an interpretation of Z only comes through W. For example, Z > 1 is equivalent to W > 6. [Wikipedia](https://en.wikipedia.org/wiki/Z-factor) calls such an assay ""excellent"", which seems reasonable enough if 6\\*2 =12 standard deviations of separation is ""excellent"" (though such terms are basically arbitrary). But even this interpretation is not precisely correct, unless the two distributions have equal standard deviation. And then the asterisk:\n\n\\*W is asymptotically absolute value of a normal distribution under the stated assumptions, but the scale is unusual because Z involves the sum of standard deviations. If I were allowed to define Z, I\'d scale by the square root of the sum of their squares instead, in order to make W *standard* normal. So Z seems largely heuristic, without a precise interpretation.\n\nEdits: Corrected my bad arithmetic and some statements following from it.']",16,1,https://www.reddit.com/r/statistics/comments/11lxufj/zz_factor_and_highthroughput_drug_discovery/
824,2023-03-08 21:23:24,Between groups design [Q],"I am doing a report which is a between groups design (2 groups) and testing 2 dependent variables. When doing histograms/q-q plots , do I need to do a separate one for each group and each dependent variable, or can I do both groups together and one dependent variable. Thank you so much in advance

Edit: I will gift anyone that helps 😊
Please I am desperate","['[deleted]', 'Thank you so so so much. Do you reccomend I add all the figures in my results section, or just my appendices']",1,1,https://www.reddit.com/r/statistics/comments/11lwb0g/between_groups_design_q/
825,2023-03-08 20:53:17,[Question] Graphing moderation analysis,"Hello, stats wizards!

Bear with me here, I’m very new to this-

I ran a moderation analysis with three IV, one being the interaction between one of the IV’s and the DV. The analysis came back with a huge effect of the interaction, so now I need to unravel it. I need to graph it that shows the different levels of the IV towards the DV for high and low on the moderator, but I’m not sure how to do that. If it helps, I can use Excel, Google Sheets, SPSS, or PSPP for graphing. 

Any guidance greatly appreciated!","[""In that case there is zero point in testing/graphing the effect of the IV at arbitrary levels. Why not graph Johnson-Neyman intervals instead? It's much more informative and actually in line with the original research design."", ""Is the moderator continuous, or categorical? It's unclear from the post."", 'Oh sorry, my mistake. It’s continuous']",1,3,https://www.reddit.com/r/statistics/comments/11lvmk4/question_graphing_moderation_analysis/
826,2023-03-08 20:39:19,[Q] An alternative non parameteric test for Mann whiteny test,"In the field of medical research, if the data is not normally distributed, they jump and do the Mann whiteny test. However, I don't think that Mann whiteny test is testing the difference in the median like some sources say. Is there a non parameteric better option better than Mann whiteny test?","[""MW test doesn't test the differences in the means. Perhaps look at bootstrapping?"", ""It's not testing the difference in the medians. Who says it is? What is the null hypothesis you want to test?"", '> It\'s not testing the difference in the medians. Who says it is?\n\nIt\'s very common in ""applied"" statistics textbooks (e.g. for the social sciences) to describe the MW as a test of medians, which it is in the specific case of a pure location shift alternative (which is probably not a reasonable assumption in the majority of cases people use it for).', 'You could do a randomization test of the difference between means (Mann-Whitney is just a rank randomization test). [Here](https://www.lock5stat.com/StatKey/randomization_1_quant_1_cat/randomization_1_quant_1_cat.html) is an online calculator. .', 'Quantile Regression may be what you are looking for! or moods median test.']",4,23,https://www.reddit.com/r/statistics/comments/11lvbvt/q_an_alternative_non_parameteric_test_for_mann/
827,2023-03-08 16:24:41,Is this a Kruskal-Wallis? [Q]," So to compare the effect of receiving treatment on a dependent variable measured using a Likert scale (0-7),  using such data:

|\#|Group|Rating at baseline|Rating at endline|
|:-|:-|:-|:-|
|1|Treatment|2|5|
|2|Treatment|3|3|
|3|Control|4|7|
|4|Control|5|9|

I was told to use the following formula:  
\[(%  of respondents in the treatment group choosing a rating at endline - %  of respondents in the control group choosing a rating at endline) - (%  of respondents in the treatment group choosing a rating at baseline - %  of respondents in the control group choosing a rating at baseline)\]

He built a frequency tables to carry out these calculations, and said that this is a Kruskal-Wallis. However, when reading up on Kruskal-Wallis, it seems to be something different, and not applicable for this case. I would appreciate it  if anyone can help me understand what is happening here.","['To estimate the mean difference l, but no idea how to calculate p now.', 'Difference in Difference (DiD) can also be an option. Check assumptions.', 'Why go the Kruskal Wallis route when you have paired data (2 repeated measures) from two groups and you want to apply a test for data not assuming the data is normally distributed? You need the [Wilcoxon signed-rank test](https://en.m.wikipedia.org/wiki/Wilcoxon_signed-rank_test)', 'You was told to use the formula to achieve what?', 'See also duplicate question: https://stats.stackexchange.com/questions/608808/estimating-treatment-effect-on-a-variable-measured-on-a-likert-scale-at-two-poin']",12,9,https://www.reddit.com/r/statistics/comments/11lqqku/is_this_a_kruskalwallis_q/
828,2023-03-08 12:48:41,[Q] How do i do a mann whitney u test by hand?,"for my high school stats analysis paper, i need to do a mann whitney u test by hand but i'm having trouble. i know there is a formula but i'm getting really confused bc my data has tied ranks, and a sample size of 50. every time i do the math by hand, i end up getting a different answer than what the software i use to check says?? i am confused lol. 

can someone please send me an example of mann whitney u by hand for tied ranks?? if someone could even PM me i would send over my raw data so they can just do it for me/show me how to do it that would be so helpful because it's making me stuck😕","['https://www.youtube.com/watch?v=BT1FKd1Qzjw\n\nThis example has ties in the data.', ""That's awesome you're learning that in high school! I think the other post has your answer (sort, label, count labels, lookup statistic)"", 'step 1 is sort the data all together but keep the group labels with the numbers -- easiest by staggering the rows, like so:\n\n     A:    3          5      7  8              23\n     B:        4  4     5           9  10 \n\nThen write the corresponding ranks (when tied, you average the ranks):\n\n     A:    1            4.5       6 7              10\n     B:        2.5 2.5     4.5           8  9\n\nThen the easiest way to calculate the statistic is to use the Wilcoxon rank sum form; you can convert to U by a simple shift afterward if you must have U rather than W. Then with n=50 and tied ranks, use the normal approximation but make sure to adjust the variance for the ties.', ""Here's an example from my text book. I am an old man learning statistics!\n\nhttps://imgur.com/a/hLnoCCP"", ""that's awesome!!! it's very challenging though. my dad is also just learning stats as part of his masters program""]",9,5,https://www.reddit.com/r/statistics/comments/11lmnb6/q_how_do_i_do_a_mann_whitney_u_test_by_hand/
829,2023-03-08 12:42:05,[Q] easiest non parametric significance test?,i'm a highschool student and i need to a statistical analysis for my class. i need to do a non parametric significance test by hand ... so i need something easier. i tried to do mann whitney u test by hand but i'm having a lot of trouble and i'm confused because i have ranks.,"[""The easiest one I know of is the Tukey Quick test, a.k.a Tukey-Duckworth test.\n\nhttps://en.wikipedia.org/wiki/Tukey%E2%80%93Duckworth_test\n\n> i'm confused because i have ranks.\n\nHaving ranks doesn't make it hard.\n\nYou mean you have *tied* ranks, right?\n\n\n\nIt's not going to get all that much easier than a Mann Whitney, to be honest.\n\n(If you're able to choose a different data set, get one that's *way* smaller and where there's no ties. )"", ""I think the Mann-Whitney test is a good candidate.  It's fairly easy to calculate and understand the results.  The only thing I would caution about is what the test is actually testing for.  It's not usually a test of the median.  (Even though this is commonly stated on websites.)  It's a test about if the values in one group tend to be higher than those in the other group.  (Or more simply, if the distributions of values are different between the groups.)  If you can use data that doesn't have tied values, the procedure will be slightly easier.  It's also easy to double-check your work with free software or online calculators."", 'See: https://www.reddit.com/r/statistics/comments/11lmnb6/q\\_how\\_do\\_i\\_do\\_a\\_mann\\_whitney\\_u\\_test\\_by\\_hand/', ""LOL i'm so sorry i meant tied ranks"", ""I meant to mention that the Tukey Quick test assumes no ties; if there's lots of ties you have to adjust the critical values for the pattern of ties, which would make it more effort than the Mann-Whitney (for which an adjustment is already available in large samples).""]",9,6,https://www.reddit.com/r/statistics/comments/11lmia7/q_easiest_non_parametric_significance_test/
830,2023-03-08 08:48:17,[Q] How to rate players in 5-a-side football?,"Hi everyone!

I hope this is a good place to ask such question. If it's not, I am really sorry. Anyway, I have a group of 21 players, we play 5-a-side football (futsal) every Tuesday and we're looking for a perfect, but simple way of dividing players into teams so the odds would be 50-50.

We started by rating each other in Google forms and we got some ratings. The best player is rated 8.7 and the worst one 3.9. It's pretty simple to divide people into 2 groups with similar sum, but it may not be perfect. Maybe some player is underrated and he's actually worth more... Maybe someone injured himself and his rating should decrease... There are many factors, but right now we only use these:

+0.1 rating if you won
-0.1 rating if you lost
-0.03 rating if you didn't come
-0.01 rating if it was a draw.

We are currently looking for a better model and I wonder if you guys have any ideas. Thanks!","[""The Elo system for chess is a good example for such a system, although it's a 1 vs 1 game. You can use its adjustment formula but apply it to the summed strength of a team. New players can get a larger factor for Elo changes so they get to their right value faster."", ""A good team is greater than the sum of its parts. Some people might work well together, others might not. But if we ignore that:\n\n* I wouldn't subtract rating for a draw.\n* If the (predicted) stronger team wins, maybe change the rating less.\n\n> Maybe some player is underrated and he's actually worth more...\n\nThen teams with him should win more than they lose, which means over time his rating will increase. That could take a long time, however."", ""Thank you for the answer!\n\nI definitely agree with the first part, but we have to ignore it.\n\nI agree for draws.\n\nHow much should I change ratings for example if team with rating 40 beats team with rating 39.6?\n\nThe last part is probably the most problematic. We have a guy who most of us didn't know well and he got himself rating of 7.0, while he deserves at least 8.0. So now most of the games team with him wins, but it will still take some time until he reaches his normal value.""]",0,3,https://www.reddit.com/r/statistics/comments/11lh739/q_how_to_rate_players_in_5aside_football/
831,2023-03-08 05:44:18,[Q] Are cross tabulation tables simply tables with labels on both the row headers and column headers?,"Something like this (a distance table):

&#x200B;

`-------|-US---|-Brazil`

`US-----|-0----|-4500--`

`Brazil-|-4500-|-0-----`

&#x200B;

Since it has labels on both the row headers and column headers, is it a cross tabulation table?","[""Maybe...  Cross tabulation simply presents the data in categories or sub-categories. For example, if you are measuring the mean income of some population, cross tabulation might present the mean income for each gender category, or for each combination of, say, gender category and age category.  The results could be presented in a two-way table in this case, say, with gender on one axis and age category on the other axis.\n\nFor example, the following table, which I assume are counts, not a variable like *Income*:\n\nhttps://www.researchgate.net/profile/Hina-Jaffery/publication/286239437/figure/tbl1/AS:669232815673371@1536568945745/Age-and-Gender-Cross-Tabulation\\_W640.jpg\n\nFor your example, you could consider your results to be cross-tabulation data, across two categories of *Nation*.  But you wouldn't need a two-dimensional table.  I'm not sure why data would be presented that way."", ""Your table could be a crosstabulation if you wanted to calculate the (weighted bird flights or whatever) distance From and To a particulate country\n\nIF you had more countries in the lists, it would be more informative. Nonetheless, I believe it could be seen as a crosstab. I've built something like this with the number of people who live in region A but get their vaccination in location B. This would allow for some more interesting figures to occur in the table. \n\nOf note: if you have the same exact countries From as well as To, and the numbers for From>To are the same as To>From, you only need the upper or lower triangle of that table, optionally excluding the diagonals are they're all 0. Edit to add that in your case, you only needed the 4500 value as a distance from US to Brasil."", 'Well, there can be more countries in the variable, and then we can see the distances between the countries. I used a 2x2 just as an example.', 'Thanks!', 'Then it sounds like what you are presenting maybe wouldn\'t be ""cross tabulation"".  Maybe ""pairwise table of distances"", or something like that.']",2,6,https://www.reddit.com/r/statistics/comments/11lcduq/q_are_cross_tabulation_tables_simply_tables_with/
832,2023-03-08 04:24:09,[Question] International Beerio Kart Championships of the World: Power Rankings Help!,"**TL;DR: My friends and I have a stupid hobby that's getting out of control and I need your help spiraling it further. Please help me create a fair power rankings system (using the attached spreadsheet for reference) for the Beerio Kart tournaments we host.**

[**https://docs.google.com/spreadsheets/d/1CS5pWnmgS8wIZAvFQL4cc\_jHWbTZ\_khS/edit?usp=sharing&ouid=114408781303577995971&rtpof=true&sd=true**](https://docs.google.com/spreadsheets/d/1CS5pWnmgS8wIZAvFQL4cc_jHWbTZ_khS/edit?usp=sharing&ouid=114408781303577995971&rtpof=true&sd=true)

Dear members of the Statistics community,

I call humbly upon the statisticians, mathematicians, programming aficionados, excel experts, sports analysts, and power rankings enthusiasts of this great community to assist me with a vital task -- creating a fair and representative power ranking formula for the International Beerio Kart Championships of the World.

A little background: my buddies and I were trapped at home Thanksgiving of '21 for a fourteen day COVID quarantine. We were saddened by a missed opportunity to see our families, but with competitive spirit running through our veins and a surplus of leftover PBR from a party we threw (which was undoubtedly what gave us COVID), we found solace in roughly two weeks straight of fierce competition in the best drinking/video game pair to ever exist: Beerio Kart. For the uninitiated: Beerio Kart is Mario Kart, however, you need to finish your beer before the end of each race, and you can't drink and drive (i.e. chug and control your character simultaneously). Our version of the game has many extra rules and sub-rules, however, that's the basic premise of the game.

After two weeks of this, we needed an outlet to determine who was truly the best of us, and thusly the International Beerio Kart Championships of the World were born. It started with a modest eight competitors, but interest has increased steadily over the past three years and in recent events we've had as many as 58 competitors fighting to compete in a 32 person bracket (surplus competitors play in Play-in Prix's for entry into the main bracket). We've now had 75 people play in official brackets and obtain power rankings, and close to 100 participate in the events overall. For a little context into how the tournaments are run, four competitors participate in each Grand Prix, and the top two competitors advance from each round until the championship. In the preliminary rounds, players must drink a beer on races two and four of each Grand Prix, and in the finals all four races are drinking rounds, thusly the final four competitors must drink a minimum of 10 beers to win the tournament.

As tournaments got larger and more intricate (and people started complaining that they were seeded unfairly), we realized we needed an objective ranking system to seed players so that the Prix's leading up to the championship were fair and quantitative. This background brings me to the hallowed undertaking I beseech your help with: **please help me figure out how to do this.**

We've tried a few formulas, but we are but amateur statisticians and none have felt like they effectively capture a player's skill level.

First we tried the following formula: ibkc power ranking = 0.33t/60n + 0.33z/60 + 0.33y/60, where:

1. 60 = the maximum number of possible points scored in any given grand prix
2. t = total points accrued over all past tournaments attended
3. n = total number of grand prix’ held in all official tournaments
4. z = average points scored per prix, per tournament, in all tournaments attended
5. y = average points scored per prix, per tournament, in all tournaments attended this calendar year

It was a good start, but it unfairly biased players who had played in more tournaments, and wasn't an accurate reflection of *current* skill level. It would be like baseball power rankings putting the Yankees are at the top because they're an ancient ball club and have won 27 World Series', even though the last time they won was 2009, or the Astros low down on the power rankings because they didn't win their first Series until 2017, even though they've won twice in the past 5 years.

We then created a formula based on Pythagorean expectation, where a players skill level is calculated by averaging their (points accrued in a prix)/(points accrued in a prix + total number of possible points in a prix). Each round of a tournament was weighted heavier than the last, and tournaments with four rounds carry more weight than tournaments with three rounds. The player's Pythagorean expectation was then averaged over all tournaments they've participated in, averaged over the last four tournaments held, and averaged over the last two tournaments held. Their power score was then calculated by averaging these three numbers together with the intention that more recent tournaments would be weighted heavier than older ones. **This is the formula that the attached spreadsheet uses.**

This new formula was better than the first but has an inverse problem -- it weighs recent tournaments too heavily and doesn't account for any rank decay from missing tournaments. For example, you can see that BAT has won 6 of 8 tournaments, but after a huge upset in the semi's, BAT did not make the finals of the last tournament, and was booted from first place overall to third. All the while, Squirt4Boyz advanced from second place overall to first, even though Squirt4Boyz didn't even participate in the last tournament.

There's all sorts of hidden columns and rows and whatnot in this spreadsheet so please dm me with any questions you might have, but please, I beg of you fine and glorious proprietors of the world's most stressful game, help me create a ranking system that makes sense. **Ultimately we need a system that reflects how many points a player is expected to score, considers that player's tournament wins, podium finishes and finals appearances, accounts for rank decay, and like in global tennis or golf rankings, has some bias for recent events.**

Thank you, friends.

Your servant,

The International Beerio Kart Championships of the World League Commissioner",[],3,0,https://www.reddit.com/r/statistics/comments/11la74u/question_international_beerio_kart_championships/
833,2023-03-08 01:52:47,[Q] How to prove the Poisson link function is a canonical link function?," So I'm a 3rd year undergraduate doing my thesisin football score models right now. In my thesis I want to include a proof of what the link function for the Poisson distribution is and why it relates the mean to our linear predictors. I'm almost there, but there is one part that most literature seems to gloss over.

So we have our linear predictors η =β0+β1\_xi1+⋯+βp\_xip and our natural parameter θ. Now I know why if η = θ then our link function is canonical, but my question is how do we prove that we have η = θ in the case of the Poisson distribution? Most literature just state that the canonical link sets this equality, or we can assume this equality for distributions that are members of the exp. family but don't actually prove why. Can anyone help?","[""Just use the definition.  Also you don't need your formula for linear predictors.  You can do just one Poisson random variable.  Then any linear submodel is also exponential family (a fact about exponential families).\n\nLook at the log likelihood for Poisson.  To be exponential family form it must be (function of data only) * (function of parameter only) + function of data only + function of parameter only.  From that you read off the canonical statistic and canonical parameter."", ' There are many resources for this and this is above my pay grade. There’s a proof here it looks like\n\nhttps://pages.stat.wisc.edu/~st849-1/lectures/GLMH.pdf#5']",13,2,https://www.reddit.com/r/statistics/comments/11l63tz/q_how_to_prove_the_poisson_link_function_is_a/
834,2023-03-08 01:19:11,[Q] Monitoring a Year-Long Sample: How to Determine When Stratum Subsample Thresholds Have Been Significantly Exceeded and Should Therefore be Adjusted?,"I'm in charge of a year-long, stratified sample where the population is estimated based on previous years but which is also quite dynamic, changing in unpredicted ways every year (furthermore, past population data are aggregated by year, making it impossible to know in advance if there are seasonal spikes or drops).

Obtaining a sample response is very resource intensive, so only the bare minimum sample size is sought and the stakeholders are hyper-interested in monitoring the sample daily to ensure no single stratum subsample has 'taken off'.

Sometimes a stratum subsample will exceed its annual estimated total early (e.g. Stratum #3 has hit it's annual expectation of 200 samples even though we're only halfway into the year). When this happens, they immediately want to shut down all sampling for that stratum, even if other strata are lagging behind expectations (thus putting at risk obtaining the overall sample size goal).

What is the proper way to monitor and govern an in-progress sample like this? Are there any objective, statistical 'threshold' tests to govern such decisions (e.g. Stratum #2 has reached 125% of the estimated annual stratum subsample before the 8-month mark in the sample therefore further sampling in that stratum should be shut down)?",[],6,0,https://www.reddit.com/r/statistics/comments/11l58bh/q_monitoring_a_yearlong_sample_how_to_determine/
835,2023-03-08 01:02:42,[Q] How do you get 90% confidence interval &. 10% level of significance? Do you use t-test?,,"['Confidence interval for what? Significance test of *what*? What are you trying to do?', 'What hypothesis? And confidence interval *for what quantity*? You need to explain your data and research question.', 'Is this homework?', 'Compute a 90% [confidence interval on the difference between means](https://onlinestatbook.com/2/estimation/difference_means.html) and a [significance test on the difference between means](https://onlinestatbook.com/2/tests_of_means/difference_means.html ). BTW, the question uses sloppy language since you don’t draw conclusions about significance. Significance is used to draw conclusions about the population.', 'Assuming that the question gives you the average profits, the standard deviations, and the sample sizes, this would be an independent samples t-test for the hypothesis test portion of it. I’m guessing you also need to do a confidence interval for the difference in average profits?']",0,28,https://www.reddit.com/r/statistics/comments/11l4sln/q_how_do_you_get_90_confidence_interval_10_level/
836,2023-03-08 00:14:46,[Q] Eurostat question,"
I have a Eurostat data “Distribution of income by quantiles”. But the percentages are different per quintile. Question: if it’s quintiles, why is it not 20% each? What is a quintile in this case? (I read their explanation, but they are unclear to me). Would GREATLY appreciate the answers, thank you!!","['Each quintile contains 20% of the population. The data here shows what share of total national income is earned by the population in each quintile. For instance the top quintile will have the largest share (and that share will be greater than 20%), since by definition the people in that 20% of the population will each earn more than someone in a lower income quintile.', 'Nah.  Maybe mock up something with different numbers ?  Or maybe someone else here can address the question.', ""Maybe it's just me, but I'm having trouble understanding the question without seeing what you're looking at.  Maybe share a screenshot of part of the table exemplifying what you're seeing ?"", 'Maybe you can share a link to the data set ?', 'See: https://www.reddit.com/r/AskStatistics/comments/11l3blv/i\\_need\\_reddit\\_wisdom\\_this\\_is\\_a\\_eurostat\\_data/']",4,10,https://www.reddit.com/r/statistics/comments/11l3ijw/q_eurostat_question/
837,2023-03-07 23:47:17,[Q] Chi-squared or Kruskal-Wallis to compare nominal variables with multiple categories to Likert scale data,"Hello,

I conducted a single survey (so only one population group) to test certain Likert-scale constructs against one another. To do that, I used Kendall's Tau-b.

However, I would also want to check whether moderating variables such as age (4 categories), gender (2 categories), and investor sophistication (2 categories) have an effect on the ordinal (i.e., Likert scale questions) variables.

I researched online and it appears that a one-way ANOVA wouldn't be a good idea since gender and investor sophistication are non-numeric by nature. 

However, a Chi-squared test or Kruskal-Wallis test seem to come back most often. 
Does someone know which one would be wiser to use? 

Thank you very much!","[""Whether you use a measure of / test for nominal-ordinal association or nominal-nominal association depends on the alternative of interest.\n\nHowever, if you're looking at correlation with actual [Likert scales](https://en.wikipedia.org/wiki/Likert_scale) (a sum or average of items) rather than with individual items, you will already have had to assume the items were interval in order to add them (since doing so asserts equivalence relations that impose interval scaling) -- in which case you might well consider other measures of association/correlation.\n\nIn general, marginal bivariate (x,y) relationships can be highly misleading in terms of whether a variable would be best included in a model with other covariates. See Simpson's paradox on wikipedia (especially the first plot)"", 'So, is your dependent variable the results of several items combined together ?', 'No. Every single Likert-scale based question is its own variable and will be tested separately with the moderating variables.', 'Is your dependent variable a *scale* per se.  That is, several Likert-type items combined together into a numeric value ?  Or is it an individual Likert-type item ?', ""Because your dependent variable is a single Likert-type item, it's usually best to treat this variable as ordinal in nature (ordered categorical).\n\nIf you want to investigate the effect of your other variables one at a time, Kruskal-Wallis would be appropriate, treating \\*Age\\* as a nominal variable.  A Dunn (1964) test could be used as a post-hoc test.  The Wilcoxon-Mann-Whitney test could be used if the independent variable has only two levels.\n\nHowever, as a caveat, you may want to treat *Age* as an ordinal variable.\n\nIf you want to investigate multiple independent variables in the same model, using ordinal regression is relatively easy in some software platforms.  This is analogous to a *general* linear model, or e.g. two-way anova.""]",1,9,https://www.reddit.com/r/statistics/comments/11l2sf4/q_chisquared_or_kruskalwallis_to_compare_nominal/
838,2023-03-07 23:09:02,"[Q] Factor analysis, eigenvalue cutoff > 1","Hi everyone,

newbie to factor analysis here, I learned about it, but in real life the numbers are not as clean cut as in the profs examples.

I have three samples for the validation of a questionnaire in a new language. In the original, the authors identified only one factor, as only their 1st eigenvalue is > 1 and the screeplot has the distinct kink after the 1st factor.

In our analysis, the structure and magnitude of the eigenvalues is extremely similar, the only difference seems to be that the eigenvalue for the 2nd factor is > 1 (e.g. 1.04).

I would like to know if there is a rationale to exclude the 2nd factor in spite of being > 1, and if there is one, what reason would/could I give. As far as I can see, this dubious 2nd factor would be extremely difficult to interpret (i.e. no idea). Is there anything else I need to consider?

thanks in advance","[""Kaiser's criterion is famously unreliable. Perform Parallel Analysis instead, and see where it takes you."", 'The >1 rule of thumb often retains too many factors so if your second factor is not interpretable even after rotation I would not keep it.', 'I think interpretability should be sufficient. There is little mathematical basis for the <1 rule, only that 1 is the average eigenvalue in Principle Components which doesn’t strongly imply that that should be the rule for the number of factors.', 'When validating the questionnaire in a new language fit a confirmatory factor analysis to see if the factor structure originally identified (ie what the original questionnaire had) holds. You are not trying to create a new measure you are seeing if an existing measure works in a new language/population.\n\nYou can then do measurement invariance testing to see if the structure, intercepts, factor loadings and errors differ across samples.', 'Interesting, I will have a look, thanks for the suggestion.']",2,7,https://www.reddit.com/r/statistics/comments/11l1stk/q_factor_analysis_eigenvalue_cutoff_1/
839,2023-03-07 21:47:42,[Q] How exactly do I use prior information in a real example?,Suppose I do a clinical trial with a small number of subjects and run my model and estimate an effect size with standard error. Is that information the prior so that I can use to update the posterior as I release the drug out to public and gather more data?,"[""You certainly could use past studies (e.g. an RCT) as a means of developing a prior to then further study a drug.  I did this a lot in my PhD.\n\nYou just need to be a little careful.  For one, RCT's are mostly homogeneous when it comes to demographics; they are mostly men, usually north american, and a least in the studies I read usually healthy.\n\nWhen studying anything in the real world, there is a certain endogenetity that is omnipresent.  Sure, in the RCT the average treatment effect was X, but in the real world there is a reason these patients are needing to take this medication, and that may change  the average treatment effect in ways that are not wholly predictable.\n\nSo while prior information is useful and you can take previous studies as prior information, you need to be aware of the transportability of the estimate from those prior studies."", ""I didn't write this out explicitly in the papers I wrote, but yes its probably fine to inflate the uncertainty a little.\n\nA better approach would be to do a prior predictive check to see that your model is giving credence to plausible scenarios before seeing data"", '1) If you are comfortable, could you share any paper you did using this approach?  \n\n2) Would you inflate the prior precision (not sure how, but using gut feel?) to account for the homogeneity issue your mentioned?', 'This is the sort of thing that makes weakly informative priors a good idea....', ""BY weakly informative, what do you mean?\n\nWeakly informative priors (assuming they are in the neighbourhood of observed effects and are not just diffuse priors for the sake of being diffuse) are great, but if you're not going to use prior information and your model is relatively simple I don't see the benefit of being Bayesian.""]",1,8,https://www.reddit.com/r/statistics/comments/11kzsrt/q_how_exactly_do_i_use_prior_information_in_a/
840,2023-03-07 20:01:13,[Question] MUltiple Correspondence Analysis MCA,"Hello World! I am building a thesis based on the MCA analysis but my supervisor is not satisfied with the way i am describing the way Eigenvalues work as well as Total and Explained Inertia. I will attach the parts in case someone can provide any help. I have tried to elaborate as much as possible but i cant find that many papers that describe how MCA works and youtube is not usefull at all. Thank you very much for your time.

[https://imgur.com/7DfuWgp](https://imgur.com/7DfuWgp)

[https://imgur.com/nYE931E](https://imgur.com/nYE931E)

[https://imgur.com/0YOlRhU](https://imgur.com/0YOlRhU)

[https://imgur.com/lC32rY2](https://imgur.com/lC32rY2)",[],1,0,https://www.reddit.com/r/statistics/comments/11kxg8t/question_multiple_correspondence_analysis_mca/
841,2023-03-07 16:18:19,[Q] How do you get the probability of model (prior) in Bayesian statistics?,"In reading about Bayesian methods, I often see the following:

P(model | data) is proportional to P(data | model) \* P(model). 

where model are more specifically model parameters like mean and variance. But how do you get P(model) in practice? 

Also is it possible to interpret ""models"" as distributions (normal, t, cauchy), or more generally neural networks / random forests, etc.? I assume it would not be possible to get a closed form solutions for the latter, so how would I get the P(model) in these cases?","[""This is where domain knowledge comes in. You have to supply priors in the form of PDFs for all the parameters in your model. In practice it often doesn't matter, and people often check how their priors affect the posterior by fitting the same model with different priors. \n\nThe reason that most Bayesian models don't have a closed for solution is the part you've left out: P(data). For most things that are interesting to model, this contains an integral we can't solve analytically, so we bypass it with MCMC."", 'The prior encodes your beliefs about your model parameters before considering your data. It\'s ultimately subjective but there are several common choices.\n\nIt\'s common to use a uniform distribution between limits that gives equal probability to any plausible value of the parameters. There\'s other more complex solutions to this but the general idea is to ""let the data speak for itself"" and minimally bias your posterior.\n\nOr perhaps you have previous measurements? Suppose someone else published their own values of these parameters from another experiment. You could use their posterior as your prior.\n\nOr maybe you just know some stuff which you could loosely put a distribution to. Maybe you know the effect of one parameter is much smaller than another so you alter your basic uniform distribution to give no probability to parts of parameter space where they\'re close in value.\n\n\n\nSo that\'s how you choose a prior and then your other concern is how do you get the equation. Well, it\'s often not necessary. It\'s rare to do these things analytically and it\'s easy to code up functions that do things like check parameter A is at most half of B, returning 1 if it is and 0 probability if not.', ""Maybe a few extra words on domain knowledge and priors. People are often 'afraid' of priors because they think they may be biasing the model, or don't quite understand how they could have a prior without previous experiments. But in general, it is very rare that we don't have some prior knowledge about the possible values of our parameters. I like Aki's example. If I ask you to guess how much money in euros I have in my wallet, you have no previous measurements of my wallet, have never seen it, and have no idea how much money I make. At the same time, any reasonable person would put the probability of there being more than 10000€ in any wallet as extremely low, and would rather put most of the probability at between 0 and 1000€ (or around those numbers). \n\nThis is to say, we usually have so idea of what we can expect from our models, and what reasonable and unreasonable values are."", 'I feel like there is some confusion here concerning the formula in the OP. Generally, when one starts learning about Bayesian methods Bayes\' formula is given as \n\nP(parameters|data) = P(data|parameters) P(parameters) / P(data).\n\nHere, P(data|parameters) captures the model, as it gives the likelihood of observing the data given a specific set of parameters. \n\nNow in the OP, ""parameters"" is replaced with ""model"" which leads me to believe that they may be reading texts dealing with Bayesian Model Averaging (BMA), where one tries to mix over different model specifications. In the simplest case, this boils down to something like ""which covariates do I include in a linear regression model"" but can also be something more complex like ""which likelihood is best suited to my data"".', ""In simple cases, it's handy to choose a prior that is convenient and plausible. A conjugate prior yields the same distribution form for the posterior. E.g., for binomial data, choose a beta distribution for probability of success. An beta \\* binomial gives you another beta, with updated parameters.""]",55,22,https://www.reddit.com/r/statistics/comments/11ktf93/q_how_do_you_get_the_probability_of_model_prior/
842,2023-03-07 13:56:21,[Q] Multinominal Probability with Value Given to the Trial Results," So, I'm trying to figure out some statistics for this game I'm playing. This is it how it works. Every time you buy an item, you'll have a 50% chance of getting a common worth 11, 30% chance of a rare worth 16, 15% chance epic worth 22, or 5% chance legendary worth 44. The total of all your items value equates to the amount of resources per second you get. So high value is good. Max items you are allowed to have is 150, so players will often compare their 150 item totals and see who's lucky and who isn't.

Average item value will be 15.8, so at 150 items the average should be 2370 between all players. Hence, that should be the 50th percentile and then all other results should follow a bell curve. I'm trying to figure out how to figure for percentile, or I supposed I should say chance, of scoring higher or lower than a specific value that's not the average. For example, what percent of possibilities are above/luckier than a total of 2400?

Doing research I learned how to use binominal probability to figure out the chance of getting a specific number, or more/less of a specific number, of a certain item rarity. So then I tried multinominal probability but I could only figure out the chance of a specific result as the chances having values make it confusing. Anyone able to help me figure this out?","['> Hence, that should be the 50th percentile and then all other results should follow a bell curve. \n\nThe *mean* is **not** the *median* in most cases. Certainly not here.\n\nI don\'t know what you mean by ""bell curve,"" but most people interpret that to mean ""Normal distribution"" which this **is not**.\n\nNow, it is true that the larger the number of items, the more symmetric and Normal-like this distribution will become. So at a certain point it may not be a huge issue to apply a Normal approximation as the other comment suggests. (More on that to follow.) But you cannot take that for granted.\n\n> I\'m trying to figure out how to figure for percentile, or I supposed I should say chance, of scoring higher or lower than a specific value that\'s not the average. For example, what percent of possibilities are above/luckier than a total of 2400?\n\nThere are three approaches to ""what is the probability of this range of values?"" for a distribution like this, or to asking the reverse question of ""what range of values has this probability?"" (this second form is about quantiles/percentiles).\n\n1. Brute-force computation. We know how to write the *equation* which you want an answer to, it is the sum over all sets of items which have the desired range of values (greater than 2400 in your example, but we could also ask about values between 2200 and 2300, or anything else) of the probability of getting that set of items. Generally the math does not work out nicely with things cancelling out to get simple, but we can always make a computer do this for us. We can make a program which enumerates all item combinations which produce a value in the range of interest and sums up their probabilities. This gets hard/slow/burdensome when the number of items gets large, because there are many, many item configurations which could produce the value. But if you wanted to know about the probabilities for, say, a few dozen items, this would get you the exact answer.\n\n2. Simulation. You\'ve got a nice straightforward set of rules for how the items come about, so you can write code to simulate this. By simulating many many times, you get samples from the distribution you want to know about, and you can answer questions by simply counting how often that happens in the samples. Simulating comes with some error (called Monte Carlo error in the literature), but you can estimate and control that error by choosing the number of simulations carefully. Or just choose a really big number and hope for the best. Here are a few lines of R code which allow you to simulate this:\n\n    nitem <- 150\n    nsim <- 1e6\n    p <- c(0.5,0.3,0.15,0.05)\n    v <- c(11,16,22,44)\n    tots <- sapply(1:nsim,function(x){\n      sum(v[sample.int(4,nitem,TRUE,prob=p)])\n    })\n\n\n3. Find an approximation. The other comment suggests appealing to asymptotic normality, which would mean you could compute the probabilities in anything from Excel to stata to python. There will always be error in this case, called approximation error, and it cannot be controlled, it\'s just a fact of approximating. I simulated one million times and compared that to a Normal approximation. The Normal approximation is not *terrible* around the center of mass, but there is still some asymmetry that it can\'t capture. The Normal approximation has the 25th, 50th, and 75th percentiles being 2307.5, 2370, 2432.5 while the simulations have those being 2306, 2367, and 2431. (Note that the mean, 2370, is not the median, 2367 here, but the Normal forces it to be.) If we ask what the probability is of >2400, the Normal says 37%, while the simulations say 36%. The less-probable the values in question, that is, the more extreme and into the tails, the worse the Normal approximation will be. (To be fair, the simulations will _also_ have trouble with particularly rare events unless you use a really large number of replicates.)', 'You have already calculated the expected value of 15.8. So you still need to calculate the variance for your discrete random variable. You can do that in this way: [https://nzmaths.co.nz/category/glossary/variance-discrete-random-variable](https://nzmaths.co.nz/category/glossary/variance-discrete-random-variable)\n\nThen you can apply central limit theorem (sample mean is approximately normally distributed with mean 15.8 and the variance divided by n which in this case is 150). Then you can calculate the probability of getting a sample mean greater than 2400/150.', 'That helps make a lot more sense of it thanks. Been awhile since I used R but I\'ll mess around with it to fire up a simulation. \n\nI do have two follow up questions. First, apologies for the use of terminology ""bell curve,"" high school statistics was a long while ago for me lol. In the normal approximation is the 50th percentile different than the mean because the odds of getting on one side of the mean value isn\'t truly 50%? For example the odds of getting 75 or more common is actually 54% and not 50%. \n\nSecond, between simulation and approximation by using a normal distribution, which one do you think would be considered better? In actuality there is no where to know how many items in total have been bought, but we know how many resources were cashed out and the top 500 in the country and your own state. Using that knowledge, I could approximate that there\'s a minimum of at least 1,000,000 items bought upwards to maybe 2,000,000. Would the way of figuring out how normal this would be, be to simulate it? As for people with 150 items, which is often what\'s compared, I would approximate there\'s 3,000-5,000 people.', 'Thanks for the link. That should be very helpful.', ""> In the normal approximation is the 50th percentile different than the mean because the odds of getting on one side of the mean value isn't truly 50%? For example the odds of getting 75 or more common is actually 54% and not 50%. \n\nYou're conflating something like three different things here.\n\nIn *truth* the mean is not the median here. The mean is higher, you can think of that as being because of the low-probability expensive legendary items. This drags the mean up more than it affects the median.\n\nThe Normal approximation *assumes* that the mean is the median. Any time you make a Normal approximation and ask what the median and mean are, you will get the same answer. Because the Normal is one of those places where the mean is the median, which is also the mode. So, the Normal approximation must be wrong about two of these values. And since we make Normal approximations using the mean and variance, this means that the Normal approximation will be wrong about the median (which should be 2367, not 2370) and the mode (which should be 2354, not 2370).\n\nNeither of these have to do with the odds of getting 75 or more commons, which the Normal approximation is unconcerned with. The Normal approximation only approximates the *total value* of a given number of items, *not* the composition.\n\n> Second, between simulation and approximation by using a normal distribution, which one do you think would be considered better? In actuality there is no where to know how many items in total have been bought, but we know how many resources were cashed out and the top 500 in the country and your own state.\n\nIt seems like you're changing the goalposts here a bit. \n\nSimulations give you a powerful approach for getting answers when there are not good approximations available. As soon as you have to do a lot of simulating, such as cases where the value depends on something which is unknown so you do the simulation repeatedly for ranges of values, the cost of it can quickly get prohibitive if you're not careful.\n\nIn the situation we've been considering where you know the total number of items, simulating gets you very precise answers at little cost as long as you're not asking about extremely rare events (though you might be able to enumerate those and get them exactly). So the Normal approximation seems to me like more error than it's worth here. I personally turn first to simulation for problems like this and only to approximation if that is too slow/burdensome to work.\n\n> Using that knowledge, I could approximate that there's a minimum of at least 1,000,000 items bought upwards to maybe 2,000,000. Would the way of figuring out how normal this would be, be to simulate it? \n\nNow you're going from probability to statistics. \n\nSimulations from knowns (values of items, probabilities of getting each, independence per-item, number of items) to unknowns (probabilities of total values). Probability in general does the same, you ask about unknown outcomes of known data-generating processes. Statistics asks about the process of generating the observed data. It turns that on its head (which is why you need to know probability *before* doing statistics).\n\nThis is very much a regime where simulations can become incredibly burdensome. You can do statistics entirely with simulations, approximate Bayesian computation (ABC) comes to mind. But it can be more effort than it's worth, and with thousands or millions of items a Normal approximation is probably going to work pretty well.""]",4,5,https://www.reddit.com/r/statistics/comments/11kqwix/q_multinominal_probability_with_value_given_to/
843,2023-03-07 08:28:48,[q] How can all numbers show up with equal frequency?,"I once heard a statistician say that all numbers ultimately show up with equal frequency, and this is why he himself chooses to play lottery with numbers that have NOT been drawn as frequently, to increase his chances of winning. The logic behind this is: numbers that haven’t been drawn will show up next because all numbers are supposed to be equal. 

I’ve never understood this and still don’t. I’ve tried googling this too, nothing. For the sake of argument, let’s keep the lottery scenario as an example: how is it possible all numbers are supposed to show up in equal frequency? If true, I’d imagine this would happen in an unlimited timeframe, so in this light, is the statistician really increasing his lottery winning chances by filling in least-frequently drawn numbers in hopes of them showing up next? 

This is lottery example so number 0 doesn’t exist.

EDIT: 13h into the post and I'm finding myself somewhere between the paradox of
a) the Gambler's fallacy (belief that the next random event if more likely/less likely to occur due to past events)
b) law of large numbers (events balance out over large amount of trials/tests)
c) law of small numbers (cognitive bias referring to the tendency to draw broad conclusions based on small data)","[""In the long run ( thousands of drawings), each number should be drawn an approximately equal number of times... but that doesn't mean they will come up with exactly equal frequency.\n\nFurthermore, the statistician you mentioned is mistaken to believe that numbers that haven't been selected recently are more likely to come up.  That is an example of the gambler's fallacy (https://yourlogicalfallacyis.com/the-gamblers-fallacy)\n\nEach number has the same probability of being drawn at the beginning of each drawing."", 'Seems like a statistician of dubious quality if they’re making these kind of claims. This is a great example of the [gambler’s fallacy](https://en.m.wikipedia.org/wiki/Gambler%27s_fallacy).', '> how is it possible all numbers are supposed to show up in equal frequency?\n\nThey aren\'t.  They only have equal *expectation* of showing up in upcoming draws.  The idea that a random process ""remembers what happened"" and doesn\'t do that in the future, is literally named *The Gambler\'s Fallacy*.  Your friend is incorrect.', '> In the long run ( thousands of drawings), each number should be drawn an approximately equal number of times\n\nThis is not so (at least not quite the way you said it, for all that you might have intended something else); choose two of the numbers (say 11 and 34 for example). Then the difference in the *number* of times those values appear will diverge as n goes to infinity.\n\nSpecifically let B11(s) be 1 if ""11"" appears in draw s (0 if not), and define B34(s) similarly. Now let   \nX11(t) = B11(1)+...+B11(t) and let X34(t) = B34(1)+...+B34(t) ...   \nso X11(t) and X34(t) are the number of times that those two numbers (11, 34) appear in the first t draws.\n\nNow as t grows large, the average value of |X11(t) - X34(t)| *grows* in proportion to t^(1/2). That is, the *number* of appearances actually diverges as t grows, heading off to infinity (albeit slowly).\n\nThe same applies for any such pair\n\nWhat converges is the *proportion* of times each one appears.\n\nLet P11(t) = X11(t)/t and similarly for P34(t)\n\nThen for large t, on average |P11(t) - P34(t)| decreases in proportion  to t^(-1/2) (i.e. it converges to 0, though again slowly).\n\nThe distinction between the proportions and the *raw* *numbers* is crucial, and it\'s the scaling by *t* that does it.', ""> I once heard a statistician say that all numbers ultimately show up with equal frequency\n\nThis is a (bad) misunderstanding of the law of large numbers. Are you sure they were a *statistician*? (i.e. someone with an actual stats major?)\n\nI ask because there's pretty obvious reasons why this statement doesn't make sense. \n\n> this is why he himself chooses to play lottery with numbers that have NOT been drawn as frequently\n\nThis is straight-up Gambler's fallacy caused by that misunderstanding. I'm tempted to call bull. Either (a) this person isn't a statistician at all, or (b) they are one (somehow) but sound like a pretty poor statistician\\*, or (c) they once knew their stuff but have somehow forgotten it (*drugs are a terrible drug* so the wise man said) or (d) they're pulling your leg.\n\nhttps://en.wikipedia.org/wiki/Gambler's_fallacy\n\n---\n\n\n\\* I mean, getting the LLN and its implications wrong is one thing but how does a statistician fail to recognize plain old gambler's fallacy? It's not even slightly disguised. Someone who thought that the Gambler's fallacy was correct reasoning would send our clients broke. \n\n[For that matter, how does a statistician fail to recognize the problem with a regular bet which in just about every place I've heard of pays about 60 cents on the dollar? If you want to gamble, there's way better bets than that. Even within that style of lottery there's way better schemes than that.]""]",5,23,https://www.reddit.com/r/statistics/comments/11kjgye/q_how_can_all_numbers_show_up_with_equal_frequency/
844,2023-03-07 07:42:12,[Q] correlations with multiple entries per participant,,"['I would include the multiple observations in my model in your cat example. I’d call the owner or household a random variable.', 'Thank you anyway!', ""Yes, it's a problem. What to do about it... I hope smarter people comment here...\n\nAs long as you know which participants responded, and how many times, I can think of two possibilities:\n\n1. Only  use each participant's first entry. Then you're good. Normal correlations (if the data supports it).\n2. Model everything as multilevel data, run a simple model, generate polychoric correlations or some other effect estimate."", 'How many multiples are we talking about here and do you have a way to identify them? \n\nIf it is small enough relative to your sample size, you might want to consider omitting them if you cannot run models controlling for the many to 1 relationships.', ""I see, but you wouldn't be able to run a simple correlation, you would have to model the nested nature of the data. That was what I was thinking.""]",9,15,/r/AskStatistics/comments/11khdpm/correlations_with_multiple_entries_per_participant/
845,2023-03-06 21:16:04,"[q] If I have yes/no/unknown survey, is it proper to recode unknown to missing?","A know it’s a dumb question. A little more detail, case-control study where one question has 8% “unknown”. 

What to use multiple imputation, if okay to recode to missing, as that 8% would remove to my strata if dropped.","['It is a nominal variable with 3 levels.', 'It’s not missing data, though. They answered the question.', 'No, because you are blending two different things.', ""I've noticed a lot of questions recently about removing data or recoding data. The answer is just about always no. You can't remove data because you don't like it or because it makes your analysis more difficult."", 'This is the way']",24,13,https://www.reddit.com/r/statistics/comments/11jyo1z/q_if_i_have_yesnounknown_survey_is_it_proper_to/
846,2023-03-06 16:06:57,[Q] Need help w/ an odds question,"Let's suppose that for each iteration of a dice roll, there's a 40% chance of rolling X (this is a hypothetical dice - maybe a d10 - where it doesn't really matter what X is).  

What are the odds that after 35 rolls, X actually appears 80% of the time rather than 40%?  Also, what is the formula that calculates that so I can try to figure this out for myself in the future?  I've got a decent background in stats but I can't figure out how to math this one.","[""You can model this as a biased coin, 40% heads and 60% tails.\n\nYou can use the normal approximation to the binomial distribution so:\n\nvar(p) = p(1-p)/n\n\n= 0.4(0.6)/35\n\n= 0.00686\n\nand the standard error is the square root of that, so:\n\nse(p) =0.08281\n\nIf the observed value for p is 0.8 with a true underlying value of 0.4,\n\nz=(0.8-0.4)/se(p)\n\nYou should be able to take it from there.\n\nBear in mind that if this question has been prompted by observing an extreme value once out of many tries, the probability of this occurring isn't very interesting. If you give rare things enough chances to happen, they will happen. If, on the other hand, you said that someone was cheating and tested it by observing their next set of dice rolls coming up X 80% of the time, that would be compelling evidence that they were cheating (absent any other explanation)."", 'This is distributed Binomial with probability of success p=0.4. So P(Y=28) [because you want a 80% of the rolls out of 35 to be a success: 0.8*35=28 successes]\nP(Y=28) = (35 choose 28)* [(0.4)^28]* [(0.6)^7]= 0.00000135643\n\nPlease correct me if I’m wrong. The formula is just the PDF of the binomial distribution. I gave the probability that you roll an X 80% of the time in 35 rolls with the probability of rolling an X being 0.4.', 'Thank you very much - I appreciate it.', ""I thought I had a general feel for statistics, but clearly I was mistaken. I guess we'd put me about high school level maybe. I will definitely take your word on this one"", 'So, just plugging in your number, the odds of this happening are about 1 in 735k, correct?']",3,7,https://www.reddit.com/r/statistics/comments/11jslji/q_need_help_w_an_odds_question/
847,2023-03-06 06:42:24,[Q] Which statistical test to compare differences in time series data?,"Hello, I am interested in testing whether the difference between monthly values in two financial market indices is statistically significant. I have only taken a few statistics courses and would greatly appreciate some guidance.

I have 452 months of paired values and the differences are close to being Normally distributed. The distributions of the values themselves are unimodal and right-skewed. I initially thought that a paired t-test might work, but the monthly data points might not be independent (testing for autocorrelation?).

Thanks.","[""You're going to need some form of model for the observations in order to make any progress. I agree that dependence is very likely to be present (though it's hard to say much more with almost no context); expertise in the specific area may well be important to coming up with reasonable models."", 'If you have censoring (drop outs) then you may have to do use an exponential or Weibull distribution to do a log rank test.', 'if youre interested only in the main effect of the two financial index differences, you may try fixing time using fixed effects linear modelling (check out the felm or plm package in R) to control for time invariant effects', 'Survival analysis methods such as log-rank test or cox proportional hazards regression models', 'No need for censoring for a log rank test.']",3,5,https://www.reddit.com/r/statistics/comments/11jgci3/q_which_statistical_test_to_compare_differences/
848,2023-03-06 04:19:49,"[Q] About to get a bachelors in MIS with only intro stats taken, but figure I’d like a MS in Statistics because it is more interesting. How do I prove to graduate admissions that I am competent with prerequisites? Would I need to spend more tuition money for formal statistics + math college classes?","Also, in case you may ask, how do I find statistics more interesting if I’ve only taken introductory statistics? 
I believe I got discouraged from statistical coursework due to a bunch of formulas and Greek letters that intimidated me, even though I previously took calculus. It wasn’t until much later in my academic study where I talked to an alumni that works with data analytics a lot in higher education that suggested I may have learned statistics poorly, where if I had first learned conceptually such as statistical intuition, I would have appreciated the field more. As I am working on data analytics projects to put on my GitHub profile, I found my interests to increase in statistics as a way to view the world rather than just a means to getting an entry level analyst job, such as consuming news or scientific articles and questioning their findings based on intuition on data sets, how they collected data, presence of holes in their data, statistical significance, etc.

Edit: with my recent luck posting questions in subreddits it seems I’ve appeared to be asking dumb or annoying questions, but before you downvote, please understand I did my best to look through this subreddit and even other subreddits for someone else asking my exact question.","['>I believe I got discouraged from statistical coursework due to a bunch of formulas and Greek letters that intimidated me, even though I previously took calculus.\n\nThe highest ranked statistics programs require a good understanding of theory.  Dong well in calc 1-3 and linear algebra hold more weight in admissions than doing well in basic statistics and analytics classes.  There are some programs that have de-emphasized math skills, mostly down further in the US News rankings.  That does not mean that you cannot get a good education from those programs, just letting you know that the top statistics programs are probably not what you are interested in.  Have you thought about a degree in data science or analytics?  It sounds more in-line with what you are interested in.', 'Thank you for your reply. I have considered a masters in data science and analytics, I just put Statistics as the preferred after reading older posts on this subreddit about statistics being a better degree.\nUpon further reflection I probably can see myself going for an Analytics Manager route than a data scientist due to the disparity in math level and the business aspect of my undergrad.']",1,2,https://www.reddit.com/r/statistics/comments/11jbyxy/q_about_to_get_a_bachelors_in_mis_with_only_intro/
849,2023-03-06 02:31:41,[Q] Should I keep the outliers in my dataset?,"I'm using a multiple linear regression model with three predictors and a continuous outcome. When I run the model with the complete dataset, I see no significance, however, when I remove the outliers (+/- 2 standard deviations of the mean) I see that predictor #1 now shows significance. I have a feeling this is a false positive and the outliers should actually be included in the model, or is there maybe a different way I should detect outliers in this problem?","[""I don't know the context, but usually I'd say that outliers are expected to exist and we should keep them.\n\nI'd only exclude outliers when I have a good reason to do so, not because of the way they are impacting significance. For example, if the outliers don't make sense, maybe there was a measurement error."", 'if something was procedurally off for that replicate, the data for that whole replicate have to be removed and redone.  If it’s a real result though, the data (all) have to stay in as far as I know.  If one datapoint changes the answer, people where I work say that means either not enough data were collected, or the wrong analysis was used to evaluate the data.', 'Unless you know they’re bad data reports, keep them. Just try using models that are robust to it.', ""You need to keep them unless you think that they are result of a poor measurement or some kind of error. It's typical for a lot of data sets to contain outliers that are meaningful data that tell you something useful about what's going on.\n\nJust as an example, say that you're working with some economic data relating to income. Maybe most people are within two standard deviations of the mean income but some people are Bezos, Gates, etc. Even though those are outliers, they are really important data and any economic model that doesn't account for the fact that some people are really, really rich is not going to be a good model.\n\nYou can't start removing things from your data set to try to make a parameter seem significant. If it isn't significant, then it isn't significant. The model that you're using may not be the best way to represent this data."", 'Hello, maybe try using a robust regression with t distributed resiudals to account for outliers.\n\nHere is a guide in python: https://austinrochford.com/posts/2015-03-08-robust-regression-t-residuals.html']",2,16,https://www.reddit.com/r/statistics/comments/11j6vgr/q_should_i_keep_the_outliers_in_my_dataset/
850,2023-03-06 00:27:35,[Q] Publication reviewer wants me to run a post hoc power analysis. How should I proceed?,"Hi all, 

I am publishing my MA psychology thesis in a peer reviewed journal. Part of the revisions requested was to run a post hoc power analysis to determine if I had enough power to justify the number of hierarchical linear regressions that I conducted. From what I understand, post hoc power analyses are a bit rubbish? Can someone please explain why they are rubbish; and if I was to do the post hoc analysis, would that be possible in SPSS? 

Thank you.","[""These references show why they are rubbish and the reviewer will almost certainly back down if you send the references to them, perhaps with a few relevant quotes.\n\nHoenig, John M. and Heisey, Dennis M. (2001), The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis The American Statistician 55:19-24. DOI:10.1198/000313001300339897\n\nLevine M, Ensom MH (2001) Post hoc power analysis: an idea whose time has passed? Pharmacotherapy 21:405-409 DOI: 10.1592/phco.21.5.405.34503\n\nGoodman SN, Berlin JA. (1994) \nThe use of predicted confidence intervals when planning experiments and the misuse of power when interpreting results. Ann Intern Med 121:200-6. doi:10.7326/0003-4819-121-3-199408010-00008 (Erratum in: Ann Intern Med 122:478. doi:10.7326/0003-4819-122-6-199503150-00029)\n\nThomas L (1997) Retrospective power analysis. Conservation Biology 11:276-280. DOI: 10.1046/j.1523-1739.1997.96102.x\n\nYuan K-H, Maxwell S (2005) On the post hoc power in testing mean differences. Journal of Educational and Behavioral Statistics 30:141-167. DOI:10.3102/10769986030002141\n\nWalters SJ (2008) Consultants' forum: should post hoc sample size calculations be done? Pharm Stat 8:163-169 DOI: 10.1002/pst.334\n\nMiller, J. (n.d). What is the probability of replicating a statistically significant effect?. Psychonomic Bulletin & Review, 16(4), 617-640.\n\nGreenland, S. (2012). Nonsignificance plus high power does not imply support for the null over the alternative. Annals of Epidemiology, 22(5), 364–368.\n\nWilkinson, L., & Task Force on Statistical Inference, American Psychological Association, Science Directorate. (1999). Statistical methods in psychology journals: Guidelines and explanations. American Psychologist, 54(8), 594–604. https://doi.org/10.1037/0003-066X.54.8.594\n\nHow post-hoc power calculation is like a shit sandwich | Statistical Modeling, Causal Inference, and Social Science [Article here](https://statmodeling.stat.columbia.edu/2019/01/13/post-hoc-power-calculation-like-shit-sandwich/)"", 'The main purpose of a power analysis is to keep you from doing a costly experiment which has little chance of being conclusive even if your expected effects are real. It doesn’t reflect on the validity of your results. Nonetheless, it’s true that if most submissions to a journal are either very low power or have true null hypotheses, the number of significant low power articles may not be that different from the number of articles with type I errors. However, increasing the number of low power experiments without increasing the number of articles with type I errors obviously lowers the type I error rate in the journal.', 'I am saving this post just for this answer. Thanks for sharing', 'You are right about it being a bit rubbish. Some view it as a method of finding the power “necessary” to find statistically significant differences for your initial hypothesis. But applying this after your data has been seen and analyzed removes the randomness the analysis relies on. Since the goal is to find the likelihood of finding a statistical difference this is meaningless after the fact. \n\nIf you want to do more reading on it I’d recommend: https://gpsych.bmj.com/content/32/4/e100069, and it’s first four references.', ""I would do the analysis since the software is free, but then include a wall of text with citations about why it is not good practice to do this in a post hoc manner and you would greatly prefer to not have this as part of your manuscript as such and merely are providing the analysis for curiosity's sake.  \n\nShould be able to do this with G*power, and if not, R.  Both are free.\n\nHold your sources incase you have to reply to the editor about the post hoc analysis.  Frankly, the reviewer should have been able to do this themselves as well if they cared.""]",65,19,https://www.reddit.com/r/statistics/comments/11j17hq/q_publication_reviewer_wants_me_to_run_a_post_hoc/
851,2023-03-06 00:07:01,[Q] Movement Ecology: How to determine which Hidden Markov Model?,"I am modeling the movement of an animal using hidden Markov Models (HMM). The animal is in one of two modes (traveling and foraging). In each mode, I have a HMM consisting of two states, parameters generated from moveHMM in R using the movement step length and turn angles. The model fits well, and I can use it to generate movements.

Now, I want to observe the animal moving and then determine what mode they are in. I have a sample of, say, 100 steps, and I can compute step length and turn angles for that sample. How do I decide that it fits HMM1 (foraging) or HMM2 (traveling)? The way I have been doing it is to take the cumulative distribution function of step length and comparing it to the CDF of the different HMMs, using either Wasserstein (earth movers distance) or Kolmogorov-Smirnov. It works ok, but of course I'm completely ignoring turn angle. I could do the same with turn angle. But how do I calculate the joint distance, incorporating both step length and turn angle?

(Here's a youtube video discussing the topic in general: [https://www.youtube.com/watch?v=WELTpbB5BuU](https://www.youtube.com/watch?v=WELTpbB5BuU))","[""Based on your description, this sounds to me like an [xy problem](https://xyproblem.info/).  Why can't you build one HMM where your hidden states are traveling and foraging, since those seem to be of primary interest to your research?""]",2,2,https://www.reddit.com/r/statistics/comments/11j0nqr/q_movement_ecology_how_to_determine_which_hidden/
852,2023-03-05 21:43:34,[Q] Mean Residual Life,"I see that the definition of the Mean Residual Life is the integral of a Survival function and then divided by another Survival function.  I understand the denominator, but why is the numerator the way it is? Why not the standard integral for continuous pdf?","[""Are you asking why write  *∫*ₜ^(∞) S(x) dx / S(t)  \nrather than *∫*ₜ^(∞) (x-t) f(x) dx / S(t) ?\n\nIf so, that's because they're equivalent expressions; it's an\nidentity sometimes called the Darth Vader rule. Its proof is fairly a standard undergrad probability exercise\non expectation for non-negative random variables (where here (x-t) is the non-negative quantity)\n\nThere's a proof here: \nhttps://math.stackexchange.com/a/4000018\n\nor here: https://thirdorderscientist.org/homoclinic-orbit/2013/6/25/the-darth-vader-rule-mdash-or-computing-expectations-using-survival-functions\n\nAlso see:\n\n https://www.researchgate.net/publication/234465628_The_darth_vader_rule\n\n(This may have been the first use of this name for it but the result is considerably older.)"", 'Thanks!  This is what I was looking for!', '>  do you know whether there is generic way to draw from the Residual Life Distribution\n\nIf you know S(t|t>s)\n\nthen let gₛ(t) = S(t|t>s).\n\nGenerate Tᵢ = gₛ^(-1)(1-Uᵢ) where Uᵢ is standard uniform (if g is such that g^(-1)(0) is finite then you don\'t need to flip U (i.e. you could use U directly rather than subtract from 1)\n\nThen I believe Tᵢ should be a random draw from the conditional distribution of t given t>s.\n\nIt\'s important to be careful to distinguish between t and t-s though; if you want to know about the distribution of ""years of lifetime left when I\'m s years old"" you\'re looking at t-s, not t. If you want to know about the distribution of age at death given I\'m s years old"" then you\'re looking at t, not t-s.\n\nThis will affect what you do: if you\'re generating from that conditional survival function, you may need to subtract s (depending on which quantity you were after) -- but in many cases you\'ll actually find yourself generating not T|T>s but  T-s|T>s  in which case you won\'t subtract s (it\'s already subtracted); but you might then need to *add* s if you want that conditional age at death\n\nYou\'ll see exactly what you\'re doing, because if your generated values start at 0 you\'re looking at time left, if they start at s you\'re looking at age at death (total life).', ""Because it's conditional on X reaching a certain value. The denominator you are seeing is the conditional event."", 'Is there a similar trick for the variance?']",2,9,https://www.reddit.com/r/statistics/comments/11ixgwe/q_mean_residual_life/
853,2023-03-05 12:13:49,"[D] Need RCTs or Observational studies that explicitly mention ""statistically significant but not clinically significant/meaningful"" to dispel a misunderstanding","I  am having an argument with my dad, who is a clinician. I said  interpreting results solely based on statistical significance is  unwarranted because with enough sample size, anything will become  statistically significant. I have shown him paper after paper explaining  the difference as well as a systematic review actively utilising the  concept. He remains obstinent and continues to argue uncharitably.  Anyway, his current requirement is for **primary studies** that have  explicitly utilised the concept within their study design and reported  it in that manner.

Does anyone have any examples?","['https://pubmed.ncbi.nlm.nih.gov/10485679/ is an example. There\'s certainly more recent studies to be found though, this was just the first I found on a quick search.\n\nThe effects are all (very!) statistically significant:\n\n> mean annual rate of episodes of acute otitis media, 1.4 vs 2.1 (P<.001)\n\nBut the conclusion is essentially negative, because in the context of the side effects and costs, averting half an episode a year is not ""clinically significant"":\n\n> Our study showed limited and short-term efficacy of both adenoidectomy and adenotonsillectomy; given the risks, morbidity, and costs of these procedures, these data suggest that neither operation should ordinarily be considered as a first surgical intervention in children whose only indication is recurrent acute otitis media.\n\nAs a clinician, I\'m surprised by your father\'s position. Clinical vs statistical significance is something I hear very often from other clinicians. If the pharma rep comes to peddle a drug costing $100,000 with NNT 5,000 (odds ratio 0.98, 95% CI 0.97–0.99, ""it\'s statistically significant!""), you\'d tell them to fuck right off.', ""Not sure if this tackles your request, but this [article](https://www.jto.org/article/S1556-0864(20)30477-9/fulltext) has some interesting content and examples of statistically significant vs clinically significant.\n\nI like that the approach of the author includes two sides of the coin. When the statistical design of the trial is sort-of design to detect modest effects, and another one when a stringent threshold failed to capture a clinically significant effect.\n\nThere's a bunch more out there I am sure."", '>Could you tell me what terms you searched for, and on which platform? I would like to be able to gather more evidence!\n\nI went looking for systematic reviews on clinical vs statistical significance then went looking through the articles they reviewed favourably.\n\nOver in /r/medicine they made the reasonable comment that big pharma-sponsored trials might not be explicit about clinically insignificant results. You could try searching on Twitter (#MedTwitter), they are usually a bit more merciless there about dodgy results.\n\nOncology might be a good field to look. Contrary to the stereotype, the good oncologists tend to be pretty against making patients miserable to achieve nominal gains in survival.\n\n>I cross-posted this on r/medicine. You can take a look at the top responses of the thread here, its not very encouraging: \n\n:(\n\nI suppose ""anything"" was a bit vague, obviously you meant ""arbitrarily small effects"". I don\'t think it was worth making a fuss about.', ""I see where you are coming from, but I'm not convinced – I would still expect you to find statistical significance in only about 5% of experiments.\n\nYes, your study with N = 2 million will be powered to detect very small differences. But with N = 2 million, you will expect your random partition to produce only small inter-group differences in the first place.\n\nWe can confirm this by simulation:\n\n```\nimport numpy as np\nfrom scipy import stats\n\nnp.random.seed(0)\n\nnum_significant = 0\n\n# Repeat the experiment 100 times\nfor _ in range(100):\n\t# Randomly sample 2 million people\n\tparticipants = stats.norm().rvs(2_000_000)\n\t\n\t# Randomly partition into 2\n\tnp.random.shuffle(participants)\n\tgroup1 = participants[:1_000_000]\n\tgroup2 = participants[1_000_000:]\n\t\n\t# Do a t test\n\t_, pvalue = stats.ttest_ind(group1, group2)\n\t\n\t# Was it significant?\n\tif pvalue < 0.05:\n\t\tnum_significant += 1\n\nprint('Of 100 trials, I found {} significant differences'.format(num_significant))\n```\n\n> Of 100 trials, I found 7 significant differences"", 'being a clinician as well as a PhD in Epidemiology and Biostatistics, I would say that you are on the right path. Clinically meaningful difference is a very nuanced concept. We use it all the time.\n\n""I would change your statement, ""I said, interpreting results solely based on statistical significance unwarranted because, with enough sample size, anything will become statistically significant"" to, ""IF THERE IS A TRUE DIFFERENCE, No matter how small, with enough sample it will become statically significant.""  \n\n\nClinically meaningful is a convergence of opinions of many physicians and patients in the area of interest./disease so it will be hard to find primary evidence in quantitative research. You might want to look for qualitative research. It will likely be interviews of multiple providers arguing on what is ""clinically meaningful"".. and then a mixed methods study that leads into quantitative analysis to prove if such clinically meaningful difference exist between treatment 1 vs treatment 2.']",32,29,https://www.reddit.com/r/statistics/comments/11in628/d_need_rcts_or_observational_studies_that/
854,2023-03-05 00:49:36,[E] Time Series or Econometrics Course/Book,"Hi I am look for some recommendation for book on Time Series!

I am hitting myself in the head for not taking a course on Time Series given my interest in financial economics.

I've looked at the book by Hyndman but didn't find it quite rigorous. I was recommended the book by Tsay and another by Chris Chatfield and Haipeng Xing; are those any good?

Any course with videos are also much appreciated!","['Another book you might consider is \nHamilton, analysis of time series\n\n\nIt seems more rigorous than tsay.', ""Duckduckgo.com - > forecasting principles and practice fpp3 hyndman\nYou're welcome"", ""Tsay is decent if somewhat unclear at times. (Pros: Complete, quickly covers the main points and provides data that you can use to verify your understanding.)\n\nYou can supplement it with Shumway Stoffer (though their focus is not strictly financial time series). \n\nI do not recommend Hamilton because it spends way too much time deriving formulas, the notation is attrocious (imo)  and if you are familiar with stats it won't add a lot to your understanding."", 'The book on time series applications by shumway is pretty good', ""Recommend Tsay and some others over Chatfield and Xing, honestly. Chatfield and Xing's errata page no longer even works online. Some of its provided solutions are not very clear and at that point, would have rather not had the unclear solutions at all. \n\nI find [Time Series Analysis by Ord and Kendall](https://www.amazon.com/Time-Charles-Griffin-Maurice-Kendall/dp/0195207068/ref=sr_1_1?crid=3PDWZS11UZWWW&keywords=time+series+charles+griffin+book&qid=1677996602&s=books&sprefix=time+series+charles+griffin+book%2Cstripbooks%2C157&sr=1-1) (which Chatfield and Xing cite) to be a good one with a good balance of rigor and intuition. The exercises are dated, obviously, but the fundamentals of frequentist time series analysis are motivated very well.""]",11,5,https://www.reddit.com/r/statistics/comments/11i5kig/e_time_series_or_econometrics_coursebook/
855,2023-03-04 23:01:35,[Q] what statistical package to use to compare average annual percent change?,"What package on RStudio is best to use to pool and analyze AAPC in a meta analysis? I figured it would be metamean since I’m comparing different averages?

Thanks!",[],1,0,https://www.reddit.com/r/statistics/comments/11i187a/q_what_statistical_package_to_use_to_compare/
856,2023-03-04 21:25:35,[Q] Do you need to take theoretical statistics courses for grad school?,"So I am currently doing an undergrad in economics, and I am taking a lot of math courses offered by the math department. However, the statistics courses I took are applied statistics for business/economics offered by the business department, would this hurt my application eligibility for a master's in statistics?","[""Lots of folks who go to grad school for stats had few (if any) stats classes in undergrad. Many were undergrad math majors. As long as you've had a full sequence of calculus and a course in linear algebra, you should be fine. For a PhD in statistics, you'd also ideally have real analysis."", 'I will be taking a full year of linear algebra and calculus 3 in the upcoming academic year. But calculus 1 and calculus 2 I took were also focused on business and economics applications only where trigonometry and geometry are not taught. Would that be a disadvantage?', 'I think you’ll be fine as long as your GPA is decent (at or above the listed requirement for the specific program), and you meet the other prereqs given by the school. Of course a good application with decent LORs is important too. \n\nI’d check out r/gradadmissions if you haven’t already.', ""Here's a sneak peek of /r/gradadmissions using the [top posts](https://np.reddit.com/r/gradadmissions/top/?sort=top&t=year) of the year!\n\n\\#1: [This should be common practice!](https://i.redd.it/9uhgf2z8agga1.jpg) | [56 comments](https://np.reddit.com/r/gradadmissions/comments/10uir49/this_should_be_common_practice/)  \n\\#2: [Finding out decision with my daughter](https://v.redd.it/t2am80pfksm81) | [81 comments](https://np.reddit.com/r/gradadmissions/comments/tbviqd/finding_out_decision_with_my_daughter/)  \n\\#3: [I only applied to four PhD programs. Three super elite ones and one safety. Finally heard back from the safety: rejected](https://i.redd.it/9c40wfndroia1.jpg) | [50 comments](https://np.reddit.com/r/gradadmissions/comments/11460de/i_only_applied_to_four_phd_programs_three_super/)\n\n----\n^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| ^^[Contact](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| ^^[Info](https://np.reddit.com/r/sneakpeekbot/) ^^| ^^[Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/o8wk1r/blacklist_ix/) ^^| ^^[GitHub](https://github.com/ghnr/sneakpeekbot)""]",33,4,https://www.reddit.com/r/statistics/comments/11hz337/q_do_you_need_to_take_theoretical_statistics/
857,2023-03-04 15:10:21,[E] Resources for Kernel Regression? (Kernel Smoothing),"I have to give a presentation about Kernel Regression for a Regression II class, but I can't seem to find good resources. Is there any book you guys have found useful in understanding this topic?","[""Many references are good on various aspects of the topic (there's a lot of things one might talk about), but chapter 5 of Wand and Jones' *Kernel Smoothing*, and chapter 6 of *Elements of Statistical Learning* are perhaps places to start.\n\n\nThere's many sets of notes and talk slides around on this topic; some are quite helpful.\n\n\nAdditive models can be based on different smoothers, including kernel smoothers, so some of those resources may be helpful as well."", ""You might try looking in textbooks relating to nonparametric statistical methods. Larry Wasserman's All of Nonparametric Statistics is a good resource, and content in chapters 4-6 might be of interest to you. Chapter 4 gives an intro to smoothing, chapter 5 is about nonparametric regression, and chapter 6 talks about density estimation. Good luck on your presentation!"", 'I loved Applied nonparametric regression (1990) by W. Härdle', ""This sounds like it's just the thing I need, thank you so much!""]",1,4,https://www.reddit.com/r/statistics/comments/11hsnlw/e_resources_for_kernel_regression_kernel_smoothing/
858,2023-03-04 12:54:20,[Q] Kruskal-Wallis and Mann-Whitney different p-value?,"Hello everyone! I would like to start this post by saying I am very statistics illiterate 😅 I am evaluating some data (3 groups) to see if the method used to generate the data was statistically different between each other at a 95% confidence. I’ve attached a photo of an example group of real data I have. The sample size is small and cannot be increased. I am using an online calculator to do a Kruskal-Wallis test (from statistics kingdom) and am really looking into the group comparison p-values (i.e x1-x2, x1-x3, x2-x3). However, I am getting a p-value > 0.05 for 2/3 comparisons. When I individually compare the groups using a Mann-whitney U test the p-values are < 0.05. Can anyone explain why this is? I appreciate any help!!!!

Photo of sample data for reference:

https://imgur.com/gallery/0El2uMB","['Your p-values can’t all be smaller than 0.05 with a Mann-Whitney. You have two groups with 3 observations each, the smallest achievable p-value is 0.10 for them.\n\nDunn’s and Mann-Whitney are different tests, it’s normal that they give different results.', ""So here you're comparing 3 populations.\n\nIf you compare populations one by one against each other using three different t-tests (Mann Whitney U) then you will introduce error.\n\nThe correct test would be a non-parametric ANOVA - which the Kruskal Wallace should work. So the ANOVA will compare all three groups in 1 test and will spit out a p-value which will determine whether or not you reject the null hypothesis that there is no difference between groups. If you reject the null hypothesis the test isn't showing which groups are statistically dif from each other, but that a significant difference exists between at least two groups."", ""If whatever program they used employs the asymptotic Normal approximation for the Mann-Whitney test, then it would produce a p-value that is *just barely* less than 0.05 even for the group 2 vs group 3 comparison.\n\nThe value of U would be 0 or 9, and the asymptotic mean and variance are 4.5 and sqrt(63/12), respectively. This gives a z-score of 1.964, or a two-tailed p-value of 0.0495.\n\nNot that an asymptotic Normal approximation should be used for two groups of size 3. But it could explain OP's results/comment about that."", 'Look at how the Mann-Whitney U test works. Two samples of 3 observations are small enough that you can enumerate the possibilities (with a computer).\n\nIf you do so, you’ll see that (under the null) U will have the following probabilities:\n\n* p(0) = 5%\n\n* p(1) = 5%\n\n* p(2) = 10%\n\n* p(3) = 15%\n\n* p(4) = 15%\n\n* … (the centre of symmetry is 4.5 so probabilities for U=5 to 9 are the same as above)\n\nSo you can never have a p-value below 10% for a two-tailed test (or 5% for a one-tailed test) because even the most extreme values of the test statistic (0 and 9) are achieved 5% of the time.', 'how do you calculate the smallest achievable p-value?']",19,18,https://www.reddit.com/r/statistics/comments/11hpyia/q_kruskalwallis_and_mannwhitney_different_pvalue/
859,2023-03-04 07:43:03,[Q] Looking for help,"I'm doing something out of my own curiosity. I'm having a hard time making sense of this information. Language problem maybe.

its from site [https://www.fbi.gov/file-repository/2015-ncic-missing-person-and-unidentified-person-statistics.pdf/view](https://www.fbi.gov/file-repository/2015-ncic-missing-person-and-unidentified-person-statistics.pdf/view) 

Looking at the table on page 3 

total file transactions: 1,813, 755. - so I assume this means the total number of reports

entries: 634k (is this the ""active"" list of missing persons?

Cancelled & cleared: 579k (is this the number of  reports that were dismissed, or report is cancelled? ie false alarm?)

Locates (LM): 54.9l (Assuming this means persons that went missing and were found.)

Modified (MM): (no idea)

Modified supplemental : n/i

Cancelled supplemental: n/i

Queried missing person file: n/i

Queried NCIC system wide: n/i

Looking at this table what's the math to find the total number of missing persons whom were not found?

&#x200B;

Thank you","['I think the number you want is the number at the top: Active entries. \\~85k. That is the number of currently missing people in the list. Entries is just the total number of entries in the list, which includes people who have been found or entries that have been canceled for another reason.\n\nThe specific number of total missing persons who where not found may not be possible to calculate here. Depends on how they maintain the list, and what sort of events can de-activate an entry: being reported as found, yes, but time might also do it not sure how the list works.', 'Hmm thanks. May have to ask folks that specifically deal with this']",0,2,https://www.reddit.com/r/statistics/comments/11hj0ec/q_looking_for_help/
860,2023-03-04 04:41:16,[Q] How would clustering help in this study?,"Let's say I have a study that wants to understand how crime activity and the economy at different geographic unit levels (county and zip code) are related. How could cluster analysis help? We already conduct a Spearman rank correlation, and later on, if there's enough correlation, will begin a causal analysis.

The data (which are highly skewed):

* 4.5 million anonymized crime data points across the United States collected from 2011 - 2022 labeled with the time, coordinates, the type of crime, and who committed the crime.
* Each county in the United States is labeled with average income, median income, and some other economic variables for 2022.","['Hierarchical/multilevel SEM can do the job. Please don\'t be confident that a ""correlated enough"" data can be sufficient to proceed with causal inference. Are really two different realms', ""You might find there are two clusters (just an example), one which is high crime <--> small economy, and the other is high crime <--> blooming economy.\n\nThe criminal profile between these two clusters might be different. So if the government want to take measures to reduce crime rate, your cluster analysis's result could guide targeted intervention strategy."", ""Cluster analysis is helpful for two reasons. \n\nFirst, Spearman rank correlation is not going to be optimal, since I doubt the relationship between your two variables is monotonic (but you didn't specify the variables you put in that Spearman rank correlation, so I don't know for certain--I'm just speculating).\n\nSecond, and perhaps most importantly, multilevel models are *made* for situations like this. I did a lot of work like this for my master's thesis. I'm assuming you're running a regression model to determine causality (which, by the way, is going to be very difficult with cross-sectional data, but that's a different discussion). One of the assumptions of regression modeling is independence of observations. Your observations are **not** independent, since they were collected from the same neighborhood, which could exert an influence on the observations. (I mean, that's part of your hypothesis, right?). If you don't account for the clustering, your results could be biased."", 'You could incorporate that into the model, yes. Just make sure you select a suitable model. “Cluster analysis” is honestly a broad term. The other commenter mentioned SEM, but that is just one approach to go about it.', ""Thanks for the caution and correction! The multilevel SEM sounds like a good idea, however, we don't have individual-level data per se. Our crime data points mirror events Someone (group host) who has committed a crime (hosted an event) can be charged with many at various points on the map (multiple events), but we can only connect the crime to the person---a single column for unique ID's.\n\nIf we pulled the column for unique ID's we could derive a count of crimes and each crime type count.\n\nIf I understand your recommendation correctly? I'm a research student working at a  lab, so while I know implementation, I'm still getting up to speed on statistical reasoning and design.""]",2,7,https://www.reddit.com/r/statistics/comments/11he9cv/q_how_would_clustering_help_in_this_study/
861,2023-03-04 01:30:12,[Q] What statistical test do I use to test the relationship between two Likert scale-based questions?,"Hello,

I did a survey using Qualtrics where I try to understand the effet of different variables (e.g., perceived usefulness, perceived ease of use, etc.) on the intention to use robo-advisors. Moderating variables are age, gender, and prior investment experience.

All questions (except for the moderating variables) were given as a 5-point Likert scale, both the different constructs (e.g. for perceived usefulness: ""I find robo-advisors useful"" => Please rate on Likert scale), as well as the intention to use assessment (""I would use robo-advisors"" => Please rate on Likert scale).

I know that I can test the statistical significance of the relationship between my moderating variables (e.g. gender) and any of the Likert-scale questions using a Chi-squared test, since the moderating variables are categorical and my Likert scale questions are ordinal.

However, what test can I use to compare the relationship of two Likert-scale based questions with each other? E.g. the statistical significance of perceived usefulness on intention to use, when both survey questions were given in a Likert-scale rating format (as explained hereabove).

Thank you very much for your time!","[""Kendal's Tau (B) or Spearman's Correlation Coefficient would both work to quantify the relationship/correlation between the two ordinal variables."", ""Have a look at ordinal regression as well as polychoric correlations. Both methods are for starters and it really depends on what you'd like to do with the data first and foremost. Regardless of technology. What is your goal? What is your ultimate question?"", ""Mutual information is what I would use here. Nice, discrete state space, and it's sensitive to nonlinear relationships in data. \n\nMake sure you compare the empirical MI to a null distribution, but it should be very easy to compute."", 'Like other commenters have mentioned, a regression or correlation is probably your best bet (particularly the former since you can measure the effects of any moderating variables). \n\nHowever, you need to make sure to account for the directionality of your hypothesised relationship. It sounds like your study design is presuming users will be more inclined to use robo-advisors based on certain factors, so a one-tailed hypothesis in any analysis might be needed.', ""Thanks a lot! I'll look into those. Any reason to choose one over the other?\n\nAnother comment also suggested polychoric correlations, do you believe it would be less appropriate to use here?""]",25,10,https://www.reddit.com/r/statistics/comments/11h7fcw/q_what_statistical_test_do_i_use_to_test_the/
862,2023-03-03 17:46:22,[Q] Statistical chance of winning in a bot game simulation,"I am programming an elo system and I want to make bots that play a generic game, but I want the bots to be able to have a skill level so that elo makes sense. My goal is to be able to calculate the probability player1 will win over player2 given a their skill levels. My idea for how the bots play a game is as follows:  
Assign a skill level to each player where the possible skill level values are between 2 and 99. (2 to 99 is chosen to avoid games where no player can win)

Generate a random number between 1 and 100 for each player and compare that number to their skill level. If the random number is less than or equal to their skill level, they get a ""yes"", otherwise they get a ""no"". A winner is determined when one of the players has a ""yes"" and the other player has a ""no"". Otherwise, we generate a new set of numbers and compare again.

Here is an example:  
Player1 has skill level 70, player2 has skill level 40

A random number is generated for player 1. If that number is less than or equal to 70, player1 gets a value ""yes"", otherwise a value ""no"". A random number is generated for player 2. If that number is less than or equal 40, player2 gets a value ""yes"", otherwise a value ""no"".  
Suppose player1 is generated the value 50 and player 2 is generated the value 10, then the game will repeat since both got a ""yes"". Now suppose player1 is generated the value 80 and player2 is generated the value 72, then the game will also repeat since both got a ""no"". Finally, suppose player1 is generated the value 10 and player2 is generated the value 80. Then player1 wins because he is assigned the value ""yes"" while player2 is assigned the value ""no"".

I created a test program that will simulate this game for the two players 100k times for different skill levels just to see what the averages look like. Here is some of the data: (I'll make player 1 more skilled for readability)

|Player1 skill level|Player2 skill level|higher skill player win %|
|:-|:-|:-|
|20|10|\~68.8%-69.5%|
|90|80|\~68.8%-69.5%|
|75|70|\~56.2%-56.4%|
|99|98|\~66.7%-66.8%|
|70|10|\~94.5%-95.5%|
|70|70|\~50%|
|62|39|\~71.6%-\~71.8%|
|68|62|\~56.2%-56.8%|

My question is, how would you go about calculating the probability that the higher skilled player will win given player1 skill and player2 skill levels? I tried calculating it by letting P = higher skill player wins, Q = low skill player loses and then doing P\*not(Q), but that's as far as I've gotten.","['I think you just divide by all the terminal states. Take 20/10 as an example P(20 wins) = .2 * (1-.1) / ((.2 * (1 - .1) + .1 * (1 - .2)) = .69', ""In an ELO system the probability of a win can be determined by:\n\n    exp = (home_team_rating + home_field_advantage - away_team_rating) / 400\n    team_a_prob = 1 / (10 ^ -exp + 1)\n\nhome team advantage doesn't matter here, so we can remove it. The 400 as a constant just scales the values, it's based on a 1500 centered ELO system - if we reduce it to 40 we get reasonable ish predictions\n\n&#x200B;\n\n|A|B|Win %|\n|:-|:-|:-|\n|90|10|97.5%|\n|60|40|71.5|\n|55|50|55.7%|"", ""ahh okay thank you, I was over thinking it. I'm never sure when I need to go into combinatorics verses just sort of going through the problem like this verses moving to a distribution model."", ""cool thank you! I'll study it a bit more so I understand it better. Programming can only get you so far if you don't know the math.""]",9,4,https://www.reddit.com/r/statistics/comments/11gwp5k/q_statistical_chance_of_winning_in_a_bot_game/
863,2023-03-03 09:21:30,[Q] Negative Binomial Regression vs Zero-Inflated Poisson?,"I'm attempting to work with a dataset involving NFL statistics for quarterbacks. I want to find significant relationships between physical and collegiate attributes and professional, count statistics. However, there are many quarterbacks in my dataset who have never played professionally, meaning that there are an outsized number of zeros in my dataset. I'm wondering whether a negative binomial regression or zero-inflated poisson would be more appropriate in this instance.

I'm leaning towards poisson, because I believe that a theoretical reason behind all these zeros is the round the player was drafted in. Players drafted in earlier rounds are a bigger investment, and thus their teams would have more incentive to play them, whereas the patience for players drafted in later rounds would be lower. However, I'm not sure whether this is a solid-enough foundation to run zero-inflated poisson, or whether this is even what zero-inflated poisson is designed to measure. 

If anyone has any advice on this or any other aspects of my project, I'd be very appreciative!","['Just run a ZINB model or hurdle NB. Poisson assumptions are almost never met.', 'Poisson and Negative Binomial are two different count models that can both be zero inflated. To see which model to use, you need to see if your dependent variable is equi- or overdispersed. If the mean and variance or your DV are the same, it’s equidispersed and you use Poisson. If variance is greater than the mean, use negative binomial.\n\nAs for zero inflated or not, you need to think about the causes behind the zeroes. Does each case in your data set have the same chance of a 0 in the DV? For instance, if you wanted to predict the count of school buildings in each census tract, you’re going to be zero inflated because there’s no reason to put a school in a census tract with 0 people. Those 0s are qualitatively different.\n\nAnother example that I’ve seen is with fishing. If I want to count the number of fish caught in 15 lakes but 5 of the lakes are too shallow for fish to live there, of course 0 fish will come from those lakes. I can’t attribute the 0s to any other factors. Those lakes are qualitatively different.\n\nI know a lot more about statistics than I do football so I can’t tell you from your description if I think your model is zero inflated. But hopefully this helps you think through it.', 'Yes, just exponentiate the coefficient. Same interpretation as Poisson.', 'Check dispersion first.', 'Since you’re dealing with count data with a preponderance of 0s, why not compare the results of a ZIP model and a ZINB model? I wouldn’t be surprised if they yielded very similar results.']",25,17,https://www.reddit.com/r/statistics/comments/11gn4sm/q_negative_binomial_regression_vs_zeroinflated/
864,2023-03-03 05:13:22,[Q] Odds and Probabilities: What Am I Doing Wrong?,"I learned the following probability concepts:

\- Fractional odds for an event = P(E)/(1-PE)  
\- Probability for the event to occur: P(E) = a/a+b

\- Fractional odds against an event = \[1-P(E)\]/P(E)  
\- Probability against the event: P(Ec) = b/a+b

Out of curiosity I wanted to look at the implied probabilities of a sporting event.  
Online bookmakers here in France give decimal odds.

Say the bookmaker provides the decimal odds of 1.3 for team A to win.  
Now, unless I am mistaken:

\- The implied probability of team A to win is 76,9%  
\- The implied probability of team A to lose is 23,1%  
\- The implied fractional fractional odds for the team to win is 3/10

For some reason, when I try to run a check on the implied probabilities using the formulaes at the very beginning of this post I get:

\- Probability for the event to occur: P(E) = a/a+b = 3/13 = 23,1%  
\- Probability against the event: P(Ec)= 1 - 23,1% = 76,9%

Long story short, I am getting the opposite results?  
It's actually driving me crazy and I was hoping someone here could help me see what I am doing wrong.

Thanks!","['A decimal betting odd of 1.3 includes the original stake so the odds for are actually 0.3, or 3/10. \nTherefore the correct way to solve for the probability is 3/13 = 23.07%.', 'No worries. \n\nBut isn’t that counterintuitive?\n\nWhat I mean is, with odds of 1.3 for Team A and odds of 8.00 for Team B, doesn’t that mean that probabilities are in favour of Team A winning, hence it should have the 76.9% probability of winning and not the 23.1% you solved for?\n\nI’m not saying you’re wrong, I got the same result but that seems contradictory and I can’t explain why I’m getting those results.', ""Yeah, you're right; I fixed the issue of the difference in definition between the two kinds of odds but reintroduced the other problem, going back to thinking in terms of odds for rather than against."", ""~~Bookmakers often quote odds-against not odds-for. If something had a 1/4 chance, 'fair' gambling odds would be given as  3.0 (3-1 against) not 0.333, ie [1-p]/p not p/[1-p].~~\n\n~~Note also that implied probabilities from  bookmakers odds won't add to 100%, since they're trying to make a profit~~\n\nedit: I missed that you had *decimal odds* (I saw the word; I misunderstood its implication -- I didn't know that it was a term referring to one of the sets of odds that are distinct from fractional odds). The implied probability for decimal odds is different. So ignore what I originally wrote above"", ""I have no idea what you're talking about. \nIf odds in favor is \n\n     O=P(E)/(1-P(E)) \n\nthen probability in favor is \n\n     P(E) = O/(1+O) \n\nso in this case, 1.3/2.3 = 0.565""]",1,11,https://www.reddit.com/r/statistics/comments/11ge7ui/q_odds_and_probabilities_what_am_i_doing_wrong/
865,2023-03-03 05:06:05,[E] Current or recent TAMU MS stats online students,"Hey – 

I'm considering applying for this program and I've got a few questions I'm hoping some current or recent students can answer.

1) How accessible are the professors? How much did you interact with them directly? Did it feel more like a traditional classroom experience or a MOOC?

2) How much of the coursework is project-based? 

3) Did you get to know any of the other students in the program?

4) What programing languages did you use?

5) How applied vs. theoretical did you find it to be? Was there much ETL learning or was it primarily analysis in a setting where you already had all of the data you needed in the format you needed it in? Maybe a better way to put it was: How data science-y was the program?

6) I noticed that there's a new ML course being offered. Did ML show up in other courses?

Thanks so much and feel free to DM me if that's easier.","[""I attended 2017 - 2019 and I imagine it has changed since then. I think some of the professors I had are no longer there and the coursework I also imagine has changed. With that said, it was a great program and I was overall really happy with it. I had started out wanting to work as a statistician but post-graduation I got a data science internship and now work as a data scientist. During my time, I had taken 1 course that had python in it and had never done any thorough ML projects; that I learned in my internship and on-the-job training. \n\nSo let's see if I can take a stab at these:\n\n>1.How accessible are the professors? How much did you interact with them   \ndirectly? Did it feel more like a traditional classroom experience or a   \nMOOC?\n\nIt varied, but each course had a 1-hour virtual office hour session each week. These were great and well-attended. Some professors were also really active answering questions on the Blackboard forum. These forums were usually broken up by homework assignment or lecture topic, so asking about a formula or something got good responses. \n\n>2. How much of the coursework is project-based?\n\nVaries course-to-course. A few of the courses I took had a final project; one of these was the time series course. Most of them were standard courses/exams, so you can navigate electives as you feel works for you.\n\n>3. Did you get to know any of the other students in the program?\n\nSomewhat. I'm just naturally reserved and never went out of my way to meet friends in undergrad courses, so saying I never did it at TAMU could be misleading. But, we had a slack channel and I connected with some people to study for the comprehensive exam. I hear there's a Discord channel now and if you want to be more active/social, the opportunity seems to be there.\n\n>4. What programing languages did you use?\n\nAside from that one course of super-basic Python, I used mostly R and SAS. I still regularly use SAS at my current job, so it worked out well.\n\n>5. How applied vs. theoretical did you find it to be? Was there much ETL   \nlearning or was it primarily analysis in a setting where you already had  \n all of the data you needed in the format you needed it in? Maybe a   \nbetter way to put it was: How data science-y was the program?\n\nSome courses were super theoretical, most were a healthy balance. If you're looking to avoid theoretical topics, this isn't for you. But if you're looking to get into data science, then I would say stick to the theoretical. You can teach yourself how to build a GBM, but it's more difficult to teach yourself the theory behind a GBM, and in a data science role, you need to do both.\n\n&#x200B;\n\n>6. I noticed that there's a new ML course being offered. Did ML show up in other courses?\n\nVery high-level. I remember one course we used SAS which had a click-and-drag feature to build decision trees, GBMs, Neural Nets, and we compared the results. Again, we mostly used to it discuss the concepts behind them. That one Python course had a bit deeper, but not much, and certainly nowhere near a model build that you can find on Kaggle. But, that said, they may see where the field is going and may have created some more ML courses.\n\nHope this gave some help and feel free to DM me if you have any other questions. I have no regrets and really enjoyed my time there; it was a great program and I wouldn't be where I am now without it/the great professors I had."", '1) professors and TA are quite accessible via office hours and email \n\n2) very little if not 0 for me so far \n\n3) you could but I haven’t since I work so much \n\n4) R but sometimes I sneak in Python for fun \n\n5) not much ETL so far which I get way more than enough of at work anyway. Most stuff is applied statistics and a good amount of theory \n\n6) so far no but I’m only in the core courseload so the ds focus classes are coming up', '1. Traditional classroom. Weekly assignments. Exams. Lectures to watch.\n\n2. Maybe an elective or two.\n\n3. Yes.\n\n4. R, SAS\n\n5. Traditional statistics with some data science electives. Mix of theory and practice. Mostly pre re-sorted data.\n\n6. ML was only for PhD students when I went. There are two courses that most people would call ML.', 'I graduated from the program in December 2022. I was working full time while attending and finished in 2.5 years (started Summer 2020). I was able to move from Engineering into a DS role as I was finishing up.\n\n1.\tI personally did not interact with the professors all that much since I did not attend many live sessions due to my schedule. They were definitely available if I would have needed them though.\n2.\tHighly depends on the classes you take. I think I ultimately had 4 classes that I would consider project based, 2 of which were data science focused. One building a prediction model for NBA games, the other trying to project the remaining useful life of rechargeable batteries.\n3.\tThis was the one thing I missed when compared to my undergrad. Everybody I worked with in group projects worked really hard, but I never got to really know them or get to be friends with them. Just hard when everybody is living in different states and you only meet for an hour per week. Like others have mentioned, there is a fairly active Discord server that has different channels for each class.\n4.\tR is the heavy use, although if you go down the biostat route you will take classes with SAS from what I heard.\n5.\tFrom what I could tell, based on new courses that were starting to be offered as I was finishing up, they are actively trying to grow the applied course list. Outside of the projects, almost all of the data provided was pre-cleaned so that the actual methods could be practiced. \n6.\tI took two ML specific courses as electives. They were essentially two sides of the same coin where one was the theoretical backbone of the methods and the other was the application. They were both taught by the same professor,  Dr. Homrighausen, who is fantastic teacher. Other than that, the only other spot ML really showed up (outside of Linear Regression and the Penalized versions) was in my final project class.', ""This is so helpful, really appreciate your reply. I'll for sure Dm you with follow ups if I think of any.""]",15,16,https://www.reddit.com/r/statistics/comments/11ge13b/e_current_or_recent_tamu_ms_stats_online_students/
866,2023-03-03 03:52:18,[Q] Will a month+ vacation after graduation hurt my chances of finding work later?,"Sorry to post this here, i just want insight from people in the field that ill be going into.

My parents really wants to take me back to their native country for vacation after graduation. It sounds exciting for me, but iv been really worried that i wont find a great job after graduation, so i really dont want to hurt my chances further by taking alot of time off. I figured maybe it might be better to work for a couple years and save some of my vacation days to do that instead.

Im getting a bachelors in Statistics and minors in Economics and applied math, with average grades. That sounds cool to an employer, but personally i feel holey unprepared. Im worried a vacation might make me feel that even more so.

There is a job fair happening at my uni soon, im not sure how to let them know that i wont be immediately ready for work after graduation. I wonder if just mentioning that i want to chillax a bit will make me undesirable. I graduate this spring.","[""You're about to finish four years of investing in your education/career. Give yourself a month off to invest in your happiness. The jobs will still be there when you get back, but you can apply while you're on vacation as well if you want. For most positions, employers expect to wait at least two weeks after you accept an offer so you can wrap up at your previous position. They may not expect you to have a job that you're leaving since you're coming out of college, but different schools also graduate at different times in the season, so similar effect."", ""At least in the United States, it's very common for new graduate jobs to start a few months after most students graduate."", 'Yeah, vacation, sure…\nYall are getting hired within a month as a new grad???', 'This. I’m 47. Take the time off, if you don’t you will regret it later.', 'It won’t make a bit of difference to getting a job. As long as money’s ok for a few months, enjoy your life!']",0,11,https://www.reddit.com/r/statistics/comments/11gc3yo/q_will_a_month_vacation_after_graduation_hurt_my/
867,2023-03-03 01:57:15,[Q] Single Response Significance Testing,"I'm embarrassed to post this but I've been researching for quite a bit and cannot figure out the answer.  For context, I'm a marketing professional who knows enough about statistics to be dangerous, but would be considered intermediate at best in the real world. 

Let's say I have a simple single-response question: ""Which is your favorite ice cream flavor?"" People can only pick one answer, and the options are Chocolate, Vanilla, Strawberry and Something Else. 

I survey 100 people with this single question, and end up with 50% choosing Chocolate, 40% choosing Vanilla, 10% choosing Strawberry and 0% choosing other. 

What I'm trying to learn is whether the difference between Chocolate (50%) and Vanilla (40%) is statistically significant at 95% confidence based on my n=100. For the purposes of this question, I have no idea what the actual distribution of this answer is among the general population - hence why I'm surveying. 

Weirdly, I know how to do this if the question is structured as a multiple response: ""Which ice cream flavors do you like? Check all that apply."" In that case, it's my understanding I can conduct an independent samples T-test. It feels like it's plausible that I could do the same thing here. 

TL:DR - Within a single response question, what test do I run to tell whether the difference between the % who chose A and the % who chose B is statistically significant?","['Keyphrase: categorical outcome', ""You'd normally do a proportions test, which is easiest done by conditioning on choosing only one of those two options; of those that expressed one of the two preferences you're comparing, compare the proportion of one of them (say chocolate) to 1/2. \n\nThis can be done as a z test or a chi-squared test, or (more exactly) as a binomial test. It could even be done as a t-test.\n\n\nThere's also the problem that this sort of data doesn't tell you about Chocolate/Vanilla preference among people who ranked strawberry first (so if you're trying to say that people prefer chocolate to vanilla, you may be misled; if you add the preference between the two options of the people that ranked them 2 and 3, you may well flip things). (This is the difference between a first past the post election and ranked choice type voting; if you want to say which of two options would be preferred, a first past the post election won't give it to you)\n\n[Beware also of this smaller paradox: the mere *presence* of a third choice may alter some people's preference between two options. (e.g., presented with only two options, someone may be unable to pick a preference and essentially choose at random, and say 'chocolate', but merely adding the option of 'strawberry' may resolve their chocolate/vanilla dilemma in favor of vanilla; this is not a mere hypothetical, it sometimes happens).]\n\n> In that case, it's my understanding I can conduct an independent samples T-test. \n\nWell, not quite; (1) the responses are counts; (ii) a subjects choices in the multichoice are dependent (dependent on subject), and (iii) that data wouldn't tell you about preference between options that are both selected or both unselected."", "">Well, not quite; (1) the responses are counts; (ii) a subjects choices in the multichoice are dependent (dependent on subject), and (iii) that data wouldn't tell you about preference between options that are both selected or both unselected.\n\nThank you very much for your thoughtful reply. I really appreciate it!\n\nAnd you're definitely right on with the callouts on the question constructs. Obviously if the goal is to get to the truth the answer is far more nuanced.""]",2,3,https://www.reddit.com/r/statistics/comments/11g97ud/q_single_response_significance_testing/
868,2023-03-03 01:55:47,"MS program with no comprehensive exams, what’s the catch? [Q]","I have gotten into two MS programs in statistics which are both funded. Wake forest, and Miami (OH). The Miami of Ohio program requires a comprehensive exam and a thesis. The wake forest program has a choice of thesis or no thesis, and has no comprehensive exams at the end of year 2. 

Right now I’m leaning towards wake forest solely because they have no comprehensive exams. However, I wonder if there’s a catch to this. Do you think they just make the actual course curriculum way harder? Is it a red flag of sorts if a MS stats program doesn’t require comprehensive exams?  These are both not online by the way.

EDIT: These are both *not* online. I made a typo","[""It's whatever. The thesis/no thesis distinction is more important. Nobody ever asks how your comps went."", ""It varies. The thesis itself may not, esp outside of academia. I work in government and have a PhD thesis - ain't nobody want to see it. The process of writing a thesis helps no matter what you do. It's really more about having had research experience than the writing itself, and while research can happen without a thesis it's probably not as long or defined as the research that goes into writing a thesis. \n\nYou don't need a MS with thesis to get into PhD programs. Undergrads get accepted into PhD programs. Remember that all experience is helpful experience (an MS thesis will make a PhD thesis feel more approachable) but there is a time cost associated with collecting more experience, so you have to decide what is best for you."", 'Our program just dropped the comprehensive exam. Why? Cause it’s dumb. We all know a single massive exam is not a good measure of your real understanding of the content. It’s an example of your ability to memorize.', ""Not really needed for normal industry jobs, but if you are pursuing an academic career (research, PhD, etc) then you'll need a thesis"", 'The Miami of OH program allows you to skip the exam if you get a high enough GPA in the classes it covers.\n\nMy thesis got a publication out of it which was nice.']",10,19,https://www.reddit.com/r/statistics/comments/11g96jh/ms_program_with_no_comprehensive_exams_whats_the/
869,2023-03-02 23:06:24,[Q] Help me understand Two Way Anova,"Hey!

So i need to understand how apply a Two Way Anova. Is my setup (see below) totally effed?  
The data i have is:   
10 patients with Measurement of 5 different brain areas  (Dependent variable?).   
During Fasting/Fed states (Independent variable?)  
And then again after X intervention i have measurements of the same brain regions during the fasting and fed states.

Set up;  
I was thinking: ""Fed/Fasting state"" on the 2 rows. 5 diffrent brain areas in 5 subcolumns (matched across row) and the columns representing before and after.   


I feel really lost. first day using statistics software. wanna bash my head in so help is much appreciated :)  Im using Graphpad Prism. to do a Two-way Anova to get the interaction effect.","['Guessing from the information you provided, you would want to do a two-way repeated measures MANOVA. You have multiple dependent variables, so you need to do a multivariate analysis. I think nobody here will be able to guide you to how to do that with your software and dataset. Your supervisor/tutor should be helping you there', 'Well, people study years to use stats, so no surprise you are struggling. By the way, you have measures in the same patient, violating independence assumption of anova so this is not the appropriate model for you anyway. Seek support of a statistician irl.', '>10 patients with Measurement of 5 different brain areas\n\nI mean this. You have 5 measures in the same patient. Those measures are not independent. They are nested.', 'So would this work:   \n2 main columns for before&after (Independent variable = time).  \n5 Rows for brain areas (Brain activity = Dependent variable).  \n2 matched subcolumns for fed/fasted (so that same brain area during the same state can be compared before and after intervention. Independent variable = Fasted/fed).  \n\n\n?', '>you have measures in the same patient\n\nWhat do you mean? i said i have measures from 10 patients..']",0,6,https://www.reddit.com/r/statistics/comments/11g4zna/q_help_me_understand_two_way_anova/
870,2023-03-02 22:20:41,"[Q] Layperson here. Is it possible to run a multiple regression when the independent variables are ""unequal""? Not sure how to explain this properly"," [https://imgur.com/a/kWMSzch](https://imgur.com/a/kWMSzch)

This is representative of the content set I am working with. With each variable, Leads is my dependent variable but as you can see the number of categories in the independent variables is different from each other (which is what I meant by ""unequal"").

Can I run a multiple regression on these three variables? I want to see if all three variables taken together are descriptive of Leads.

If I am missing any information, then I can provide more detail as needed.

Thanks all.","['Yes, you can run a multiple linear regression!', 'Do you have leads broken down by each *combination* of your three variables?', 'This *table* is not very useful for (joint) statistical analysis (unless you want to think of these as multi view data, but you just don’t have enough columns to get a lot out of that), but if you have the full data row-wise in a spreadsheet elsewhere, where each customer has their associated Tier, State, Age, and Lead indicator in a row, then you can perform logistic regression.']",1,3,https://www.reddit.com/r/statistics/comments/11g3y68/q_layperson_here_is_it_possible_to_run_a_multiple/
871,2023-03-02 20:04:47,[Q] Causal inference with dynamic treatment?,"Hi guys,

I'm studying causal inference with dowhy package. I'm reading the example of membership reward program [\[Link\]](https://www.pywhy.org/dowhy/v0.5.1/example_notebooks/dowhy_example_effect_of_memberrewards_program.html). The notebook takes the example of signup month i = 3

But, what will happen if everyone has different signup months? Do we run the model independently for every different signup month? Or we can have a dynamic model with dynamic signup month?","['Tbh i would just control for fixed effects, but that is what makes me applied and not a statistician', 'Remind me', 'The outcome is time-dependent so as long as you don\'t think that the specific eg season is a confounder you can use only one lagged model. This is not a ""Dynamical treatment"" as we refert to DT as treatment varying in a time manner for example dose escalation of a drugs for the same patient.', 'wdym by control for fixed effects?', 'Can you elaborate more about lagged model? Why it works well in this case?']",5,10,https://www.reddit.com/r/statistics/comments/11g13ua/q_causal_inference_with_dynamic_treatment/
872,2023-03-02 18:55:02,"[Q] How are flux, divergence, and curl of vector fields used in statistics?",,"[""I never needed those concepts in statistics. They are useful to know(ex: modelling systems behavior) , but if you focus on stats only, it won't be useful.\n\nThere might be some niche exceptions, but that's what they are. Exceptions."", 'Those objects have a more physical and geometrical interpretation that isn’t used much in statistics. The differential objects you encounter in statistics are mostly things like the gradient, Jacobian matrix and the Hessian matrix.', 'I have never seen them applied in Statistics.', 'The generalizations of these concepts (exterior derivative and differential forms) show up in some areas of probability theory. But this is mathematical statistics / theoretical statistics and not something you would use in applied stats. \n\nhttps://en.m.wikipedia.org/wiki/Information_geometry', 'This answer looks like it was generared by chatGPT.']",43,14,https://www.reddit.com/r/statistics/comments/11fzw9p/q_how_are_flux_divergence_and_curl_of_vector/
873,2023-03-02 18:24:52,"[Q] Assumptions for MANOVA failed, what are my options?","I want to conduct a one-way MANOVA with many DVs (around 9) and one fixed factor as IV that consists of six groups. Since I first have to check for many different assumptions, this is where I am having trouble.

First I removed outliers, then I checked the normality and almost half of them showed deviation from normality based on Shapiro-wilk and/or Kolmogorov test. By log-transforming the data I still have a couple of non-normal variables.

If I were to ignore it, there still are quite a few other problematic results:

1. Correlations – some of them are in the range (0.2 – 0.9), while some are very low.
2. Box's M is < 0.001 so this test also failed
3. Levene's test of equality of variances has also showed that half of them are significant, while the other half is not

My question based on all of this is what are my options? Firstly, can I ignore that normality test and just say that we looked at Q-Q plot which does seem ok.

Regarding correlations, can I maybe separate DVs into two MANOVAs where I only put those which do show correlation?

What can I do about Box's M and Levene's test?","['MANOVA is obsolete. SEM has taken its place. See e.g., Huang, F. L. (2020). MANOVA: A procedure whose time has passed?. Gifted Child Quarterly, 64(1), 56-60.', 'Or https://quantitudepod.org/s2e09-manova-must-die/', 'If you can formulate a null hypothesis you should be able to cook up a monte carlo resampling test']",4,4,https://www.reddit.com/r/statistics/comments/11fzey8/q_assumptions_for_manova_failed_what_are_my/
874,2023-03-02 18:22:47,[Q] What tests/parameters describe the variances within differently distributed sample groups?,"For example, I have data on how tired people are when they wake up in the morning. Each participant logged their level of tiredness on a scale from 0-100 each morning over a few weeks, and ultimately I will look into determinants recorded.

However, initially I just want to examine the variance between each individual's readings for tiredness each morning to see the general variation for each participant. For an individual obviously the range and difference in range, mean and median are useful, but median and mean won't effectively look at the overall population as each person will have a different 'baseline' for general tiredness e.g. someone might regularly get 8 hours and wake up feeling awake each morning, so would generally be around 90 each day, whereas someone might get less sleep and struggle with insomnia so would have readings closer to 10 each day.

Standard deviation and coefficient of variation seem logical, but otherwise I'm unsure what to look into. I'm not sure ANOVA is relevant as I'm not interested in the difference between different individual's data.

I am hoping to determine whether imputation can be used to replace missing values, so ideally am hoping differences are not significant.",[],2,0,https://www.reddit.com/r/statistics/comments/11fzdtt/q_what_testsparameters_describe_the_variances/
875,2023-03-02 17:26:47,[E] The Brier Score Explained,"Hi guys,

I have made a video [here](https://youtu.be/BiaebXlgfNQ) where I explain what the Brier Score is and how it is computed

I hope it may be of use to some of you out there. Feedback is more than welcomed! :)","['""Hi, I\'m spamming my videos across dozens of likely and unlikely subreddits. There\'s no downside, so who cares! Any sockpuppets want to give me a hand, I\'ll scratch your back if you scratch mine.""', 'That was pretty brutal, but I guess I deserve some of that critique. Thanks!', 'Thank you!', ' Following!']",2,4,https://www.reddit.com/r/statistics/comments/11fyg3x/e_the_brier_score_explained/
876,2023-03-02 16:39:15,[Q] Is it worth making the jump from spss to stata?,"I am currently working on several databases. However my pi plans to add more data years that will expand the record to around 100 millions entries with around 200 variables.

I am currently handling 30m via spss. Is it better to shift to stata?","[""Why specifically are you considering Stata? It's what i was initially taught, i would not recommend it to anyone over R today for my area."", 'R is free, constantly updated, and can read spss and stata files.', ""Always shift from SPSS to something, anything else.  \n  \nStata is proprietary, but if it's common in your field, then probably has useful libraries. Otherwise, R."", 'SPSS < Literally doing stats by hand < Stata <<< R', 'Yes.\nIt’s better to go with R. \nCause free.']",12,17,https://www.reddit.com/r/statistics/comments/11fxnln/q_is_it_worth_making_the_jump_from_spss_to_stata/
877,2023-03-02 14:55:50,[Q] Mean vs median,"Hi guys, I have a question.

If the mean GPA of a sample is 3.5, and the median of that sample is 3.6. If a student is randomly chosen from that sample, what would his expected GPA be?

Just made up this question out of curiosity. I'm inclined toward the mean, but I'm not sure how the median would be any worse.

Thanks!","['The ""expectation"", as is defined in probability and statistics, is equal to the mean. \n\nIt can be confusing terminology for some distributions, including skewed ones like in your example. The ""expectation"" is not necessarily the value you\'d expect to get if you randomly sampled. Another example: the expected value of a dice roll is 3.5.', ""Often statistics can be very fuzzy for a number of reasons, usually because people are not very explicit in their terminology. You yourself asked for expected value, when you wanted something quite different.\n\nIf I had to predict what the value would be, I'd take the mode. That maximises my probability of being correct in my prediction.\n\nIf I was asked to predict what the value would be, and was awarded points for being close to the correct value in a linear fashion, I'd do as the book author suggests and take the median. The reason for this is that we want to give the value, x, which minimises the mean absolute difference between itself and the samples.\n\nIe. we want x such that the sum(| GPA_i - x |) is minimised, where GPA_i labels the GPA of student i. This turns out to be the median.\n\nThe mean, on the other hand, minimises the square difference."", 'If you actually mean to ask about expected value, simply look at the definition of expected value\n\nhttps://en.wikipedia.org/wiki/Expected_value', ""I think the author was trying to explain that the median is a robust statistics but the mean it's not being very sensitive to outliers."", 'The decision to use the median very much depends on ""in a linear fashion"". The median minimises the sum of the distances to the data points. The mean minimises the sum of the squared distances to the data points. These statements can be proved with basic calculus.']",1,11,https://www.reddit.com/r/statistics/comments/11fvt40/q_mean_vs_median/
878,2023-03-02 09:16:29,[Career] What is needed for a career as an environmental statistician?,I have a BS in applied statistics and am looking to pursue a master’s degree in biostatistics. Are environmental statistics considered public health? Would an MS in biostatistics help me towards a career as an environmental statistician or should I look into MS programs in environmental data science/applied statistics? Are environmental statisticians just statisticians who work in environmental contexts?,"['A stats MS program might be better. There are some spatial people I know in biostats who have done research on air pollution, but most environmental stuff happens in stats departments.', ""I think it's a mildly unusual career path for someone who has studied *only* statistics; a lot of environmental types will have done their undergrad in geology or biology, then switched to statistics (or, perhaps, done the opposite, done undergrad in statistics and then grad in some specialized area like hydrology.)\n\nIt won't be impossible if you have the science background to understand the type of environmental statistics you're doing. But it may be hard to get an interview if they don't see anything environment related on the transcript or resume."", ""My friend works for the US govt and counts wildflowers and writes papers.\n\nI can't remember which department, but it's not public health."", 'I was a military statistician with just a BS in biostats.', 'Biostats, public health, & epidemiology are human-based; environmental programs are much more varied and have different concentrations to choose from depending on the university. Env programs I’ve encountered cover many subjects including statistics, because it does help dramatically to have the scientific background when choosing a statistical setup. Env fate & transport (in air, surface water, or groundwater), toxicology, chemistry, ecology, risk, treatment, econ, policy… all possibilities to choose from. Env stats needs more professionals! Statistical methods are transferable from human-focused fields, though they might not be as varied. Also see [this other relevant post](https://www.reddit.com/r/statistics/comments/x17778/q_environmental_careers_in_statistics/?utm_source=share&utm_medium=ios_app&utm_name=iossmf).']",16,8,https://www.reddit.com/r/statistics/comments/11fofz7/career_what_is_needed_for_a_career_as_an/
879,2023-03-02 05:59:57,[Q] Which Statistical Test to Use?,"I'm working on a project currently with both the independent and dependent variables being binary. For example, the data compares sex as the independent variable (male or female) vs existence of symptoms of \[disease\] (yes or no). Which statistical test would be best to use for this case?","['Chi-square test, most likely.', 'If you want an OR, you can use a logistic regression', 'Depends on your hypothesis. If you’re trying to see if there’s any significant association between gender and existence of a disease, chi square would be your best bet. If you’re trying to see if gender is a potential predictor of said disease, dichotomous logistic regression would be an option', ""Look up Chi square test of independence and Fisher's exact test (better for small n)."", 'Chi square test']",9,16,https://www.reddit.com/r/statistics/comments/11fji30/q_which_statistical_test_to_use/
880,2023-03-02 04:28:04,Question about multiple comparisons [R],"Hi, I’m stumped on how to analyze some data, and I suspect that others in my field may be incorrectly correcting for multiple comparisons. I’ll try and briefly describe the situation below: 

We record 21 spatially separated channels of brain activity while presenting a stimulus a few hundred times. We then use a circular statistic, the Rayleigh test, to say whether the distribution of phases (one phase value for each time the stimulus was presented) of the brain activity, in response to the stimulus, is significantly different than a hypothetical random phase distribution. So we end up with a lot of p-values from lots of Rayleigh tests (21 channels of brain activity). 

What, if any, corrections should be made for multiple comparisons?

I’ve tried to read up on this, and have been doing stats for years, but it’s just not clear to me.","[""Yes, you should correct for mc. The EEG (I presume) channels are typically  correlated though (as sources project to many channels), making bonferroni and friends too conservative. I'm much better versed in the literature for time resolved analyses, but for phase you probably could just use fdr. \n\nWhat exactly do you think people misunderstood in the field?"", 'Thanks for the input. FDR looks pretty good. The misunderstanding I’m referring to is multiple people recommending that I correct for the number of stimulus presentations (e.g alpha/#trials) which seems incorrect as those aren’t hypothesis tests.', ""Indeed, that does not make any sense. You correct for the number of tests.\n\n From a theory side, the troendle mc would be a little bit more powerful than FDR, it works only if you use permutationtests, which you do. It is implemented in the permuco package in R - but I haven't seen anyone use it in your use case, might be harder to publish"", 'Interesting. I’ll look into it, thanks for your help.']",3,4,https://www.reddit.com/r/statistics/comments/11fgz1p/question_about_multiple_comparisons_r/
881,2023-03-02 01:04:35,[Q] how to calculate interclass correlation,hello all! i love using the psych package in R to get the ICC which i understand to be intraclass correlation. is there a way to also extract the inter? or another package that gives both? ty!,"['You use the same formula for calculating both types.  If the values you estimate your ICC on comes from the same rater -> intrarater.  If they come from different raters -> interrater reliability. \n\nBoth types are intraclass correlations.', ""I think that's just your between-groups correlation.\n\nThe statsBy function might do that for you."", 'this blew my mind and solved my problem, thank you so much!', ""I'm happy it helped you. You might want to read this paper. I think its one of the best on the subject.\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC4913118/"", 'oh so literally like a pearsons! that makes sense']",5,6,https://www.reddit.com/r/statistics/comments/11f92xe/q_how_to_calculate_interclass_correlation/
882,2023-03-01 23:36:21,[Q] Help calculating the Pre-post Δ (Standard deviation),"Hey all, 

I'm working on a meta-analysis and I'm kind of stuck. I'm trying to calculate the delta of a given group's pre- and post-intervention standard deviations. Can someone please help out? Very much appreciated!

For example: 

Preintervention 22.29 (8.39)

Postintervention 19.29 (8.21)

Pre-post delta -3.00 (4.73)

How does one calculate and arrive at 4.73?","['Assuming 4.73 is the standard deviation of the change score, you can\'t calculate it without having the correlation coefficient between pre and post. The formula is shown [here](https://training.cochrane.org/handbook/current/chapter-06#section-6-5-2-8) (scroll down to the subsection ""Imputing a change-from-baseline standard deviation using a correlation coefficient"").\n\nTo summarize: In order to calculate the SD of the change, you need the SD from pre and post as well as the correlation coefficient between pre and post.', 'Thank you so much! I missed that sub-chapter']",3,2,https://www.reddit.com/r/statistics/comments/11f6u3y/q_help_calculating_the_prepost_δ_standard/
883,2023-03-01 19:35:52,"[Q] G*Power - Difference between ANOVA Repeated measures, between factors and ANOVA: Repeated measures, within-between interaction","Hi,

I have a mixed-ANOVA design: 2 groups (experimental & control) x 4 time points (pre, post, post2, post3), and am using G\*Power to calculate the required sample size.

My understanding is that the ""ANOVA Repeated measures, between factors"" should be used if I am interested in the sample size required to detect the main effect of the group factor, and that the ""ANOVA Repeated measures, within-between interaction"" should be used if I am interested in the sample size required to detect the interaction effect (group\*time).

However, the sample size required when I used ""ANOVA Repeated measures, within-between interaction"" is lower than the sample size required when using ""ANOVA Repeated measures, between factors"" in G\*Power, which is weird as we generally require larger sample size to detect an interaction effect.

Am I missing something here?","[""G\\*Power is working properly.\n\nCheck out pages 18 and 19 of the paper (which i copied the following from) [https://www.journalofcognition.org/articles/10.5334/joc.72/](https://www.journalofcognition.org/articles/10.5334/joc.72/):\n\nG\\*Power's recommended sample size is only valid under the following conditions:\n\n1. the interaction effect is as big as the main effect (i.e., the equivalent of a fully crossed interaction), \n2. you compare designs in which you only have one observation per condition (so, more levels of the repeated measure gives you more information), and \n3. you are only interested in the overall interaction effect of the omnibus test, not whether the pattern of pairwise differences agrees with what is in the population."", 'Thanks so much, this helps a lot!', 'I don’t know but I hope G*Power’s calculation includes an epsilon correction such as Huynh-Feldt? If a correction for the almost-certain violation of sphericity is not used then the Type I error will be inflated.']",4,3,https://www.reddit.com/r/statistics/comments/11f1ifo/q_gpower_difference_between_anova_repeated/
884,2023-03-01 19:02:08,[Q] Is it bad if my two-way ANOVA between subjects factors aren't even?,"I have triple-checked my assignments for the groups and there's no way they are wrong, and yet I end up with 32/30 and 32/30 in my IV groups for between-subjects factors. I see every example with even numbers in each so I'm wondering if this is a problem","[""You're probably fine. Those are pretty similar group sizes and ANOVA is robust to minor violations like this."", 'ANOVA does not assume equal sample sizes although power is greatest with equal sample sizes. The examples you see probably have equal n because the calculations are much more complex with unequal n and unequal n requires one to choose the type of sum of squares you are using.']",3,2,https://www.reddit.com/r/statistics/comments/11f0wot/q_is_it_bad_if_my_twoway_anova_between_subjects/
885,2023-03-01 11:05:13,[Q] Basic question about confidence intervals,"Hey everyone! I'm a PhD student that is
INCREDIBLY bad at stats (yeah, this should go well), and I have a question about confidence interval. Since my undergrad, I’ve always been taught that a confidence interval that includes 1 is considered to be not significant, but in my current course (and I believe the course I took in prep for this one), my professors have said that the data suggests significance unless the CI ""crosses"" 0. I was even having a conversation with one of my attendings the other day about a study we reviewed and I commented that the data was significant because the Cl didn't include 0 and she corrected me to say ""doesn't include 1"". So now l'm all sorts of confused. We're using SPSS for this particular example, and it's even suggesting significance with a Cl of 0.51-21.16 with a p=.04. So, which one is it?","['It depends what is the parameter of interest and what is the null hypothesis. Usually if you’re interested in a mean difference, your hypothesis test will specify the null: mean difference = 0 and so if your CI does not include 0, your result is significant. But other times you might be looking at odds ratios where the null is instead that OR = 1 (because OR = 1 implies equivalent odds between the two groups in question and is thus a natural choice for the null).  In this case, your results are significant if your CI does not cross 1.', 'Others have resolved your confusion, I\'m just here to add a more general tip.\n\nYou can\'t learn statistical methods by rote, you need to understand the underlying ideas. It can feel like a scary subject but it\'s just ""advanced common sense"" with a little bit of maths needed to express some of the ideas precisely enough to be useful.\n\nHere, we\'re interested in whether the confidence interval is consistent with the null hypothesis. A 95% CI which does not contain the null value is (typically) deemed inconsistent with the null hypothesis (and will usually also have a p-value <0.05, as they\'re usually both computed from equivalent methods). The null value is usually 0 for a difference in means, or 1 for a ratio statistic.\n\nWhile I\'m here, it would be remiss of me not to suggest you also try to get to grips with what a p-value actually is (and isn\'t) and why the p<0.05 standard is inadequate in many (most) contexts. This is a very useful explainer which might also give you some ideas on how to turn the often rather abstract ideas you will be taught into concrete examples you can understand and therefore remember and apply properly: [The p value and the base rate fallacy](https://www.statisticsdonewrong.com/p-value.html).', 'Are you testing an odds ratio or an absolute effect?', 'A CI of a ratio would be not significant if it included 1. This is because if a/b ~ 1 then a must be similar and n value to b.  \n  \nSimilarly, a CI of an absolute effect or a difference would not be significant if it included 0. This is because if a - b ~ 0 then a must be similar in value to b.', ""If you're calling something *significant* you're doing a hypothesis test. In that case the value you look at as causing you to reject H0 when it's outside the interval *depends on the hypothesis*.\n\nIn some situations that might be 1 in other situations it might be 0. In still other circumstances it will be something else.\n\nWe can't tell your what your hypothesis is, and you have not given us enough information to guess \n\n(I will, say, however, that if you're 'bad at stats', don't use CIs to do tests, since it does often happen that they cause confusion/errors when trying to carry out tests.)""]",37,6,https://www.reddit.com/r/statistics/comments/11esfz7/q_basic_question_about_confidence_intervals/
886,2023-03-01 10:36:16,[Q] Looking for a bit of guidance about statistics and bets," Hi, I know the title might appear not that serious, but i am looking for a legit response.

What statistical subjects should I master in order to bet with a controlled risk?

What would be the learning roadmap for this?

Is there a specific programming language I should learn to develop models?

I apologize if the post looks out of place but again, I am serious about this, thank you for your time.","['>What statistical subjects should I master in order to bet with a controlled risk?\n\nDecision theory (expected utility, the ""Kelly criterion"") etc tells you how to make decisions under uncertainty in an optimal way. Optimal means there is a clearly defined objective. You need to reflect on whether popular objectives are in line with your goals though. Maximising wealth growth isn\'t going to be the same as minimising the risk of a bad loss.\n\nApplying these theories at minimum requires assessments of what events might happen, and usually assessments of probabilities that they happen. If you feed bad assessments of how likely something is into your expert decision model, you get back a bad decision. Where do you get good assessments from when making bets? You need to have as much information that helps you predict events as possible. And you need to turn these into probabilities. Regression models are the subject to learn for this. The world\'s best model used on easily available public data will be less useful than someone\'s shrewd guess based on private, insider info.\n\nOnce you have all that, you need to take into account fees when making bets and other constraints (eg bet size with a bookmaker, or not getting to bet if you are too successful). You should be able to bet on the best odds available for an event to maximise your return, taking these constraints into account. \n\nIf you appropriately take that into account, you can think about whether you\'re doing any of this much better than other bettors. If no, it is not likely you\'ll be a consistent winner. Maybe buying a government bond yields a better risk adjusted expected return for you.', 'I would first get comfortable finding and manipulating data. This can often be the most time consuming. \nAfter that basic probability, including conditional probability and Bayes law. \nI use R predominately and it works great but there are many that prefer Python for languages. \n\nIm not suggesting that is your best end game, but it would be a great place to start.']",1,2,https://www.reddit.com/r/statistics/comments/11ert31/q_looking_for_a_bit_of_guidance_about_statistics/
887,2023-03-01 08:50:54,[Q] Difference between Masters of Statistics and Masters of Science in Statistics?,"Apologies in advance if this is a repetitive question - I've tried looking for the answer and can't seem to find it.

What's the difference in degree with programs that are called ""Master of Statistics"" versus ""Masters of Science in Statistics""?

I've heard mentioned in some places that Master of Statistics is considered a ""lesser"" degree than a MS, but I don't have concrete reasons why. Can anyone help me understand the difference (if there is one)?

Thank you!","['These types of distinctions make no difference at all. \n\nMasters is a masters. People who care are likely gatekeeping.', 'Where I study, the MS Stat degree is more in-depth than the Master of Stat degree.\n\nFor example, the Master of Stat degree includes courses like: introductory probability, introduction to statistical inference, applied regression analysis, and applied multivariate analysis. Note the use to introductory and applied.\n\nWhile in the MS degree, courses are: probability theory, parametric inference, linear models, multivariate analysis.\n\nMS stat also requires a masters thesis to complete, while master of stat requires a simpler research paper.', 'At least in the US, I thought they were all masters of science in statistics at accredited schools', 'Nah, it varies quite a bit school to school. You can get a Master of Arts in stats from Boston University, for example.', 'can you link those degrees? we can look at the course requirements and stuff and can prolly answer better']",2,7,https://www.reddit.com/r/statistics/comments/11eph0z/q_difference_between_masters_of_statistics_and/
888,2023-03-01 06:22:41,[Q] Please explain alpha equivalence like I'm 5,"Have recently covered transformations to stabilize the variance in SLR. The transformations are all stated such as ""If var(ei) is alpha equivalent to Xi\^2..."". I understand that transformations are necessary in some instances to correct a model, but what does alpha equivalence entail? I have tried looking up relevant articles/videos but none of their explanations are really making much sense to me. Something about the bounding of named variables? Thanks in advance - just want to be able to wrap my mind around it.",[],2,0,https://www.reddit.com/r/statistics/comments/11elwpo/q_please_explain_alpha_equivalence_like_im_5/
889,2023-03-01 05:48:47,[Q] What classes are the general prerequisites for Stats/Data Science Masters/PhD programs?,"I have heard generally about probability, linear algebra, calc, and real analysis. Is there anything I should add to this list? Also, what kind of math classes should I focus on that would be the most helpful for grad school? I am doing okay in real analysis, but it is definitely difficult and I am not sure if I want to keep taking a lot more proof based classes since I don't need too many more to graduate.","['it varies by grad programs... take a look at the programs you want to apply to and see what they require for admission.\n\nOne gap you should look at is programming. For DS, you would want to pick up phyton for example.. while Stats, perhaps R or SAS or STATA...', 'Calc I-III and Linear Algebra is pretty much the bare minimum, other programs may want more - PhD programs will value the Real Analysis course work', 'A big one missed here: Probability.', 'Master: General Cal (usually offered for general life science), Linear Alg, statistical theory\n\nPh.D.: Mathematical route Cal (up to multivariate), Linear Alg (intro + advanced), Real Analysis, Discrete mathematics, Probability + Statistical Inference, computational statistics, statistical methodologies (regression, anova, time series analysis, GLM, etc.),  capstone project, and REU.\n\n**This is the US standard**. However, the requirement can be less stringent in the EU.', 'Struggling with your math?  Please check your chats once I have shared some valuable information that can help .']",0,7,https://www.reddit.com/r/statistics/comments/11el10u/q_what_classes_are_the_general_prerequisites_for/
890,2023-03-01 05:21:42,[Question] Sample sizes are not the same,"I am working on an app and trying to understand how many people need to make it through my feature in order to feel confident in the conversion data. My constraints are:

95% Confidence Level  
20% Margin of error  
Using 50% for population cause i'm not sure  
Unlimited population

This tool says i need a sample size of 25:  
[https://www.calculator.net/sample-size-calculator.html?type=1&cl=95&ci=20&pp=50&ps=&x=70&y=18](https://www.calculator.net/sample-size-calculator.html?type=1&cl=95&ci=20&pp=50&ps=&x=70&y=18)

This one says i need a sample size of 2900:  
[https://www.optimizely.com/sample-size-calculator/#/?conversion=10&effect=20&significance=95](https://www.optimizely.com/sample-size-calculator/#/?conversion=10&effect=20&significance=95)

They are not exactly the same calculator since one takes into account baseline conversion, but i am not sure i understand why the sample sizes are 100x different.

What is causing this discrepancy?","['There are two big differences between these two calculators.\n\nOne is that the first calculator is aimed at the case where you collect a single sample and compare it to a fixed target. It told you that, if the true rate is 50%, a sample of size 25 is sufficient to ensure that your sample will usually give you an estimate between 30% and 70% (50±20%).\n\nThe second calculator is aimed at A/B testing, where you collect two samples and see if they differ from each other. As a rough rule of thumb, if you need N samples to compare against a fixed target, you\'ll need 2N samples in each of two groups to compare them against each other with the same precision.\n\nThe other difference is your second calculator\'s effect size is *relative* change in conversion rate. It told you that you need two groups of size 2900 to distinguish between a 10% and a 12% conversion rate (p=.12 being 20% more than p=.10.)\n\nYou don\'t say what conversion rate you actually expect; whether your ""20% margin of error"" is relative or absolute; and whether you have a fixed target you are trying to meet, or are comparing your app to another one to see which is better. You\'ll need the answers to those questions to decide which calculator is right for you.', 'Now if you\'d only used one, you\'d have no doubt, right? This is why online calculators are dangerous; to get a number out you only need to know enough to put viable numbers is, not whether it fits your specific use case.\n\n1. Baseline conversion rate is certainly one issue, yes. \n\n2. The first link is for a *confidence interval for a single proportion*. The second is for a test comparing two proportions.\n\n3. The use of the word *relative* in ""minimum relative change in conversion rate"" concerns me. What do they mean by *relative* there? (Clicking the ""?"" was no direct help, since the link is dead and you just end up at a generic looking page). I can make assumptions about what they intend, of course, but I would want to see *exactly* what they think it means.\n\n using the wayback machine ( web.archive.org ) does seem to give some\ninformation, but not all of the page properly renders (it seems to rely on other assets that aren\'t showing up). It\'s probable there\'s an explanation for any remaining difference in there.\n\n---\n\n""Statistical Significance\n95%""\n\nOkay, important tip: Beware any tool that is written by people who *don\'t even understand what a significance level in a hypothesis test is*. They are very, very likely to get less basic things *even more wrong*.']",1,2,https://www.reddit.com/r/statistics/comments/11ekc9a/question_sample_sizes_are_not_the_same/
891,2023-03-01 05:12:25,[Q] Does it make sense that the R-squared on three variables combined is substantially less than the R-squared for each variable individually?,"I am not a statistician and I only have a vague, long-ago understanding of these concepts :)

I built a very simple predictive model based on three variables. I used Excel to calculate the R-squared for each variable and they came out to be 67%, 96%, and 83%. But when I combine all three variables into one ""super-variable"", the R-squared comes out to be 21%. Just wondering if that is an unexpected outcome.

My layperson's point of view is that if each variable is highly predictive then all three combined should be similarly predictive.

EDIT:  To clarify what I meant by ""super variable"", suppose my independent variables were Zip Code, Industry, and # of Employees.  Each of those on its own generated the R2 values above.  My so-called super-variable combined all three together, such as ""90210+Construction+25 employees"".  When I ran the combined variable against my dependent variable (which happens to be sales), then that's where I got the R2 of 21%.

If it matters, because not every combination is represented, there were a lot of zeros.  But when I took them out, the R2 went to 20%","['> I used Excel to calculate the R-squared for each variable and they came out to be 67%, 96%, and 83%.\n\nIt sounds like you were running three separate models:  ""x1 and y"", ""x2 and y"", ""x3 and y""\n\nYour next step should not be to combine the variables into one \'super variable\'.  Your next step should be a *multiple linear regression* where ""x1, x2, x3, and y"" is your model, and you can see all three variables together. \n\n> But when I combine all three variables into one ""super-variable"", the R-squared comes out to be 21%. Just wondering if that is an unexpected outcome.\n\nI think so.  Your formula to create the \'super-variable\' is not working.  If you think that some relationship between the variables might be helpful, then try adding a fourth variable like x4 = x2 * x3.  But you want a good reason for this!\n\nIn general, *adding variables to a model, not combining them* will increase r^2.  Also look at *adjusted* r^2, because if that doesn\'t go up, your added variables are less useful.  \n\n> My layperson\'s point of view is that if each variable is highly predictive then all three combined should be similarly predictive.\n\nNot necessarily.  Depends on the how the variables themselves are related, and then how they are combined.', 'OP summed the independent variables into a single column, and then ran a univariate regression.  Your intuition is sound, but your execution was flawed and resulted in a very noisy single predictor.   \n\nIf only there were a way to find the ""best"" combination of your predictors?   Well the answer to your question (in a particular sense of ""best"") is MULTIPLE LINEAR REGRESSION.  \n\nother replies are confused because they assume OP ran a multiple regression and obtained a lower R^2.', 'Imagine a model y ~ x1 has an r sq of 0.9, and then you have x2 = -x1 + e. The model y ~ x2 may have r sq of 0.8 because of the relationship between x1 and x2. Now we create a super variable called z = x1 + x2. Guess what, the model y ~ z only has r sq of 0.05', 'Is this an *adjusted* R-squared? Is it being calculated on the test set?\n\nEDIT: And what on earth is a ""super-variable""?', 'Define ""supervariable"". Did you add them to your model (y = x1 + x2 +x3) or did you literally add up the values of the three variables into one column and regress that on the dependent variable? Because if its the latter, its no wonder because its wrong.']",12,19,https://www.reddit.com/r/statistics/comments/11ek3do/q_does_it_make_sense_that_the_rsquared_on_three/
892,2023-03-01 04:52:08,[Q] [E] [C] Resources for reading research in HR/People Analytics/IO Psychology,"Can anyone recommended journals or resources for research in the area of IO psychology/people analytics?

I want to better understand how researchers in this field determine ""acceptable""/""significant"" correlations, testing methods, factors in estimating a type II error rate, determining 1-tail vs 2-tail t-tests in practice (I know the difference on paper, but I understand it is often subjective), etc.",['You could start with the Journal of Applied Psychology.'],2,1,https://www.reddit.com/r/statistics/comments/11ejk7i/q_e_c_resources_for_reading_research_in_hrpeople/
893,2023-03-01 03:54:02,[Q] Interpreting negative differential entropy,"It's well-known that differential entropy can be negative - does anyone have any good heuristics for interpreting this case?

For instance, we know that, for a Gaussian RV X:

h(X) = ln(σ\sqrt(2πe))

Obviously, any σ < 1/(\sqrt(2πe) will results in h(X) < 0. Is there significance to this value? It seems like a highly peaked distribution will have negative differential entropy, but it's not obvious why this should be the case, or the interpretation in terms of uncertainties.","['I think the take-away is negative differential entropy on average means your random variable is \'tighter\' than the continuous uniform distribution on \\[0,1\\].\n\na) So differential entropy is actually not strictly what you get when you take the limit of the riemann sums of discrete entropy, it\'s missing a -log(dx) term. Informally, if we take the Riemann sum of discrete entropy, we get lim\\_{|P| --> 0} \\\\Sigma\\_P -p(x\\_i\\*dx)log p(x\\_i\\*dx), which is equal to  \\[\\\\Sigma\\_P -p(x\\_i\\*dx)log p(x\\_i)\\] + \\[\\\\Sigma\\_P -p(x\\_i)\\*log(dx)\\] = \\[differential entropy)\\] - \\[log(dx)\\]. Anyway.\n\nb) one way I think about it is in terms of the pdf of the ""corresponding uniform"" distribution, eg if a random variable has differential entropy H(X), then the uniform distribution with entropy H(X) has constant probability value 2\\^{-H(X)}, so we see that eg a uniform distribution on \\[0, 0.1\\] will have p(x) = 10, and H(X) = -log\\_2 (10). So then any continuous random variable that\'s on average ""tighter"" than \\[0,1\\] has negative differential entropy.\n\nc) another way to intuit it is maybe that the entropy (when it\'s computed using log\\_2) corresponds to the average message size, or number of bits or binary decisions that we need on average to single out or uniquely specify a sample from a distribution. So the discrete uniform distribution on the integers \\[\\[1\\],\\[2\\]\\] has entropy 1, the continuous uniform distribution on the real subset \\[0,2\\] has entropy 1, so one binary decision for both of those. \\*But\\* the discrete uniform with all mass concentrated on a single value has entropy 0, and similar for continuous uniform on real subset \\[0,1\\]. So again, I guess anything tighter than \\[0,1\\] has already been narrowed down to only needing one binary decision and to specify it any further than that gets us negative differential entropy.\n\nI\'m not saying this is super insightful, but maybe different ways to intuit it.']",8,1,https://www.reddit.com/r/statistics/comments/11ei3c3/q_interpreting_negative_differential_entropy/
894,2023-03-01 03:43:18,[Q] What to do when 2/3 Quartiles are the same?," I have a 500K data set that counts customers and their trips into Cabernet Sauvignon. I working to replicate a report I saw that groups your best customers by the quartile of their spend and the quartile of their trips.

The problem I'm running into is when I calculate the quartiles using Excel, I keep getting 1,1,2, ad when I exclude single visits, I get 2,2,4 ...

my gut is telling me to make my quartiles 1,2,4 but I don't want to fudge the data... I'm attempting to make this analysis repeatable, so how do I solve for the duplicate quartiles? Do I leave them in and my resulting scatter plot will only have 6 sections?","[""There is no solution that does all the things you want to do / avoid at the same time. Fixed quantiles don't work so well as distributional  summaries with highly discrete data (like counts with small means)\n\n\nI didn't follow what you meant about 6 sections at the end. What are you doing?"", 'What do you mean by “solving the issue”? If the lower 75% of trips taken was one, then naturally the lower 25 and 50 as well. Just leave out the 25 and 50 then.', '&#x200B;\n\n|3|Potential customers|best customers|best customers|\n|:-|:-|:-|:-|\n|2|Infrequent customers|Potential customers|best customers|\n|1|worst customers|Infrequent customers|Potential customers|\n||1|2|3|\n\nWith the Y-axis as Trips/Visits quartiles, and the X-axis as Spend quartiles, I want to create a distribution of customer IDs', ""I ended up doing 1, 2, and 3+ trip shoppers, but I'm going to revisit if trips is the right measure I'm looking for"", ""That really should go up in your post, for any other person who's trying to understand what you're asking.""]",1,7,https://www.reddit.com/r/statistics/comments/11ehtp9/q_what_to_do_when_23_quartiles_are_the_same/
895,2023-03-01 02:14:26,"[Q] How do you decide when a logistic regression model is ""good""?","Hi, I didn't really cover this in school but it's now coming up in my job... I've looked online without much success, I think I just need someone to explain it to me.

One view I've seen says that basically the only thing that matters is the classification rate on the test data: we use the model to get probabilities, and define a cutoff for what the response should be. Then the model is good if we correctly guess a lot of them right. (In my case the cutoff is just 0.5, and I've been seeing if we correctly guess more than half of them)

That makes sense, and it's easy to follow, but I feel like there should be other things we should be using. 

For example, in R I can use the summary() function on the model and it tells me the residual deviance and the AIC. All I know about these stats is that lower is better, but how much lower?

If it says that the null deviance is 3000 on 2340 degrees of freedom and the residual deviance is 2800 on 2335 degrees of freedom, then is that good?

Are there other stats I should be calculating?","['Most answers provided thus far are from the prediction point of view. ""Goodness of fit"" is assessed through the discrimination capacity of your currently fitted model. There are other Goodness of fit assessments which are not aimed at prediction, rather goodness of fit, and does not require test-validation samples, very alike of the goodness of fit of traditional linear regression model. graphical inspections of residuals and so on.\n\nI think the most straightforward method to assess goodness of fit for these models is by means of the likelihood ratio test. In the traditional approach you compare your model with one that does not have covariates, only an intercept (a NULL model) .\n\ncheck [this](https://stats.stackexchange.com/questions/6505/likelihood-ratio-test-in-r) out.', ""> One view I've seen says that basically the only thing that matters is the classification rate on the test data: we use the model to get probabilities, and define a cutoff for what the response should be. Then the model is good if we correctly guess a lot of them right.\n\nThat is the sort of view that is (in my experience) common among machine learners, but uncommon among statisticians.\n\nWe use the model to get probabilities. The model is good if the outcomes that actually occur are the outcomes the model says are most probable.\n\nBut imposing a cutoff and just counting right and wrong answers is throwing away information. Getting a 51%-chance prediction wrong is a much smaller deal than getting a 99%-chance prediction wrong. \n\nIt is also a bit unfair to judge a model by a criterion it did not seek to optimize. If you fit a model by maximum likelihood, you judge its goodness of fit by some likelihood-related method, not by counting correct predictions. (If you fit models by maximizing the number of correct predictions, sure, judge them on that basis... soon enough you'll soon run into a pathological case that will scare you out of ever using that fitting method again.)\n\nYou aren't going to find a one-size-fits-all formula for what constitutes a good fit. (You won't for linear regression either - you'll just find more standard options.) All of the usual comparison tricks are available, and that is where things like AIC shine. If a simpler model does much worse, but a more complicated model fails to do much better, you have some confidence you've done the best you can do with the data at hand."", "">If it says that the null deviance is 3000 on 2340 degrees of freedom and the residual deviance is 2800 on 2335 degrees of freedom, then is that good?\n\nYou can't use this for binary data goodness of fit. Look up Hosmer-Lemeshow for an alternate test.\n\nAIC is useful for comparing models. In that case, lower is better. Alone it's not useful. \n\nYou can use k-fold cross validation to improve your current classification testing strategy. \n\nI realize now my responses are all out of order with your original questions. Lmk if unclear."", 'Deviances absolutely can be used to compare nested models', 'Not in half, an 80-20 training-test split is standard.']",63,18,https://www.reddit.com/r/statistics/comments/11efku4/q_how_do_you_decide_when_a_logistic_regression/
896,2023-03-01 02:05:22,[Q] When should one compute RMS or average of an oscillating non-negative time series?,"I am working in a lab and we are trying to compare an oscillating quantity over a time window (it is more or less a sawtooth pattern, but is biological data so not a perfect sawtooth) to some other baseline quantity over time (which is constant during the time window).  Thus, we want to calculate some overall value for the fluctuating time series and compare that to the constant baseline value. 

&#x200B;

I was wondering if under this circumstance taking the RMS over time would be better than simply taking the average. I know for sinusoids that oscillate around y=0, taking the average yields a value of zero which is not helpful. 

&#x200B;

I also know that RMS is a metric for dispersion (so not good for central tendency?) so I am not sure if it is better than a non-zero average computation. However, we are basically doing a t-test where one distribution has some variance (oscillating sawtooth) and the other distribution has effectively zero variance (constant over time). So in that case wouldn't dispersion also be important?

&#x200B;

Any insight is appreciated!","[""Without knowing exactly what your question is, it's a bit tricky to know what the best metric is. \n\nIf you want a holistic measure of dispersion, why not compute the differential entropy? It has some nice intuitions behind it, and since it's based on probability distributions rather than direct values it behaves nicely in many contexts. You can compute it easily using Gaussian assumptions, or if you want a non-parametric estimator, the Kozachenko-Leonenko method is one I'm fond of.""]",3,1,https://www.reddit.com/r/statistics/comments/11efd0c/q_when_should_one_compute_rms_or_average_of_an/
897,2023-03-01 01:58:48,[Q] When should I use Mann-Whitney U test vs. Wald-Wolfowitz test,,"['Those tests are used to answer *completely different questions*. The opening paragraphs of the respective wiki articles ([Wald-Wolfowitz](https://en.wikipedia.org/wiki/Wald%E2%80%93Wolfowitz_runs_test) and [Mann-Whitney](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)) do a good job of explaining the general use cases.', ""Im glad this question was asked as I've never heard of the Wald-Wolfowitz test"", 'I totally forgot about that test, I saw that in my bachelors, and have never used it in real life.', 'A.K.A. the ""runs"" test']",6,4,https://www.reddit.com/r/statistics/comments/11ef6uu/q_when_should_i_use_mannwhitney_u_test_vs/
898,2023-02-28 23:53:49,[Q] Can a logistic regression result be imported to R?,"I have a logistic regression that was run in a program that uses R for the stats. I don't have access to the individual level data, as the program takes care of that. But is there a way to take the output from this and put it in R to be able to use the predict() function on it in R?","[""Probably not what you're looking for, but it's pretty simple to just type the equation. P(y|x) is just 1/(1+exp(-1\\*(b0 + b1\\*x1 + b2\\*x2 + ...)))."", 'You can save the model as an R object.', 'I will try this thank you!', '> `-1*(b0 + b1*x1 + b2*x2 + ...)`\n\nunary minus ... `-(b0 + b1*x1 + b2*x2 + ...)` should work correctly', 'If it gives you the weights of the model, yes. If it just gives the predictions, no.']",8,12,https://www.reddit.com/r/statistics/comments/11ec15z/q_can_a_logistic_regression_result_be_imported_to/
899,2023-02-28 23:53:47,[S] Changing Axes Range in Sigma Plot,"Hey y'all!

I'm currently using sigma plot to create some graphs, and I am having a bit of an issue with scaling the axes. Currently, the axes are set up such that there is a starting/baseline value and an upper value, with the bars being positioned at the starting value.

I am wondering whether there is a way to change the axes such that it shows a range of values above \*and\* below the starting/baseline value? E.g. if zero was the baseline it would show both positive and negative values above and below, respectively. This way my bars could ""point"" above and below the baseline value, if that makes sense.

Thank you!","['This is not really a statistics question, it is a software question. And, I doubt that many ""statisticians"" use it... I have only seen Chemists use it, personally. Previous posts to /r/statistics relating to SigmaPlot haven\'t gotten any responses in the past 5+ years.  I see a few responses in /r/chemistry , /r/biology , and /r/labrats, but this  seems to be a very niche product.']",2,1,https://www.reddit.com/r/statistics/comments/11ec14q/s_changing_axes_range_in_sigma_plot/
900,2023-02-28 18:07:44,[Q] Is Huffman Encoding suitable for fair dice-rolls using coin tosses?,"Assuming a fair coin, if I am simulating a six-sided die roll, are the following rolls an even distribution of probability as if it were a fair die?

1 - T/T

2 - T/H

3 - H/T/T

4 - H/T/H

5 - H/H/T

6 - H/H/H

The part I am not quite understanding is if the required H-prefix for 3-6 affects the overall probability so this would be weighted in a biased manner.

Use case is using Diceware for a game but wanting to use coin flips instead of dice but maintain the 1/6th probability whether it's the 2 (compressed) or 3 flips.

The other option that I don't like is to always flip 3 coins and discard any HHH or TTT flips.

How can I achieve this?","['This would not be a uniform distribution, as the probabilities of 1 and 2 are both 1/4, not 1/6.  You would need to “re-roll” (re-flip) certain outcomes like your last method, even if that seems undesirable.', ""No finite sequence is guaranteed to work as you have 2^N equally likely option after N rolls and 2^N is never divisible by 6.\n\nA simple algorithm that's still optimal in terms of the expected number of rolls:\n\n* 1 - TTT\n* 2 - TTH\n* 3 - THT\n* 4 - THH\n* 5 - HTT\n* 6 - HTH\n\nIf you get HH, start over. Compared to your algorithm this has the benefit of saving a (useless) third flip in the 1/4 case that we need to start over."", 'Okay cool thank you', ""If you insist on a fair die, you will *always* end up discarding flips at some point, since there's no positive integer m such that 2^m is a whole positive multiple of 6. If you want a finite number of steps to generate a number, there's no option but to discard at least a few results (either 2 or 4) out of the 2^m.\n\nNo matter how clever you get with use of coding schemes, you can't get around this issue.\n\ne.g. toss 5 coins (or one coin 5 times); you can get 32 distinct sequences, all equally probable (assuming a fair coin and independent tosses). The next smallest multiple of 6 is 30 (5 outcomes each), so two of the 32 results must be left out.\n\nOdd m means no fewer than 2 outcomes must be discarded, even m means no fewer than 4 outcomes are discarded. e.g. toss a coin 20 times, then after you allocate the 2^20 outcomes evenly into six bins (by whatever mechanism) there will be 4 outcomes left over."", 'Okay awesome thanks']",1,6,https://www.reddit.com/r/statistics/comments/11e2fxm/q_is_huffman_encoding_suitable_for_fair_dicerolls/
901,2023-02-28 17:28:40,[Q] How do I make a ratio to distinguish the rows from each other?,"https://i.imgur.com/pYmY1Wq.jpg

I have three columns (G,H,I). In columns K and L I’ve created two ratios (K represents G/H), while L represents (H/I). 

The ratios are my attempt to distinguish each of the numerical sets in each row from each other. ie: assigning a numerical value to the ratio of the values in row 168 to tell it apart from row 171, 174, etc. Is there another way to relate the values in G, H, I according to a correlation type formula?

I’m basically looking for ratios to classify each row of values and distinguish them from each other.


Let’s say columns G represents red marbles, H represents blue marbles, I represents green marbles, and each of the rows represents a jar. I am trying to classify each jar by placing a number on it, with the number (or two separate numbers) being a coefficient found by involving addition/subtraction/division of the numbers in each of the jars.

 I’m trying to find numbers to classify each of the jars that are different enough from one another such that you can distinguish the ratios of colored marbles in the jars clearly and with no overlap.",[],1,0,https://www.reddit.com/r/statistics/comments/11e1u24/q_how_do_i_make_a_ratio_to_distinguish_the_rows/
902,2023-02-28 08:18:55,[Education] MSc Biology in Biomedical Data Science + MSc (Bio)statistics?,"**TL;DR:** offered a fully-funded Master’s position with a CS/ML faculty prof, with a project in my area of interest (AI/ML in biomedical science). However, as a biomed major, I’d have to be doing the masters as an MSc in Biology, rather than MSc in CS. How transferable is this to industry, provided I’m okay with another 1-year MSc in (bio)statistics/data science if needed? Alternate plan is to do an accelerated BSc in CS.

Hey everyone. I finished my Bachelor’s in Biomedical Science in Canada, and for a year after graduation, had worked in wet lab research. [After taking the time to explore my careers and interests](https://www.reddit.com/r/statistics/comments/108ltqp/education_computer_science_biostatistics_health/), I was planning on doing an accelerated bachelor’s in CS + a master's in biostatistics. Through this, my goal was to do research using data science within a biomedical/clinical context, while also having robust credentials for industry if things go awry.

However, I recently got offered a fully-funded Master’s position by one of my professors. They are primarily appointed in the CS department, with research in ML for drug discovery, cancer biology, etc. The project aligns very well with my interests, however, as my current credentials are in biomedical science (not CS), he is only able to offer the position through a Master's in Biology, as opposed to a Master's in CS.

That being said:

* How transferrable would this MSc in Biology be towards the general CS/DS/ML industry? 
* Overall, provided my interests, would this be a wise decision career-wise?

I’m also fine with doing another internship-based MSc in (bio)statistics or data science after this program, if need be (e.g. if the MSc in Bio isn’t recognized). I would have enrolled in those directly if it weren’t for missing prerequisite courses (which I plan on taking in the BSc in CS or MSc in Bio).","['The name on the degree doesn\'t matter nearly as much as your project portfolio at the end of the program. If you can point to a Github and/or published projects you should be in good shape. \n\nAlso, while you shouldn\'t  *lie* on your CV, you could probably get away as listing the Masters as something like ""computational biology"" depending on what classes you take and what your advisor says. My PhD is *technically* in Psychology (since that\'s what my University offers), but my CV reads ""computational neuroscience"" because all the classes I took were computational, all my research is computational and published in journals like PLoS CB or Interface, etc. I\'ve never actually taken a psychology course. So far, no one has ever looked askance at it. \n\nIn general, I would definitely take this deal. The mentor clearly thinks highly enough of you to make the offer, it sounds like it\'s relevant to your interests, and the real-world difference isn\'t huge.', 'If the worst case scenario is a funded MSc compared to an accelerated bachelor’s in CS + a master’s in biostatistics…it seems like if everything goes extremely poorly you could still get a second MSc in biostatistics just like you originally planned. I’m not Canadian so there could be nuances, but there seem to be no downsides with the funded MSc - and I agree with the other poster that what you do matters more than the name of the degree.']",6,2,https://www.reddit.com/r/statistics/comments/11drjbv/education_msc_biology_in_biomedical_data_science/
903,2023-02-28 07:12:48,[Q] is per capita income and total income continuous or discrete variables?,"Question says it all. I have to build a model for class, and I’m getting mixed opinions online. Sorry if it seems trivial lol. Still a novice at this.

Thanks!","['Continuous until it gets binned into categories like ""small"", ""medium"", ""large"", then it\'s discrete ordinal.  Don\'t bin it.', ""I would treat them as continuous. While not strictly continuous, very few real world situations will have truly continuous values.\n\nA quick question I ask myself is 'are there often multiple entries per value' to determine if I should treat the variable as continuous. For example, I would treat 'number of children' as discrete because I would expect to see multiple people with 2 or 3 children. Total income I would treat as continuous because I would not expect more than one or two people to make exactly 74251 per year."", ""If you have millions of people then you'll get multiple people with an income of 74251, but the difference between 74251 and 74252 (or 74251.01)  is irrelevant so it doesn't matter if they have exactly the same income or something else, for all practical purposes it's continuous."", 'What definitions do you have for *discrete* and *continuous*?', 'To be fair, every continuous variable in practice is interval censored']",2,8,https://www.reddit.com/r/statistics/comments/11dpzvo/q_is_per_capita_income_and_total_income/
904,2023-02-28 07:03:29,[Q][D] Probability of choosing 4 tiles all at once vs. one at a time,"My girlfriend and I were playing Azul the other night, and a friendly conversation was sparked from the game. For those of you unaware of the game, Azul, it has a component that requires you to choose tiles from a bag at random (without replacement) to play. Naturally, classical questions of probablity tend to form when playing this game. Before I ask the question, here's the basics of the bag of tiles and the random choosing method: 

- There are 100 tiles in total placed in a bag.

- There are 5 different tile colors (blue, red, orange, white, and black) and 20 of each tile color (so the color types are evenly distributed). 

- All tiles are exactly the same size and weight.

- To start a game round, you pick sets of 4 tiles and place them in groups (for two players, you will do this five times, thus picking five sets of 4 randomly chosen tiles).


**Here's the questions:**

*Imagine I am trying to calculate the probability of one of my tiles being blue when I pull out a set of 4. Is there a difference in the probability of choosing a blue tile if I grab a set of 4 random tiles one at a time vs. If I grabbed all 4 randomly at once?* 


*Furthermore, another question that came up. Again, imagine I am trying to calculate the probability of choosing at least one blue tile in a set of 4. Is there a way to choose 1 tile from the bag, not look at it, and calculate a new probability of at least one tile being blue with the information that I already pulled a tile before that (despite not knowing what color of the previously pulled tile was)?*

I think there may be a problem with the fundamental question being asked. Would anyone have some light on how you would approach responding to/answering these questions? I tried Googling this sort of problem but was lost. Any resources would be helpful too!

Thanks so much!","["">Imagine I am trying to calculate the probability of one of my tiles being blue when I pull out a set of 4. Is there a difference in the probability of choosing a blue tile if I grab a set of 4 random tiles one at a time vs. If I grabbed all 4 randomly at once?\n\nNo. The only way it could possibly matter is if you looked at each tile as you drew out each one, and wanted to update probabilities based on what you observed.  Since you aren't doing that, then no.\n\n> Furthermore, another question that came up. Again, imagine I am trying to calculate the probability of choosing at least one blue tile in a set of 4. Is there a way to choose 1 tile from the bag, not look at it, and calculate a new probability of at least one tile being blue with the information that I already pulled a tile before that (despite not knowing what color of the previously pulled tile was)?\n\nNo. The only way the probabilities can change is if some information is learned about the tile that was removed, versus the tiles still in the bag.  If you don't look at it (or in some other way learn something about it), the tile might as well still be in the bag."", '> Is there a difference in the probability of choosing a blue tile if I grab a set of 4 random tiles one at a time vs. If I grabbed all 4 randomly at once?\n\nNo, if they\'re well mixed, the probabilities (considering the full set of 4) are the same whether you draw them all together or one-by-one. \n\nNot looking at tiles as you draw will change nothing from drawing them all at once.\n\n(If you examine them as you go, you do get information about the outcome as you go but that doesn\'t change whole-of-draw probabilities like ""chance I draw at least one blue one in the draw of 4"".)', ""Ahh, perfect. This is what I was trying to explain, but just couldn't get at for some reason.\n\nI think she philosophically wants there to be a difference or something. But without the information, there just isn't any way to really get at what she's trying to determine. \n\nThanks so much!"", 'Perfect. Thank you!']",1,4,https://www.reddit.com/r/statistics/comments/11dprua/qd_probability_of_choosing_4_tiles_all_at_once_vs/
905,2023-02-28 05:16:24,"[Q] When reporting B and Beta in a hierarchical regression table, which value gets the asterisk?","I'm looking at my output for a hierarchical regression in SPSS, and I've been instructed to report B, Standard Error B, Beta, and Change in R Square.  On SPSS, I can tell whether the change in R square is significant because on the Model Summary, it has the last box labeled ""Sig. F Change"". However, when looking at the Coefficients output table, I see there is a significance value/box, but which value is this connected to when I'm reporting it in my table?

&#x200B;

I have my table as:

B (SE B) | Beta | Change in R Square

I know where to put the asterisk if the change in R square is significant, but for the Coefficients table, where does the asterisk go? B or Beta?",[],0,0,https://www.reddit.com/r/statistics/comments/11dn4x6/q_when_reporting_b_and_beta_in_a_hierarchical/
906,2023-02-28 04:19:07,[Q] Help understanding a statement,"Hello!

I'm reading a paper and got puzzled by one of the statements in the analysis of a regression model. I was wondering if someone could please explain to me what the authors mean here: 

*""There is a moderate association between socioeconomic disparities and achievement gaps: the R2’s from the models in figure 6 are .41 and .38 for white-black and white-Hispanic gaps, respectively (implying that the correlation between district-level achievement gaps and an index of racial socioeconomic differences I, is roughly .62–.64).""*

Basically, I am not sure how they  assert that the correlation is ""roughly .62 - .64"" from the R2 (.41 and .38 respectively). How is the correlation calculated here?","[""take the square root of the R2. that's it"", ""They computed r from the square root of R^2 \n\n    > sqrt(c(.38,.41))\n    [1] 0.6164414 0.6403124\n\n1. That only works for simple regression; I can't tell from your quote whether that was the case here.\n\n2. It ignores the fact that correlation might be negative; you have to match the sign of r to the sign of the simple regression coefficient."", 'Thanks!', 'Thanks a lot, that’s very helpful!']",2,4,https://www.reddit.com/r/statistics/comments/11dloyw/q_help_understanding_a_statement/
907,2023-02-28 04:05:51,[Question] Help with GLMM in R," Hello,

I need some guidance in doing a GLMM in R and was hoping this subreddit might be able to help me out!

Background:  I'm a master's student working on the data analysis part of my thesis  and need to run a GLMM in R, but I have very little experience in R (or statistical analysis in general if I'm being totally honest). So I need  someone to explain this stuff to me like I'm 3 years old in the most  basic baby terminology possible.

My  data is on the impact of urbanization on bird foraging; my comparative  variables are site type (natural vs urban), individual site (9 different sites), observation time per visit (mins), and migration period (during or post); my response variables are # of bird species seen per visit, and # of bird-fruit interactions per visit.

I  get the general idea that it will be formatted as something like,  \[response variable\] = \[comparative variable 1\] + \[comparative variable 2\] etc, and that I'll need to specify which variables are random effects and which are fixed effects and so forth. My problem is that I have NO  IDEA whatsoever how to start doing this in R. The coding stuff is just so obtuse to me, and it seems like all the guides I see online for GLMM are assuming a base level of familiarity with coding in R which I just don't have.

If anyone has a suggestion for like, what packages to use, and/or tutorials on how to tell R what  to do with the data, I will be forever in your debt.

(also please no suggestions for me to use other software for my analysis, my thesis advisor wants me to use R)","['If you are truly a total beginner to programming I would recommend doing an introductory online course in R (e.g. [datacamp](https://www.datacamp.com/courses/free-introduction-to-r)). Unfortunately if you have zero background in this area it is going to be quite difficult to jump straight into your analysis.\n\nIf you have managed to successfully load your dataset into R (and done any necessary pre-processing steps like removing or replacing missing values), GLMM is pretty easy to use. Regression models in R are specified with the following format:\n\n    DV ~ IV1 + IV2 + ...\n\nWhere DV is the name of the column in your data frame containing your dependent variable (DV), and IV1, IV2, etc. are the names of different columns, each containing one independent variable (each will be modeled as a fixed effect). Random effects in GLMM are specified in the following format:\n\n    list(~0 + RE1, ~0 + RE2)\n\nWhere again, RE1 and RE2 are column names corresponding to the variables you want to model as random effects (note you can have any number of random effects, each separated by a comma and including the ~0 prefix). If your dataframe is named ""data"", to use GLMM you just need to use the function glmm() after importing the library, and then use summary() to see the results:\n\n    library(glmm)\n    glmm_model <- glmm(DV ~ IV1 + IV2, random=list(~0 + RE1, ~0 + RE2), data=data)\n    summary(glmm_model)\n\nNote that **you cannot just copy-paste what I just wrote**. You need to change the variable names (DV, IV1, IV2, RE1, RE2) to match the column names in your dataset, which I am assuming is stored in a variable named ""data"". You can add or take away arbitrary numbers of IVs/REs.\n\nAll of this (and more) can be found in the [GLMM package documentation](https://mran.microsoft.com/snapshot/2017-08-20/web/packages/glmm/vignettes/intro.pdf).', 'I know you’re not going to use other software, but while working on your R code I recommend using Jamovi as a “sanity checker” in the sense you can compare outcomes to be sure you’re getting the data worked out like you want. They have a GLMM package (GAMLj) that is based on the lmer package, and which you can ultimately use in R if necessary.', ""Read these papers on GLMMs in ecology:\n\nhttps://peerj.com/articles/4794/\n\nhttps://www.sciencedirect.com/science/article/abs/pii/S0169534709000196\n\nIf your school's library (or PI) has it, this book is also a really helpful introductory resource with many real world examples and accompanying R code for similar research problems:\n\nZuur, A. F., Ieno, E. N., Walker, N. J., Saveliev, A. A., & Smith, G. M. (2009).\xa0Mixed effects models and extensions in ecology with R. New York: springer.\n\nFrom the information you provided, it sounds like you need a GLMM with a Poisson link (to model count data as a dependent measure) and maybe a couple of random effects, which are terms that account for hierarchical relationships, clustering, or dependencies/auto-correlation among observation units in the data (e.g., repeated measurements for the same individuals or locations over time). In R, the most popular function for fitting such a model is glmer() from the lme4 package, which is concisely covered, alongside the necessary theory you should know to be able to understand what you're doing, in the above mentioned resources."", 'Absolutely.  You need to know what to ask. But it gives you a good baseline. Sometimes the code and results are simple solutions,  sometimes very complex. But, play around with it.', 'If your thesis advisor wants you to use R, why are they not providing any training or resources for how to use R?\n\nI like lme4 package, btw']",13,10,https://www.reddit.com/r/statistics/comments/11dlcvd/question_help_with_glmm_in_r/
908,2023-02-28 03:47:24,[D] Is Bruce Bueno de Mesquita legitmate?,"Some of the claims he makes about his models are quite drastic ie: ""predicting 90% of events"" and yet the methods and the way he describes them publicly are fairly basic.","['He\'s a legitimate political scientist.\n\nAt the same time, predicting political events can be very easy. You can get 90% accuracy by just predicting that the next month will be the same as this month, e.g. ""will Putin still be in power next month? Yes.""\n\nMuch harder is predicting momentous events. Your accuracy metric has to take that into account.', '>It\'s hard to argue with CIA statements, and they can make very good publicity.\n\nThose aren\'t official CIA documents...""The citation is not, in fact, an official CIA assessment of the performance of his methodology. It is an article published in an internal trade journal for analysts called Studies in Intelligence. The article, by Stanley A. Feder, entitled Factions and Policon: New Ways to Analyze Politics, examines the use of a pair of tools that political analysts used in the mid-1980s. Given the source and nature of the document, it is inaccurate for Bueno de Mesquita to claim a CIA endorsement for his work. Aggrandizing the source of his citations undermines the credibility of the rest of his claims.""', ""https://en.wikipedia.org/wiki/Bruce_Bueno_de_Mesquita:\n\n> A declassified assessment by the Central Intelligence Agency rated his model as being 90 percent accurate\n\nIt's hard to argue with CIA statements, and they can make very good publicity.\n\n> yet the methods and the way he describes them publicly are fairly basic\n\nSo what? Simple models can be very accurate. The obvious one I can think of is Einstein's E=mc^2 formula. Basic yet accurate.\n\nAlso, describing them in a basic way just shows that he is a good talker.\n\nI don't know his work, but nothing seems strange to me so far."", ""Ya, I think his self-cited accuracy is misleading at best, per a TED Talk he gives he gets half a 1/4 of his predictions he makes wrong while saying his model has 90% accuracy, and it doesn't take into account past events. It just feels like a garbage in, garbage out model."", 'I am fairly skeptical simple models can subjectively predict complex political predictions. There is an inherent difference between predicting human behavior vs physics.']",2,5,https://www.reddit.com/r/statistics/comments/11dkwh3/d_is_bruce_bueno_de_mesquita_legitmate/
909,2023-02-28 01:49:47,[Question] What are some go-to statistics resources beyond basic textbooks?,"I am searching for some intermediate/upper intermediate statistics resources to deepen and broaden my skills in statistics. My academic background is in psychology and experimental design, and I work as a data analyst. I am familiar with the 'basics' of statistics (e.g., NHST, distributions, linear and multiple regression) and have done some work with principal components analysis and factor analysis, multiple imputation (although I am not fully aware of how this works), and confirmatory analyses. But as I get further and further into my job I feel like I am reaching the limit of my stats and data knowledge and I want to further my skills. What are some resources you recommend? I'll include a list of books I have read and used below to give you an idea of what I have access to and what my knowledge base is like; hopefully someone who knows of these resources can recommend some deeper insights.

* Mediation and Moderation (Andrew Hayes)
* Learning Statistics with SPSS (Andrew Field)
* Statistics for the Behavioral Sciences 7th edition (Gravetter & Walnau)

I have been looking at the Elements of Statistical Learning by Toshibriani(spelling?) et al., 50 topics for data analysts, etc. I do not have a strong math background so I have run in to some limitations in that regard. Any recommendations are appreciated!","['The statistics notes series in the BMJ is good: [https://www-users.york.ac.uk/\\~mb55/pubs/pbstnote.htm](https://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm) ; more for clinicians but written (mostly) by Doug Altman/Martin Bland.', ""ESL, by Tibshirani et al is a fair jump and the book focuses on statistical learning which is not likely to be directly useful to you (at least not yet), unless you're specifically  interested in topics like prediction and classification. (However I would suggest paying close attention to a couple of early sections that *are* quite relevant to a psych person,  specifically parts relating  to not using the same data for inference and/or prediction as was used to choose the model)\n\nIf you want to understand statistics beyond the level you you indicate (understanding something of the why and how and what else the might be) you probably will need some basic stat theory  for which you will need at least some mathematics (calculus would be a big help to get started but for regression/ anova, etc, some familiarity with vectors and matrices would be important)\n\nA basic book on probability  and mathematical statistics would then be my suggestion (some mathematics, mostly algebra, but well below the level of ESL)... I'll come back with some possible suggestions but there's dozens  of similar books\n\nEdit: maybe take a look at *An Introduction to Mathematical Statistics and Its Applications* (Larsen & Marx), it's about as simple as it gets while still actually covering topics that would help to give a start on some actual statistical grounding. It doesn't seem to mention the Neyman Pearson lemma (but *does* cover likelihood ratio tests, which is kind of odd), nor does it seem to cover Wald tests, nor score tests, so it's maybe *too* basic but if you can manage it, there's always the possibility of going back later on to pick up missing topics.\n\nAn alternative would be the book by Wackerly, Mendenhall and Scheaffer (edition doesn't matter with these books, grab an old one without much worry).\n\nI'd suggest maybe trying to get a look in a university library if you can, check out early chapters and see if that would be manageable.\n\n(What you will miss with these is exposure to simulation. Maybe Hardin and Cenka, *Introduction to Modern Statistics* might be gentle enough to get started there - available free online, if you can't find it let me know. An alternative would be to pick up some R, perhaps via Navarro's [*Learning Statistics with R*](https://learningstatisticswithr.com) -- a book on basic stats aimed at undergrad psych students though it does touch on some very basic simulation ideas -  and then try to go on from there with some deeper simulation notes, of which there are many; there's an R package for the book, by the way)\n\n\n\nAfter that probably a good book on applied regression. Maybe Fox? (Fox is a social scientist, specifically a sociologist). There's a companion book (Companion to Applied Regression) and package (`car` in R)\n\nThen after that basic grounding you should be in a position to read books on a much wider array of topics without getting lost.""]",3,2,https://www.reddit.com/r/statistics/comments/11dhyco/question_what_are_some_goto_statistics_resources/
910,2023-02-28 01:49:17,[Q] I need a help to find the source of the base of industrial inflation index data I'm using. Anyone can identify where this series comes from?,"Hi! I'm working with the data base of industrial inflation of the image below:

[https://imgur.com/a/XxrnuNW](https://imgur.com/a/XxrnuNW)

Does anyone know where can I find the rest of the historical data related?

It was said to me that this table data was obtained in the Bureau of Labor Statistics, and it's an Industrial Goods index. Searching the Bureau's site, I couldn't find a match.

I'd be very grateful if anyone can help me with that! :)","['I checked bls.gov and couldn\'t find an ""Industrial Goods Index"" (didn\'t look every hard), and wanted to confirm you meant the US Department of Labor, and not some foreign country\'s?', ""[FRED](https://fred.stlouisfed.org/categories/32455) has over 10,000 Producer Price Indices.\n\nIt's probably one of them.....have a look."", ""I'm sure you've already noticed but the dates are in a foreign language and they're using commas to separate the decimal part, which makes it unlikely it comes from a US source. Of course, it could be translated from a US source, but the source might be non-US entirely"", 'Thanks for helping!\n\nI got this numbers from a report that stated that the source of the index was IMF, but it was discontinued, being replaced by the ""same"" index from the american Bureau of Labor Statistics.', 'Yes, well noticed.\n\nThe info is from an international report elaborated by Brazil and Paraguay together.\n\nHowever, in the report its stated that the source of the data in the table is the american bureau of labor statistics.  \n\n\nEdit: the data is intend to reflect the american inflation, because the report deals with dolarized quantities.']",1,6,https://www.reddit.com/r/statistics/comments/11dhxy4/q_i_need_a_help_to_find_the_source_of_the_base_of/
911,2023-02-28 01:41:15,[Q] Compounding Interest Help,"Hey, I was wondering if there is a formula out there that can help me.

Let’s say I do something 20 times and I have a 5% chance to get the result I want. Out of the 20 times I go through each result what is the percent that I would get the result?

Is there a calculator app out there where I can put a number of attempts and a percent for each attempt?

Wanting to use this for a ton of rpg games that I play but I wasn’t able to google correctly to find an easy formula or calculator to help me out.","['This is more like a probability question, not a compound interest question. Since you are trying 20 times, the distribution of getting the result is ranging from 0 to 20. If your question is what’s the probability that you get “at least one result”, the answer is 1 minus the probability that you don’t get any result out of 20 trials, which is 1 - (0.95)^20 = 0.6415.', 'Not sure if I got my thoughts out well, but was just curious how many attempts I need to make at something before I get close to getting pretty good success', '> Let’s say I do something 20 times and I have a 5% chance to get the result I want. Out of the 20 times I go through each result what is the percent that I would get the result?\n\n1. Do you mean ""get the result *at least once*""?\n\n2. Are the trials independent?   (Edit: oh, for it\'s TTRPGs? then yes, almost always independent)\n\nIf yes to both, then 1-(19/20)^20 (or roughly 1 - 1/e, since n isn\'t small and p = 1/n here)\n\nmore generally, for probability of success p in each trial and n attempts, P(at least one success) = 1 - (1-p)^n\n\n(This is just ""what\'s the probability I don\'t fail all of n times in a row?"")', 'Yeah, I wasn’t sure how to word it. Hah this isn’t anything formal, just to help me with video games.\n\nThanks for that!']",0,4,https://www.reddit.com/r/statistics/comments/11dhr3q/q_compounding_interest_help/
912,2023-02-27 23:36:43,[E] Help finding resources about Markov Kernels,"I am getting Markov kernels introduced for the first time in a causality course and I am struggling to understand them. We have exercises to practice like ""show the product of markov kernels is a markov kernel"", ""composition of markov kernels with gaussian densities""... but I just do not know how to apporach them. 

For example, I see the definition of the product of Markov kernels, and then a remark about it not being commutative. I get the idea of why the product of Q(W|T) and K(W|T) should not be commutative, but I can't mathematically write it down. This happens often, I get what the idea should be, but get completely lost trying to write it down.

The only resource we are given are some lecture notes with no solutions and most proofs are left as exercise to the reader, and after reading the definitions 10 times I still can't figure out anything. This new Markov kernel setting to generalize probability spaces is making me completely lost, and being unable to find resources to go little by little or at least some examples of how to solve some exercises is making me desperate. 

I am looking for either some videos/notes/books that go really slow about markov kernels, or some basic exercises with solutions about them.","['Thanks for such a wonderful reply! TheGratitudeBot has been reading millions of comments in the past few weeks, and you’ve just made the list of some of the most grateful redditors this week! Thanks for making Reddit a wonderful place to be :)', 'https://www.jstor.org/stable/2290486\n\nThis paper provides an introduction to Markov kernels in a clear and concise manner, with several examples and exercises that are suitable for beginners.', 'thanks for the reply, but when I click the link I get a review of a book, is the link wrong? or are you reccomending the book it is reviewing?', 'Oh, sorry! Yes, ""Markov Kernels and Their Applications"" by Srinivasa Varadhan']",3,4,https://www.reddit.com/r/statistics/comments/11deqjd/e_help_finding_resources_about_markov_kernels/
913,2023-02-27 21:42:54,[Q] Would this be a moderator or mediator?,"Let’s presume that there is a positive relationship between the intensity of a stressful event and negative mood, such that the more intense a person experiences a stressful event, the worse their mood will subsequently be. 

Would distress tolerance (the ability to tolerate stress), mediate this relationship (i.e., explain it), such that individual differences in the ability to tolerate distress is the reason why stress intensity relates to mood. Or would distress tolerance moderate this relationship, such that the ability to tolerate distress would weaken the strength of the relationship between stress intensity and mood. 

I’m having trouble wrapping my head around it, because they both sound plausible.

Thanks.","[""The stressful event doesn't get explained away by the stress tolerance. \nYou can have a high stress tolerance and still show reaction with your mood.\nAnd at lower stress tolerance,  your mood may be more low.\n\nAlso sounds like a moderator to me"", 'Sounds like a moderator to me']",1,2,https://www.reddit.com/r/statistics/comments/11dc5b0/q_would_this_be_a_moderator_or_mediator/
914,2023-02-27 18:42:56,[Q] What sort of time series model would be appropriate for a binary variable?,"Hey everyone, I'm just curious if there are any resources on time series models for binomial variables. For instance, analysing the landing page of a website and seeing how many visitors actually go ahead to make a purchase. Google wasn't very helpful for me!

&#x200B;

My initial thoughts were to construct a multi level model where time is just another variable.

&#x200B;

Thanks.","['Is it really binary or is it binomial/poisson? Sounds more like you have count data, unless I am missing what aspect of this process you want to model.\n\nI.e. Do you want to model aggregate behavior (purchases over tien period) or individual behavior (likelihood a person makes a purchase after X time on site)', 'Sorry, I meant binomial for modeling aggregate behaviour. Will update the question!', ""So if I understand correctly, you're interested in modelling how many users from a cohort of say 100,000 will get a binary outcome (e.g. a signup) over time?"", ""I feel like there's a way to transform these into a single binomial variable with mean (X-Y) and then test if that transformed distribution's p value is 0... my other guess would be some kind of chi-squared testing?"", 'Simple linear Regression with X as whatever variable (maybe time) and y as the binary of 0 = no purchase and 1 = purchase.']",24,6,https://www.reddit.com/r/statistics/comments/11d8uwl/q_what_sort_of_time_series_model_would_be/
915,2023-02-27 18:18:15,[Q] Percentile and sorting.,"I have two questions. Suppose I have a numerical dataset (n=100).
1. Is pth percentile the value whose ""magnitude"" as well as relative positioning is greater than p% of the data, and less than 1-p% of the dataset? (These two conditions should be simultaneously satisfied ?)
2. Is the only way, mathematically speaking, of finding a percentile to fully arrange all the data in ascending or descending order? I've searched online and almost all methods involve some sort of sorting. (Like partial sorting, full, heaps)

My apologies in advance, I'm not very bright when it comes to statistics. Just starting to learn it.","['> 2. Is the only way, mathematically speaking, of finding a percentile to fully arrange all the data in ascending or descending order?\n\nNo, you can avoid sorting all the data with a suitable algorithm. But with n=100, just sort the data (you have a computer right?), unless you want a fairly extreme quantile (in which case just pull out the largest or smallest few values)', 'It isn\'t hard to answer the question ""What percentile of my data does this response value fall into?"" without sorting - go through all of the data and find the proportion that falls on either side of your chosen observation, and you should be able to calculate the percentile from there. I\'d venture to say it\'s not feasible to answer the question ""What response value is the Xth percentile in my data?"" without sorting - percentiles are  proxies for order statistics, and you\'ll need to know the ordered form of all of the data/variable of interest to answer this. I\'d assume there are methods for pretty good approximations, though I\'m not personally familiar with them.', ""2. You don't have to sort all the data. Look up quickselect, e.g. You mention partial sorting yourself."", ""Well I am trying to find VaR(Value at risk) for a dataset, the definition states, it is the worst loss for a given confidence level. So if I'm correct, it means for a C.L of 90%, it must be the worst loss. Do i have to sort the full data to find this? I was thinking sorting only the relevant 90% of the data would do, but then the question is which 90%? Would appreciate any insight."", 'Yes, i was asking, whether there is a way to find percentile without sorting of ""any"" form.']",3,5,https://www.reddit.com/r/statistics/comments/11d8gku/q_percentile_and_sorting/
916,2023-02-27 14:46:25,[D] Scaling dummy variables K means,"Hi. I had a discussion recently with a coworker. We are running a K means clustering algorithm. He said that when dummy variables are made into 1s and 0s,these columns must be scaled in a specific way. If variable A has 17 categories - names of cities. Then after turning into 17 columns of 1s and 0s (Python does it this way). He said that the sum of variances of those columns should be num_cathegories - 1. Thus 16. If the categories are 11, then sum variances should be 10. He proposed centering the dummies for each dummy column by substracting the mean and then dividing by the square root of the columns mean. I have looked into literature for this but haven't found anything. He is senior to me so I did what I was told without understanding it. Can you help.","['He got me on the first half. The n-1 thing is right. If you think about it when we have sex (M, F) as a feature in practice it’s two categories but you end up with one column. The reason we do it like like that has do with the least square error method and linear algebra in general. I can go in details if you want. \n\nAlso forgot to mention but the are two types of categorical variables ordinal and nominal. What we are talking about here is nominal. An example of an ordinal categorical variable is Likert scale. Every time you see question about how much did you like this, it’s usually likert. \n\nHaving said that we don’t normalise the nominal variables because it makes no sense.\n\nAlso I don’t know if in K-means using n or n-1 columns makes any difference but in general have n-1 columns contains all the information and then nth column is redundant. In some methods that causes a problem in others it goes unnoticed.', ""What first half are you refering to? I understand that Python and R have dufferent number of created dummy columns. A variable with 12 city names will make 12 columns in Python and 11 in R. But still don't understand why they should be divided by sqrt(mean).""]",6,2,https://www.reddit.com/r/statistics/comments/11d53i6/d_scaling_dummy_variables_k_means/
917,2023-02-27 12:37:52,[Q] How can i reasonably get access to JMP to learn/practice?,"My company (and some that I've seen job postings from) use JMP for data analysis, I'm interested in learning it, but i cant seem to find a way to practice. I have access to the virtual lab through an online course, but the VL is not able to open data files, handle excel files, etc. I highly doubt my company would let me use a license as is not really in my job responsibilities. Any way i can get some hands on practice? I dont mind paying a reasonable amount just not the $2-3k for the full license. Thabks","['[Here\'s the link to the free trial](https://www.jmp.com/en_us/download-jmp-free-trial.html).\n\n\nIt\'s fantastic software.  The price is justifiable based on actual time savings and based on conceptual leverage.  It takes less time to do something in JMP than in the alternative, so multiply your hourly rate by the time saved and you\'ll get to a cost-benefit analysis that is in favor of the annual expense.  For leverage, you\'ll be exploring new methods of analysis that you weren\'t aware of, will get better answers from a pile of data, design better experiments and will find more truths than with other software.  \n\n\nThe leverage benefit is harder to quantify, so you\'ll need an anecdote that comes from your usage during your free trial: ""I would never have seen this (fill in your result here) if I hadn\'t used JMP, and the fact that I have seen it has such-and-such economic benefit to the company.""  This can be a hard sell that depends on your status and influence, so my approach has been to sell JMP to management on the basis of time savings with the fan-boy enthusiasm that lets people see this is god for morale.']",1,1,https://www.reddit.com/r/statistics/comments/11d2u7k/q_how_can_i_reasonably_get_access_to_jmp_to/
918,2023-02-27 10:23:38,[Q] Advice on JMP versus R,"Hi everyone, I'm in my second semester of my Biostatistics MS, and I'm running into a problem. Due to the fact my undergraduate degree was in Biology, I'm fairly new to statistics. I did fine in my first semester (Probability I, Introductory statistics I) as math comes easy to me, and I wasn't required to use any software in these classes. However, now that I'm in introductory statistics II, my professor incorporates R into most aspects of the class. I plan on beginning to learn R soon, but as of right now, I don't have even close to enough experience with it to complete this class. 

Upon asking my professor, he said that although we use R, I'm free to use whatever software I want as long as I can answer the questions. I found that JMP (another software I have never used) was much easier to grasp, and I've been getting through the class with the help of online tutorials. Is this a trick that is going to last for me? Are there limitations to JMP that are solved with R that I will face in a class like this? Any information would be very helpful. Thanks!","[""Although coding may seem overwhelming at first, manipulating and visualizing data in R is just a few Youtube videos away. Don't limit yourself, don't waste your time, learn R now while you actually have a reason to use it."", 'JMP is a very capable stat program and it is doubtful that it would not be able to do everything covered in your second semester. JMP is the only existing stat program written from the ground up to have a GUI so, as you have discovered, it is very easy to learn. You could work on learning R while doing problems in JMP. You might also be interested in [JMP and R integration.](https://www.jmp.com/en_us/events/ondemand/mastering-jmp/jmp-and-r-integration.html)', ""Invest time learning R, it will going to bring you closer to a lot of tools used in a lot of fields: genetics, bioinformatics, ecology, you name it. You won't regret it."", ""If you're looking for a stop-gap Stats software while you learn R, try JASP. It's a free statistical analysis software which runs on R. \nhttps://jasp-stats.org/"", ""For me, JMP is for exploration of data and R is for modelling.  Both can be used for either, but JMP is for designing and analyzing experiments, choosing custom designs, making charts and graphs faster that I could ever learn graphing in R, quickly altering the type of analysis to understand visually what's happening in a data set.  I already know JMP so I go there first for manipulating tables, control charts, probability charts, reliability analysis, process capability analysis.  \n\n\nI use R for modeling of sampling distributions, simulating probability models, experiments and characterization of Monte Carlo analysis.  I switched to R for these tasks because to run random simulations in JMP is tedious and coding in R is more flexible.  Graphing in R is grunt work compared to JMP, but it's woth learning the rudiments so you don't have to go back and forth between R and JMP.""]",6,19,https://www.reddit.com/r/statistics/comments/11d06l5/q_advice_on_jmp_versus_r/
919,2023-02-27 08:30:45,[Education] SOHCAHTOA to Gradient Descent,"\[Education\]

Hello,

I am studying machine learning, and I am trying to piece some concepts together to align my understanding of them individually. Here they are:

Concept 1: All machine learning is a geometry problem. All machine learning is the fitting of a line/curve/plane/surface in dimensional space.

Concept 2: SOH CAH TOA, the penumonic for remembering the calulcation of the sine, cosine, and tangent of the acute angles in a right triangle. Note: right triangles are used to find the minimum distance between a point and a line in 2D space; the minimum distance between the point and line is the length of the tangent line from the point to the line. The tangent line forms a right angle with the line.

Concept 3: In simple linear regression, we find a ""line of best fit"" amongst data points (x,y) in a 2D space. The function of a line is y=mx+b. Thus, our predictions using this line are yhat\_i=mx\_i+b. The line of best fit is the line that minimizes MSE, the mean squared error. The error is the difference between the actual values (y\_i) and the predictions (yhat\_i). Thus, we effectively minimize the sum of (y\_i - yhat\_i - mx - b)\^2 across all i data points (x,y). To minimize a function with respect to a variable, we take its derivative, set this equal to 0, and solve. Note: our variables in the MSE function are m and b as x and y are realized values of our data points. Thus, we minimize the MSE with respect to m and b. When doing this, it is clear the values of m and b in this context are directly solvable using the data. Thus, we can directly solve the values of m and b which minimize MSE using the data points. The resulting line y=mx+b has value values of m and b such that the sum of the lengths of the tangent lines from all points to the line is minimized.

Concept 4: Maximum likelihood estimators. I somewhat forgot what these are but I know the MLEs for m and b in simple linear regression should result in the same values as what is found following the process in concept 3.

Concept 5: Where we take the derivative of the cost function, set it to 0, and solve for the coefficients of our line using our data points in simple linear regression, we use the gradient when work in higher dimensions or in nonlinear context. Why? What is the gradient? How is it related to the derivative and minimum distance of points to the function plane/surface?

Concept 6: The ""linear"" in linear regression refers to the relationship of the parameters/coefficeints (m and b) to the independent variable (y). y=m(x\^2)+b is still linear regression; it is polynomial regression which is linear regression. However, y=(e\^m)x+b is not linear regression. The relationship between any indepdent variable and the dependent variable can be nonlinear!

Concept 8: We often should, but do not necessarily need to, normalize the independent variables in linear regression so their mean is 0. In all other linear models, we must normalize the independent variables. We should always normalize the dependent variable(s) to avoid vanishing gradient. Why do we normalize in linear models? StandardScaler() vs MinMaxScaler() and use in linear models vs ANNs?

Concept 7: Multiple linear regression is different from multivariate linear regression. Example of multiple linear regression: y=mx+kz + b. Multivariate regression: multiple dependent variables. Multiple polynomial linear regression: y = mx + kz\^2 + b.

...

I feel like I am getting some grasp of what is happening theoretically, but I am hoping you can poke holes in my understandings and/or align them.

Thank you to anyone who provides input!","[""Concept 1: I don't think clustering algorithms are equivalent to fitting a surface in dimensional space. Or at least, I haven't thought of them that way before. You're more separating your data points into multiple clouds, which I guess is fitting multiple surfaces in dimensional space, but the boundaries of those surfaces are sometimes fuzzy, and it's certainly a set of poorly behaved surfaces from a mathematical perspective.\n\nConcept 5: You want to take multivariable calculus to understand the gradient thoroughly. It's an extrapolation of a derivative in 2d (a slope), used in multidimensional space. The derivative of a three dimensional surface at any given point is a plane. That plane has many possible slopes. The gradient is the steepest slope of all those possible slopes. Similar logic extrapolates to higher dimensional spaces. One reason to care about it because we want to know which combination of x variables (which direction in X space) results in the fastest change in y. But there are plenty of other reasons to care too.""]",0,1,https://www.reddit.com/r/statistics/comments/11cxsh5/education_sohcahtoa_to_gradient_descent/
920,2023-02-27 04:57:05,[C] Master's in Stats: UWashington vs. NYU," Hi  stats community, I was hoping for some insight into potential master's  programs for statistics. I was accepted into three programs thus far:  UWashington M.S. in Statistics - Advanced Methods and Data Analysis,  NYU's M.S. in Applied Statistics for Social Science Research, and  Columbia's M.A. in Statistics. The final program that I am hoping for admission is UC Berkeley's M.A. in Statistics.

I  didn't receive funding for any programs, but will receive in-state tuition for UW (which is a nice bonus) and I will not need to go into debt for any of them. I've heard good things about UW's program  especially, and bad things about Columbia's on reddit (which is why I'm not considering it as strongly). I was wondering if anyone had insights into NYU's program, or if you have any general advice when considering various master's programs. I am currently leaning towards UW mostly because of the curriculum, but NYU is also of interest to me because it is a statistics program hosted within a humanities school (Steinhardt), and I think I am interested in the social sciences/policy aspect. The  price difference is quite large though, so I would definitely want to make sure I get my money's worth from NYU.

Thank you for any advice, and also if you think the Berkeley program is also strongly worth considering (should I get in), please do share!","[""None of these programs are better than UW's and all of them are more expensive. UW all the way."", 'UW for sure. One of the best Statistics programs in the world. Plus you receive in-state tuition.', 'The faculty. I was in UW before and attended a couple of stats courses. But I wouldn’t recommend any program with a quarterly system. You don’t learn anything.', ""UW has a legendary stats department. Best professor I've ever had went there."", 'I’ve been at UW for grad program in a stats adjacent field and not only is the in state tuition going to be beneficial but UW is absolutely dripping with TA/RA funding so there’s a good chance you’ll be able to get tuition covered. Also the courses and profs (Biostats anyways) have been excellent in my experience']",13,20,https://www.reddit.com/r/statistics/comments/11csgbr/c_masters_in_stats_uwashington_vs_nyu/
921,2023-02-27 03:54:06,[Q] How to integrate the sample size when building the average placement for characters in a video game?,"Hello,

I'm currently trying to build some stats for a video game (Hearthstone Battlegrounds). At the start of the game, the player chooses a hero, and their final placement is between 1 (they won) and 8 (they were the first to lose).

I want to build an ""average position"" stat for each hero, to give players an overview of how strong each hero is. This is done pretty easily.

However, some heroes are much more popular than others, which means that their stats should be more accurate (data points roughly vary between over 50k for the more popular ones, to below 4k for the least popular ones). Showing only the average position doesn't reflect this.

Is there a way that I could modify my final ""average position"" stat to have it reflect the uncertainty of the sample size?

I tried to look into the standard deviation, but I get really high values (over 2, while the means is at roughly 4.5), so I'm not sure how to use this value (I still would like the final value to be between 1 and 8, to be more easily understandable).

Thank you in advance for any advice :)","[""The standard deviation is showing you how wide the distribution of positions is. That doesn't decrease with larger sample size because this is the real spread of positions people get (your estimate of that spread gets more precise). To estimate your uncertainty of the average, divide the standard deviation by sqrt(N).\n\nhttps://en.wikipedia.org/wiki/Standard_error#Exact_value\n\nhttps://www.statlect.com/fundamentals-of-statistics/mean-estimation"", 'Thanks a lot for the explanation, and the links!']",2,2,https://www.reddit.com/r/statistics/comments/11cqu9o/q_how_to_integrate_the_sample_size_when_building/
922,2023-02-27 03:46:41,[C] Industries that often hire statisticians.,"What are some industries besides biotech, biostats or finance that commonly hire statistics MSc graduates?  I’m getting more and more interested in engineering companies or some kind of urban planning. Is it common for people with formal statistics training to be offered opportunities in an engineering company or something similar if they don’t have engineering training? If you work outside the above listed fields as a statistician please list your industries in this thread.","['Lot of federal places hire statisticians such as Census, Marine bases, etc.', 'Lots of federal agencies hire statisticians. from recently looking for jobs, I saw openings at faa, dot, and irs. A lot of bigger companies have at least a few statisticians on staff too. I currently work at an aerospace manufacturing company, and we have a few dedicated stats people on staff.', '>Is it common for people with formal statistics training to be offered opportunities in an engineering company or something similar if they don’t have engineering training?\n\nThen you need to take chemometrics if it is available. Survival analysis also will likely be involved in the aforementioned domain (engineering). As long as where there are data available, and the generating process or forecasting needs to be studied/performed in a rigorous manner, there will be demand for statisticians.', 'government apparently', 'Media and survey eg- Nielsen']",6,11,https://www.reddit.com/r/statistics/comments/11cqnkw/c_industries_that_often_hire_statisticians/
923,2023-02-27 03:07:49,[Question] What statistical techniques can I use for binary output data where the input variables are categorical?,"I’m used to performing statistical analysis when all input and output are continuous but now I have event count data (zero or one) for a dozen or so feature variables that are all categorical. 

I would like to do things like figure out which feature variables most correlated with the output like a sensitivity analysis or pca, but the input data is categorical and not necessarily ordered. I would also like to compute something like correlations between a variables but how do you do that when the variables are categories and have no ordering? Any suggested techniques to look into? The bottom line is I want an automated approach to figure out which categories are most important for the events of interest. 

One technique I know of is one-hot-encoding but the input space seems like it could blow up pretty fast since some of my categorical variables have 10 or 20 distinct values.","['> I’m used to performing statistical analysis when all input and output are continuous but now I have event count data (zero or one) for a dozen or so feature variables that are all categorical. \n\nThis is unclearly worded. Do you have a single output variable that is binary? You also have multiple categorical predictors it seems. Is your output data a count that just happens to be 0/1 and could possibly be higher, or is it intrinsically 0/1 valued?\n\n> I would like to do things like figure out which feature variables most correlated with the output like a sensitivity analysis or pca, but the input data is categorical and not necessarily ordered. \n\nNot really sure why you mention either PCA or sensitivity analysis here.\n\n> I would also like to compute something like correlations between a variables but how do you do that when the variables are categories and have no ordering? \n\nIf you really need a correlation like measure between categorical predictors, Cramer’s V is typically a decent option.\n\n> Any suggested techniques to look into? The bottom line is I want an automated approach to figure out which categories are most important for the events of interest. \n\nThis is a totally different task than mentioned above. The simplest way to do this would be a simple regression of the outcome on the categorical variables coupled with effect size measures.\n\n> One technique I know of is one-hot-encoding but the input space seems like it could blow up pretty fast since some of my categorical variables have 10 or 20 distinct values.\n\nThis may or may not be an issue depending on how much data you have total and how many values are in each of the various predictor variables. Regularization may be an option if it does end up being an issue.', 'Thank you for this. For continuous variables regression and sensitivity analysis are deeply connected. So that is why I might have confused the two. \n\nIn my case I have a single output that is a failure yes or no. So f(x1, x2,   …) = 0 or 1 where x are discrete categorical. \n\nCan you provide a reference for this effect size measures? Are there other ways of regressing categorical inputs that doesn’t involve one hot encoding? Thanks this is helpful.', '> Are there other ways of regressing categorical inputs that doesn’t involve one hot encoding\n\nYou have some choices exactly how you do the fitting and model building. But if you have a category with 10 levels and no other structure, you will do something that is mathematically equivalent to estimating 9 coefficients.\n\nIn the old days that might have been manually grouping similar items into fewer categories; in modern times it might be regularizing (either forcing a certain number of those coefficients to be zero, or restricting the sum of those coefficients to some small number.)']",1,3,https://www.reddit.com/r/statistics/comments/11cpoi2/question_what_statistical_techniques_can_i_use/
924,2023-02-27 01:37:46,[D] Do you think it's a good idea to first try some traditional statistical models when approaching a machine learning problem?,"Do you think we should give a try to traditional statistical models (e.g. Linear Models) before moving on to more complex machine learning algorithms when approaching a machine learning problem? IMO, traditional statistical models give you more space and flexibility to understand your data. For example, you can do many tests on your models, calculate different measures, do some diagnostic graphs, etc...

What do you think? Would love to hear your opinion.","[""It is absolutely meaningful and useful to make baseline comparison models, and to report results for more complex models relative to the baseline. \n\nMy experience has been that you can get about 80% of the way there (or more) with a simple linear regression / logistic regression / lookup table etc. That has been a sobering experience for me; I'm much less enthusiastic these days.\n\nMy conclusion has overall been to emphasize the large-scale organization of data and results. Try to get the big picture straight, then look for specific places where a specific model is called for. Good luck and have fun."", 'your first stab at a predictive modeling solution should always be some sort of moderately naive solution that can be achieved quickly. Operationally, this accomplishes several things:\n\n1. It quantifies the ""low hanging fruit"" in the problem. If you are able to capture enough of the variance with your naive model, there might be significant diminishing returns investing more time and effort in the problem. The naive solution might be good enough for your needs. Congratulations, the problem was easier to make headway on than you anticipated.\n\n2. The quick and dirty solution gives you a benchmark to measure against. You shouldn\'t just care how good your sophisticated model is: you need to justify the time you put in doing it the hard way by demonstrating how much more effective that approach was than modeling the problem an easier way might\'ve been.', ""Large casualty insurer's risk models are an example. Models that estimate the expected loss on insurance contracts, and directly feed into pricing them.\n\nThese models are still, in general, generalized linear models, for regulatory reasons."", "">My experience has been that you can get about 80% of the way there (or more) with a simple linear regression / logistic regression / lookup table etc. That has been a sobering experience for me; I'm much less enthusiastic these days.\n\nFor context.\n\nIn some industries 0.1% of the way is millions of dollars"", 'Transforming individual features to be palatable and well scaled for a NB or linear model should be the baseline.  \n\nI had such a problem, and with some clever algorithms for transformation of the univariate features plus an appropriately constrained linear model, the final one with a small number of features beat a tree ensemble on original data, and likely would be more stable through time.   The setting was a regulated application where the model had to perform stably many years without change.\n\nOften complex models might outperform in time, but out of time they can collapse. \n\nThe simple models can still be the core of the solution with isolated addons. With human effort, the nonlinearities might be feature engineerable back to new features which work in the linear model.\n\nPadawan:  uh stuff it into excel and fit a line\n\nNew Data Jedi: autoML with latest tree ensemble tweaks with self supervised deep learning for hidden representations, we need 200 GPUs to train\n\nData Yoda: wise features and logistic regression it is']",58,36,https://www.reddit.com/r/statistics/comments/11cnctc/d_do_you_think_its_a_good_idea_to_first_try_some/
925,2023-02-27 01:28:05,"[Q] If I have a high sample size (2500 data per group of a total group of 3) and the data have no normality and unequal variances, should I then apply Kruskall wallis? Or what hypothesis testing method should I use in those cases where sample size is huge?","I have a time series data where at certain point the time series gets stabilized, what I want to do is prove statistically that in this stabilized zone, these 3 groups are different between them, yet I cant do an anova since the data dont have normality and their variances are unequal.

I heard that for these cases Kruskall wallis is appropiate, yet the data is huge and havent read what hypothesis testing method use when the number of data is huge.

I am plotting the trayectory data in root-mean-square deviation against time and in an interval of time the trayectory is stabilized and I want to prove statistically that these groups are different in this stabilized zone.","[""> yet I cant do an anova since the data dont have normality...\n\nThe lack of independence is probably a far bigger problem than the lack of normality.\n\nYou haven't explained what these data actually are, or *how* you're trying to compare the groups (are you specifically interested in *mean* differences? Then Kruskal-Wallis is definite the wrong choice), so it's hard to give advice. What exactly are your data and research question?"", ""Normality is the least of your worries here.\n\nYou don't have independence and you say yourself that you don't have identical distributions, both of which are more critical. \n\nThe data are also matched on time, which you seem to be ignoring. With that in mind, the marginal distribution of the data are not really relevant."", 'Im studing a covid spike protein and mutations on the covid spike protein.\n\nI calculated the root-mean-square deviation of the sequences of this protein which forms a time series of the values in Angstrom and time in nanoseconds.\n\nI identified a stabilization zone where you can distinctly see that all these groups differentiate, now I want to prove it in a statistic way yet the data which is about 2500 per group, dont have normality nor variance.\n\nMy research question is if in the stabilized zone the mean in angstroms of the mutated spike proteins differ from the mean in the no mutated spike protein.\n\nExample would be like this image (a) zone.\n\nhttps://pubs.acs.org/cms/10.1021/acs.jcim.2c00562/asset/images/medium/ci2c00562\\_0008.gif\n\nI wanted to calculate the mean and put in my findings that these groups have a different mean but the mean doesnt have normality nor variance and I cant say in a statistically way that they are different, I know anova is robust for non normality but only if the variances are the same, which is not the case. So I believe my only option is using the median, yet I am not too much into statistics and dont know if there is a theorical obstable for this, if I am violating an assumption, I dont know, or if there is another hypothesis test that I can use which I dont know or other analysis. I am not specialized in statistics, thats why I ask.', 'Here is an image like what I am doing, in the picture (a)\n\n[https://pubs.acs.org/cms/10.1021/acs.jcim.2c00562/asset/images/medium/ci2c00562\\_0008.gif](https://pubs.acs.org/cms/10.1021/acs.jcim.2c00562/asset/images/medium/ci2c00562_0008.gif)\n\nfrom 100 ns to 200 ns stabilized zone, I want to say in a statistical way that they are different.\n\nYou can clearly see in its trend they are different, but how to say it in a statistically way?\n\nAre there special hypothesis testing from time series which I dont know?\n\nI wanted to use an anova but the assumptions fail.\n\nI know time affects independence, but once reached the stabilized zone it is supposed that time doesnt matter anymore nor affects the variable since it is now stabilized.']",2,4,https://www.reddit.com/r/statistics/comments/11cn2to/q_if_i_have_a_high_sample_size_2500_data_per/
926,2023-02-26 23:54:49,[E] ISL second edition solutions manual wanted,Anyone able to get their hands on what seems to be an extremely precious (if at all existent!) commodity and is willing to share? thanks :),[],2,0,https://www.reddit.com/r/statistics/comments/11ckjto/e_isl_second_edition_solutions_manual_wanted/
927,2023-02-26 17:02:34,[Question] ways of calculating R^2," First time posting here, I'm confused about the ways of calculating r-squared, I've read some articles regarding r-squared and how to calculate it. some says R2 is just pearsons R squared, which makes sense. Also, I could just use this formula **R2** **= 1 - (RSS/TSS)**. Usually, I just calculate r2 using pearsons r. does using the pearsons r method to get the r2 applicable for multiple linear regression? because you cant calculate pearsons R with multiple independent variable. I'm just a student.","['For linear regression all those formulas should give the same value.\n\nFor generalized linear regression, for example logistic regression different definitions lead to different formulas that also give different values.', "">For linear regression all those formulas should give the same value.\n\n.... but only in the case where the regression has exactly one independent variable and a constant.\n\nAs the OP notes, with multiple independent variables, you've got multiple Pearson's r."", 'Yeah as far as i know, R² is just r² in linear regression. In mulitiple lineare regression you add the increments in R² for each new predictor, using semi partial correlation. Dont know if this was what your asking, im first year statistics too. I can also look up the more percise formula if you like. Cheers :)', 'R is equal to the r obtained when a linear combination of predictors is chosen to maximize r. You get the same coefficients (up to an arbitrary multiplier) as you get when you minimize squared errors.']",9,4,https://www.reddit.com/r/statistics/comments/11c9ozm/question_ways_of_calculating_r2/
928,2023-02-26 10:35:23,[C] Bachelor in Statistics,"I would like to know what do you think about a about in a bachelor in statistics for both, industry work and academy, and how does this compares to a bachelor in math","['Statistics is severely underused in the business world and I think a lot of businesses are starting to realize this. \nI work for one of the largest retailers in my country and there are very few statisticians on staff. \nI end up taking on that role often for other teams.', ""A bachelors of stats is much more useful than a bachelors of math. As a statistician you can provide value to many businesses using stats skills. The world is only getting more and more data and there is still value in analyzing it. A Stats degree is especially useful if you combine this with a little CS (e.g. you know SQL, how to scrape and wrangle data).\n\nMaths isn't so useful in and of itself until you get to the PhD level, unless you combine it with something like engineering."", 'I only graduated 1.5 years ago, so haven’t got much to go on, but jobs weren’t easy to get. \nNone the less, I got hired as an analyst, but when they discovered how much value I could bring outside of the roll they hired me for, things started to ramp up. \n\nI will find out in a month or so about a raise. \nI’m Expecting a 30% raise over the next 3 years.', 'Asking this question in a stats forum brings certain biases — I’m surprised no one has yet to mention this given the sub. \n\nI’m going to go against the grain and say a bachelors in stats is as (un)employable as a bachelors in math — because jobs where you’ll  be expected to do reasonably technical work involving stuff you learnt in undergrad will generally require graduate degrees.\n\nI’ve personally found the maths curriculum to be quite enjoyable due to its variety. We have topology, number theory, abstract algebra, complex analysis, various flavours of geometry, harmonic analysis, etc. each of them it’s own world, and often you’ll have enough electives to cover most of the required/important stats courses. \n\nIt might be helpful to glance at the course catalogue for both departments, select all the courses that interest you, and let the courses tell your whether to do maths or stats degree.', 'I graduated with a Bachelor’s in Statistics last year. As far as finding a job. I had two offers for work pending graduation about 7 months before I graduated. Meanwhile I had classmates who also where in the same program as me with no job prospects. I find that my most useful resources where school career fairs, networking, and persistence.']",8,23,https://www.reddit.com/r/statistics/comments/11c351a/c_bachelor_in_statistics/
929,2023-02-26 07:32:46,[Q] Distribution of Regression Coefficients (Bayesian vs Frequentist),"I was having a conversation with a coworker regarding the difference between a frequentist and bayesian regression formulation. My thought was, that in bayesian modelling, all coefficients in a regression are random variables that follow some posterior distribution, while in the frequentist model the coefficients are point-estimates of the population parameter. My coworker pointed out that the coefficients of a vanilla frequentist regression follow a t-distribution, and therefore my differentiation didn't hold.

While my coworker may be technically correct, my intuition tells me that there is something more to this. For instance if you were to make a prediction in a frequentist model, you would use the mean beta(s) to calculate the response y, whereas if you were to make a prediction with the bayesian model you would sample from the random beta variable(s) & use those samples to calculate a posterior prediction. In the bayesian case it seems we are actually treating betas like random variables, whereas in the frequentist modelling we are not.

Is there something i'm missing here? What is the technically correct differentiation between this bayesian model and frequentist model?","['> My coworker pointed out that the coefficients of a vanilla frequentist regression follow a t-distribution, and therefore my differentiation didn\'t hold.\n\nYour co-worker is mistaken; under the usual assumptions, estimated coefficients follow a normal distribution (though with unknown variance) not a t. However, you\'re not comparing like-with-like anyway. *Conditional on the data* the estimated coefficients are not themselves random variables, while in Bayesian statistics, the conditional posterior distributions for the parameters are. So when you condition on the data for both, your original distinction holds up  --- but it\'s not clear to me what the point of the distinction is.\n\n> What is the technically correct differentiation between this bayesian model and frequentist model?\n\nYour use of the word ""the"" there confuses me. I would not label that distinction you made with ""the"". Possibly with ""a"", in some situations.\n\nThe fundamental difference is simply how you treat probability (as a notionally infinitely long-run frequency or as a formal way to summarize relative degrees of belief). Everything else pretty much follows from that distinction.', '>For instance if you were to make a prediction in a frequentist model, you would use the mean beta(s) to calculate the response y, whereas if you were to make a prediction with the bayesian model you would sample from the random beta variable(s) & use those samples to calculate a posterior prediction.\n\nThis seems to mistake estimating the mean function **E**(y|**x**) with estimating quantiles of ""future"" y values, which are not the same thing.  Sure, if you allow the Bayesian analyst to give you estimates from the entire *posterior predictive distribution*, and you only allow the frequentist analyst to give you the estimate of the mean function, the first will appear to be using more uncertainty than the latter.  But the analogous ask is for the Bayesian analyst to give you the MAP estimate or posterior predictive mean or something similar.', '>The coefficients of a frequentist regression follow the t-distribution   \n>  \n>across realizations of the regression.\n\nnot true.\n\nthere are subtle things that are being confused or misunderstood. \n\nFirst thing : regression coefficient are parameters. In frequentist approach those parameters are fixed and unknown. In a Bayesian paradigm those are random variables.\n\nEstimators: when you fit a model in the frequentist approach you do so by calculating estimators. Those estimators are random variables which indeed have a distribution (t-distribution in this case). In the Bayesian context you use as estimator some parameter of the posterior distribution, like the expected value or the median.\n\nlet us set this clear with an example: estimating the expected value. in the frequentist approach you use the mean, and as you may know, in the case of normal distribution the mean have t-student distribution that you use to make confidence intervals and hypothesis testing. In this case, the expected value is the parameter (fixed and unknown), the mean is the estimator (with a distribution which allows us to derive CI and tests). I will let you to complete this example in the case of the Bayesian estimation procedure, you can find it elsewhere.', "">My coworker pointed out that the coefficients of a vanilla frequentist regression follow a t-distribution, and therefore my differentiation didn't hold\n\nESTMATES of the coefficients follow a distribution, not the coefficients themselves in a frequentist framework"", 'For the frequentist the regression coefficients are unknown fixed quantities (they do not have a distribution), the *estimates* of the regression coefficients are random variables, and follow a t distribution, assuming you also had to estimate the variance.']",36,21,https://www.reddit.com/r/statistics/comments/11bz8hk/q_distribution_of_regression_coefficients/
930,2023-02-26 05:16:52,[Q] What test to use to determine similarity between data sets,"I would like to compare two data sets representing self-report and partner-report values. We both took a personality test (I know they're BS but it was in good fun) and then took the test again through the lens of the other person. We want to see ""who knew the other person better"". Each test gave numerical values for 5 categories, then broke down each category into 5 subcategories with numerical values. I'm not sure how to phrase this next part, but it looks like I got higher values on both tests I took (rating myself and them). Is there a way to compare that as well? Formulas that could easily be entered into excel would be bonus. Thanks!!!",['Mean squared error maybe. \nConsider your attempt at answering for her as “predicted” and her answers as actual. \nDo the same for her and then compare MSE'],1,1,https://www.reddit.com/r/statistics/comments/11bvztb/q_what_test_to_use_to_determine_similarity/
931,2023-02-26 03:17:19,[S][R] Hidden Markov Model implementation in R and Python for discrete and continuous observations.," Hidden Markov Model implementation in R and Python for discrete and continuous observations. I have a tutorial on YouTube to explain about use and modeling of HMM and how to run these two packages.

Code:

[https://github.com/manitadayon/CD\_HMM](https://github.com/manitadayon/CD_HMM) (in R)

[https://github.com/manitadayon/Auto\_HMM](https://github.com/manitadayon/Auto_HMM) (In Python)

Tutorial:

[https://www.youtube.com/watch?v=1b-sd7gulFk&ab\_channel=AIandMLFundamentals](https://www.youtube.com/watch?v=1b-sd7gulFk&ab_channel=AIandMLFundamentals)

[https://www.youtube.com/watch?v=ieU8JFLRw2k&ab\_channel=AIandMLFundamentals](https://www.youtube.com/watch?v=ieU8JFLRw2k&ab_channel=AIandMLFundamentals)","['Cool.  Does it work for a single time series or multiple observations with many subjects (like a mixed model or multi-level model)?', 'How does it handle the heterogeneity?', 'It does handle multiple observations.', 'Great job! Do you have others?', 'Oh spammer, please go away. Funny how the sockpuppet replies are always the same. ""Great work! How do you handle <detail xyz>? Can I subscribe to your channel?"" Can\'t wait until spammers are using ChatGPT to generate 500 word responses, that\'s going to be glorious and/or disastrous.']",28,5,https://www.reddit.com/r/statistics/comments/11bt3hh/sr_hidden_markov_model_implementation_in_r_and/
932,2023-02-26 02:47:56,[Q] Probability of 2 events happening together given a set of probable events,"My friend and I are making a maze game where we want doors to close at random as you play. Essentially every minute a timer goes off and sets off a probable chance that a door will close.

We want to know what the statistical chance of any 2 doors closing at the same time is. We both struggle with stats and couldn't solve the question so here is the question:

Given a set of 20 doors, each with a 1/10th chance to close, what's the probability that exactly 2 doors will close?

I tried mapping this to another problem: ""Add the numbers 1-10 to a hat and draw a number at random 20 times with replacement. What's the probability you will get 5 exactly twice?"" But I wasn't sure if this is an equivalent problem or not. I couldn't solve it either.. stats is my weakest subject.

How would this be solved? We would like know so we can fiddle with balance of the maze so we would like to see how this specific problem is solved, but also the general approach. What if I wanted to know exactly 3 doors? Or what about 3 or more doors and not just 3?

Thanks a bunch for any insight!","[""the formula for the binomial distribution is exactly what you're looking for: [https://en.wikipedia.org/wiki/Binomial\\_distribution](https://en.wikipedia.org/wiki/Binomial_distribution)\n\nintuitively you can think about it this way, you have 2 doors that close and 18 doors that do not close so (1/10)\\^2(9/10)\\^18 \\[independent events so you just multiply there probability\\] but thats not all, you also have to consider every combination of 2 doors opening and 18 doors closing as well so you multiply it by the number of ways you can choose 2 from 20 objects which gives you the probability of exactly two doors opening. \n\nthis might be easier to think about in the three door case, if you are looking at the probability that exactly one door is open, then do the same thing and recognize that either the first door can be open or the second or the third (3 C 1)."", 'This sounds like Bernoulli trials. The probability of exactly two successes in 20 trials given P_success = 0.1. We solve this by saying 0.1^2 * (1-0.1)^18 * 20!/(18! 2!) ~= 0.285 or about 29%', ""If you're good at coding but not stats, a solid thing to learn is how to simulate things with random numbers. It allows you to get the answer to arbitrarily complex stats problems without very complex maths.\n\nFor this problem, we can represent a door by drawing a random number between 0 and 1. If it's less than 0.1, the door is closed, if it's greater it remains open. That gives it a 1/10 chance of closing.\n\nFor twenty doors just pull 20 random numbers and count how many are less than 0.1. That's how many close in one simulation.\n\nTo get the chance of exactly two doors closing when twenty have a 1/10 chance: do the above simulation thousands of times and count what fraction of the time you get 2 doors closing.\n\nIt's super simple and whilst this problem is easy to solve analytically, simulations don't get much harder than this but some problems can get extremely hard to solve analytically."", ""Thank you! I'll research this a bit more to understand it better. I ran some sample generation and got around that value, but I didn't know how to calculate it"", ""awesome thanks! Yeah I have a math degree too which is funny, but I'm just horrible at stats. The way of thinking to me is so different than proofs. I feel like my intuition is always wrong for how to start a stats problem or how to visualize it. Thanks for this explanation""]",6,6,https://www.reddit.com/r/statistics/comments/11bsef1/q_probability_of_2_events_happening_together/
933,2023-02-26 01:29:13,[E] Why is Linear Algebra required for Statistics?,"I am enrolled in a MS in Applied Statistics program that did not require Linear Algebra. Most of my classes are heavy on the ""applied"" part of statistics where we are using code to perform statistical testing/analysis etc. The Statistics and Probability courses we are required to take are particularly difficult though. Most of the computations just utilize a lot of calculus, but the concepts are very tough to grasp.

I've not taken Linear Algebra so I am curious about if having taken that would have helped my understanding of the concepts, and in particular which concepts would be clearer with that background?","['Linear algebra might be the most important traditional math course for understanding many modeling techniques. Your data and coefficients are a bunch of matrices.', ""The reason it's important OP is because when you take all the equations you know from basic statistics, and try generalise them to multiple dimensions, you need vectors. Instead of a single slope for linear regression, you have a vector of regression coefficients for each variable.\n\nLinear algebra gives us mathematical tools to deal efficiently with all those vectors. It's the next step up from high school algebra.\n\nAnd the reason we care about multivariate problems is because in statistics nothing is ever just a function of one thing. Everything is an interplay. Univariate statistics is not enough."", ""Yes - I can't imagine doing a proof in stats without linear algebra under your belt."", 'Or implementing a model without using matrices', 'Years after starting my PhD I finally got around to learning linear algebra and I thought two things: 1) this is actually more straightforward than I thought and 2) where the hell have you been all my life?']",39,42,https://www.reddit.com/r/statistics/comments/11bqhfw/e_why_is_linear_algebra_required_for_statistics/
934,2023-02-25 12:52:34,[Q] What is a test to see if two groups of different dependent variables with a shared independent variable are possibly related?,What is a test to see if two groups of different dependent variables with a shared independent variable are possibly related?,"['Maybe look into the theory of causality and Bayesian DAGs for this one. I don\'t believe there is a simple test because it\'s hard to have access to the set of all possible sources of cause.\n\nOtherwise if it\'s simply correlation check you\'re after look at the sample data for p(a|b) and p(c|b) and use any relevant correlation check where ""b"" would be your ""shared independent variable"" and a and c are your groups of variables. \n\nYou can for example check for correlation between a and b and c and b, then between a and c and ""you can conclude"" b could be a common factor of effect between a and c somehow.  \n\nIt\'s really hard to give a precise answer because the writing of the problem is not so precise', 'A simple correlation matrix could show you if they have any relation to each other.', 'You might want to look into canonical correlation analysis. But, from your phrasing, you sound fairly new to stats so that may not be an option. Other suggestions here are good.', 'A simple test: suppose your two dependent variables are X1 and X2, and the independent variable that ""drives"" both is Y. \n\nIf I(X1;X2|Y) > 0, then there is dependency that exists between X1 and X2 beyond that which is provided by Y. \n\nYou can significance test this by shuffling Y a large number of time and doing a NHST of the true value against the null distribution.\n\nNB: this can get complicated if X1 and X2 also ""cause"" Y. If you can assume that X1 <- Y -> X2 then you\'re fine, but if X1 <-> Y <-> X2, then conditioning can get really difficult.', 'Maybe PCA would help? You could also use information values to see if a dependent variable can help explain the other one. My fair guess would be that, due to them sharing an independent variable, you kinda have to look for a somewhat powerful IV.']",14,5,https://www.reddit.com/r/statistics/comments/11bcdpr/q_what_is_a_test_to_see_if_two_groups_of/
935,2023-02-25 11:37:42,[D] Ordered vs un-ordered axes? Weird question.,"Ordered vs un-ordered axes?

My only point is to do a comparison, as to why if i plot a graph for ordered or un-ordered(randomly ordered) x axis, the former depiction is termed ""correct"" and latter termed ""wrong"". Why must ordering affect the interpretation of relationship between these two variables? Also, if i were to ever use this un-ordered x-axis to plot norm dist, would the area under a normal dist sum to 1? Would it even be a normal dist or just a meaningless cloud.
I guess I'm asking, why favour the ordered distribution? Because at the end the pdf or cdf or edf dictates the area under curve, and we know, changing the order of x axis changes the distribution shape, then why must we be biased toward saying the ordered x axis along with its respective probability density must give the correct depiction everything else is not accurate?","['You use plots to help make sense of data. In the vast majority of cases, an unordered x-axis will not help you make sense of anything.\n\nInstead of musing in the abstract, make some plots and see for yourself.']",4,1,https://www.reddit.com/r/statistics/comments/11bay6w/d_ordered_vs_unordered_axes_weird_question/
936,2023-02-25 10:54:41,[Q] Working with a known type II error rate,"I'm having a bit of trouble getting my head around this. I have two datasets. The first has my dependent variable and the explanatory variables from data collection with 18,000 participants. The second has a sample of participants where we double checked the validity of the dependent variable. We identified a 25% type II error rate.

Any suggestions on how we can proceed? Right now I'm at the brute force method of just randomly removing 25% of negative cases. Surely there's a better way, right?

[Edit for more details:]
In our large sample dataset, we sent a message to phone numbers we suspected used WhatsApp. 

If the message was marked as sent/read these were confirmed WhatsApp users. If it did not, we initially recorded these as NOT WhatsApp users.

However, when we explicitly call them to confirm the negative outcome, we find that 25% of those we call have WhatApp, but use a different phone number.

So, we can estimate (for now) that 25% of our large sample negative outcome cases are in fact positive.

There's no need to confirm our positive cases because we have a read receipt.

It's a study on factors that determine WhatsApp usage in developing countries by the way.","[""> We identified a 25% type II error rate.\n\nCan you explain what you mean by this sentence in completely explicit terms, in detail, please? I don't see how it's possible to do what the words say, which makes me wonder if you mean something else."", '""Type II error"" is not a synonym for ""false negative"". That\'s why it has a different (and woefully clunky) name.\n\nType II error is a concept used in designing experiments. It\'s the probability of missing a difference at significance level alpha if there is a true underlying difference of delta (D). It\'s conditional on there being a meaningful difference (D) to find, which is why we don\'t call it the false negative rate because that depends on the probability of there being a difference to find.\n\nThe false negatives in your dependent variable are not likely to be random; that is, people who use a different number for WhatsApp are likely to be different from people who don\'t use WhatsApp at all. So just arbitrarily removing some negatives won\'t remove the bias.\n\nIf you don\'t have the resources to systematically check all the negatives, I\'d start by working out how the false negatives are different from the true negatives and ponder from there.', ""Sure. In our large sample dataset, we sent a message to phone numbers we suspected used WhatsApp. \n\nIf the message was marked as sent/read these were confirmed WhatsApp users. If it did not, we initially recorded these as NOT WhatsApp users.\n\nHowever, when we explicitly call them to confirm the negative outcome, we find that 25% of those we call have WhatApp, but use a different phone number.\n\nSo, we can estimate (for now) that 25% of our large sample negative outcome cases are in fact positive.\n\nThere's no need to confirm our positive cases because we have a read receipt.\n\nIt's a study on factors that determine WhatsApp usage in developing countries by the way."", 'Okay, so you have\n\n- a known sample of WhatsApp users\n- a contaminated sample that (you estimate) contains 25% users, 75% non-users.\n\nCan you use that to correct for the estimation bias caused by the use of the contaminated sample?']",1,4,https://www.reddit.com/r/statistics/comments/11ba2yg/q_working_with_a_known_type_ii_error_rate/
937,2023-02-25 08:41:23,[Q] Misuse of p-value wiki article is bugging me,"item 2:

https://en.wikipedia.org/wiki/Misuse_of_p-values#Clarifications_about_p-values

""The p-value is not the probability that the observed effects were produced by random chance alone.""

The text after it doesn't correct the sentence. In fact, I think the text after it is pretty useless and confusing. 

As we all know, the p-value is the probability of obtaining the observed result or a more extreme one if Ho is true.

But isn't ""by random chance"" a perfectly valid layman's way of saying ""if Ho is true?""

Doesn't that mean the sentence becomes correct as long as you change it to as follows:

""The p-value is the probability that the observed effects (or more extreme ones) could be produced by random chance alone.""

If so, I think they should at least add the above sentence to the wiki.","['> But isn\'t ""by random chance"" a perfectly valid layman\'s way of saying ""if Ho is true?""\n\nNo, it doesn\'t automatically imply that; random chance operates under both hypotheses. Indeed lay people tend to specifically *not* interpret it that way, which are largely the people the article is for.\n\nYou would need to specifically talk about the operation of random chance along with H0 being true -- indeed, note that H0 is not necessarily a *nil* null even if you would like to interpret it to be one.\n\n> I think they\n\nWho is ""they"", exactly?', ""> but in frequentist statistics a p value is never a probability\n\nIt is definitely a probability. When the null is true, the test statistic has a specific distribution, and the p-value is the probability, under that distribution, of observing a test statistic greater than or equal to the observed value. Note that this interpretation doesn't depending on whether you're a frequentist or a Bayesian."", 'As I already pointed out in my previous comment, not all nulls are ""nil""; that is, you can have ""real phenomena"" under both H0 and H1.', 'Your last comment highlighted the problem of implied things - because you showed that you are implying the wrong thing.\n\nYour null hypothesis doesn\'t have to be ""no significant relationship"". It can be anything. It can be ""this drug is equal to this other drug"". It can be ""the probability of this process is as expected from theoretical predictions"". A different null hypothesis will lead to a different p-value, so it matters what you use.', '1. The null hypothesis is absolutely integral to the definition of a p-value.  ""By random chance"" does not acknowledge the critical role of the null.\n2. A p-value is not a probability unless you know the true distribution; we wouldn\'t be doing hypothesis testing if we knew the true distribution. I believe that Bayesians treat it somewhat differently, but in frequentist statistics a p value ~~never a probability~~ *is never a probability the observed result (or the null) is true*. Instead frequentists call it a likelihood.\n\n[https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/)\n\nThere are extensive debates on how to describe the p-value so that laymen can understand it. The lack of precision about p-values is a common frustration among formally trained statisticians, and a regular cause for concern about non-statisticians using statistical models.\n\nThat said, there\'s a pretty good chance I misspoke about the true definition of a p-value in this comment. Hopefully a statistician with even closer ties to formal statistics will point it out.\n\n(Edited to continue my caveat in the first sentence of point 2 to the second sentence, because indeed, I was insufficiently precise.).']",0,22,https://www.reddit.com/r/statistics/comments/11b7723/q_misuse_of_pvalue_wiki_article_is_bugging_me/
938,2023-02-25 06:48:07,[Question] Which is the right statistical test for a cost and benefit study?,"Hello fellow redditors,

I am collecting data about businesses and their costs and benefits that are associated with implementing sustainable practices in their business. 

I was wondering, which would be the right/best statistical test to use for a study of this type? 

Thank you in advance.","['I think this is bordering the field of decision analysis. David C Skinner has a nice introductory text. Hope you get something to guide your steps there.', 'Thank you 🙏🏻', 'You could test the hypothesis that the net benefit is more than a trillion dollars', 'Tests are consequences of hypothesis. No hypothesis no test. So there is ""not right test for the problem"" because you are not formulating any hypothesis. Of course ""tests"" are only one tool you could use for your problem.']",0,4,https://www.reddit.com/r/statistics/comments/11b4ja6/question_which_is_the_right_statistical_test_for/
939,2023-02-25 06:18:54,"[C] As a statistics major interested in the health sector, should i go for a minor in public health or bionformatics?","Hi! I'm currently a Stats majour and i've been considering adding a minor. I'm interested in both working in the medical field or environmental field as a statistician or analyst. the issue is that i can't afford to go to grad school any time in the near future and plan to move from the east coast to west coast after graduation, so getting a job right out of school is important for me. at the same time, i know how some employers tend to be wary of new grads with only a bachelors in stats.

 i've already completed 4/9 classes required for the public health minor (did them in high school). the bio-info minor at my school is about 24 credits and has courses like coding in python, mathematical bio, and independent research (in genetics iirc) which have their own set of pre-reqs. i'd say the bii-info degree might be more useful; however, there's the emphasis on genetics which i'm not totally invested in. what are your thoughts?","['data analyst in medical and public health research here. I took bioinformatics in college and have never applied any of it. I found it very computational and specific. I think you can’t go wrong with public health as it goes hand in hand with stat, especially if you’re already interested in it!', 'Also a DA in in healthcare, have my MS in Bioinformatics.  I agree and think that public health might suit OP better.  That said, I really enjoyed studying bioinformatics.  It may be a lot more wet lab oriented depending on where you go to school.', 'Bioinfo is a pretty specialized field. Most bioinfo jobs would require ms or phd. If you don’t want to become a bioinfomatician, just learn some python/coding skills on the side. Don’t bother with the degree.', ""Thank you! I've been seriously considering and plan to speak with an advisor on a combination degree :)"", 'If you aren’t particularly interested in bioinformatics than go for the Public health minor. Public health is a wide field and there are never enough statisticians.']",3,6,https://www.reddit.com/r/statistics/comments/11b3umb/c_as_a_statistics_major_interested_in_the_health/
940,2023-02-25 04:02:44,[Q] Admitted to Claremont Graduate University and University of Washington for MS Statistics - Difficulty Comparing and Choosing," 

Hello Statisticians,

I recently was admitted to both Claremont Graduate University MS Statistics & Machine Learning and the University of Washington MS Statistics - Advanced Methods and Data Analysis.

I am going into graduate school with the intention of academically preparing for potential pursuit of a Ph.D. in Statistics. If possible, I would prefer to work for 1-2 years before then proceeding to my Ph.D. Presently, I am having difficulty comparing the two programs in terms of academia preparation and in terms of career prospects.

I was wondering if anyone here has experience with Claremont Graduate University or the University of Washington?

I would love to hear about your thoughts!","[""Don't know much about Claremont. But I know UoW is a good school. Lots of interesting research and great lecture notes I've pulled from them. I have my MS but plan on submitting a PhD app there eventually."", ""Consider the lifestyle of each city too. In Seattle, you won't need a car to get around most of the time and there's a lot happening there to do in your free time"", 'As a UW alumnus I\'m biased, but it is an outstanding program and you won\'t lack for opportunities for pure and applied research (outside of the fact that an MS is very time constrained compared to a PhD).\n\nSome things to consider:\n\nUW ranks in the top 20 largest universities in the US, and is about 25x the student population of Claremont. \n\nUW consistently ranks among the top 5 universities for winning grant funding and in the top 2 for NIH funding (if interested in health/bio work).\n\nUW has a large number of micro grants, incubators (both technological and social), as well as mentorship and spinupp/startup programs across the university, some wholly within an academic/student body, some via private/public partnership. This is especially true of STEM departments which maintain close relationships with industry.\n\nBeyond The UW\'s statistics program (7th), the university holds high ranking colleges and programs across STEM.\n\nSeattle is home to multiple global leaders in ML research and implementation and not slowing down. Similarly, world class participants in health statistics, civil and industrial engineering, nautical engineering, aero-space, weather, physics, and others. Depending on your desired application area, the city alone would be why to come.\n\nTogether, the above means there is no comparison in opportunity of exposure to projects and access to resources and expertise.\n\nHowever, I would more consider the culture of the two institutions. I find The UW to be on the far end of requiring an ""entrepreneurial mindset"" in order to succeed. You won\'t likely be handheld into a lab, project, and thesis. Faculty are friendly (though time constrained), and you can approach them and ask all kinds of things, but you need to think for yourself about what you need and what you will ask for in any meeting. With few exceptions, your thesis work will be entirely your baby, and you will need to discover and plan out which skills you need to build, and then find classes, labs, and projects to participate in in order to build those skills and push your thesis and career forward.\n\nFor people who are comfortable doing their own leg work to build their support networks and project with little or no guidance, UW is extremely effective. But for people who want strong guidance, or to function in a team/system and leave with a degree, UW will leave them lost and feeling unsupported (as well as abused, since they certainly will be able to work in a lab, but a thesis won\'t come directly from that.) I believe the UW model is particularly harmful to people from backgrounds that rely on positive reinforcement, high levels of modelling ""right"" solutions, and high levels of ""superfluous"" engagement beyond the ""business at hand"", and/or who expect that their managers/mentors will reliably guide the student to the student\'s end goals (a degree) if the student serves the mentor faithfully. This means biasing towards higher rates of failure for many from disadvantaged backgrounds, many foreigners, and first generation students. I think many STEM programs at The UW are rather oblivious to how extreme they are in this regard, but they know they are not the norm. I\'d say their CS programs are especially wracked by these problems, and so have an abysmal rate of retaining female students. \n\nAlso, the size and complexity of UW, combined with being a relatively under resourced state school (STEM departments get lots of money, but core academic funding is actually quite tight), means that navigating UW bureaucracy can be, at best, annoying, and sometimes Sisyphean. They are always working to better things, but they are also regularly downsizing administrative staff support, meaning there are a lot of things you will have to figure out and execute/do, on your own that many other schools may gracefully take care of for you, or at least be able to explain and navigate you through. You need to budget bureaucracy into the timeline for everything you do at The UW.\n\nIn return for all of this you will receive massive amounts of freedom and, where you can convince people to work with you or share/give resources, opportunity to execute pretty much anything you can imagine. \n\nAlso, Seattle is a baller city, and Washington is one of the highest functioning states in the county with financial, human, and natural resources galore and sound (enough) leadership and plenty of scrappy (and non-scrappy) disruptors to keep the wheel from spinning off.', 'If writing to faculty at The UW, Please note, it is absolutely not UoW. It is UW, or The UW.', 'UW’s stats department is ranked #7 on US News.']",32,44,https://www.reddit.com/r/statistics/comments/11b0ka2/q_admitted_to_claremont_graduate_university_and/
941,2023-02-25 03:57:40,[Q] Two confidence intervals. One is narrower than the other implies it follows a certain distribution ?,"Okay so, I have two 95% confidence intervals. One is obtained from the bootstrap distribution using quantile function in R with a confidence interval of ( 81000, 100000) and another is based on the asymptotic theory where I used the approximation of the estimator to be normally distributed since the sample size was large and the resulting confidence interval was much narrower like (89905,90094). Does this imply the bootstrap distribution is a normal distribution because the 95% asymptotic CI is much narrower than the 95% CI of the bootstrap distribution?","['No. Parametric models, because of assumptions, have less uncertainty. Whether the assumptions hold or not is another issue.', 'I dont see why that would follow.']",2,2,https://www.reddit.com/r/statistics/comments/11b0frj/q_two_confidence_intervals_one_is_narrower_than/
942,2023-02-25 02:44:27,[E] Gradient Boosting with Regression Trees,"Hi guys,

I have made a video on YouTube [here](https://youtu.be/lOwsMpdjxog) where I explain what gradient boosting is and how it works.

I hope it may be of use to some of you out there. As always, feedback is more than welcomed! :)",[],3,0,https://www.reddit.com/r/statistics/comments/11ayopm/e_gradient_boosting_with_regression_trees/
943,2023-02-25 00:27:14,[Q] How does one directly convert risk difference to risk ratio (also relative risk),I'm analysing a study in the medical field and struggling hard with this.,"[""You can't, unless you know some additional information.  If you know both individual risks, then that is easy.  If you know one measure of association (RR, RD, OR) and one of the individual risks, then you can convert.  But they are nonlinear functionals of two variables, so unless you know something else, you cannot do it.\n\nTo see this, note that if p0=.2 and p1=.4, then the RD = .2 and the RR is 2.  If p0=.4 and p1=.8, then RD = .4 and RR = 2.  So the same RR gives you different RD."", ""There's no magic formula and you can't do it directly unless you want to make it more complicated than necessary. To calculate RR you need the proportions on each arm, which might be reported directly, or indirectly, or not at all.""]",1,2,https://www.reddit.com/r/statistics/comments/11avehv/q_how_does_one_directly_convert_risk_difference/
944,2023-02-24 23:42:39,[Q] Bayesian Networks,"Hey,


I am currently working on a master thesis in
which I want to estimate heightmap values of soil dumped from an excavator
into the dumping region inside of a truck. I would like to predict the
height map value Xij(t) of the soil in a given 1cmx1cm cell ij inside this
dumping region, i.e. estimate the heightmap of soil inside the dumping
region. I decided to use a bayesian Network approach.



To do this, we measured 5 variables: Y1-Y5, which resemble variables such
as the dumping point, volume in bucket, soil distribution in bucket and
the velocity with which the bucket is opened. All these variables Y are
continuous and also the heightmap values Xij are continuous.



Moreover, we have observed values Oij which are values estimated from a
sensor on the excavator and we also measured the groundtruth data after
each dumping, let us call them Gij.



So, I handcrafted a probabilistical graphical model in which I have an
edge between Y1,…,Y5 to Xij, an edge from Xij to Oij, and also a time
dependency, i.e. the estimated value of soil height in a cell ij at time
point Xij(t-1) has an edge to Xij(t).



I have attached a visualization of the graphical model I handcrafted.

Now, I want to learn the conditional probability distributions between all
these variables and my heightmap values X. Since all my variables are
continuous, I would pick a continous distribution, e.g. Gaussian, and
learn the parameters of this cpd via mle. In order to do so, I would use
the groundtruth data G I recorded.

Then, since the next step would be to learn a posterior distribution and
P(X^t+1 given Y^t+1, O^t+1, X^t) is hard to estimate cause in the
denominator we have to build the integral over X^t+1, I thought about
using approximate methods such as MCMC or VI to generate predictions and
then measure goodness of fit via rmse.



Does this approach make sense from your perspective?



Best regards,
Ne","['Are regression tasks usually performed via Bayesian networks?', ""I dunno -- this seems overly complex, to the extent that it's going to be very hard to get to any results. Also, I don't see anything at all about the physics of the problem -- building in as much as you can about the actual mechanics of the situation is going to greatly constrain the possible results, and therefore is going to be a huge win computationally speaking.\n\nMy advice is to start out with a simplified model of dumping a scoop of incompressible granules on a flat surface. What is the shape of that pile? There has probably been work on that before. Now assume that the location is not fixed but distributed according to some distribution. How do the granules pile up after a number of scoops have been dumped? How does it change the picture to assume the scoops are not the same size but, again, distributed according to a distribution? \n\nAdditional physical considerations. What if the material is compressible? What if the granules are sticky? Etc.\n\nBy the way, if you work from a physical model, you will find there are free parameters which correspond to things in your world. E.g. the angle of repose of loose material. This is a very good situation to be in, because it means, above all, that other people will understand what's going on. Good luck and have fun.""]",1,2,https://www.reddit.com/r/statistics/comments/11aucxr/q_bayesian_networks/
945,2023-02-24 21:57:33,Intuition about standard errors and serial autocorrelation [Q],"I was reading a paper recently that said something along the lines of: ""if a panel data set demonstrates serial positive autocorrelation, unadjusted standard errors will be too small. This is intuitive because the model believes there is more information than there really is.""

This was in the context of DID regressions. Could you help explain this intuition for me?","['Autocorrelation creates strong positive (negative) effect sizes for a series of 30+ observations. At some point the direction flips to negative (or positive). And then it eventually flips again. The flips never cancel each other out, and the resulting short term and long term trends have nothing to do with x influencing y in any useful manner. \n\nSo you end up with spurious effects. They happen to have very small standard errors, because the “artificial” trend vastly outweighs the random fluctuations in the data.', 'Thanks so much, this is a good answer. So would the fact that negative serial autocorrelation leading to too high standard errprs be that any trends between X and y and obscured by the negative relationship of the errors?']",1,2,https://www.reddit.com/r/statistics/comments/11arzvu/intuition_about_standard_errors_and_serial/
946,2023-02-24 21:52:28,[Q] How to check randomness of a binary pattern?,"I have a simple problem, I have a strong of 1/0 that's 100 bits long. I expect the pattern to be random, and NOT 10101010etc (or 100100100 etc). Basically shouldn't have any noticeable random.  Expecting something random like 1001100001001101110010.


My question is , how can ""calculate"" the randomness of such string, with a known length of string, and known # of transitions (ie. How many transitions from 1>0 or 0>1). Just looking for a quick and simple idea and threshold.","['There is an amazing thread on how to do this here https://stats.stackexchange.com/questions/574333/how-can-i-determine-which-of-two-sequences-of-coin-flips-is-real-and-which-is-fa/574359#574359', ""Under pure randomness there's a 0.5 chance of transitioning in each spot and all subsequent spots are independent. There are 99 spots where a transition can occur so the number of transitions follows a binomial distribution with n=99, p=0.5. So for example random sequences will have between 40-60 transitions 95% of the time."", 'That was a great read, I wonder if OP will realize that their expectations of randomness is the opposite of actual randomness.', '>\tMy question is , how can “calculate” the randomness of such string, with a known length of string, and known # of transitions (ie. How many transitions from 1>0 or 0>1).\n\nIs the following string random:  0101010101010101010101 ? Likely not. To measure “randomness” you’d have to check distribution of 0 vs 1 also 00 vs 01 vs 10 vs 11 and so on for longer lengths. \n\n>\tJust looking for a quick and simple idea and threshold.\n\nIf you are programming a simple approximation is to apply the best  string compression function you have available and measure compression ratio. For **most** strings the compression ratio will be your best measure of randomness. There are exceptions but they likely do not matter for your use case.\n\nTo figure out a threshold generate a large sample of random strings and see how well they compress and pick your threshold empiracly based on how many truly random strings you want to reject. This will depend on the exact compression algorithm - compression algorithms are designed and optimized to efficiently spot patterns so why not use them for that purpose?', 'Look into Kolmogorov complexity and algorithm probability. As others have pointed out any binary string is as likely as any other when viewed  as a random variable, but when viewed as an object it’s algorithmic probability can differ']",23,17,https://www.reddit.com/r/statistics/comments/11arw5e/q_how_to_check_randomness_of_a_binary_pattern/
947,2023-02-24 21:42:43,[Q] Using marignal means to better understand three-way interactions and test specific hypotheses,"My question relates to the use of the emmeans package in R to better understand a three-way interaction in my linear mixed model.

 I have structured the dataset in such a way that each patient has four repeated measurements (t=0, t=0.5, t=1, and t=2 years) for each of the six domains of the BASDAI, resulting in a total of 24 observations per patient. 

 The variables in my model include:

* ""basdai"": the outcome variable, which represents a treatment response.
* ""gender"": the independent variable of interest
* ""time"": a covariate indicating the time of the measurement (t=0, t=0.5, t=1, or t=2 years).
* ""domain"": a variable indicating the domain of the BASDAI (1-6).

Research question: Does the magnitude of the sex differences over time vary in the components of the BASDAI after treatment?

To answer this question, I built the following model in R:

    library(lme4)
    library(lmerTest)
    mixed_final_model = lmer(basdai ~ 1 + gender + time + domain + gender*time + gender*domain + time*domain + gender*time*domain + (1 | ID) + (1| country), data = dat, REML = F, control=lmerControl(optimizer=""bobyqa""))

Now I am trying to better understand the results of the three-way interaction (gender\*time\*domain) by using marginal means of the emmeans package. Thus far, I have this code:

    emm_model5  <- emmeans(mixed_final_model, ~gender*time*domain, ref_level = c(domain = 1))
    comparison_sex_differences_domain <- contrast(emm_model5, ""pairwise"" , by = c(""domain"", ""time""))
    comparison_sex_differences_domain

This gives me the following [output](https://ibb.co/fxPpv9Z) for the first 3 domains (the other 3 not shown).  

I think the null hypothesis being tested in the contrast function is whether the mean difference between males and females at different time points within each domain is significantly different from zero. However, what I am interested in is whether the magnitude of the sex differences in domain 2 is different from that in domain 1 at each time point. For instance, I want to know if the mean difference between males and females at t=0.5 in domain 2 is significantly different from that at t=0.5 in domain 1. Similarly, I want to compare the mean difference at each time point in all other domains with that in domain 1. 

I hope I was able to clarify myself and that someone can explain to me how to test these hypothesis.","['To get at the three-way interaction you’ll need to run an additional contrast.  You’d do this the same way as your two-way contrast, but by “time” alone and using your two-way contrast as the input.', 'Okay thanks so I use emm_model5 as parameter of the contrast and by = time?\n\nWill try it out on monday. Could you explain why this would work? Pairwise comparison? Or would trt vs. Ctrl make more sense (to compare with domain 1?). Also, I thought mine was a three-way-interaction contrast because I included that in the first emmeans function as a parameter, but you are saying it is a two way contrast. Please explain.']",5,2,https://www.reddit.com/r/statistics/comments/11arou1/q_using_marignal_means_to_better_understand/
948,2023-02-24 04:42:29,"[Question] I see some people calling the analysis of confidence intervals (as opposed to p-values) ""Bayesian Statistics"": why?",What is the relation between confidence intervals and Bayes' Law?,"['Who do you see calling the analysis of confidence intervals Bayesian statistics?', ""Are you sure they're use *confidence* intervals and not *credible* intervals?"", 'Asking the real questions here. I have never seen someone refer to analysis of confidence intervals as Bayesian...', 'You might be seeing credible intervals (Bayesian interval for parameters) or even prediction intervals (which might arise in either paradigm) rather than confidence  intervals. Or the person might be confused.\n\nCan you point to an example?', ""I would say that the defining characteristic of Bayesian statistics is that the result is a joint posterior distribution of the parameters. From that, you can generate point estimates for the parameters, credible intervals for the parameters, and predictive distributions of new data from the same generating process, among other things.\n\nBayes's Theorem and Bayes's Rule are two ways to write a mathematical fact. Using either of them does not imply that you are doing Bayesian statistics.""]",5,21,https://www.reddit.com/r/statistics/comments/11a85us/question_i_see_some_people_calling_the_analysis/
949,2023-02-24 04:35:51,[Q] Techniques for spotting patterns in a sequence of events,"Not too familiar with statistics beyond a few requisite undergrad courses from years back, so I'm trying to figure out what techniques/terms I should be googling (if there are any) in order to achieve what I'm trying to do. If there are any statistics/programming libraries available to help me tackle this issue, I'd love links as well (mostly familiar with Python and to a lesser extent MATLAB, not too familiar with R).

I have a pseudorandom number generator that spits out integers from 1 to 5, one at a time. I'm free to generate as many numbers from it as I want to get as large of a sample size as needed. Numbers are recorded in the order I fetch them from the generator, that forms my sequence of numbers. I have no access to the code behind the number generation.

The suspicion is that this number generator is not truly random.

I'm trying to analyze this sequence of numbers I get from the generator and possibly identify any patterns that may exist in the sequence. Things like the following:

1. If there is a maximum number of times a number disappears from the sequence before it appears again. E.g. there is never a case where a sub-sequence (of say 20 numbers) does not contain a 3 in it at some point.
1. If there is consistently some number of events in non-overlapping consecutive sub-sequences (of the same length, if that makes it easier) where the number of occurrences of each number is equal. E.g. for non-overlapping consecutive sub-sequences (of say length 45) each number is always guaranteed to occur exactly 9 times.
1. If there is an abnormally high chance that the same number occurs consecutively. E.g. the same number appears three times in a row at abnormally high rates.

I'd like advice on how to determine whether results I get from determining the above are statistically significant, and how many samples I would need to take to determine that with a reasonable degree of confidence.

---

My initial thoughts for how to approach this were to just dump the sequence into a list, and repeatedly iterate through the list with a bunch of counters and keep track of all of the things I mentioned above. But this seems pretty brute-forcey, so I'm wondering if there's a better way to approach this.","['Statistically, there is no real way to test a sequence for ""true randomness"", as all patterns are uniformly equiprobable in an RNG. You can test specific kinds of randomness (e.g. frequency of each digit, or of all subsequences, etc) but any individual test would always have specific kinds of non-randomness that it can\'t detect.\n\nI would suggest the most powerful method would be to fight black-box with black-box, train a model to predict the next digit based on previous digits. If the model can perform better than random, then you have good evidence of some non-random pattern (although we couldn\'t say *what kind* of non-random).', ""All patterns are possible, some are more likely to occur by chance than others.  What's the reason for checking the randomness of the random number generator?  Would you learn more if you went to the source of the algorithm and code for that generator?  Whoever wrote it has had to withstand critique of its quality.  And, if you're writing your own, you'll get to learn about what those critics are looking for."", 'Gotcha, I\'ll definitely consider that approach! Though when evaluating the trained model, what would be the right way to measure ""better performance"" in this case?', ""The generator is a black box. The goal is to test if the generator is truly random or not.\n\nI know all patterns are possible with a true random number generator. I'm trying to verify if the one I'm working with is one or not. If it isn't a TRNG, I know it's likely to follow one or some of the patterns I listed."", 'Accuracy > 1/N, for N possible characters (i.e. 10% if the RNG is giving digits 0-9).\n\nIf you are really able to generate a large number of examples for testing, you should have a good idea of the true accuracy and whether or not it converges to 1/N.']",1,7,https://www.reddit.com/r/statistics/comments/11a8025/q_techniques_for_spotting_patterns_in_a_sequence/
950,2023-02-24 02:41:37,[Q] Likelihood of new data after Bayesian regression,"I have a variable Y, that is determined by some predictors X\_i. After establishing the priors, posteriors, and the parameters that determine the effect of the predictors on the parameters of Y, I did a bayesian regression with historical data to get a distribution for all the parameters. Now, I want to know the likelihood of a new observation, given my model. This new observation is of both the predictors and the Y. 

I have done likelihoods for deterministic parameters, but never on a bayesian regression where the parameters are random variables. I am unsure if I can actually even compute that likelihood. My idea was to sample from the parameters, and for each sampling, along with the new measure, obtain a sample of Y based on the model and the new measurement. Then with a big enough sample, estimate the distribution of this Y|model,new X and see how my new measurement of Y fits in there. It makes sensd to me that this must work, but probably people here have already dealt with something similar and can give me insght. Any comments about the apporach, how you would do it or resources I can look into that deal with this are appreaciated :)","['The key concept you need is posterior predictive distribution.  Most books on bayesian inference will describe it.  How you actually compute it depends on the model and the approach to inference.  It sounds from your post that you can sample from the posterior so you can also use that to generate samples from the posterior predictive distribution with your new predictors.  Give that sample you can estimate a density if you want to, there are lots of ways to do this.', 'See this book, specifically the section on Bayesian linear regression:\n\nhttps://mml-book.github.io/book/mml-book.pdf', "">  this Y|model,new X and see how my new measurement of Y fits in there.\n\nIsn't this just the predictive posterior distribution? This is easy to calculate, especially if your priors are gaussian."", 'I am unfamiliar with that term sadly. Well how do I calculate it? I am specially confused on conditioning on distributions of parameters instead of actual parameters', 'Will do, thanks!']",1,5,https://www.reddit.com/r/statistics/comments/11a570b/q_likelihood_of_new_data_after_bayesian_regression/
951,2023-02-24 02:03:00,"Statistical Learning, Bayesian Statistics, or Time Series Forecasting [Q]","Hello, I was looking at the curriculum for one of my grad programs and I noticed there was a choice of 1 extra elective, and it was a choice between statistical learning, which uses ESL, Bayesian statistics, which uses BDA3, and then time series forecasting, which uses brockwell and Davis. 

Which of these do you think is the best course that would be the most useful?","['I would take Bayesian statistics or time series. There are limitless good machine learning resources derived from ESL out there for self teaching but learning Bayesian statistics will change the way you look at data while time series is a valuable specialty that is difficulty to self teach (at least imo)', 'Statistical learning here isbasically machine learning.  I would go for that.', ""Are you in a master's or Ph.D. program?"", 'Took all three and the Bayesian one was by far my fave. That being said, the other two were more immediately useful.', 'Definitely Bayesian statistics']",2,7,https://www.reddit.com/r/statistics/comments/11a487y/statistical_learning_bayesian_statistics_or_time/
952,2023-02-24 01:39:03,[Question] What information should I stay current for Data Science/Machine Learning?,"I’ve studied a decent amount of Statistics alongside Computer Science. Namely, the basic intro stats class, Regression and Correlated data w/ R, and two Theory of Statistics classes based on Mathematical Statistics with Application by Wackerly.   
 

I always read that Computer Scientists should learn more statistics, especially if they are interested in data science/machine learning. So, as someone with some background, I was wondering what topics should I stay current on so that I am not an embarrassment to my people (jokingly)? I plan on doing a review of statistics and then diving into Machine Learning.","['The main thing is to get at least a 400-level 2 semester, calc-based sequence.  It sounds like you have done that. Are you in a program, or online?', 'An easy starting point would be Introduction to Statistical Learning and Elements of Statistical Learning. Both are free and I’m sure you can find lectures based them floating around.', ""For the stats I've already done, I was in a program. I am going to attempt using online material to learn more about Machine learning and other things. I don't remember many of the proofs from the 400-level theory courses, how important are those?"", 'You want to be facile at using the concepts.', ""Start with 'Elements of Statistical Learning'.""]",1,11,https://www.reddit.com/r/statistics/comments/11a3m5f/question_what_information_should_i_stay_current/
953,2023-02-23 23:36:26,[Question] What percent of statistics are actually made up?," I know this is the go-to joke response, but does anyone know the actual percentage of statistics that are made up? Is there an accurate measure for this?

If you reply with a made-up statistic I will cry.","['This sub is about the field of statistics, not the factoids called statistics.\n\nIf you want a serious answer, you will need to define your population:\n\nfactoids cited in the media (which media?) \n\n+/- factoids cited by establishment figures (which kinds of establishment figures?) \n\n+/- ordinary people spouting off (on social media or also at work/in the pub?) \n\n+/- researchers being very bad/dishonest at their jobs (which researchers, and what jobs?) \n\n... etc etc.\n\nThen you will need to define what you mean by ""made up"". The vast majority of claims made are false but they\'re often based on a bad analysis of good data or a good analysis of incomplete data, not just pulled straight out of someone\'s arse.\n\nOnce you\'ve defined your terms, you will reaiise that neither the numerator nor the denominator are countable. And that it doesn\'t really matter. What matters is being able to distinguish between made up nonsense, unreliable analyses, and claims that might have a decent shot at getting us closer to the truth.', 'The number is not significant.', 'garbage in garbage out. Statistics are not made up, data are.', '69% \n\nI made that number up tho', 'Yeah! Stats, bitch!']",0,18,https://www.reddit.com/r/statistics/comments/11a0ky2/question_what_percent_of_statistics_are_actually/
954,2023-02-23 23:21:04,[Q] Chi-square test ( Equal probability or equal length ),"Hi, I'm currently trying to figure out whether I should carry out an equal probability chi-square test or an equal length. I have a set of data that I want to test to check if it fits a lognormal distribution.   
I was told to minimize arbitrary choices when carrying out the test. After carrying out both tests, I found that I needed to combine cells when using the equal-length method as there were expected frequencies below the value of 5. Is this what the question meant by arbitrary? Secondly, my p-values were different for both methods but still reached the same conclusion (accepting the null hypothesis).","['Okay, thanks, I see, the thing that was missing was \'I am doing a chi-squared *goodness of fit test* of a lognormal distribution.\'\n\nIt\'s conventional to call the size of bins by the term \'width\', rather than \'length\'. Between those two things I was a little confused.\n\nI\'ll give two different responses, depending on whether this is just ""I\'m doing this for a class exercise and I don\'t really care if it\'s a bad idea"" (go straight to answer 2, do not read answer 1), or whether it\'s ""I\'m trying to learn to do something sensible"" (go to answer 1 first, which explains a little of why this isn\'t in the class of more or less sensible things.)\n\n1. Treating it as real piece of statistical work:\n\n  *Yikes*. This is really not a great idea. \n\n  (a) How did you estimate the parameters of the lognormal so you can compute the bin probabilities? I presume you took logs and calculated sample mean and variance on the unbinned data, right? The problem is if you do it that way, *you no longer have a chi-squared test*. You can at best find upper and lower bounds on the p-value that way.\n\n  (That you don\'t have a chi-squared test has been understood for about 70 years, possibly considerably longer, but I\'d have to do some research to figure out exactly when. Certainly by the early 50s Chernoff and Lehmann were writing about the particular consequences on the test of estimation from unbinned data so at least that long)\n\n    (b) Even if you have a fully specified distribution, this has pretty *terrible* power for continuous distributions (and even for discrete distributions). \n\n     However, it\'s actually pretty powerful for the multinomial case it was designed for, if the sample size is decent; it can suffer from bias problems at small to medium sample sizes (or sometimes even surprisingly large sample sizes if the bin-probabilities are sufficiently unequal).\n\n   (c) If you really want to test lognormality, take logs and use a general test of normality that doesn\'t discretize the distribution (Chen-Shapiro is decent enough for an omnibus default, more specific advice depends on what sort of alternatives you most want power against. A Shapiro-Wilk is okay as a slightly weaker alternative if you can\'t do Chen-Shapiro).\n\n   (d) Final piece of actual statistical advice: even the more powerful goodness of fit tests are not particularly good ways to answer the sorts of questions people typically use goodness of fit tests for. In general there\'s something better to do.\n\n2. Alternatively, if this is a course where you just want to learn the mechanics needed to pass the subject ... then:\n\n   Equal probabilities is better than equal width (Mann and Wald, 1942), though naturally that advice assumes you\'re not doing parameter estimation. (They also gave advice on the number of equiprobable cells to use; I presume you chose alpha at 5% in which case you want roughly 1.88n^(2/5) bins. So roughly about 12 bins at n=100, vs about 20 bins at n=400. n=100 is on the low side though, power will be poor; I\'d hesitate to do anything less than 20 observations per bin and even that\'s not great. The power is low so you need a lot of data to have a good chance to pick much up.)', ""> I'm currently trying to figure out whether I should carry out an equal probability chi-square test or an equal length. \n\nI think you're omitting some context here that you're assuming we'll be aware of.\n\nDo you mean that you're choosing to bin a continuous variable? Why would you do that?"", ""Because the data is continuous. It's not discrete. We are testing goodness of fit"", 'Hey sorry for the late reply but thank you for giving me a detailed response. Not sure If could reference a reddit post on my assignments haha', 'Indeed you should not use that comment\\* as a reference; but if you wanted a reference, why ask that way at all (which gave no hint of requiring a reference)?\n\nIf you required a reference, I gave an explicit reference in my comment; Mann and Wald 1942^([1]); an alternative would be the book by D\'Agostino and Stephens \n\n---\n\n[1] for which a quick google should get you to   \nMann, H.B. and Wald, A. (1942),  \n""On the choice of the number of class intervals in the application of the chisquare test"",   \n*Ann. Math. Statist.*, **13**, 306–317\n\n---\n\n\\* (BTW did you read the subreddit guidelines, specifically the one about homework?)']",1,5,https://www.reddit.com/r/statistics/comments/11a07vr/q_chisquare_test_equal_probability_or_equal_length/
955,2023-02-23 22:50:25,[Q] Thoughts on pursuing statistics in the future?,"I (18m) am in my last year of college in the UK, about to sit my A-levels.

Going into college in 2021 i had absolutely no idea what I wanted to do for the future and almost arbitrarily chose my A-levels based off of subjects that seemed interesting. I ended up choosing Psychology, Economics, and Statistics.

I’d always thoroughly enjoyed maths for most of primary/secondary school, even getting a grade 9 in GCSE maths (the highest grade possible for those unaware of the UK exam system). However, through a mix of an unlikeable maths teacher from ages 14-16, not knowing what I wanted to do, and perhaps pure laziness, I decided against choosing A-level maths, which I do now regret but there’s no changing the past.

This was fine until I started my Statistics course and it has been far and away the most enjoyable course I take. I frequently get the top marks in my class on practice exams and I feel I have a good understanding of some statistical concepts ( https://imgur.com/a/l0Y2zfi for a picture of what the course involves).

Now, a few months away from finishing my A-levels, I decided I wanted to continue down the path of statistics and applied for 5 University’s mathematics courses in the hopes of eventually developing into statistics along the way. This does mean that no matter which Uni I go to I’ll have to complete a foundation year first, as a result of not having A-level maths, which I am honestly excited for! I’ve received offers from all 5 to study there so it’s only a matter of choosing what would be the best for me.

I hope to continue to study and learn about statistics, it has been without a doubt the most fun I’ve had learning in a long time. Potentially one day I could develop a career in a statistics field.

That is all I have to say, I just wanted to know if this sub had any thoughts about it and whether this could be a good route for me?

Thank you :)","['I study data science. Fancy, modern name for statistics with elements of programming. It’s 80% math, statistics, data analysis etc. Now… I’m mid 30, based in UK, mid management lvl in private sector company working with broadly speaking data that can be sliced and diced to show myriad different KPIs (key performance indicators). Studying on-line. My view on statistics and data analysis is that… private sector managers/directors even auditors are morons - they have other skills on broad “how to run a business” + I wanted to sound harsh. It’s such a shame that so few ppl can wrap their heads round advanced statistical concepts that could help improve so many things in non high-tech, still big companies. It hurts me when ppl can’t read % properly, and it’s a shame that with my adhd and allergy to very hierarchical and formal academic environment, I couldn’t find place for myself and got stuck here. \n\nBut it’s not about me. It’s about enormous gap between academics and real world making money places doing all sorts of activities to keep everyone going, but going against the research and scientific findings. In very many places, your degree is basically a proof that you are eloquent enough, and smart enough to learn your role in work environment from the bottom using very little of your learned core skills and it’s even when your role matches your degree. Similar with psychology when it comes to management/leadership positions. Good manager is not the one who can do one kind of job best, it’s someone who can piece together team that can do things beyond capacity and capability of a single individual no matter how smart that individual is. \n\nIn my opinion, usage of statistics in real world compared to its fantastic and powerful range of tools is like… I don’t know… having a jet but travelling on a bike. On a bike not on foot cause bike is like %… everyone is glued to % and average and median/frequency measures on big data sets causes blank stares and inability to act on it. It’s always “we need to keep it simple” regardless time and resources wasted.\n\nIt’s my first post here so I’d be glad to hear statisticians that are 1. not working for gov 2. were working for gov now are teaching and 3. other teachers, 4.  Are in sector by definition being created from statistics like… I don’t know commercial surveys/research. Are there any statisticians that managed to add value as analyst, actually using advanced statistics in business decisions making process? I’m asking out of curiosity and to learn about other people views and experiences, for op, it could be good way of learning future career options and opportunities. Thanks if you managed to read it all:)']",0,1,https://www.reddit.com/r/statistics/comments/119zhrk/q_thoughts_on_pursuing_statistics_in_the_future/
956,2023-02-23 19:08:57,[Q] Kmeans VS Kmodes,"Hello everyone!

I’m was wondering about the relationship between kmeans and kmodes. 

I have binary data where both 1 and 0 contain the same value. I can use simple matching coefficient (SMC) to obtain similarity. This could then be clustered using Kmeans.

Is this exactly what Kmodes does as well?

My question came to be since I need to get the similarity matrix anyway to calculate silhouette score. Since I already have it, running Kmeans directly seems appropriate rather than using an implementation of Kmodes. But this is only if my understanding of the algorithms is correct.

Thanks for the help in advance!","[""From my understanding, k modes functions like k means but with the L1 distance, which makes the minizer of the criteria function modes instead of centroids. K modes is more robust against outliers and I think it is more used on categorical data, but both can be used.\n\nIn clustering there's rarely a definitive best way to proceed because it is used for exploration purposes and usually there is little information about our data. I would probably use K means by default and if you are worried of extreme values try k modes."", 'K-means is a clustering algorithm that partitions data points into K clusters based on their similarity to the centroid of each cluster. K-means assumes that the data points are continuous and normally distributed, which makes it suitable for numerical data. However, K-means can be less effective when dealing with categorical data, as there is no meaningful way to calculate the distance between categorical variables.\r  \n\r  \nK-modes is a clustering algorithm that is specifically designed to handle categorical data. K-modes replaces the mean of each cluster in K-means with the mode, which is the most frequent value of each categorical variable. K-modes calculates the distance between data points using a matching dissimilarity measure that counts the number of mismatches between categorical variables. K-modes can be more effective than K-means when dealing with categorical data.\r  \n\r  \nIn summary, K-means is a clustering algorithm that works well with numerical data, while K-modes is a clustering algorithm that is specifically designed for handling categorical data.']",3,2,https://www.reddit.com/r/statistics/comments/119v4rt/q_kmeans_vs_kmodes/
957,2023-02-23 14:03:38,[Q]Any relation between a Jefferys uninformative prior and the Cramer-Rao lower bound?,"I’m a fresh masters student and I’ve noticed that the Cramer-Rao lower bound is the inverse of the Fisher Information, while the uninformative Jeffrys prior is proportional to the square root of the Fisher Information. These seem like completely separate topics to me. Is there some kind of intuitive link between these two? If there is please explain at the level of an advanced undergraduate. I find much of the posts of stack-exchange to be slightly too advanced for me.","[""Other that what you already noticed, no.\n\nThe Jeffreys (note spelling) prior is derived from the change-of-variable theorem for distributions (the one with Jacobian determinants).   That is why multivariate  Jeffreys priors involve the square root of the determinant of the Fisher information matrix.  Since Fisher information transforms like a tensor under change of parameter, using this definition gives the Jeffreys property: the same recipe gives the same prior (transformed by change of variable) in all parameterizations.\n\nThe Cramer-Rao lower bound is about asymptotics of maximum likelihood and ultimately arises from differentiability under the integral sign argument that says the variance matrix of the first derivative vector of the log likelihood is minus the expectation of the second derivative matrix of the log likelihood, so the CLT applied to the first derivative of the log likelihood (score vector) has the Fisher information matrix as its asymptotic variance.  The inverse in the asymptotic variance for the MLE comes from solving (inverting) the likelihood equations to obtain MLE.  The Cramer-Rao lower bound (or the Hajek convolution theorem) says the MLE has lowest possible variance (under some assumptions), so that doesn't add anything to why inverse Fisher information in the first place.\n\nSo I don't see it.  Of course the reason why Fisher information transforms like a tensor and the differentiation under the integral sign equality do have something to do with each other.  So there is some connection, but I wouldn't call it intuitive."", 'Following this, would love to read answers.', ""They both make use of the Fisher information because both are related to the underlying geometry of a particular statistical model. The Jeffry's prior is related to the concept of a Haar measure on a statistical manifold, which is roughly like a uniform distribution over the parameter space, taking into account that the geometry of the parameter space (encoded by the Fisher information metric) may be different from that of ordinary Euclidean space. In the case of the Cramer-Rao bound, bounds for estimation accuracy are often related to the local geometry (e.g. the curvature) of a statistical model, and so they'll naturally relate to the Fisher information in some way.\n\nTLDR: The Fisher information determines the geometry of a particular statistical model, which means it appears in many different contexts related to the estimation of the parameters of the model."", 'Well no not really, but actually yes\n\nJeffreys\' prior is a type of uninformative prior that is invariant under reparameterization of the model. In other words, it is a prior that does not depend on the specific parameterization of the model and is only based on the geometry of the parameter space. Jeffreys\' prior is defined as the square root of the determinant of the Fisher information matrix, which is a measure of the curvature of the likelihood function.\n\nThe Cramér-Rao lower bound (CRLB) is a fundamental limit on the variance of any unbiased estimator of a parameter in a statistical model. The CRLB is based on the Fisher information matrix, which measures the amount of information that the data provide about the parameter of interest. The CRLB is the inverse of the Fisher information matrix and provides a lower bound on the variance of any unbiased estimator.\n\nThe relationship between Jeffreys\' prior and the CRLB is that the CRLB is equal to the inverse of the Fisher information under Jeffreys\' prior. This means that Jeffreys\' prior achieves the minimum variance bound for any unbiased estimator of the parameter of interest. In other words, Jeffreys\' prior is the ""best"" uninformative prior in terms of achieving the smallest possible variance for any unbiased estimator.\n\n&#x200B;\n\nSo sure it has a relatinship, but it might be a bit of a stretch', 'Same']",42,9,https://www.reddit.com/r/statistics/comments/119q81m/qany_relation_between_a_jefferys_uninformative/
958,2023-02-23 12:32:20,[Q] Applied math/genetics double major or master's,"Hello all. I'm currently an applied math major hoping to go into a biostatistics PhD program. I've taken a good amount of biology and genetics classes so I'm pretty much on track for a genetics double major but if I continue I'll have to take 17-19 hours every semester until I graduate, and some summer classes. My school also has a program where you can take some graduate courses in undergrad to get a master's degree in 5 years. This sounds good but I'm worried because in order to be accepted to take graduate courses you have to get As in both courses of the real analysis sequence and I've heard horror stories about those classes. Another problem is that the master's degree would be in mathematics not statistics, but there is a concentration in biostatistics. Does the concentration matter for master's degrees? Would it make more sense to go for the master's degree or do the double major? Thanks in advance.","['I don\'t have too much of substance to say, but I will say this: if you are ever ultimately concerned with getting a job in industry, ""double major"" means next to nothing compared to just having a Master\'s degree.  Human beings reading resumes, and especially the god damned computer algorithms that are actually reading resumes don\'t really know or care what a \'double major\' is.  They are just scanning for keywords like MSc or Phd', 'It sounds like a chance to get a MSc in 1 year, which means also saving a lot of money. It’s a great deal.', 'If you are going to a PhD program the top schools want good grades in advanced math courses. The more analysis you can take the better. A rigorous probability course will help bridge the gap between analysis and probability.']",2,3,https://www.reddit.com/r/statistics/comments/119oh59/q_applied_mathgenetics_double_major_or_masters/
959,2023-02-23 09:59:52,[Question] Calculating effect size for Jonckheere-Terpstra in R,"I have used the JonckheereTersptraTest function in R and got a significant p-value. The only results provided by the function are JT = 159.5 and p-value = 0.005. I would like to calculate effect size, but cannot figure out how to do this in R. Any help would be appreciated!",['theres an app called gpower that my one class used to calculate effect sizes and power. just look it up and google it. I know its not R but thats one way to do it'],1,1,https://www.reddit.com/r/statistics/comments/119l5pq/question_calculating_effect_size_for/
960,2023-02-23 09:46:01,[Q] Can I convert a non continuous 4 point likert scale to a continuous 5 point likert scale?,"Hopefully this is the right place. I apologize if not.

A 5 point likert scale survey, 1-5 was conducted. A second survey was conducted but the '3' choice was removed. So people could pick 1,2,4,5. I've read about the formula to convert a 4 point likert to a 5 point likert scale, but I don't know that it works if the values aren't continuous? 

Thanks.","[""I don't think it's a statistical issue. You already have the data on the same scale, except the option for neither agree nor disagree option was removed. 1 maps to 1, 2 to 2, 4 to 4, 5 to 5 with the exact same labels. Min, max, midpoint of the scale is the same, so no point in transformation. \n\nFrom a psych point of view what happened to the people who would have responded 3 except you removed the fence-sitter response option? Numbers/stats aside, doesn't that make it a different survey?"", ""> continuous 5 point\n\nThis confuses me. If a scale has 5 points (1,2,3,4,5), how can it be continuous? You can't choose 1.4142... or 2.7818... or 3.1416... can you?\n\nDo you mean *contiguous* instead?"", "">  Someone was trying to justify that the surveys are comparable, but I didn't think they were.\n\nI would argue that any comparison between results of the two surveys are suspect *at best* because of this. Any attitude measure that doesn't allow a neutral response is probably going to skew more positive than one that does."", 'I think the idea that one can, for practical purposes, treat a scale as continuous is not the same thing as it being continuous. The key question in most cases is whether the test is sufficiently robust to deviations from an assumed distribution (typically normal which obviously is continuous) to provide a valid test. The meaningful distinctions are between discrete and continuous and between ordinal and interval but not between ordinal and continuous.', 'You are formulating a theory of measure question. The answer is ""it depends"" and in particular it depends on the type of thing you are measuring. Velocity of you car can be converted to a continuous scale but things like ""Do you like something"" is not continuous in its essence so changing the scale will break everything']",0,8,https://www.reddit.com/r/statistics/comments/119kukw/q_can_i_convert_a_non_continuous_4_point_likert/
961,2023-02-23 07:23:40,[Q] Can a ratio between MAE and RMSE show anything?,"I am forecasting sales and let my models have forecasting metrics of: 

Model 1: MAE = $10000; RMSE = 12000$

Model 2: MAE = $10000; RMSE = 13000$

By taking a ratio of these two together (Thus getting 1.2 and 1.3) intuitively that tells me my first Model has less large-scale individual errors in my forecast. By seeing a lower ratio between the two, is that a fair assumption to make?

I am still learning stats through my undergrad, so any help is appreciated.

Thanks","[""It's a function of the entire distribution of errors including those close to the middle, but more affected by the tail. RMSE/MAE is always at least 1, of course."", 'Yes, it shows kurtosis.']",9,2,https://www.reddit.com/r/statistics/comments/119hgue/q_can_a_ratio_between_mae_and_rmse_show_anything/
962,2023-02-23 06:50:33,[Q] What's wrong with this coin-toss reasoning?,"Suppose I'm playing a fair coin-toss game. I start with an initial balance of **$10** and each time I flip a coin and it lands heads, I win $1, and if it lands tails I lose $1. The expected value of the game is clearly $0 and my expected balance is **$10** .

At some point in the game, it's likely I'll have a balance of **$11** from getting heads. The game restarts with my new balance:

Suppose I'm playing a fair coin-toss game, I start with an initial balance of **$11** and each time I flip a coin and it lands heads, I win $1, and if it lands tails I lose $1. The expected value of the game is clearly $0 and my expected balance is **$11** .

At some point in the game, it's likely I'll have a balance of **$12** from getting heads. The game restarts with my new balance, and so on...

My expected balance of this sequence of games appears to be infinity which contradicts the initial premise that the expected value of the game is $0. What's wrong here?","[""You're as likely to start a new game with a balance of 9 as with the balance of 11"", ""What you're missing is that the same logic works in the negative ($9, $8, etc) direction."", '>What\'s wrong here?\n\nAs the others have mentioned, you\'ve conditioned this on the fact that you win each game, which is clearly not realistic.\n\nBut there\'s another issue that the others haven\'t mentioned. Presumably, the game stops if your balance hits 0. In this situation (and assuming your opponent has infinite money) your expected balance after an ""infinite"" number of games is 0 because this is a 1D random walk with a sink at 0. It doesn\'t matter how much money you start with, this expected balance doesn\'t change!', ""> What's wrong here?\n\nI mean, sure it's going to look like your expected balance is infinity when you decide you're just going to ignore every negative outcome."", 'You’re assuming that the sequences (ending on 11, then on 12, etc.) are independent from each other when they’re not. The start of each sequence depends on the outcome of the previous one, so the likelihood of achieving the continuously increasing values decreases as the number is sequences increases.\n\nIn other words, what’s the probability of a sequence ending on 12, *given* that it started on 11?']",5,14,https://www.reddit.com/r/statistics/comments/119glhw/q_whats_wrong_with_this_cointoss_reasoning/
963,2023-02-23 05:08:45,[R][E]I'd like to determine what effect a policy has had on student enrollment in multiple programs (electives) within all secondary schools of several states. What statistical knowledge will I need to learn?,"I'd like to determine what effect an educational policy has had on a specific set of high school programs by state. Below is the known information that I'd like to compare:

* 5 states
* School size (categorical)
* Program type offered within each school (categorical)
* Enrollment in each program (continuous)
* Demographic of program enrollment (income, race, etc.) (categorical)

I assume that it is possible to assess this information over the span of a decade to determine what effect, if any, the policy had each type of school and program, based on how rigorously the state implemented the policy (assuming there is a rubric already devised to rate implementation).

What type(s) of statistical modeling will I need to learn?","['What you are asking is fundamentally a causal question.\n\nBefore you start thinking about statistical models, you\'ll want to think very carefully about *all* potential causal influences on enrollment. Do you believe you\'ve captured all of these sufficiently to isolate a causal effect of the policy on enrollment? Why / why not? It would be helpful to draw a DAG to make your assumptions concrete.\n\nAt this point you\'ll want to consider whether you can even reasonably estimate a causal effect of the policy given the data you have available. The answer to this might be no. In fact, given you say you already have the data (as listed in the post), I\'d hazard to say the answer *is* no, given there appear to be potentially important confounds missing (regional economic and political indicators, population densities and changes, etc). \n\nIf you are somehow in a position to gather the relevant data influencing enrollment, maybe then you can start thinking about modelling approaches.\n\nThere are a number of different methods that *could* be applicable for this type of question - regression discontinuity, differences-in-differences, and synthetic control approaches all spring to mind - but FWIW, I think D-Juice is right. You\'d probably be better off collaborating on this problem with subject matter expert education researchers, statisticians, and/or econometricians, rather than trying to do it all yourself with rudimentary skills. To do an analysis like this credibly takes a years and years of expertise and experience. Just sticking what data you have into a multi-level model is the wrong way to go.\n\nThere are a number of nice freely available causal modelling textbooks in recent years that you may want to read, including [""The Effect""](https://theeffectbook.net/index.html), [""Causal Inference: The Mixtape""](https://mixtape.scunning.com/), and [""Causal Inference: What If?""](https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/).', ""This is like asking how many steps there are up to the 10m diving board before you've learnt how to swim.\n\nAssuming you already have the contextual knowledge to do this work properly (ie you are a fully qualified educationalist with substantial experience in the system you want to research), and if you won't work with a fully qualified statistician on it, you will need to do an MSc in applied statistics, get a job as a statistician researching education, working with fully qualified statisticians for at least 4-5 years, and then spend a year or five putting in grant applications in the hope that someone will fund it.\n\nOr, again assuming you are a fully qualified educationalist, you could contact statisticians who already know what they're doing and ask if they are interested in a collaboration. And spend one to five years putting in grant applications in the hope that someone will fund it."", 'Let\'s say hypothetically you find an explicitly non-causal ""association"", however you choose to define that statistically.\n\nWhat would that mean to you? What conclusions would you draw about the policy? What purpose would that estimate serve to you or anyone else you present this to? Asking genuinely here.\n\n[This paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5888052/) might be worth reading r.e. associational vs. causal language in studies.', '> you will need to do an MSc in applied statistics, get a job as a statistician researching education, working with fully qualified statisticians for at least 4-5 years [to become qualified to do it yourself]', 'Honestly, I\'m actually more convinced based on this response that yours is a causal question, but let\'s set that aside and assume you\'ll only refer to it as ""associational"".\n\nThe point made in the original response was that it\'s incumbent upon you to really think through all the possible influences on the enrollment outcome if you want to rigorously isolate and determine the effect of the policy. You\'ll want to do this *before you fit a model* and decide whether you really have the right data at hand to answer your question. If not, your association could be worse than worthless - it could easily be actively misleading due to totally unaccounted for confounds. \n\nI understand that you\'ve put a lot of thought into the influence of states. I\'m not in a position to appraise that because I\'m not a subject matter expert in this area. I will say that you\'d want to apply the same kind of critical reasoning to potential influences you\'ve mentioned here that you don\'t seem to have already in your dataset (e.g. funding to schools). I hope the textbook references above are helpful and good luck.']",2,14,https://www.reddit.com/r/statistics/comments/119d8am/reid_like_to_determine_what_effect_a_policy_has/
964,2023-02-23 04:17:22,Effect size for change in proportions and variance [Q],"In Python, using `statsmodels.stats.proportion.proportion_effectsize()`, the only arguments are p1 and p2. There is no accounting of sample size / variance. Fyi I believe this function is computing Cohen's H.

Why is this? The variance of a sample proportion goes down as sample size goes up. In Cohen's D, this is accounted for by dividing by the pooled standard deviation.

Why does this not matter with a change in proportions?","['The estimated  effect size is an estimate of the effect size in the population. The sample size used in an experiment does not affect the population effect size.  You can’t increase the population effect size by increasing n even though increasing it increases power.', ""> In Cohen's D, this is accounted for by dividing by the pooled standard deviation.\n\nNo, in actual Cohen's d (e.g. see his book), you have  \n\nd = (μA - μB)/σ   (for the one-tailed case) or \n\nd = |μA - μB|/σ   (for the two-tailed case) \n\n(Cohen actually writes m for μ in the book but he clearly defines it to be the population mean, μ; it's not clear why he changes the convention to Roman letters in the numerator - and in the symbol for the parameter itself - but retains the convention of Greek letters for the parameter in the denominator. <Shrug> )\n\nThere's no adjustment for sample size; it's the number of original population standard deviations the two mus are apart. If you're computing an estimate of d based on a sample (but beware, there's a lot of ways people seem to misuse those), you may have been fooled by the presence of n's in the *weighting* of the two sample variances to produce the pooled variance, but the pooled standard deviation estimate you end up with is still just an estimate of σ.\n\n---\n\nProportions: if you know the population proportions, then you know the equivalent of σ. You can determine everything from the two population proportions. \n\nHowever, Cohen uses *h* (which you mention), which is:\n\n h = 2 arcsin √p1 - 2 arcsin √p2\n\n(with absolute values added for the two-tailed case)\n\nwhere p1 and p2 are (explicitly) the population proportions. He uses capital letters for these, but that breaks another statistical convention, and we're already breaking one by using lowercase Roman. (The 'proper' convention, almost universal when I was a student, was to use Greek letters (π, pi) here as well -- i.e. π1 and π2, but that has been all but lost from basic books and non-mathematical people often seem to get confused if I stick to that)\n\nThis does make some sense, though I think there's something of a gap in how he reasons to it; it's certainly possible to argue for other measures. \n\nAgain, everything would be determined from those population proportions."", 'Thanks for this detailed explanation. For Cohen’s D, I guess what I meant was that it accounts for precision by accounting for standard deviation. \n\nBut with effect size for sample proportions this does not appear to be the case. Same effect size for 10/100 —> 20/100 as 100/1000 —> 200/1000 even though the confidence intervals around these sample proportions would be different. \n\nThis seems odd to me, maybe I am missing something?', '""width of confidence intervals"" have to do with estimation uncertainty, and nothing to do with effect size']",1,4,https://www.reddit.com/r/statistics/comments/119b1ti/effect_size_for_change_in_proportions_and/
965,2023-02-23 01:41:38,[E] Is it worth studying statistics as a second bachelors ?,"Hello 

I am from India and will be graduating with a bachelors in mechanical engineering degree but I have always wanted to study bachelors in math/statistics but couldnt because I got below average marks in my 12th grade. 

I am thinking of doing an online bachelors in statistics from an American university. 

Is it worth studying a second bachelor in statistics and are online bachelors equivalent in academic rigor as compared to the one offered in face-to-face mode ? 

Thank you","[""Get a Masters! It's less time and a higher degree. You'll have the math prerequisites covered almost certainly by your engineering background.\n\nLook at online options, there are many. Penn State, Texas A&M off the top of my head, probably others. The math requirements are less than you think - couple semesters of calculus and linear algebra or diff eq usually."", 'Get a MS in stats. Your background is suitable.', 'Masters or minor. 2 bachelor degrees is a waste of money', 'I was considering doing this and like many others are suggesting, it would be less time and less money to go for a masters degree, which would benefit you more in the future than a second bachelors.', "">I didnt study proof based math\n\nThis isn't a prereq for most masters degrees.""]",1,11,https://www.reddit.com/r/statistics/comments/1195yg8/e_is_it_worth_studying_statistics_as_a_second/
966,2023-02-23 01:41:28,[Question] ANOVA groups,"Hello, statisticians!

I’m looking for feedback on how to divide my groups for a one way between groups ANOVA. 

Hypothesis: that participants who score higher on a measure of social needs will have higher scores on a depression measure. 

My first thought was to divide participants into three groups based on social need (low need, moderate need, and high need) and compare their depression scores in the ANOVA.

Or should I compare the depression scores as one group and the needs scores as another group?

Any guidance greatly appreciated!","[""Hi,   \nI am not sure is this is a right case for ANOVA.  \nBased on your description, data seems to be {(xi, yi); i = 1,2,...n}  \nwhere xi = social needs score and yi = depression score for the i-th individual.  \nFor this case, the question seems to be whether the scores X and Y are associated.  \nIf the scores are discrete (e.g. 5 point scale (1=very bad, 2=bad  3=neutral, 4=good, 5=excellent)), then you have 2-way contingency table with (i,j) th cell having value n\\_ij, \n\nwhere n\\_ij is number of subjects who scored i in Social Needs and j in Depression.   \nThen you can do different tests of association (say Chi-squared).  \nThere the null hypothesis is X and Y are independent and you will test whether based on your data you can reject the null.   \nFor ANOVA, typically, you 'assign' a treatment to a subject.   \nIn your case nobody 'assigned' a subject a score of say 2 for the depression score.   \nIt was a measurement. So I think test of association is the right way to analyze this."", ""it sounds like you want something like correlation or regression. If you're interested in monotonic rather than linear relationships then Spearman or Kendall correlation would be suitable (among other options), but if there are other important covariates, bivariate correlations will generally be misleading and you need something more sophisticated for that."", 'This is a great comment, thank you. Very helpful! I may have been too narrowly zoomed in on the one test even when it didn’t make the most sense. I just did a stats test flowchart and ended on simple regression, so I think I will try that! Thank you!']",2,4,https://www.reddit.com/r/statistics/comments/1195ybn/question_anova_groups/
967,2023-02-23 01:08:32,[Q] Very different results when predictors are coded as counts vs frequencies.,"I don't want to explain my real data, but I will make up an example that is equivalent.

Let's say people are judging the quality of paintings and can say ""like"" or ""dislike"". 
Paintings can have five colours in them: red, yellow, blue, green and/or orange. The paintings can be any size.

**Count Approach**

Let's say I categorize each square cm based on its colour (and that in this example each square cm can only be one colour). Because paintings can be of different sizes, the totals will differ across paintings. I create a model that is the number of square cms that are each colour. 

So my model is: *Liked = TotalRed + TotalYellow + TotalBlue + TotalGreen + TotalOrange*

Analyzed like this, my results are something like this:



Term | B
---|---
Int | .8*
TotalRed | -.2
TotalYellow | .04
TotalBlue | -.5
TotalGreen | -.5*
TotalOrange | -.3*

*= p < .05



**Proportion Approach**

Let's say for each painting I calculate the proportion of the painting that is each colour. This time I leave out blue because it is the least frequent and I want to avoid perfect redundancy among my predictors.

So my model is: *Liked = PropRed + PropYellow + PropGreen + PropOrange*

Analyzed like this, my results are something like this:

Term | B
---|---
Int | -.4
PropRed | 1
PropYellow | 1.5*
PropGreen | -.01
PropOrange | -.15

*= p < .05

Some predictors change signs, different predictors are significant, and my intercept changes.

How do I know which one is ""more right""?

Edit: I should add that the colour example was just an example. I'm actually dealing with language data.","['What you have here is a compositional covariate and you are trying to distinguish between whether or not you should be using absolute and relative compositions.  \n\nThere’s not a clear statistical answer to which one is more appropriate, and you should make a decisions on the basis of domain knowledge and your actual question of interest. \n\nThe absolute compositions, the actual counts, from my rough guess based on your color example, are probably not the more appropriate one to use, as the predictions of this model suggest changing the size of paintings to change how much people like do not like them. I am just guessing this is probably not what you would want.\n\nThe relative compositions provide you with an interpretation regarding how like/dislike probability will change as you change the underlying proportion.\n\nThere’s a caveat to the relative proportions method though, the method you are using of directly using proportions is probably not appropriate. The covariates are highly dependent on one another, and face hard constraints on the simplex as they vary. Because of these your model is likely going to be wildly misspecified and the results will be misleading. Instead, I suggest using the ILR transform (isometric log-ratio) to provide an alternative covariate representation where your model can be more feasibly well-specified.', 'Vision scientist with background in color vision and colorimetry here.  \n\nI would start by making sure my colors are properly mapped into a linearized set of coordinates called tristimulus values.  If your paintings are displayed on an monitor, then you\'ll have to tackle the gamma-correction as well.  \n\nOnce all that is done, I would use multinomial distribution to model your data.... This gives you a set of frequency parameters AND a count so you don\'t have to choose between the options you provided.    By grounding my choice of model and analysis in mathematical theory, I don\'t need to bother with the crisis of ""which is more right?""\n\nHere are some links to the colorimetry stuff to get you started.  \n\nhttps://en.m.wikipedia.org/wiki/Colorimetry\n\nhttps://en.m.wikipedia.org/wiki/CIE_1931_color_space\n\nhttps://en.m.wikipedia.org/wiki/Gamma_correction#:~:text=The%20gamma%20correction%2C%20or%20contrast,sensors%20that%20usually%20respond%20linearly.\n\n\nHappy hunting', 'I especially love this answer because it shows how important domain expertise can be in framing a statistical question.\n\nBecause I assumed the “colors” were stand-ins for some other compositional characteristic I didn’t even begin to consider anything regarding color space representations, but this answer is great in showing what important considerations there are if in fact the “colors” are actually colors!', ""If you're using least squares regression they're both bad models since they both model your outcome as continuous and conditionally normal and you should be using something like logistic regression which models the probability of success of a Bernoulli (binary) random variable. More generally you need to check that the assumptions of the tool you're using are satisfied by the data and data-generating process you're modeling.\n\nIf you establish that both models are valid in that respect (e.g if you're using logistic regression then the log-odds of your outcome needs to be ainear function of your predictors, which you generally establish based on knowledge of the data-generating process), then it becomes a matter of what question is most appropriate in practice. Number of blue square inches is not the same as proportion of blue square inches. Decide which question is appropriate and ask it. Asking separate but tangentially related questions and scratching your head when the answers are different won't get you anywhere. If the model you want to use for your preferred question isn't valid due to violated assumptions, you can adjust your question or use a different model."", 'You\'re throwing away information in the proportional approach - the ""size"" of the painting (sum of all the colours).\n\nIf you include a sum/size predictor in the proportional approach, your coefficients between the two approaches will be equivalent assuming maximum likelihood estimation (MLE) without any regularization. ""Equivalent"" here means the same distribution within a statistical model, just represented in different parameterizations.\n\nEDIT: the above isn\'t necessarily correct because the transformation between counts -> proportions and sum is nonlinear. Linear transformations of predictors are equivalent to dual linear transformations of the coefficients, but that\'s not useful here']",31,19,https://www.reddit.com/r/statistics/comments/119547u/q_very_different_results_when_predictors_are/
968,2023-02-22 23:47:14,[Q] Sample size calculation with repeated measures,"Is there a particular equation for calculating size with matched data/repeated measures?

I'm trying to calculate sample size for a study and the equation I've been working with is this:

n = 2 × (Zcrit + Zpower)\^2 × SD\^2 / (Effect size)\^2

However it gives a much larger n than that calculated with G\*power, so I was wondering what the appropriate equation/calculations would be.

The variables I've been using have come from a previously conducted study, and are as follows:

Mean 1 = 4.5; Mean 2 = 6.1, SD1 = 1.9; SD2 = 3.0; r = 0.5

Any help would be greatly appreciated!","['How are you calculating the ""SD"" and ""Effect size"" in your equation?', 'we were told to use pooled SD and difference between the means for effect size\n\nso SD = 2.511 and d = 1.6', ""This is likely the source of your problem. The paired t-test (I'm assuming you're talking about the t-test based on using d - but correct me if I'm wrong) relies more on the standard deviation of the differences than on the pooled standard deviation. Take a look at how the [G\\*Power documentation](https://www.psychologie.hhu.de/fileadmin/redaktion/Fakultaeten/Mathematisch-Naturwissenschaftliche_Fakultaet/Psychologie/AAP/gpower/GPowerManual.pdf) defines the standard deviation (page 48, section 19.1)."", ""thank you! i think i am talking about the paired t-test. Is there a different formula for calculating sample sizes for paired t-tests, or is it the same as the one i've been using? i've come across [this equation](https://www.statsdirect.com/help/sample_size/paired_t_test.htm) but i'm not sure what tn-1,a-2 and tn-1,B are."", ""Hi,  \nI have this problem:  \nI have a bag with 15 balls inside  \n8 green balls  \n7 red balls  \nI draw 5 balls from the bag - one after the other (so the last one matter)  \ngreen ball gives me 1 point  \nthe red ball takes from me 1 point  \nmy point balance can't be negative  \nwhat is the distribution to finish the game with 0, 1,2,3,4, or 5 points?""]",2,5,https://www.reddit.com/r/statistics/comments/1192z6u/q_sample_size_calculation_with_repeated_measures/
969,2023-02-22 06:08:47,Hi I need more statistics [R],"I currently have 81 statistic. I need more than 81 statistics. Pls give me some staistics

[https://forms.gle/JxMeYmXsBuBjBhST6](https://forms.gle/JxMeYmXsBuBjBhST6)",['https://en.wikipedia.org/wiki/Self-selection_bias\n\n(Also see https://en.wikipedia.org/wiki/Statistic )'],0,1,https://www.reddit.com/r/statistics/comments/118glye/hi_i_need_more_statistics_r/
970,2023-02-22 04:40:54,[Q] Percentage of a Percentage,"What is the appropriate way to represent a change in percentage. For example, if an outcome occurred 55% of the time one month and 35% the next month, would the appropriate representation be a 20% change or a 36% change (rounding up for simplicity)?","['""20 percentage points"" is the clearest way to present it imo. You could say 36%, but it will almost surely be misinterpreted by some readers, unless the context is very clear.', 'For changes in a percentage percentage point for large changes and basis point for small changes would be clear terminology. Eg, the outcome occurrence decreased by 20 percentage point\n\nIt also makes sense to talk about relative changes. For quantities in percent, I\'d avoid using percent to express this though, I\'d use ""increase by a factor of"" etc', 'https://en.m.wikipedia.org/wiki/Basis_point is exactly for this purpose', 'I guess my question is about transparency. Especially because the total number of outcomes is different in each month. 300+ one month (55% of which were the outcome in question) compared  to 150 the next month (35% of which were the outcome in question).\n\nFrom reading yours and other responses I’m thinking the most transparent way to show this is to show the raw totals along with presenting it as a 20 percentage point reduction in negative outcomes.', ""By talking about a percentage in each month (55% first month, 35% next month) you're already obscuring the denominator before you even begin to talk about the change between months. What's more important to convey? The proportion of the time this outcome occurred or the total number of times it occurred?""]",8,6,https://www.reddit.com/r/statistics/comments/118efby/q_percentage_of_a_percentage/
971,2023-02-22 04:04:29,[Q] Is statistics a good major?,"I am a freshman and I just decided to switch my major from history to statistics. I originally wanted to become a history professor but the job outlook is too poor so that’s why I switched. I am wondering if statistics is a good major for me.

I like statistics and data a lot, and I also like to do research. My biggest passion is learning. I have only taken 1 stats class in HS where I got an A, but I often look at statistics in my free time. I am pretty decent at math, but I haven’t done calculus yet (doing it this summer). I don’t love to do math, but I don’t hate it. When it comes to statistics I prefer to analyze the data rather than calculate it. I particularly enjoy statistics in the social sciences, demography, and also probability models. I am also considering trying out biostatistics.

Based on this information should I stay in stats or should I switch off of it and do something else?","['My stats major (2021 grad) was 98% very heavy math. \nThe stats you took in HS will likely have 0 place in the degree. \nAs far as analyzing results and data from actual (or manufactured) data, it was close to nothing. \nI would narrow your search to a school that focuses on applied stats - it may help.', 'Define ""good"".\n\n> I don’t love to do math, but I don’t hate it.\n\nStatistics is a quantitative discipline and so you\'re going to be doing a lot of math.\n\n>When it comes to statistics I prefer to analyze the data rather than calculate it\n\nYou\'re going to be user a computer a lot in statistics, so I would recommend you learn to code too.\n\n>Based on this information should I stay in stats or should I switch off of it and do something else?\n\nYou\'ve not really given us much to go on, but if you don\'t love math and can\'t invision yourself taking a full load of math classes I would say stats is not for you and you might instead do well in a discipline that uses stats as a tool but does not make it their primary focus.', 'Figured I should point out the obvious. Statistics is a subset of mathematics. Essentially 100% of the major will involve rigorous mathematics. Do with that what you want.', 'Disagree on both points but honestly this topic has been discussed to death and I’m not interested in rehashing old arguments.', 'I\'m biased for obvious reasons, but have you considered political science? Otherwise, i would strongly suggest you look into it, though i\'m aware it\'s not as prestigious in the US as say stats, biostats, economics etc.\n\nDepending on the institution, we do a fair amount of stats with the option of choosing the exact amount you want on top of the basics. We (almost) always do so with a directly applied focus on real issues, ie. broadly what i belive you mean by ""preferring to analyze the data"". We also work a lot with historical issues and theories on historical developments.\n\nI would not continue with stats given the (though limited) information you have given her. Find something applied instead.']",55,78,https://www.reddit.com/r/statistics/comments/118dncb/q_is_statistics_a_good_major/
972,2023-02-22 02:07:09,"[Q] If I need to guess the value of a random sample, should I use the mean or the median of the population for the smallest expected guessing error? (Or some other metric?)","Assume I have the whole dataset, so I can see the distribution and calculate any descriptive metric I need, and each sample has an equal chance to be the one I need to guess.","['The mean minimizes the squared deviation whereas the median minimizes the absolute value of the deviation.', 'It depends on how the costs / rewards of error in guessing are structured. How much error results in what outcome?\n\nIf you need to maximize the chance of getting it exactly (in a discrete variable), where the reward is 0 if you are wrong even a little (or the loss jumps as soon as you are wrong by even a little), you would go for mode.\n\nIf the cost is proportional to the average absolute error you would use a median. If the loss is proportional to the average squared error you would use a mean. ... and so on. You maximize the reward or minimize the loss or whatever as suits your specific situation. \n\n> smallest expected guessing error? \n\nDo you mean smallest expected absolute error, smallest expected squared error, or something else?', 'Thanks for the comment! I meant absolute error', 'And the reward is 0 and cost is directly propprtional to absolute error', 'Thanks for the answer! There is no guarantee that it’s symmetric and most likely it is not.']",4,8,https://www.reddit.com/r/statistics/comments/118bu01/q_if_i_need_to_guess_the_value_of_a_random_sample/
973,2023-02-21 23:13:56,[Q] I want to know if my marketing strategy was effective in improving brand image...,"I want to know if my marketing strategy was effective in improving brand image. 

I have the proportion of surveyees liking the brand, for e.g. 30%. I then conduct a marketing campaign and did another survey. Now the proportion of a different set of surveyees liking the brand is 35%. I want to know if the increase is significant, what test do I use? 

&#x200B;

Since different people answered the survey, it is not a paired sample right? What test should be used?","['A two-proportion Z-test', 'Assuming both were random samples, compute the standard deviation and see if 5% is a significant increase (ie, 2 or 3 SDs depending on how sure you want to be).', ""While the time difference may have ramifications for your study in toto I'm not sure how/why it would affect the actual test if all you're doing is comparing the proportions of two independent samples, as you appear to be doing. (Though disclaimer: I'm just a student here and others may know better).\n\nShouldn't your null be that there is no difference in proportion while the alt is that there is a difference? So:\n\nH0: p2 = p1\nH1: p2 != p1"", ""What's your N?"", 'One tailed is not ""more powerful"" than two tailed. It\'s less conservative.  We say test/model estimator A is more powerful than test/model estimate B only when comparing type I errors at the same alpha level. There is a whole literature on the meaning and proper use of one tailed test. Essentially,  you\'re saying, from a Bayesian perspective, that the probability that a true effect is negative is zero. You can\'t say you know the marketing strategy couldn\'t have possibly had a negative effect of any magnitude. You\'re using a one tailed test because of its anticonservatism, which is fine, but your level of evidence is much weaker. You must clearly state this in reporting.']",6,14,https://www.reddit.com/r/statistics/comments/1185pzc/q_i_want_to_know_if_my_marketing_strategy_was/
974,2023-02-21 21:39:07,[Q] Unmasking Chance: Investigating the Randomness of Multiplayer Game Winners,"I have a simulation of a **multiplayer game** (N=50) in which each participant's behavior is characterized by a vector of parameters drawn from a **Multivariate Normal distribution**. In each session, there is **one single winner** and I want to prove that the winner of the game is random and not influenced by any particular participant’s vector.

I have access to as much data as needed (via simulations), including the parameters vector of each participant and who won.

What statistical or mathematical methods can I use to prove that the winner of the game is indeed random and not determined by any specific participant vector or set of participants vectors in each session?

I appreciate your help!","['No finite data size will rule out every possible way the winner could depend on the vectors. You can show that selected possible options cannot have a large effect: ""players with large X are at most* 1% more/less likely to win"", ""players with large X are at most 1% more/less likely to win when facing players with large Y"" and similar options. There are far too many options to study all of them, however.\n\n\\*you cannot be *certain* about that, but you can set confidence intervals and collect enough data to effectively rule out some values.', ""You might model win-probability as a function of the vector, but you would first need to consider the ways in which the contents of that vector might potentially relate to the outcome. If the model is completely general:\n\n  P(yᵢ = 1) = f(**x**ᵢ, θ)     (where the output of f obeys bounds on probabilities)\n\nwhere person i's vector is **x**ᵢ and yᵢ is 1  when they won and 0 otherwise.\n\n\nfor some completely unspecified f, then   \n\nHowever, if (as seems quite likely, but details are lacking) the win probability might depend on other participant's vectors as well\\*, this becomes more complicated, you'd need some kind of scaling for that, but the space of possibilities for a general f are huge.\n\n>  (via simulations)\n\nThen presumably you must either know the answer already, or at least a great deal about what forms it could take. The details of what happens inside the simulation (how a winner is produced and how the vector relates to how that works) will matter.\n\nThis may at least allow you to say some things about the potential form of f; including, hopefully, what parts of the relationship needs to have unknown parameters and which parts shouldn't.\n\n\n---\n\n\\*(for one such example, consider a situation with a series of pairwise matchups, where people might consider - among other things - modelling the outcome of those individual matchups via a Bradley-Terry type model)""]",2,2,https://www.reddit.com/r/statistics/comments/1183nlb/q_unmasking_chance_investigating_the_randomness/
975,2023-02-21 21:05:38,[Q] If you were of average attractiveness...,"If you were of average attractiveness (5 on a scale of 1-10) and you improved your attractiveness by 0.1 (to 5.1), how many people of the 8 billion on the planet would you pass, assuming a normal bell curve?","['Depends on the variance of the distribution', 'You can approximate the standard deviation by taking the range/6, so using the normal distribution with mean=5 and sd=(10-0)/6=1.67, find the cumulative probability between 5 and 5.1 and multiply by 8 billion. This gets you 0.023*8billion=184million people.', 'normal distribution goes from -infinity to infinity, no matter the variance. So yes more information is required.', ""> assume normality due to the theorem of central limit\n\nThe CLT is a theorem about the difference of a *standardized average* or a *standardized sum*. How is that relevant to the distribution of the values themselves? (Taking larger samples of a population doesn't alter the distributional shape of the population values.)\n\nThe assumption of normality was given in the question\\*, so you don't need to invoke a CLT to get there. A good thing, too, because the CLT doesn't give a basis for what you wanted to use it for. \n\nYou're right that you can estimate variance from a random sample of the population of interest  and yes, in a very large sample, that estimate will be accurate enough to use as if it were the population variance. (*taking* an actual random sample from the population of the world is essentially impossible in practice, though) \n\n\\*  (it's not actually possible to have actual normality for something bounded between 1 and 10, but lets assume the OP meant approximately normal)"", 'The simple problem with this question is that we don\'t know where the ""10"" bin falls on the bell curve.  We might put 10 at 3 standard deviations, although realistically that\'s probably too far out - more than 0.15% of people would be a 10.  So you need to decide where ""10"" should fall - or get experimental data on people\'s ratings.\n\nIt\'s also worth noting that on the most standard 1-10 scale (with no 0), 5 isn\'t the midpoint - 5.5 is.  So 5.5 should be your mean.']",0,30,https://www.reddit.com/r/statistics/comments/1182z7i/q_if_you_were_of_average_attractiveness/
976,2023-02-21 16:20:16,[Q] How do I generate CI for Likert-Scale Questions for more than 2 levels? More precise: How to get value for upper percentile for chi-square distribution?,"\[SOLVED\] I have responses from a 5 response likert item and I'd like to create CI for each response.

I am using this [guideline](https://imgur.com/a/zAVKfkI) to calculate the CIs , but have trouble getting/understanding 'B': upper (a/k) 100th percentile of the chi-square distribution for 1 degree of freedom.

Is there a python script or table to look up the value for 5 responses (or other values in the future)?

Or is there a different way to calculate CIs?

Cheers!

EDIT:Used `chi2.ppf()` from `scipy.stats` to calculates quantiles of the chi2 distribution.

You can also used `multinomial_proportions_confint` from `statsmodels`

This directly calculates simultaneous confidence intervals for multinomial proportions and let's you choose between goodman and sison-glaz approximation.","['Is this a homework assignment?', 'You could use `chi2.ppf()` from `scipy.stats` to calculates quantiles of the chi2 distribution. To replicate the `qchisq` calculations from R: `chi2.ppf(1 - 0.05/6., 1)`. The `1 -` is necessary to replicate the `lower.tail = FALSE` argument from R.\n\nAn easier way is to use `multinomial_proportions_confint` from `statsmodels`. This directly calculates simultaneous confidence intervals for multinomial proportions.', 'Nope. Been out of school for 5 years.', 'Thank you very much. That is the answer I was looking for! \n\nWhat I used in the end was B = chi2.ppf(1-(.05/length), df=1)\n\nAlso playing around with multinomial\\_proportions\\_confint. What I like about this is that I can use either goodman or sison-glaz approximation for the CIs!\n\nCheers!', "">  chi-square distribution for 1 degree of freedom.\n\nYou can just do it using any stats program that implements the inverse cdf for the chi-squared. e.g. in R it's   \n`qchisq(a/k,1, lower.tail=FALSE)`  ... it's almost as simple in Excel, for example\n\nIf you only have tables, the chi-square(1) is just the square of the standard normal, so you can do it from normal tables; find the z-value that has probability a/(2k) (i.e. half of a/k) to its right, and square it.""]",6,5,https://www.reddit.com/r/statistics/comments/117ycj5/q_how_do_i_generate_ci_for_likertscale_questions/
977,2023-02-21 10:19:46,[Q] Why are my fit indices for a path model on Lavaan perfect for a serial mediation model?,"lavaan 0.6-11 ended normally after 38 iterations

Estimator ML

Optimization method NLMINB

Number of model parameters 12

Number of observations 183

Number of missing patterns 1

Model Test User Model:

Test statistic 0.000

Degrees of freedom 0

Model Test Baseline Model:

Test statistic 434.980

Degrees of freedom 6

P-value 0.000

User Model versus Baseline Model:

Comparative Fit Index (CFI) 1.000

Tucker-Lewis Index (TLI) 1.000

Loglikelihood and Information Criteria:

Loglikelihood user model (H0) -1597.019

Loglikelihood unrestricted model (H1) -1597.019

Akaike (AIC) 3218.038

Bayesian (BIC) 3256.552

Sample-size adjusted Bayesian (BIC) 3218.545

Root Mean Square Error of Approximation:

RMSEA 0.000

90 Percent confidence interval - lower 0.000

90 Percent confidence interval - upper 0.000

P-value RMSEA <= 0.05 NA

Standardized Root Mean Square Residual:

SRMR 0.000

Parameter Estimates:

Standard errors Bootstrap

Number of requested bootstrap draws 1000

Number of successful bootstrap draws 1000

I conducted a serial mediation model on lavaan for RStudio. Why is my TLI and CFI an exact 1.0? Moreover, why is the RMSEA 0 and Chi-Squared Test 0 too? Is there an issue with the validity of the findings on this path analysis if these are the fit indices?",['Because you have 0 degrees of freedom in your model'],1,1,https://www.reddit.com/r/statistics/comments/117rsbo/q_why_are_my_fit_indices_for_a_path_model_on/
978,2023-02-21 09:57:11,[Career] Master suggestions,"I have a background in informatic engineer and statistics (dual degree) and i would like to do a master,  if i want to do research what would be better to do it in math or stats?

If I do it in math i would probably come back for stats, but math is for personal satisfaction and to go inside a more theoretical aspect of both of these subjects, would this be a good idea? What else would you recommend?","[""Hey, master student here planning to go for a phd. I'll share my experience so far. I started a master in Stats and Data science, fearing a math master would be way too theoretical for what I want to do. Turns out the master I picked wasn't that much stats and more let's give you a shit ton of methods and not even touch why they work or how they can be modified/adjusted. Luckily for me, a professor advised me to change to the applied mathematics master in the same uni. Turns out the master is completely elective courses, so I get to pick all the cool stats/measure theory courses to ger a good foundation and also get some applied courses. I am loving it right now.\n\nSo my advice is: look at both math and stats programs and focus and what they offer and how they are oriented. Also, if you want to pursue a phd it might be nice to look for unis that have active departments in areas you find interesting.""]",3,1,https://www.reddit.com/r/statistics/comments/117rbk8/career_master_suggestions/
979,2023-02-21 04:03:40,[R] Per units odds ratio from logistic regression,"I have some per units odds ratio from a logistic regression for a continuous variable. Is the odds ratio for n units:

OR * n

Or as the logist regression is linear to the log(OR), is it:

Exp(Log (OR) * n)",[],1,0,https://www.reddit.com/r/statistics/comments/117iyjr/r_per_units_odds_ratio_from_logistic_regression/
980,2023-02-21 02:35:42,[Question] How to calculate leading indicators?,"Correlation gives you how coincident the indicators have been being, but what if one indicator is leading to another? How do I calculate it?","['Lagged correlation. Or if you want a more sophisticated approach that can handle multiple lags simultaneously, test for Granger causality with an autoregressive distributed lag/VAR model.']",3,1,https://www.reddit.com/r/statistics/comments/117gon7/question_how_to_calculate_leading_indicators/
981,2023-02-20 22:50:38,"[Q] Levene's test for homogeneity of variables is significant, however, I have to run two way anova, the sample size is small (n=6), What are my options?","Hi everyone, I have posted here a few days back, however I did not get any suggestions. So, posting again for some help. I have a study to analyze, it has two independent variables, one of which has 3 levels (like Car A,B,C) and the other independent variable has 4 levels (like speed P,Q,R,S) and one dependent variable (measured in 'scale').

I have to run two way anova (professor want's it two way anova to see if there are interaction effects; I already have run a one way anova using non-parametric analysis). However, the levene's assumption is violated when I run two-way anova.

I'm super stressed and have no idea what to do now. Can someone please help me. I did the transformation (log, sqrt, inverse), levene is still significant. Unfortunately, I do not have sample size greater than 50 to ignore levene's results. Also, given out time and resources, we cannot plan to do more study related to this work so that we have larger sample size.

Please note that: I have to compare data in the following fashion:

Does Car A, at speed P have more traction while driving compared to all the other combinations? I have to compare for all the different combinations with one another. I have 6 readings for each combination. My total readings are around 70 (like if I add all the readings we took, then we have around 70 readings in total. But just 6 readings per combination).","['If you had any option to refuse to perform an analysis for insufficient sample size, this would be a good time to use it. But, as you said, this is coming at the request of a professor -- I\'m assuming this is coursework, and not an independent study where you have more of a say on what will be done? In that case, it is technically true that you can fit that model to your data. If you want to address your concerns about sample size and call the whole approach into question without directly telling the professor ""no"" then what I would do is to couple the model with information on the estimation uncertainty for its predictions and the model residuals.\n\nIn short, this is a great opportunity for a teaching moment with your professor to remind them about the sample sizes necessary to detect significant interaction terms. \n\nLastly, Levene\'s Test. With 12 (4 levels x 3 levels) groups and 70 data points, I wouldn\'t have expected a significant result unless one or more groups have potential (extreme) outliers. Have you conducted a visual inspection of each group and individual data point?', 'I may have misunderstood when you said 6 readings per combination then. Do you actually have fewer than 6 for some settings? In your example with 3 readings I honestly wouldn’t put any weight on the standard deviation estimate from such a small sample. So, yes, I would be skeptical of running any test of standard deviations under those conditions.', 'The issue you’re facing is that ordinary least squares (OLS) linear regression assumes homoskedasticity of variances — that they are constant across the range of your predictor variables. When this fails, it is failing at the population rather than sample level and there is good and bad news for what this means for you in practical terms. \n\nGood news: there are methodologies for regression  that don’t use OLS and avoid homoskedasticity assumptions. Bad news: they’re more complex and you already have extreme sample size limitations. \n\nGood news: what happens if you use your OLS model even if the homoskedasticity assumption is violated? Your model will still estimate its coefficients without bias, but your standard errors _will_ be biased so the test statistics used to assess statistical significance of effects will not be trustworthy. Reference, with references for specific claims [here](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1331&context=pare).\nBad news: it sounds like your professor is more interested in testing for significant differences between settings than in building a model to estimate the level within each.', "">estimation uncertainty\n\nHello, thank you so very much for your reply. I have conducted a visual inspection, IQR and did a boxplot and the data does not have any outliers. Sometimes, the n is unequal (say, we only have 3 out of 6 readings for one of the combinations. We ran low on supplies like a required motor wasn't available and the study requirement was to repeat only 3 times. However we did repeat it 6 times just in case if we need more results). Would you suspect, that this could be the reason for levene's test to fail?"", "">of running any test of standard deviations under those conditions.\n\nYes. I will need to let my professor know about it. However, given this case, if it happens in future that i have all the same n=6, and still get significant levene's test, what are my options if I have to run two way anova?""]",2,6,https://www.reddit.com/r/statistics/comments/1178h92/q_levenes_test_for_homogeneity_of_variables_is/
982,2023-02-20 22:38:20,Why don’t most statistics departments involve themselves with ML research [Q],"I say most, because there’s some which do, lile CMU for example. However, I raised the question because I notice how lot of cs phd programs have sub disciplines of research which feel very statistical in nature. Is the only area of research closely related to ML mainly statistical learning? I always thought that stuff computer science departments are working on are similar problems to statistics departments. But it feels as though statistics doesn’t actively take part in researching these areas of ML vs classical problems like survey sampling or foundational statistics research","['They do, barring the departments that haven\'t caught up with the times.\n\nFor academic statistician to work on ML problems, there needs to some statistical underpinning.  That\'s why you don\'t see as much work on topics where the mathematical foundation is weak or where the limiting factor to adoption is good software. Some research in CS departments is ""let\'s try this variation on an algorithm because my gut says it will work"" which gets tested empirically on a well-known dataset. You won\'t see that kind of work in stats departments. \n\nAll of these reasons are why you don\'t tend to see statisticians working on neural networks, for example, but I\'ll admit my knowledge of the topic is very basic.', ""If you limit ML to neural networks and deep learning, then it'll filter out a lot of statistics departments that actually do ML-related research. ML is a large field that stretches beyond NN or DL. There's a ton of statisticians working on high-dimensional statistical modeling or graphical structure modeling with network data, and stuff like that.\n\nAnother reason you might not be seeing as much ML research coming out of statistics departments is longer journal review cycles. ML researchers tend to submit manuscripts to conferences and their proceedings, which makes reviewing orders of magnitude faster. Statistics departments tend to require papers published in peer-reviewed journals for tenure promotion, and that discourages a lot of statistics researchers from submitting papers to high-visibility ML proceedings.\n\nLastly, it's difficult to quantify the uncertainty of ML models, just yet. ML models produce incredibly accurate predictions but statistics is inherently a field about uncertainty quantification and how to do that in a principled and theoretically rigorous fashion is a million dollar question. But without any sort of confidence intervals or statistical inference, it's difficult to publish any of those results in a statistics journal."", 'It\'s an interesting question and I\'m curious to see other answers. For one it depends on what you mean by ""ML research"". If that includes things like lasso, random forest, gradient boosting, NP Bayes, etc. then many or even most stats departments have people doing ML research. (In fact some of these classical ML tools were invented by statisticians.) But if ""ML research"" = deep nets then it\'s more true. One possible reason for this is that one role of a stats department is to aid other departments in the statistical aspects of their research through statistical consulting, serving on grants, collaborating on papers, etc. My sense of the deep nets research is that there is still somewhat limited application in the scientific areas that stats departments have historically primarily served -- social and behavioral sciences, public health, medicine, engineering and physical sciences, agricultural sciences, etc. I\'m sure there\'s some, but not enough that it\'s a common request in stats consulting or on collaborative grants. If/when that changes I imagine more statisticians will incorporate deep nets into their research and departments will look to hire new PhDs with that expertise.', 'First you are making a mistake in distinguishing ""ML vs classical"" and some of what you are seeing is that, for statisticians, this is a nonsensical distinction, and so not one that most departments make.\n\nThe other side of it is that most of the approaches sweeping the market today are over 40 years old, but could not be implemented on existing hardware. The statistical side of most algorithmic modelling being applied in ML are solved problems and so scientifically uninteresting. There are problems like creating increased efficiency in avoiding or mapping local maxima, or advancing randomization, but, this isn\'t the kind of work that most people call ""ML"", and the mathematicians and statisticians contributing in those theoretical areas are in the dozens.\n\nInstead, what you are seeing is the widespread application of methods because now the barrier to apply them is low enough. In application of these methods are some interesting challenges and research areas, but they aren\'t pure statistics. Rather they are in areas of ethics, management, and human-computer engineering.\n\nI\'d say digital humanities (with NLP being the most popular application area) are where the biggest challenges are, and there is a long way to go. The theoretical underpinnings are not able to strongly link human thinking to computation, so we still rely heavily on human classification exercises and then modelling off of those. I suspect advances in cognitive science are necessary to do any better.', ""To add the slightly more applied angle, differentiating between ML and an average econometrics model these days is not only close to a meaningless distinction for many of the newer methods, it's utterly impossible for most non-academics.""]",77,25,https://www.reddit.com/r/statistics/comments/11787kn/why_dont_most_statistics_departments_involve/
983,2023-02-20 21:26:43,[Q] Why is it that continuous data cannot be used as the IV for an ANOVA?,I understand that when your IV is continuous you use regression analysis but I can’t quite understand why.,"['It can be. Then you would just end up with linear regression.\n\nMany of these tests are really just linear models, and we give them special names depending on the specific  situation. But the underlying model is the same. ANOVA is just linear regression. \n\nCheck out this explainer: https://lindeloev.github.io/tests-as-linear/#:~:text=Most%20of%20the%20common%20statistical,most%20students%20know%20from%20highschool.', ""Use regression.  Don't group the data artificially; that's a loss of information.  The loss is information about what happens between levels.  By that I mean regression will predict the response at an age level that is between young and middle aged, grouped data will not.\n\n\nBoth of those approaches use ANOVA for the hypothesis test, so there's no reason to down-grade the information."", '  By definition ANOVA is for discrete variables although mathematically it is a special case of regression.', ""Yes there would. You grouped the values into fewer categories, and that's a loss of data"", 'ANOVA answers the question ""is the variation between groups equal to the variation within groups.""  So, we need groups.  Data can grouped by levels of the IV.\n\n\nThe IV can be continuous or attribute (ordinal or nominal).  But it needs to be group-able.  Multiple data points at the same value of the IV are expected.\n\n\nANOVA can be generalizable so that what defines a group can be creative.  ANOVA is used in regression in answer to the question ""is the variation explained by the model equal to the variation not explained by the model.""  This involves grouping the variation that is explained by the y=f(x) regression equation separate from the variation expressed by the residual error.']",12,12,https://www.reddit.com/r/statistics/comments/1176ma7/q_why_is_it_that_continuous_data_cannot_be_used/
984,2023-02-20 20:36:54,[Q] Issues with log-rank test that I am struggling to get my head around,"Hi all,

I want to compare a number of treatment modalities in terms of overall survival. They're plotted on a Kaplan-Meier curve.

Issue is there's a lot of them and the N is pretty small (ranging from 10 to 30 per cohort). The curves come back non-significant if I test all of them together. But if I test e.g. the highest survival versus the rest of the cohort in a single curve, it comes back significant. Is this a valid thing to do, considering we're more interested in the best/worst combinations than whether everything differs from everything else?

Cheers","['> What alternative would you suggest then? \n\nYou can approximately adjust for the choice by treating it as all possible comparisons; that should be a bound.', ""Are you adjusting your pvalues for multiple comparisons when doing the pairwise survival comparisons? Depending on how many groups you have you could be drastically inflating your type I error. If you are adjusting and still finding significance then it should be okay to report that difference of interest.  \n\nBy best vs worst, was that your apriori hypothesis or did you only decide to test that specific difference after seeing the curves? Sometimes if you have a known control group, such as 'no treatment', you may only want to compare all the treatment groups to that one control group, which could reduce the number of post-hoc hypothesis tests you perform."", ""If you're using the sample data to 'pick out the largest' to compare with other groups, then a test which is based on *not* picking out the largest will not have the correct significance level (but a much higher one; correspondingly, p-values will be smaller than they should be if you corrected for this effect).\n\nYour hypotheses must be stated *before* you see the data, but you're using the data to choose the hypothesis you wish to test (without accounting for the impact of that data-based choice). Imagine H0 is in fact true: you should be counting all the tests you *could* have done if the samples had come out differently - under H0 every possible arrangement of samples should be equally likely, so every possible comparison counts as much toward your significance level as the specific comparison(s) you pick out.\n\nIn effect, this is a form of p-hacking."", 'Cheers, I think I did adjust but will double check. No control group unfortunately as this is a leukemia cohort that has been put through an unsupervised clustering algorithm, so the only thing I can think of is ""test an interesting looking curve against everything else in the cohort"".', ""What alternative would you suggest then? The problem is the groupings are generated by unsupervised clustering, and I have no way of knowing *a priori* what those clusters are going to look like, because they're numbered arbitrarily.\n\n\nOnly thing I can think of doing is characterising those clusters, devising a treatment approach based on them, and *then* do a KM on that""]",5,5,https://www.reddit.com/r/statistics/comments/1175lfg/q_issues_with_logrank_test_that_i_am_struggling/
985,2023-02-20 20:10:00,[QUESTION] Is it correct to use Mahalanobis distance repeatedly?,"I'm using Mahalanobis distance to detect outliers and I remove those which fall in a 5% level of significance of a chi square with p degrees of freedom, where p is the number of variables.

The thing here is, can I apply this method repeatedly or am I having FWER or other kind of problems?

For instance, in R I've built a while loop which removes outliers until none observation fails the chi squared.

    # Mahalanobis distance
    maha.F <- mahalanobis(df %>% select(where(is.numeric)), 
                          center = colMeans(df %>% select(where(is.numeric))),
                          cov = cov(df %>% select(where(is.numeric)))) 
    
    # Loop
    
    n_borradas <- 0 # count
    
    ""
    while (any(maha.F > qchisq(p = 0.95, df = ncol(df)))) {
      
      # delete
      obs_borradas <- which(maha.F > qchisq(p = 0.95, df = ncol(df)))
      n_borradas <- n_borradas + length(obs_borradas)
      df <- df[-obs_borradas, ]
      
      # calculate mahalanobis again
      maha.F <- mahalanobis(df %>% select(where(is.numeric)), 
                            center = colMeans(df %>% select(where(is.numeric))),
                            cov = cov(df %>% select(where(is.numeric))))
    }
    ""
    
    cat(""There:"", n_borradas, ""outliers"")

Should I apply a correction on signficance level, like Sidak's one or am I approaching this wrong?

If I wanted to use robust estimation of Mahalanobis distance, which distribution should I use?

This comes because I used Mahalanobis distance one time, but then I check again and some observation still falling outside the threshold.

(As a matter of fact, I'm not deleting the outliers, I'm storing them in a different data frame, but this code is simplified)","[""Well deleting the outliers in turn changes your Mahalanobis threshold.\n\nThis problem often happens in the univariate case where people exclude outliers based on Z scores, but then when they calculate Z scores again after the outliers have been removed, there are new outliers all of the sudden! Of course, because deleting the outliers has decreased the standard deviation which in turn increases the Z scores. You can keep this process going until you barely have any data left. What's happening here is just the multivariate version of that.\n\nSo you should definitely only do it once, and for a robust estimate I would have a look at the Minimum Covariance Determinant estimator and do your chi square test based on that."", 'What exactly do you want to test, for which test do you want to avoid a Type I error inflation?\n\nDo you want to test a hypothesis on the non outlier data or do you want to test if some data points are outliers vs. non-outliers?', 'You should only do the procedure once. The same goes for univariate outliers. If you remove scores with values above Z = 3.29 and below Z = -3.29 once you remove outliers. If you go through the procedure one more time, you end up removing scores that were not outliers in the first place. The same is true for the Mahalanobis distance.\n\nCheers!', ""Under the hypothesis that there are no outliers, this is clearly biased, since if you remove extreme data points that are not outliers you bias your covariance estimator towards a lower covariance. (lower in the sense of some matrix norm in the multivariate case). If there are outliers, your variance estimator is biased towards a larger covariance. Those effects work in the opposite direction and to me it's not obvious what the operation characteristics of the algorithm are, maybe someone else knows more.\n\nWhat's easier from a theoretical standpoint is to use a robust estimator for mean and covariance and then test with the chi-squared value based on the Malahanobis distance from those estimators."", 'In effect, by repeatedly doing this, you are increasing your significance level to something >5%. To convince yourself of this, lets say your current code removes 15 outliers, you could change the significance level in qchisq() to something smaller until you have the same # of data points removed as in the iterative case, i.e. 15. \n\nSo lets say that in order to do that you would have to change it to be p = 0.8. So this would say that what your current code is effectively doing is removing outliers which (to use your language) ""fall in a 20% level of significance"". So, to me, its not a matter of your approach being correct or incorrect, its a matter of choosing how aggressive you want to be in removing outliers. \n\nIf you\'re aiming to use a 5% significance level, then just do it once. \n(Whether its appropriate to be removing outliers at all is a whole different conversation, so I\'ll just leave it here.)']",11,6,https://www.reddit.com/r/statistics/comments/1175287/question_is_it_correct_to_use_mahalanobis/
986,2023-02-20 14:54:41,[Q] Will we always get statistical significance with larger and larger sample sizes?,"Imagine I’m doing a double blind RCT to find out if a treatment is working or not. The twist of it is the treatment group is also receiving the placebo pill as the control group. My contention is that as you increase the sample size to arbitrarily high enough number, you will always achieve statistical significance (p<0.05). Am I right or wrong?

The idea is that if the effect size is even somewhat off of zero (0.000005) instead of (0.000000000…), the confidence interval lower tail can get short enough such that it doesn’t cross the line of non-signficance","[""Technically, yes. But that's where the question of statistical significance vs practical significance comes into the picture. The way I always explain this is:\n\nImagine a group of doctors were interested in whether a certain, commercially available candy increases life expectancy (ignore the specifics of how it would be measured). They find that each candy consumed increases life epectancy by one minute, and the effect is significant at p < 0.001. Is there an effect? For sure. But would they go around recommending that people routinely consume the candy? No, because the effect is practically zero for what they want to achieve.   \n\n\nAnother way to look at this is to imagine you allocated the entire population into one of the two groups. If you find the difference between them is not exactly zero, you would conclude that there is a difference in the population, no significance test needed. When you only have a sample, the significance test is the only way to judge whether the difference in the population might reasonably be zero.   \n\n\nOf course, if you increase sample size, it gets easier and easier to judge whether the difference is zero or not, but it doesn't mean the test is made meaningless. You still have to judge whether the effect is important, or large enough to be meaningful."", 'Try it yourself. Simulate two iid samples from a normal distribution with the same parameters and see how many times you reject a difference in means.', 'Nice! I used the phrase “clinical significance”. A new drug reduces systolic blood pressure by 1. The results are statistically significant, but is lowering BP from 160 to 159 going to noticeably improve health outcomes? Probably not. So, statistically significant but not clinically significant. I think I like practical significance better tho.', 'Most of the time, yes. However, I suspect [this study](https://jamanetwork.com/journals/jama/fullarticle/2801827) would have a probability of rejecting the null hypothesis of .05 (or whatever alpha is) regardless of sample size. In the unlikely event a null hypothesis is true, sample size does not affect the probability of rejecting the null hypothesis.', ""Often this is true because most null hypotheses represent very extreme situations.\n\nI ran into this with some car sales data recently. Every category/make, region of sale, etc was significantly different for basically every other variable.\n\nThe effect size was always very small (close to 0) looking at confidence intervals can be helpful for this. Alternatively for other things (say a chi square test > 2x2 in size) there are metrics like cohen's W or cramer's V for interpreting effect sizes.\n\nThere is also a realm of testing that uses a minimum effect size instead of 0 or whatever extreme null is normally assumed.""]",2,7,https://www.reddit.com/r/statistics/comments/117048r/q_will_we_always_get_statistical_significance/
987,2023-02-20 11:43:38,[D] Confusion on Partial Effects/Marginal Effects,"Suppose I fit a Logistic Regression model to some data - I understand that I can now estimate the ""partial effects"" of a predictor on the response variable.

As an example, suppose my response variable is ""employment"" (unemployed = 1 vs employed = 0) and my predictor variables are ""age"", ""income"" and ""gender"". After fitting a Logistic Regression model to this data, I can take the partial derivative of the response variable with respect to the age variable. Now,  I can fix the ""gender"" as ""male"",  take the average ""income"" for all ""males"" in my dataset - and now, I can obtain a function that shows the effect of small changes in ""age"" on a male with an average income.

Have I understood this correctly? 

As I understand, this partial effect would be only applicable to males earning roughly the average salary as per my data. These partial effects might be less applicable to males earning the highest possible salary, or to females earning the average salary.

Is there some way to estimate the ""true"" average partial effects of age? Or is this the best you can do?

Thanks!","['>As I understand, this partial effect would be only applicable to males earning roughly the average salary as per my data.\n\nCan you give some insight as to how your variables are coded and 8f you included any interaction terms.\n\nIf you included age, income and gender without any interaction terms then you should get a coefficient or an odds ratio for each of those variables.    The coefficient / odds ratio for age tells you how the odds of being employed change for every unit increase in age while holding the other two constant\n\n>Is there some way to estimate the ""true"" average partial effects of age? Or is this the best you can do?\n\nIf you suspect that the relationship between age and employment differs by gender, then you would include an ageXgender interaction to test that.', 'Depends on what scale you are working on. If you are working on logit scale (and are assuming the relationships are linear), the relationship between dependent and independent variables will be a straight line, with constant partial effect for everyone.\n\n\nIf you are working on say probability scale, you are correct the partial effect depends on the specific values of your predictors. There is no ""best"" way to summarize them. Probably the most common way is to compute partial effect for all observations/respondents in the sample and average them (the so-called Average marginal effect). Some people also like to compute partial effect at the sample mean or at theoretically important value. Short overview can be found [here](https://vincentarelbundock.github.io/marginaleffects/articles/slopes.html#the-marginal-effects-zoo), longer explanation [here](https://www.andrewheiss.com/blog/2022/05/20/marginalia/).\n\n\nLastly, be mindful that [logistic regression is noncollapsible](https://www.su.se/polopoly_fs/1.341161.1501927873!/menu/standard/file/Eur%20Sociol%20Rev-2010-Mood-67-82%20%281%29.pdf), meaning that its regression coefficients depend on the amount of unobserved heterogeneity in the data. In practical terms, it means that neither the raw coefficients your computer spits out nor partial effect at the mean have a straightforward substantial interpretation. Average marginal effects are probably your best bet.', 'Yes, you can calculate it using the ""observed value"" approach. In a nutshell, you use your whole dataset (so observed values) for gender and income (values for row 1, values for row 2, etc.) and then you average over all of them.']",6,3,https://www.reddit.com/r/statistics/comments/116wsw3/d_confusion_on_partial_effectsmarginal_effects/
988,2023-02-20 08:26:08,[Q] I take AP Stats at my school and I was having trouble figuring out when I am supposed to do a confidence interval or a hypothesis test,"I’ve done them both separately but now on a test where we have to do both, I don’t know how I can identify which one to do. Is there specific language I should be looking for when I’m trying to solve the problem. The problem I’m working with right now just wants me to to “do the data provide convincing evidence”","[""All hypothesis tests are based on whether a confidence intervals includes the null hypothesis. So a confidence interval is inherently constructed every time a hypothesis test is run. Your program may not show the confidence interval, but it was a part of creating the hypothesis test. In short, either a confidence interval or a hypothesis test will give you sufficient information to answer your question, but it's more traditional to rely on the hypothesis test.\n\nI expect that your textbook/notes at some point included specific language about whether a hypothesis test (or the appropriate confidence interval) provides convincing evidence to reject your null hypothesis. I'd go look that up."", 'in my ap stats class, ""do the data provide convincing evidence"" was for hypothesis tests. confidence intervals would just say ""construct a confidence interval"". when you are trying to look for statistically significant evidence that is hypothesis testing', 'Ok, thanks. I was looking through some problems I did earlier and found those same patterns I just wasn’t sure if that was it. I guess I never payed attention to that sort of stuff', ""> I never *paid* attention to\n\nFTFY.\n\nAlthough *payed* exists (the reason why autocorrection didn't help you), it is only correct in:\n\n * Nautical context, when it means to paint a surface, or to cover with something like tar or resin in order to make it waterproof or corrosion-resistant. *The deck is yet to be payed.*\n\n * *Payed out* when letting strings, cables or ropes out, by slacking them. *The rope is payed out! You can pull now.*\n\nUnfortunately, I was unable to find nautical or rope-related words in your comment.\n\n*Beep, boop, I'm a bot*"", 'I teach AP stats. The materials from the College Board typically accept either a confidence interval or a hypothesis test to determine if there is convincing evidence against some claim about a population parameter. Your stats textbook may prefer one or the other in different contexts, but the FRQ responses and rubric provided by the College Board typically accept either method unless there is an explicit instruction specifying a particular method.']",2,6,https://www.reddit.com/r/statistics/comments/116srhj/q_i_take_ap_stats_at_my_school_and_i_was_having/
989,2023-02-20 05:15:51,[Q] How to proceed after transforming your data in spss for two way anova analysis?,"Hi everyone, I'm relatively new to this field and my background is not in statistics, however, it is a required skill for my current work. I encountered that I have a heterogeneity in my two way anova analysis. I have transformed the variables using squareroot transfeormation and have run levene's test. Now I do see that I have homogeneous results after transformation.

My question is, now when I do the two-way anova analysis, should I use those square-root transformed values instead of choosing my dependent variable for this analysis? It does sound obvious to do that, however, I couldn't find any resource online confirming this logic. So, I'm not very sure of how to proceed.

Just in case you wanted to know: I have two independent variables (say Car of type A,B,C; Speed say P,Q,R,S). For each combination (say Car A and speed P, I have 6 measurements; similarly, Car A, speed Q; car A speed R and so on...).

Please let me know your thoughts/questions or if any clarification you might need.

Thanks","['[removed]', 'Thanks for sharing this information.', '/u/charliewilson0912 is a click-farming spam bot.  Please downvote its comment and click the `report` button, selecting `Spam` then `Link farming`.  \n\nWith enough reports, the reddit algorithm will suspend this spammer.\n\n---\n>!^(If this message seems out of context, it may be because charliewilson0912 is farming karma and may edit their comment soon with a link)!<']",0,3,https://www.reddit.com/r/statistics/comments/116o6v6/q_how_to_proceed_after_transforming_your_data_in/
990,2023-02-20 04:59:05,[Q] Comparison of explained variance from two diffrent predictors,"Hello people!

I have the following hypothesis: Predictor A explaines more variance of criterion X than predictor B.

My model is a multiple regression Criterion X ~ Predictor A + Predictor B + Predictor C

Now, as I said, I want to compare the variance elucidation of predictor A and B. But how? I thought at first I could just compare the standardized regression weights, but I guess it's not that simple. Does it make sense to calculate simple linear regressions and then compare R2?

I feel like the answer must actually be very easy, but 5 hours of googling and poring over statistics books hasn't gotten me anywhere....  

Thanks a lot!

Manu","['If you want to find the unique proportion of variance explained by each IV, holding others constant, you can use partial correlation.\n\nhttps://www.statsdirect.com/help/regression_and_correlation/partial_correlation.htm#:~:text=Partial%20correlation%20is%20a%20method,other%20variables%2C%20on%20this%20relationship.', 'If you want to find the unique proportion of variance explained by each IV, holding others constant, you can use partial correlation.\n\nhttps://www.statsdirect.com/help/regression_and_correlation/partial_correlation.htm#:~:text=Partial%20correlation%20is%20a%20method,other%20variables%2C%20on%20this%20relationship.']",4,2,https://www.reddit.com/r/statistics/comments/116nsdm/q_comparison_of_explained_variance_from_two/
991,2023-02-19 19:02:06,[Q] doubt regarding the uncertainty estimate obtained through monte carlo methods," suppose i have selected a model y= f(x1,x2,x3...) and i have fitted it against some data. Now suppose that i have draw the confidence intervals around y by sampling from the posterior distribution . will these confidence bands correctly represent the quantile distribution of the data ,i.e. will there be a correspondence between the quantile distribution of the dataset to the quantile calculated from the model or in general the uncertainty is not calibrated?","['I agree with the other comment that it seems you are confusing a number of distinct issues. It\'s also possible this is just a language barrier, in which case I hope this is still helpful.\n\n> doubt regarding the uncertainty estimate obtained through monte carlo methods\n\nMonte Carlo is just a fancy way to say ""I got the answer with simulations, not calculus/algebra."" It has no real impact on anything else you\'re asking about, provided that you\'re not using far too few simulations to get the answer. For a Bayesian model, there are two such numbers you might care about. \n\n1. When fitting the model and making statements about parameter values, including *credible intervals* for parameters of interest, you care about the *effective sample size* which tells you about the Monte Carlo error in your posterior mean from using MCMC to draw samples from the posterior. There are also [approaches that can more directly tell you about how well you\'ve sampled different parts of the distribution](https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full) which will more directly about the quality of your credible intervals.\n\n2. If you\'re doing something posterior-predictive that requires simulating from the posterior distribution, you both need to have a sufficient effective sample size as in (1) and you need to draw enough samples of this new distribution to characterize it appropriately too. This isn\'t really a problem in practice, because usually people just draw one new sample per posterior sample and then as long as you\'ve got a good approximation to the posterior as in (1), you\'re probably fine here.\n\n>  will these confidence bands correctly represent the quantile distribution of the data,i.e. will there be a correspondence between the quantile distribution of the dataset to the quantile calculated from the model \n\nThere\'s a lot to unpack here. You seem to be mixing up parameters, data, and data-generating processes.\n\nLet\'s get a little example going here to make things easier to understand. Let\'s do a two-sample model, where you\'re interested in estimating the difference in means between two groups. \n\nAs long as you\'ve made sure your MCMC samples are sufficient for the endeavor at hand as per point (1) above, then your MCMC-sample-based *credible intervals* are accurate representations of the *model\'s uncertainty* about the quantity of interest, which is usually a *parameter*. For example, you\'ll get a 95% credible interval on the difference in means, which is a range in which there is a 95% probability that the difference in means lies. If you\'re a frequentist, or a certain flavor of Bayesian, you do not believe there is a true distribution here, so this interval simply represents your uncertainty about the true, but unknown, difference in means. Some will say there is a true *distribution* on the difference in means, in which case I believe (but am far from certain) that you can view the posterior distribution as an approximation to that true distribution. \n\nTo get at quantiles of the *data* you need to think about the entire model as a data-generating process. In this little two-sample model, we\'ve got a parameter for the difference in means, but we also have to specify the *distributions* (the distribution functions f\\_1(Y\\_1)  and f\\_2(Y\\_2)) for the two groups, and we have a lot of freedom on how we want to do that, as long as the means accord with the difference parameter. We could make them Gamma, Normal, Lognormal, whatever you want. We could assume equal variance, unequal variance, we could fix or estimate the variance parameter(s). These distributions serve as the _likelihood_ when estimating the difference parameter. But they also allow us to simulate from the *posterior predictive distribution*. We do this by drawing the parameters of those distributions from their joint posterior distribution, and then drawing a new dataset for each group from f\\_1(Y\\_1)  and f\\_2(Y\\_2). This is the distribution that allows us to make statements about our observables.\n\nYour model is not the true data-generating process, (almost) ever, regardless of how you\'re doing inference (frequentist or Bayesian). Models are simplifications of reality, approximations. When the model is a good approximation, then the quantities we get out of it will be useful approximations of reality. When it is a bad approximation, they will not.\n\n> or in general the uncertainty is not calibrated?\n\nI\'m not quite sure what you\'re asking here. \n\nGenerally frequentists are far more interested in whether their confidence intervals are well-calibrated than Bayesians are whether their credible intervals are well-calibrated, because confidence intervals and credible intervals are different things with different purposes. You can talk about frequentist properties of Bayesian estimation, and sometimes that can be useful. [Bayesian estimation of proportions](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Jeffreys_interval) with Jeffreys\' prior produces pretty decently-calibrated credible intervals, to the point it\'s used as an approach to approximate confidence intervals.\n\n> if i fix an approximated model that depends from some parameters f(a,b,c) why should the prediction intervals be well calibrated w.r.t the real distribution of the data? the model expressiveness may not be sufficient to accurately model the uncertainty,no?\n\nTo trot out a well-worn phrase, ""all models are wrong, some are useful."" Models are not reality. The better they approximate it, the better our inferences. But all models are at least a little bit misspecified. Some model misspecification is more harmful than others. This is why there\'s so much interest in model diagnostics, and why you hear so much about multicollinearity, heteroskedacity, and the distribution of residuals for regression models. By paying attention to model violations, we can try to quantify what parts of the model and associated inferences we can and cannot trust, and hence whether we need to build a better model or not.', 'In bayesian estimation there are no confidence intervals. ""Confidence intervals around y"" are rather prediction intervals. Quantile distribution of the data? Well this is assumed by the model, the same model that you are using for estimate parameters and build ""confidence intervals"". All in all I think you have some misconceptions around these concepts which renders your question a ""wrong"" one. I maybe wrong tho, let\'s see if anyone gives another opinion', ""it's the second point i think. \n\nfor the analytic model:\n\ni have followed this tutorial for the creation of the credible intervals [https://cosmiccoding.com.au/tutorials/propagating/](https://cosmiccoding.com.au/tutorials/propagating/)\n\ni have used the samples from the posterior distribution  generated by emcee to plot the credibile intervals of my approximated model. for each input x, i tested thousands of combinations  of parameters (a,b,c) extracted from the posterior  and stored  the output y = f(x,a,b,c).  then ,from the stored y i then found the 68 percentile bounds.\n\n&#x200B;\n\non the other side i have the usual  bayesian neural network with mc dropout. i split the dataset in a training, a validation and a recalibration set.\n\ni trained the neural network and i have tested  how well- calibrated the uncertainty is by  comparing the predicted quantiles with the observed quantiles in the validation set. The i used the recalibration set to recalibrate the uncertainties to better reflect the true distribution of the data.\n\ni have compared the accuracy of the two methods with the rmse, but i want to know if  it makes sense to test the calibration of the uncertainties obtained with the analytic model and if it makes sense to recalibrate them as i did with the neural network."", 'ah right, i meant the  predicted credible intervals. i often confuse them when i translate in english. regarding the issue, why is assumed by the model?\n\n&#x200B;\n\nif i fix an approximated model that depends from some parameters f(a,b,c) why should the prediction intervals be well calibrated w.r.t the real distribution of the data? the model expressiveness  may not be sufficient to accurately model the uncertainty,no?', 'I am not sure I\'d trust much of anything from that website. It\'s throwing up red flags within the first few sentences. ""Gaussian uncertainty"" is an entirely unclear and nebulous term at best, and at worst suggests the author either doesn\'t know what is going on or has tried to simplify to past the point of usefulness. Plus, the *functional form* of the underlying relationship between y and x has *nothing* to do with *any* of the distributions which characterize uncertainty about anything.\n\nExpressing uncertainty about *functions* can be quite difficult, in fact, once you leave the realm of simple linear models like y = mx + b. You start being confronted with the choice between expressing something easy, like pointwise uncertainty, or something difficult, like functionwise uncertainty. Pointwise uncertainty is easy, your posterior samples allow you to evaluate f() at every observed value x, and you can compute any credible interval you like. Functionwise uncertainty takes you into the land of functional data analysis, though there are tools that can be helpful, like [functional boxplots](https://en.wikipedia.org/wiki/Functional_boxplot).\n\n> i have compared the accuracy of the two methods with the rmse, \n\nThat doesn\'t strike me as a great approach for comparing Bayesian models. RMSE is a function of an *estimator* and that generally means you\'re talking about *point estimates*. Unless you\'re doing something to very explicitly take into account the whole posterior distribution, you\'re losing a shit ton of information. You could maybe think about integrating RMSE across the posterior, I guess, but there has been plenty of good work by smart people on Bayesian approaches to predictive model accuracy. [This blog post](https://statmodeling.stat.columbia.edu/2022/06/30/47790/) discusses this a bit, and mentions [this paper](https://link.springer.com/article/10.1007/s11222-016-9696-4) on leave-one-out approaches.\n\n> but i want to know if it makes sense to test the calibration of the uncertainties obtained with the analytic model and if it makes sense to recalibrate them as i did with the neural network.\n\nI am not familiar enough with neural net modeling to know what this ""recalibration"" business is about. It could be an applicable concept to arbitrary models, but it could easily be a hack used in other applications of neural nets applied to Bayesian versions with no thought given to what it means, and not really applicable anywhere.\n\nIf you have a source on what this is, I\'m willing to poke around a bit and see if I can help you figure out an answer.']",1,20,https://www.reddit.com/r/statistics/comments/1167uqh/q_doubt_regarding_the_uncertainty_estimate/
992,2023-02-19 18:58:27,[Q] correlation for path analysis (SEM),"I run a path analysis with 4 exogenous variables (A, B,  C, D) underlying my first endogenous variable (T) then -> to my second endogenous variable (X). My data satisfies all assumptions for path analysis. However, my advisor raised a question of lack of correlation between exo (A, B, C, D)  and endo (T) variables. 

Admittedly, I am somewhat lacking knowledge on whether this is necessary or problematic. My stance is that lack of correlation is not an issue. Yes correlation for bivariate relationships is needed for a regression for the same relationship. However, for path analysis this seems not necessary. 

I would appreciate if you could please advise and share any relevant literature.","['SEM, regression and correlation are essentially the same thing in a different guise. You are essentially testing for correlation (or some form of it) therefore it cannot be a precondition for the analysis. However, if they are orthogonal, SEM will not show a relationship either.']",6,1,https://www.reddit.com/r/statistics/comments/1167sb6/q_correlation_for_path_analysis_sem/
993,2023-02-19 18:41:13,[Q] A way of describing shapes,"I thought up a small ""language"" for describing some type of shapes. I want to know if there's anything similar to this language in statistics. I want to know if there's any way to search for such type of shapes in data.  

Basically all this language has is three operations: 
1. First operation is the **""dot""**: *""().()"", for example ""A.B""*. It means that two shapes are connected, glued into a single shape.
2. Second operation is **""x""**: *""()x(())"", for example ""Ax(B)""*. It means that the left shape contains the right shape. 
3. Third operation is **""10""**: *""()10"", for example ""A10""*. It means that the shape is stretched. *(Potentially this operation can be reduced to the first two operations.)*  

## Abstract examples

I want to give some examples. ""A"" here means a blue circle. ""B"" here means a red circle. ""N"" (null) here means empty space. 

https://i.imgur.com/A65KnCB.png

Some more complicated shapes: 

https://i.imgur.com/xwJ3IB7.png

**Note 1:** *yes, this language doesn't distinguish (to a degree) between touching and intersecting shapes.*

**Note 2:** *Maybe ""10"" operation can be replaced with formulas like ""A^1 x (A^0 . A^0 )"" (meaning that A^1 contains A^0 two times).*

## Practical example

I'm interested in the language above because of image classification/seeking patterns in images.  

Take this pixel image:

https://i.imgur.com/ujczuQl.png

We can use the language to describe some interpretations of the image: 
1. If *""A = blue; B = orange""*, then you can describe this image as **""A.B""**.
2. If *""A = blue; B = green, yellow, orange""*, then you can describe this image as **""Bx(A)""**. 
3. If *""A = blue; B = yellow, orange""*, then you can describe this image as **""A.B10""**.  
4. If *""A = green; B = yellow, orange""*, then you can describe this image as **""(A10.B10)x((N))""**.    

*[An illustration](https://i.imgur.com/RpS8e60.png) of all those interpretations.* *(Looks almost like [Tetris](https://en.wikipedia.org/wiki/Tetris).)*  

The idea could be taken further, of course. Instead of color pixels you could have areas with any features. 

## What I searched

[Venn diagrams, Euler diagrams, Basic set operations](https://en.wikipedia.org/wiki/Venn_diagram). 

Of course, one could define my small ""language"" with basic set operations. Still, I'm interested to know if it has been done and used anywhere. Because I'm not interested in **all** the things which can be done with sets, only in the particular patterns above.  

[Karnaugh map](https://en.wikipedia.org/wiki/Karnaugh_map). 

Looks similar to my pixelated example. But I guess I need something much simpler than this.    

[Viola–Jones object detection framework](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework), [Haar-like features](https://en.wikipedia.org/wiki/Haar-like_feature).

This is not what I described, but quite similar: looking for simple features defined with two components. 

[Conway polyhedron notation](https://en.wikipedia.org/wiki/Conway_polyhedron_notation) *(this was suggested to me)*

This *is/could be* very similar (three simple operations for describing shapes). But it's too focused on specific operations which create the shapes.   

I also feel that something similar to my ""language"" could be related to games (e.g. to **Go**). And I heard that [surreal numbers where inspired by Go endgames](https://en.wikipedia.org/wiki/Surreal_number#History_of_the_concept) (but that's quite far from statistics).  

Do you of anything else that could be related? Is there a specific way to search such patterns (like the one described above) in the data?","['i think what you are looking for is discrete mathematics. I think you could probably model this using a tree. your A/B categories would translate to a node attribute like color. the ""contains"" operation could translate to a ""depth"", like in a hierarchical structure. i guess your dot could be a relation between nodes at the same depth? and the ""stretch"" would just be another node attribute unrelated to color?\n\ntry modeling this as a graph and that should open up a lot of analytical doors.', "">I want to know if there's anything similar to this language in statistics\n\nI don't have an answer for you, but I am curious:  \nWhy specifically the field of Statistics?"", 'Because such ""shape-describing language"" may not be interesting from the purely mathematical perspective, but could be useful in analyzing data. Like **Haar-like features** I mentioned: rectangles with two colors is not an awfully interesting  concept from the mathematical perspective, but it may be very interesting if your goal is to detect faces.', ""Aha, OK. Thanks.\n\nI wouldn't know how to find this in statistics where we have uncertainty and measures of confidence (between 0% and 100%), instead of strict equations like (A.B)xN"", 'Thank you, graphs definitely make sense!']",12,5,https://www.reddit.com/r/statistics/comments/1167ht6/q_a_way_of_describing_shapes/
994,2023-02-19 13:24:31,[Q] T or Z distributions in correlation null hypothesis testing,"Hello,

I am taking a course in correlation/multiple regression, and something confusing to me came up. We were discussing null hypothesis testing and correlations.

The prof said that when we want to test a correlation against a null hypothesis of zero, we use a z distribution. But when we want to compare two independent correlations against one another (null hypothesis = 0), we use a t distribution. What's going on here? Why use a z distribution in one case and a t distribution in the other?

Please provide some references with your responses :)

Thanks!",[],0,0,https://www.reddit.com/r/statistics/comments/1162hg6/q_t_or_z_distributions_in_correlation_null/
995,2023-02-19 12:42:36,[Q] Reviewing a study with strange method choices: Modeling shenanigans? Or totally legit?,"Hi all,

I am reviewing a study on gun-related behaviors as part of my dissertation's lit review, and I am perplexed by some of the authors' modeling choices.  My intuition tells me something funky might be happening, but I'm loathe to stick my foot in my mouth in case what they've done is completely legit.  I would greatly appreciate any thoughts you all might have.


Briefly for background: this study examined the relationships among mental illness, angry impulsivity, and gun ownership and carrying behaviors.  Angry impulsivity included one or more of the following:
1. Losing one's temper and getting into physical fights
2. Getting angry and breaking or smashing things
3. Having tantrums and angry outbursts

These three items were taken from a questionnaire that screened for two axis ii cluster b disorders: antisocial personality disorder (apd) and borderline personality disorder (bpd).  In other words, these three questions are among the diagnostic criteria for these disorders in the study.  The big problem is bpd.  78% of the cases with bpd had positive responses to one or more of those three questions.

The authors created a combined dichotomous indicator of angry impulsive gun-related outcomes, such that:
```
(DV1) Angry Impulsive Gun Ownership
DV1=1 if has guns AND angry impulsive
DV1=0 if doesn't have guns OR not angry impulsive

(DV2) Angry Impulsive Gun Carrying
DV2=1 if carries guns AND angry impulsive
DV2=0 if doesn't carry OR not angry impulsive
```

Is this a case of an IV, at least in part, predicting itself?  Or does the fact that the DVs are combinations of these criteria and gun access/carrying resolve the issue? 

Since maximum likelihood estimation iterates for each regressor, would that mean that only the disorder coefficient would be problematic and the rest of the model fine?","['I feel like you’ve given us details of the inner workings without providing some basics about the input, the output, and the question.', 'Hard to tell without reading the paper myself, but yes, components of a questionnaire are usually correlated with its composite scores. \n\nI don’t really understand why the authors constructed the study in this way. What was the underlying question?', 'Fair point.  Post revised.', 'The research question in their [study](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5116908/) was:\n\n""\\[W\\]hat proportion of the angry people in the population who own or carry guns have a diagnosable mental illness?""\n\nI don\'t know how I feel about the way they sliced and diced the DVs.  One of my biggest beefs with the study is that their second DV has only 95 events, and they report odds ratios with no SEs or CIs and with no mention of separation issues, despite the data being split across over 20 IVs.']",9,4,https://www.reddit.com/r/statistics/comments/1161r0s/q_reviewing_a_study_with_strange_method_choices/
996,2023-02-19 11:37:01,[Q] What to learn in stochastic process? (and recommendations for first courses),"I started my master's in statistics 6 months ago with no knowledge of stochastic processes. I had tried to learn a little bit before the degree started but ended up focusing more on Bayesian Statistics and GLMs (useful topics in their own rights). However, it's been clear over the past 6 months I'm expected to know a fair chunk of stochastic like the back of my hand.

By an unfortunate turn of events (family crisis), I've paused my master's degree and will come back in a year to finish it off. I can see the opportunity in front of me and would like to shore up on areas I'm lacking in before I return.

Does anyone have any recommendations for topics in stochastic processes or <good> first courses to get a fundamental understanding of the topic? :)","['So, Indians doing a weirdly good job of explaining shit on YouTube to the rescue: https://www.youtube.com/watch?v=Jr8v-WulO4g&list=PL4fpys7KOcYhKPGIGIlED1nEOcdOoW9QG\n\nSeriously, though, this class is actually great, though the audio quality is not so great.', 'ocw.mit.edu has some courses on probability theory/stochastic processes at various levels.']",2,2,https://www.reddit.com/r/statistics/comments/1160jhu/q_what_to_learn_in_stochastic_process_and/
997,2023-02-19 09:42:29,"[Question] Any statistics book for lab-related stats? (LoD, LoB, LoQ, linearity, assay equivalence)",Looking at a job involving this and it's all stuff I didn't learn in my MS.,"['I held off saying anything here for a day, hoping some other folks would jump in with some recommendations I couldn\'t find. No such luck...\n\nIn stat books you\'ll be looking for mention of censored data. You\'ll find a lot more written about right-censored data (survival analysis) than left-censored (detection limit issues.)\n\nOften you need a two-stage model here: first a binary cutoff or a logistic curve for the ""is it detectable?"" question, then a distribution to describe the detected observations. Your observed data are modeled by an interaction term, the product of the detection indicator variable and the distribution you care about.\n\nOne of the traditional methods of handling censored data was \'tobit regression\' - but it appears, from the (admittedly nonrandom) sample of books on my shelf, that nowadays you\'ll only find this in an econometrics textbook, not a statistics books.\n\nIn a GLM book, you might find examples where you predict Y by a binary X1 and a continuous X2 -- but most of these won\'t be specifically detection-related, but will be things like zero-inflated models for count data. \n\nThere\'s about ten pages (section 18.5) on how to build models for censored data in Gelman and Hill\'s *Data Analysis Using Regression and Multilevel/Hierarchical Models*. Hierarchical models are a lot more well-known to Bayesians than frequentists, but they exist in both worlds. That\'s as close to a good reference as I have for you.']",2,1,https://www.reddit.com/r/statistics/comments/115y66f/question_any_statistics_book_for_labrelated_stats/
998,2023-02-19 06:53:51,[Q] quick dice question,"Say you are tossing 1 fair dice. What is the probability you toss at least one 1 and one 6, in 4 consecutive tosses? 

I believe the solution is 1 - (2/3)^4 
But I am told other wise. 

Another way I was thinking about doing it is having the combinations needed divided by the total combinations, so SUM(combinations needed) / (6^4 )
But I can’t figure out my numerator.. any helps helps","['(2/3)^4 is the probability of no 6s or 1s.   \n1- (2/3)^4  includes many cases that dont have both 1 and 6, like 1 2 3 5', ""No. The negation of “not A and not B” is “A or B.”\n\nhttps://en.m.wikipedia.org/wiki/De_Morgan's_laws"", 'No. Again, consider 1,2,3,5. It is not in the set ""no 1\'s or 6\'s"" but it is also not in the set ""at least one 6 and one 1"".\n\nThe two sets are not complements.', ""If I'm not wrong.\n\nP(at least one 1 and at least one 6) = 1 - P(no 1 or no 6)  \nThen you have the formula P(A U B) = P(A) + P(B) - P(A intersect B) \n\nShould be sufficient to solve the question."", ""My thinking is to find the probability of not rolling at least one 1 and one 6, then subtracting 1 minus that amount.\n\nP(not rolling at least one 1 and one 6) = P(rolling neither a 6 or a 1) + P(rolling one or more 6's but no 1's) + P(rolling one or more 1's but no 6's)\n\n= (4/6)\\^4 + 1/6\\*(5/6)\\^3 + 1/6\\*(5/6)\\^3 = .3904\n\nThen 1 minus that result is 60.9%.\n\nMake sense?""]",9,7,https://www.reddit.com/r/statistics/comments/115ul5g/q_quick_dice_question/
